{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    " Document Information Extraction and Structuring\n",
    "# æ–‡ä»¶çµæ§‹åŒ–æŠ½å–èˆ‡é©—è­‰ - ä½¿ç”¨ LLM é€²è¡Œæ™ºèƒ½æ–‡ä»¶è§£æž\n",
    "\n",
    "## 1. Environment Setup & Dependencies\n",
    "# ç’°å¢ƒåˆå§‹åŒ–èˆ‡ä¾è³´ç®¡ç†\n",
    "\n",
    "# === Shared Cache Bootstrap ===\n",
    "import os, pathlib, torch\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] {torch.cuda.get_device_name(0)} | VRAM: {torch.cuda.get_device_properties(0).total_memory // 1e9:.1f}GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# å®‰è£æ–‡ä»¶è™•ç†èˆ‡é©—è­‰ç›¸é—œå¥—ä»¶\n",
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q PyPDF2 python-docx beautifulsoup4 lxml\n",
    "!pip install -q pydantic rich spacy nltk\n",
    "!pip install -q pandas openpyxl tabulate\n",
    "\n",
    "## 2. Document Parser - Multi-format Support\n",
    "# å¤šæ ¼å¼æ–‡ä»¶è§£æžå™¨ - çµ±ä¸€æ–‡ä»¶è®€å–ä»‹é¢\n",
    "\n",
    "import PyPDF2\n",
    "import docx\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union, Optional\n",
    "from dataclasses import dataclass\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ab568",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocumentContent:\n",
    "    \"\"\"Structured document content with metadata\"\"\"\n",
    "    text: str\n",
    "    title: Optional[str] = None\n",
    "    pages: Optional[int] = None\n",
    "    tables: List[pd.DataFrame] = None\n",
    "    metadata: Dict = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.tables is None:\n",
    "            self.tables = []\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "class DocumentParser:\n",
    "    \"\"\"Universal document parser supporting PDF, DOCX, HTML, TXT, MD\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.supported_formats = {'.pdf', '.docx', '.html', '.htm', '.txt', '.md'}\n",
    "\n",
    "    def parse(self, file_path: Union[str, Path]) -> DocumentContent:\n",
    "        \"\"\"Parse document and return structured content\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "        extension = file_path.suffix.lower()\n",
    "        if extension not in self.supported_formats:\n",
    "            raise ValueError(f\"Unsupported format: {extension}\")\n",
    "\n",
    "        if extension == '.pdf':\n",
    "            return self._parse_pdf(file_path)\n",
    "        elif extension == '.docx':\n",
    "            return self._parse_docx(file_path)\n",
    "        elif extension in ['.html', '.htm']:\n",
    "            return self._parse_html(file_path)\n",
    "        elif extension in ['.txt', '.md']:\n",
    "            return self._parse_text(file_path)\n",
    "\n",
    "    def _parse_pdf(self, file_path: Path) -> DocumentContent:\n",
    "        \"\"\"Extract text and metadata from PDF\"\"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "            # Extract text from all pages\n",
    "            text_parts = []\n",
    "            for page in reader.pages:\n",
    "                text_parts.append(page.extract_text())\n",
    "\n",
    "            text = '\\n\\n'.join(text_parts)\n",
    "\n",
    "            # Extract metadata\n",
    "            metadata = {\n",
    "                'file_path': str(file_path),\n",
    "                'file_size': file_path.stat().st_size,\n",
    "                'format': 'pdf'\n",
    "            }\n",
    "\n",
    "            if reader.metadata:\n",
    "                metadata.update({\n",
    "                    'title': reader.metadata.get('/Title', ''),\n",
    "                    'author': reader.metadata.get('/Author', ''),\n",
    "                    'subject': reader.metadata.get('/Subject', ''),\n",
    "                    'creator': reader.metadata.get('/Creator', '')\n",
    "                })\n",
    "\n",
    "            return DocumentContent(\n",
    "                text=text,\n",
    "                title=metadata.get('title'),\n",
    "                pages=len(reader.pages),\n",
    "                metadata=metadata\n",
    "            )\n",
    "\n",
    "    def _parse_docx(self, file_path: Path) -> DocumentContent:\n",
    "        \"\"\"Extract text and tables from DOCX\"\"\"\n",
    "        doc = docx.Document(file_path)\n",
    "\n",
    "        # Extract paragraphs\n",
    "        text_parts = [paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()]\n",
    "        text = '\\n\\n'.join(text_parts)\n",
    "\n",
    "        # Extract tables\n",
    "        tables = []\n",
    "        for table in doc.tables:\n",
    "            data = []\n",
    "            for row in table.rows:\n",
    "                row_data = [cell.text.strip() for cell in row.cells]\n",
    "                data.append(row_data)\n",
    "\n",
    "            if data:\n",
    "                df = pd.DataFrame(data[1:], columns=data[0] if data else None)\n",
    "                tables.append(df)\n",
    "\n",
    "        metadata = {\n",
    "            'file_path': str(file_path),\n",
    "            'file_size': file_path.stat().st_size,\n",
    "            'format': 'docx',\n",
    "            'paragraphs': len(doc.paragraphs),\n",
    "            'tables': len(tables)\n",
    "        }\n",
    "\n",
    "        return DocumentContent(\n",
    "            text=text,\n",
    "            tables=tables,\n",
    "            metadata=metadata\n",
    "        )\n",
    "\n",
    "    def _parse_html(self, file_path: Path) -> DocumentContent:\n",
    "        \"\"\"Extract text from HTML\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file.read(), 'html.parser')\n",
    "\n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "\n",
    "        # Extract text\n",
    "        text = soup.get_text()\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Clean up whitespace\n",
    "\n",
    "        # Extract title\n",
    "        title = soup.title.string if soup.title else None\n",
    "\n",
    "        metadata = {\n",
    "            'file_path': str(file_path),\n",
    "            'file_size': file_path.stat().st_size,\n",
    "            'format': 'html',\n",
    "            'title': title\n",
    "        }\n",
    "\n",
    "        return DocumentContent(\n",
    "            text=text,\n",
    "            title=title,\n",
    "            metadata=metadata\n",
    "        )\n",
    "\n",
    "    def _parse_text(self, file_path: Path) -> DocumentContent:\n",
    "        \"\"\"Extract text from plain text or markdown\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Try to extract title from first line if it looks like a title\n",
    "        lines = text.split('\\n')\n",
    "        title = None\n",
    "        if lines and (lines[0].startswith('#') or len(lines[0]) < 100):\n",
    "            title = lines[0].lstrip('#').strip()\n",
    "\n",
    "        metadata = {\n",
    "            'file_path': str(file_path),\n",
    "            'file_size': file_path.stat().st_size,\n",
    "            'format': file_path.suffix[1:],  # Remove dot\n",
    "            'lines': len(lines)\n",
    "        }\n",
    "\n",
    "        return DocumentContent(\n",
    "            text=text,\n",
    "            title=title,\n",
    "            metadata=metadata\n",
    "        )\n",
    "\n",
    "# Test document parser with sample content\n",
    "# æ¸¬è©¦æ–‡ä»¶è§£æžå™¨\n",
    "sample_text = \"\"\"\n",
    "# Company Annual Report 2024\n",
    "\n",
    "## Executive Summary\n",
    "Our company achieved record growth in 2024, with revenue increasing by 25% to $150M.\n",
    "Key highlights include:\n",
    "- New product launches in Q2 and Q4\n",
    "- Expansion into Asian markets\n",
    "- Strategic partnership with TechCorp\n",
    "\n",
    "## Financial Performance\n",
    "| Metric | 2023 | 2024 | Change |\n",
    "|--------|------|------|--------|\n",
    "| Revenue | $120M | $150M | +25% |\n",
    "| Profit | $15M | $22M | +47% |\n",
    "| Employees | 450 | 520 | +16% |\n",
    "\n",
    "## Key Personnel\n",
    "- CEO: John Smith (john.smith@company.com)\n",
    "- CFO: Jane Doe (jane.doe@company.com)\n",
    "- CTO: Bob Wilson (bob.wilson@company.com)\n",
    "\n",
    "## Contact Information\n",
    "Address: 123 Business St, Tech City, TC 12345\n",
    "Phone: +1-555-0123\n",
    "Website: www.company.com\n",
    "\"\"\"\n",
    "\n",
    "# Create sample document for testing\n",
    "sample_path = Path(\"sample_report.md\")\n",
    "with open(sample_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# Test parser\n",
    "parser = DocumentParser()\n",
    "doc_content = parser.parse(sample_path)\n",
    "\n",
    "print(\"ðŸ“„ Document Parsing Results:\")\n",
    "print(f\"Title: {doc_content.title}\")\n",
    "print(f\"Text length: {len(doc_content.text)} characters\")\n",
    "print(f\"Metadata: {doc_content.metadata}\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc57a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Schema Definition - Structured Output Models\n",
    "# Schema å®šç¾© - ä½¿ç”¨ Pydantic å®šç¾©çµæ§‹åŒ–è¼¸å‡ºæ ¼å¼\n",
    "\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    \"\"\"Contact information structure\"\"\"\n",
    "\n",
    "    name: Optional[str] = None\n",
    "    email: Optional[str] = None\n",
    "    phone: Optional[str] = None\n",
    "    title: Optional[str] = None\n",
    "\n",
    "    @validator(\"email\")\n",
    "    def validate_email(cls, v):\n",
    "        if v and \"@\" not in v:\n",
    "            raise ValueError(\"Invalid email format\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class FinancialMetric(BaseModel):\n",
    "    \"\"\"Financial metric with value and unit\"\"\"\n",
    "\n",
    "    name: str\n",
    "    value: float\n",
    "    unit: str = \"USD\"\n",
    "    period: Optional[str] = None\n",
    "\n",
    "\n",
    "class CompanyInfo(BaseModel):\n",
    "    \"\"\"Company information schema\"\"\"\n",
    "\n",
    "    company_name: Optional[str] = None\n",
    "    industry: Optional[str] = None\n",
    "    address: Optional[str] = None\n",
    "    phone: Optional[str] = None\n",
    "    website: Optional[str] = None\n",
    "    employees: Optional[int] = None\n",
    "\n",
    "\n",
    "class DocumentSummary(BaseModel):\n",
    "    \"\"\"Complete document extraction schema\"\"\"\n",
    "\n",
    "    document_type: str = Field(\n",
    "        ...,\n",
    "        description=\"Type of document (e.g., 'annual_report', 'contract', 'resume')\",\n",
    "    )\n",
    "    title: Optional[str] = None\n",
    "    summary: str = Field(..., description=\"Brief summary of document content\")\n",
    "\n",
    "    # Entities\n",
    "    contacts: List[ContactInfo] = Field(\n",
    "        default_factory=list, description=\"People mentioned in document\"\n",
    "    )\n",
    "    companies: List[CompanyInfo] = Field(\n",
    "        default_factory=list, description=\"Companies mentioned\"\n",
    "    )\n",
    "    financial_metrics: List[FinancialMetric] = Field(\n",
    "        default_factory=list, description=\"Financial data\"\n",
    "    )\n",
    "\n",
    "    # Metadata\n",
    "    key_dates: List[str] = Field(\n",
    "        default_factory=list, description=\"Important dates mentioned\"\n",
    "    )\n",
    "    key_topics: List[str] = Field(\n",
    "        default_factory=list, description=\"Main topics/themes\"\n",
    "    )\n",
    "    confidence_score: float = Field(\n",
    "        default=0.0, ge=0.0, le=1.0, description=\"Extraction confidence\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"document_type\": \"annual_report\",\n",
    "                \"title\": \"Company Annual Report 2024\",\n",
    "                \"summary\": \"Annual financial performance report showing 25% revenue growth\",\n",
    "                \"contacts\": [\n",
    "                    {\n",
    "                        \"name\": \"John Smith\",\n",
    "                        \"title\": \"CEO\",\n",
    "                        \"email\": \"john.smith@company.com\",\n",
    "                    }\n",
    "                ],\n",
    "                \"companies\": [{\"company_name\": \"TechCorp\", \"industry\": \"Technology\"}],\n",
    "                \"financial_metrics\": [\n",
    "                    {\n",
    "                        \"name\": \"Revenue\",\n",
    "                        \"value\": 150000000,\n",
    "                        \"unit\": \"USD\",\n",
    "                        \"period\": \"2024\",\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Contract-specific schema\n",
    "class ContractClause(BaseModel):\n",
    "    \"\"\"Contract clause structure\"\"\"\n",
    "\n",
    "    clause_type: str  # e.g., \"payment_terms\", \"termination\", \"liability\"\n",
    "    content: str\n",
    "    importance: str = \"medium\"  # low, medium, high, critical\n",
    "\n",
    "\n",
    "class ContractInfo(BaseModel):\n",
    "    \"\"\"Contract document schema\"\"\"\n",
    "\n",
    "    contract_type: str  # e.g., \"service_agreement\", \"employment\", \"NDA\"\n",
    "    parties: List[str] = Field(default_factory=list)\n",
    "    effective_date: Optional[str] = None\n",
    "    expiration_date: Optional[str] = None\n",
    "    contract_value: Optional[float] = None\n",
    "    currency: str = \"USD\"\n",
    "    key_clauses: List[ContractClause] = Field(default_factory=list)\n",
    "    obligations: List[str] = Field(default_factory=list)\n",
    "    risks: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "print(\"âœ… Schema definitions created successfully\")\n",
    "print(\"Available schemas: DocumentSummary, ContractInfo, ContactInfo, FinancialMetric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. LLM Extraction Engine - Structured Information Extraction\n",
    "# LLM æŠ½å–å¼•æ“Ž - çµæ§‹åŒ–è³‡è¨ŠæŠ½å–èˆ‡æç¤ºå·¥ç¨‹\n",
    "\n",
    "import json\n",
    "import re\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "import torch\n",
    "\n",
    "\n",
    "class LLMExtractor:\n",
    "    \"\"\"LLM-based information extraction engine\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name: str = \"microsoft/DialoGPT-medium\", use_4bit: bool = True\n",
    "    ):\n",
    "        \"\"\"Initialize with low-VRAM friendly settings\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Configure quantization for low VRAM\n",
    "        if use_4bit and torch.cuda.is_available():\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "        else:\n",
    "            bnb_config = None\n",
    "\n",
    "        # Load model with device mapping for multi-GPU or CPU fallback\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name, padding_side=\"left\"\n",
    "            )\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "                torch_dtype=(\n",
    "                    torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "                ),\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "\n",
    "            print(f\"âœ… Model loaded: {model_name} on {self.device}\")\n",
    "            if torch.cuda.is_available():\n",
    "                print(f\"ðŸ’¾ VRAM usage: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading model: {e}\")\n",
    "            print(\"ðŸ’¡ Falling back to simpler extraction methods...\")\n",
    "            self.model = None\n",
    "            self.tokenizer = None\n",
    "\n",
    "    def extract_structured_info(self, text: str, schema_type: str = \"general\") -> Dict:\n",
    "        \"\"\"Extract structured information based on schema type\"\"\"\n",
    "\n",
    "        # Create extraction prompt based on schema\n",
    "        if schema_type == \"general\":\n",
    "            prompt = self._create_general_extraction_prompt(text)\n",
    "            return self._extract_with_prompt(prompt, DocumentSummary)\n",
    "        elif schema_type == \"contract\":\n",
    "            prompt = self._create_contract_extraction_prompt(text)\n",
    "            return self._extract_with_prompt(prompt, ContractInfo)\n",
    "        else:\n",
    "            return self._fallback_extraction(text)\n",
    "\n",
    "    def _create_general_extraction_prompt(self, text: str) -> str:\n",
    "        \"\"\"Create prompt for general document extraction\"\"\"\n",
    "        return f\"\"\"\n",
    "Analyze the following document and extract structured information in JSON format.\n",
    "\n",
    "Document text:\n",
    "{text[:2000]}...\n",
    "\n",
    "Extract the following information:\n",
    "1. Document type (e.g., annual_report, contract, resume, email)\n",
    "2. Title or subject\n",
    "3. Brief summary (1-2 sentences)\n",
    "4. People mentioned (name, title, email, phone)\n",
    "5. Companies mentioned (name, industry, contact info)\n",
    "6. Financial metrics (amounts, percentages, revenues)\n",
    "7. Important dates\n",
    "8. Key topics/themes\n",
    "\n",
    "Return only valid JSON in this exact format:\n",
    "{{\n",
    "    \"document_type\": \"document_type_here\",\n",
    "    \"title\": \"title_here\",\n",
    "    \"summary\": \"summary_here\",\n",
    "    \"contacts\": [\n",
    "        {{\"name\": \"John Doe\", \"title\": \"CEO\", \"email\": \"john@company.com\", \"phone\": \"+1-555-0123\"}}\n",
    "    ],\n",
    "    \"companies\": [\n",
    "        {{\"company_name\": \"TechCorp\", \"industry\": \"Technology\", \"website\": \"www.techcorp.com\"}}\n",
    "    ],\n",
    "    \"financial_metrics\": [\n",
    "        {{\"name\": \"Revenue\", \"value\": 150000000, \"unit\": \"USD\", \"period\": \"2024\"}}\n",
    "    ],\n",
    "    \"key_dates\": [\"2024-01-01\", \"Q2 2024\"],\n",
    "    \"key_topics\": [\"growth\", \"expansion\", \"partnership\"],\n",
    "    \"confidence_score\": 0.8\n",
    "}}\n",
    "\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "    def _create_contract_extraction_prompt(self, text: str) -> str:\n",
    "        \"\"\"Create prompt for contract-specific extraction\"\"\"\n",
    "        return f\"\"\"\n",
    "Analyze this contract document and extract key legal information in JSON format.\n",
    "\n",
    "Contract text:\n",
    "{text[:2000]}...\n",
    "\n",
    "Extract:\n",
    "1. Contract type (service_agreement, employment, NDA, etc.)\n",
    "2. Parties involved\n",
    "3. Effective and expiration dates\n",
    "4. Contract value and currency\n",
    "5. Key clauses (payment terms, termination, liability, etc.)\n",
    "6. Obligations for each party\n",
    "7. Potential risks or red flags\n",
    "\n",
    "Return valid JSON:\n",
    "{{\n",
    "    \"contract_type\": \"service_agreement\",\n",
    "    \"parties\": [\"Company A\", \"Company B\"],\n",
    "    \"effective_date\": \"2024-01-01\",\n",
    "    \"expiration_date\": \"2024-12-31\",\n",
    "    \"contract_value\": 100000,\n",
    "    \"currency\": \"USD\",\n",
    "    \"key_clauses\": [\n",
    "        {{\"clause_type\": \"payment_terms\", \"content\": \"Payment due within 30 days\", \"importance\": \"high\"}}\n",
    "    ],\n",
    "    \"obligations\": [\"Company A must deliver software\", \"Company B must pay fees\"],\n",
    "    \"risks\": [\"No penalty for late delivery\", \"Unclear IP ownership\"]\n",
    "}}\n",
    "\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "    def _extract_with_prompt(self, prompt: str, schema_class) -> Dict:\n",
    "        \"\"\"Extract information using LLM with structured prompt\"\"\"\n",
    "\n",
    "        if self.model is None:\n",
    "            return self._fallback_extraction(prompt)\n",
    "\n",
    "        try:\n",
    "            # Tokenize and generate\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, return_tensors=\"pt\", truncation=True, max_length=1024\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=512,\n",
    "                    temperature=0.1,  # Low temperature for structured output\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    num_return_sequences=1,\n",
    "                )\n",
    "\n",
    "            # Decode response\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            json_part = response[len(prompt) :].strip()\n",
    "\n",
    "            # Extract JSON from response\n",
    "            json_match = re.search(r\"\\{.*\\}\", json_part, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group()\n",
    "                try:\n",
    "                    extracted_data = json.loads(json_str)\n",
    "                    # Validate against schema\n",
    "                    validated = schema_class(**extracted_data)\n",
    "                    return validated.dict()\n",
    "                except (json.JSONDecodeError, ValueError) as e:\n",
    "                    print(f\"âš ï¸ JSON parsing error: {e}\")\n",
    "                    return self._fallback_extraction(prompt)\n",
    "            else:\n",
    "                return self._fallback_extraction(prompt)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Extraction error: {e}\")\n",
    "            return self._fallback_extraction(prompt)\n",
    "\n",
    "    def _fallback_extraction(self, text: str) -> Dict:\n",
    "        \"\"\"Fallback extraction using regex patterns\"\"\"\n",
    "        print(\"ðŸ“‹ Using fallback regex-based extraction...\")\n",
    "\n",
    "        # Simple regex-based extraction\n",
    "        emails = re.findall(\n",
    "            r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", text\n",
    "        )\n",
    "        phones = re.findall(\n",
    "            r\"\\+?1?-?\\s?\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4}\", text\n",
    "        )\n",
    "        dates = re.findall(\n",
    "            r\"\\b\\d{4}[-/]\\d{2}[-/]\\d{2}\\b|\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{4}\\b\", text\n",
    "        )\n",
    "        amounts = re.findall(r\"\\$\\s*(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)\", text)\n",
    "\n",
    "        # Extract potential names (capitalized words)\n",
    "        potential_names = re.findall(r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b\", text)\n",
    "\n",
    "        return {\n",
    "            \"document_type\": \"unknown\",\n",
    "            \"title\": text.split(\"\\n\")[0][:100] if text else \"Unknown\",\n",
    "            \"summary\": text[:200] + \"...\" if len(text) > 200 else text,\n",
    "            \"contacts\": [{\"email\": email} for email in emails[:3]],\n",
    "            \"companies\": [],\n",
    "            \"financial_metrics\": [\n",
    "                {\"name\": \"Amount\", \"value\": float(amt.replace(\",\", \"\")), \"unit\": \"USD\"}\n",
    "                for amt in amounts[:3]\n",
    "            ],\n",
    "            \"key_dates\": dates[:5],\n",
    "            \"key_topics\": [\"document_analysis\"],\n",
    "            \"confidence_score\": 0.3,  # Low confidence for regex-based extraction\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize extractor with fallback-friendly model\n",
    "# åˆå§‹åŒ–æŠ½å–å™¨ï¼ˆä½¿ç”¨å®¹éŒ¯æ¨¡åž‹ï¼‰\n",
    "try:\n",
    "    # Try with smaller model first for testing\n",
    "    extractor = LLMExtractor(model_name=\"microsoft/DialoGPT-small\", use_4bit=True)\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Model loading failed: {e}\")\n",
    "    print(\"ðŸ’¡ Creating extractor with fallback mode...\")\n",
    "    extractor = LLMExtractor()\n",
    "\n",
    "# Test extraction on sample document\n",
    "# æ¸¬è©¦æ–‡ä»¶æŠ½å–åŠŸèƒ½\n",
    "print(\"\\nðŸ” Testing Information Extraction:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "extracted_info = extractor.extract_structured_info(sample_text, schema_type=\"general\")\n",
    "print(\"ðŸ“Š Extracted Information:\")\n",
    "print(json.dumps(extracted_info, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Batch Processing Pipeline - Scalable Document Processing\n",
    "# æ‰¹é‡è™•ç†ç®¡ç·š - å¯æ“´å±•çš„æ–‡ä»¶è™•ç†å·¥ä½œæµ\n",
    "\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Generator\n",
    "\n",
    "\n",
    "class DocumentProcessingPipeline:\n",
    "    \"\"\"Scalable document processing pipeline with error handling\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, parser: DocumentParser, extractor: LLMExtractor, max_workers: int = 2\n",
    "    ):\n",
    "        self.parser = parser\n",
    "        self.extractor = extractor\n",
    "        self.max_workers = max_workers\n",
    "        self.results = []\n",
    "\n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def process_single_document(\n",
    "        self, file_path: Path, schema_type: str = \"general\"\n",
    "    ) -> Dict:\n",
    "        \"\"\"Process a single document through the full pipeline\"\"\"\n",
    "        try:\n",
    "            # Parse document\n",
    "            doc_content = self.parser.parse(file_path)\n",
    "\n",
    "            # Extract structured information\n",
    "            extracted_info = self.extractor.extract_structured_info(\n",
    "                doc_content.text, schema_type=schema_type\n",
    "            )\n",
    "\n",
    "            # Combine results\n",
    "            result = {\n",
    "                \"file_path\": str(file_path),\n",
    "                \"processing_time\": datetime.now().isoformat(),\n",
    "                \"document_metadata\": doc_content.metadata,\n",
    "                \"extracted_info\": extracted_info,\n",
    "                \"status\": \"success\",\n",
    "            }\n",
    "\n",
    "            # Add table information if available\n",
    "            if doc_content.tables:\n",
    "                result[\"tables_found\"] = len(doc_content.tables)\n",
    "                result[\"table_summaries\"] = [\n",
    "                    {\"rows\": len(table), \"columns\": len(table.columns)}\n",
    "                    for table in doc_content.tables\n",
    "                ]\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing {file_path}: {e}\")\n",
    "            return {\n",
    "                \"file_path\": str(file_path),\n",
    "                \"processing_time\": datetime.now().isoformat(),\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"error\",\n",
    "            }\n",
    "\n",
    "    def process_batch(\n",
    "        self, file_paths: List[Path], schema_type: str = \"general\"\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Process multiple documents in parallel\"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Sequential processing for memory management (can enable parallel if VRAM allows)\n",
    "        if self.max_workers == 1 or not torch.cuda.is_available():\n",
    "            # Sequential processing\n",
    "            for file_path in tqdm(file_paths, desc=\"Processing documents\"):\n",
    "                result = self.process_single_document(file_path, schema_type)\n",
    "                results.append(result)\n",
    "        else:\n",
    "            # Parallel processing (use with caution on GPU)\n",
    "            with concurrent.futures.ThreadPoolExecutor(\n",
    "                max_workers=self.max_workers\n",
    "            ) as executor:\n",
    "                future_to_file = {\n",
    "                    executor.submit(\n",
    "                        self.process_single_document, file_path, schema_type\n",
    "                    ): file_path\n",
    "                    for file_path in file_paths\n",
    "                }\n",
    "\n",
    "                for future in tqdm(\n",
    "                    concurrent.futures.as_completed(future_to_file),\n",
    "                    total=len(file_paths),\n",
    "                    desc=\"Processing documents\",\n",
    "                ):\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "\n",
    "        self.results = results\n",
    "        return results\n",
    "\n",
    "    def export_results(self, output_path: Path, format: str = \"json\"):\n",
    "        \"\"\"Export processing results to file\"\"\"\n",
    "        if format == \"json\":\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(self.results, f, indent=2, ensure_ascii=False)\n",
    "        elif format == \"csv\":\n",
    "            # Flatten results for CSV export\n",
    "            flattened = []\n",
    "            for result in self.results:\n",
    "                if result[\"status\"] == \"success\":\n",
    "                    flat_result = {\n",
    "                        \"file_path\": result[\"file_path\"],\n",
    "                        \"processing_time\": result[\"processing_time\"],\n",
    "                        \"document_type\": result[\"extracted_info\"].get(\n",
    "                            \"document_type\", \"\"\n",
    "                        ),\n",
    "                        \"title\": result[\"extracted_info\"].get(\"title\", \"\"),\n",
    "                        \"summary\": result[\"extracted_info\"].get(\"summary\", \"\"),\n",
    "                        \"contacts_count\": len(\n",
    "                            result[\"extracted_info\"].get(\"contacts\", [])\n",
    "                        ),\n",
    "                        \"companies_count\": len(\n",
    "                            result[\"extracted_info\"].get(\"companies\", [])\n",
    "                        ),\n",
    "                        \"confidence_score\": result[\"extracted_info\"].get(\n",
    "                            \"confidence_score\", 0\n",
    "                        ),\n",
    "                    }\n",
    "                else:\n",
    "                    flat_result = {\n",
    "                        \"file_path\": result[\"file_path\"],\n",
    "                        \"processing_time\": result[\"processing_time\"],\n",
    "                        \"error\": result.get(\"error\", \"\"),\n",
    "                        \"status\": result[\"status\"],\n",
    "                    }\n",
    "                flattened.append(flat_result)\n",
    "\n",
    "            df = pd.DataFrame(flattened)\n",
    "            df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "        print(f\"âœ… Results exported to: {output_path}\")\n",
    "\n",
    "    def get_processing_stats(self) -> Dict:\n",
    "        \"\"\"Get processing statistics\"\"\"\n",
    "        if not self.results:\n",
    "            return {\"message\": \"No results to analyze\"}\n",
    "\n",
    "        total_docs = len(self.results)\n",
    "        successful = sum(1 for r in self.results if r[\"status\"] == \"success\")\n",
    "        failed = total_docs - successful\n",
    "\n",
    "        # Calculate confidence score distribution\n",
    "        confidence_scores = [\n",
    "            r[\"extracted_info\"][\"confidence_score\"]\n",
    "            for r in self.results\n",
    "            if r[\"status\"] == \"success\" and \"confidence_score\" in r[\"extracted_info\"]\n",
    "        ]\n",
    "\n",
    "        avg_confidence = (\n",
    "            sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"total_documents\": total_docs,\n",
    "            \"successful\": successful,\n",
    "            \"failed\": failed,\n",
    "            \"success_rate\": successful / total_docs * 100,\n",
    "            \"average_confidence\": avg_confidence,\n",
    "            \"confidence_distribution\": {\n",
    "                \"high (>0.8)\": sum(1 for c in confidence_scores if c > 0.8),\n",
    "                \"medium (0.5-0.8)\": sum(\n",
    "                    1 for c in confidence_scores if 0.5 <= c <= 0.8\n",
    "                ),\n",
    "                \"low (<0.5)\": sum(1 for c in confidence_scores if c < 0.5),\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "# Create processing pipeline\n",
    "# å‰µå»ºè™•ç†ç®¡ç·š\n",
    "pipeline = DocumentProcessingPipeline(\n",
    "    parser=parser,\n",
    "    extractor=extractor,\n",
    "    max_workers=1,  # Sequential processing for VRAM safety\n",
    ")\n",
    "\n",
    "# Create additional sample documents for batch testing\n",
    "# å‰µå»ºé¡å¤–ç¯„ä¾‹æ–‡ä»¶ä»¥æ¸¬è©¦æ‰¹é‡è™•ç†\n",
    "sample_files = []\n",
    "\n",
    "# Sample contract document\n",
    "contract_text = \"\"\"\n",
    "SERVICE AGREEMENT\n",
    "\n",
    "This Service Agreement (\"Agreement\") is entered into on January 1, 2024,\n",
    "between TechCorp Inc. (\"Provider\") and Business Solutions Ltd. (\"Client\").\n",
    "\n",
    "PARTIES:\n",
    "- Provider: TechCorp Inc., 123 Tech Street, Silicon Valley, CA 94000\n",
    "- Client: Business Solutions Ltd., 456 Business Ave, New York, NY 10001\n",
    "\n",
    "SCOPE OF WORK:\n",
    "The Provider agrees to deliver custom software development services including:\n",
    "1. Web application development\n",
    "2. Mobile app development\n",
    "3. System integration\n",
    "4. Technical support and maintenance\n",
    "\n",
    "PAYMENT TERMS:\n",
    "- Total contract value: $250,000 USD\n",
    "- Payment schedule: 50% upfront, 25% at milestone 1, 25% upon completion\n",
    "- Invoices due within 30 days of receipt\n",
    "\n",
    "TIMELINE:\n",
    "- Effective Date: January 1, 2024\n",
    "- Project Duration: 12 months\n",
    "- Completion Date: December 31, 2024\n",
    "\n",
    "TERMINATION:\n",
    "Either party may terminate this agreement with 30 days written notice.\n",
    "Client retains rights to completed work upon termination.\n",
    "\n",
    "CONTACTS:\n",
    "- Provider Contact: John Tech (john.tech@techcorp.com, +1-555-TECH)\n",
    "- Client Contact: Jane Business (jane.business@bizsolve.com, +1-555-BUSI)\n",
    "\"\"\"\n",
    "\n",
    "contract_path = Path(\"sample_contract.txt\")\n",
    "with open(contract_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(contract_text)\n",
    "sample_files.append(contract_path)\n",
    "\n",
    "# Sample resume document\n",
    "resume_text = \"\"\"\n",
    "ALICE CHEN\n",
    "Software Engineer\n",
    "\n",
    "Contact Information:\n",
    "Email: alice.chen@email.com\n",
    "Phone: +1-555-0199\n",
    "LinkedIn: linkedin.com/in/alicechen\n",
    "Location: San Francisco, CA\n",
    "\n",
    "PROFESSIONAL SUMMARY:\n",
    "Experienced software engineer with 8+ years in full-stack development.\n",
    "Specialized in Python, JavaScript, and cloud technologies.\n",
    "\n",
    "WORK EXPERIENCE:\n",
    "\n",
    "Senior Software Engineer | Meta | 2020-2024\n",
    "- Led development of microservices handling 10M+ daily requests\n",
    "- Reduced API response time by 40% through optimization\n",
    "- Mentored 5 junior developers\n",
    "\n",
    "Software Engineer | Google | 2018-2020\n",
    "- Developed machine learning pipelines for search algorithms\n",
    "- Collaborated with cross-functional teams of 20+ members\n",
    "- Contributed to open-source TensorFlow projects\n",
    "\n",
    "Junior Developer | Startup Co | 2016-2018\n",
    "- Built web applications using React and Node.js\n",
    "- Implemented CI/CD pipelines reducing deployment time by 60%\n",
    "\n",
    "EDUCATION:\n",
    "Master of Science in Computer Science | Stanford University | 2016\n",
    "Bachelor of Science in Software Engineering | UC Berkeley | 2014\n",
    "\n",
    "SKILLS:\n",
    "- Programming: Python, JavaScript, Java, Go, SQL\n",
    "- Frameworks: React, Django, Flask, Node.js\n",
    "- Cloud: AWS, GCP, Docker, Kubernetes\n",
    "- Databases: PostgreSQL, MongoDB, Redis\n",
    "\n",
    "CERTIFICATIONS:\n",
    "- AWS Solutions Architect Professional (2023)\n",
    "- Google Cloud Professional Data Engineer (2022)\n",
    "\"\"\"\n",
    "\n",
    "resume_path = Path(\"sample_resume.txt\")\n",
    "with open(resume_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(resume_text)\n",
    "sample_files.append(resume_path)\n",
    "\n",
    "# Test batch processing\n",
    "# æ¸¬è©¦æ‰¹é‡è™•ç†\n",
    "print(\"\\nðŸ”„ Testing Batch Processing Pipeline:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Process documents with different schema types\n",
    "results = []\n",
    "for file_path in sample_files:\n",
    "    if \"contract\" in file_path.name:\n",
    "        result = pipeline.process_single_document(file_path, schema_type=\"contract\")\n",
    "    else:\n",
    "        result = pipeline.process_single_document(file_path, schema_type=\"general\")\n",
    "    results.append(result)\n",
    "\n",
    "# Add original sample document\n",
    "results.append(pipeline.process_single_document(sample_path, schema_type=\"general\"))\n",
    "\n",
    "pipeline.results = results\n",
    "\n",
    "# Display processing statistics\n",
    "stats = pipeline.get_processing_stats()\n",
    "print(\"\\nðŸ“Š Processing Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Export results\n",
    "output_path = Path(\"extraction_results.json\")\n",
    "pipeline.export_results(output_path, format=\"json\")\n",
    "\n",
    "print(f\"\\nâœ… Processed {len(sample_files)+1} documents successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Quality Assessment Module - Extraction Accuracy Evaluation\n",
    "# å“è³ªè©•ä¼°æ¨¡çµ„ - æŠ½å–æº–ç¢ºçŽ‡è©•ä¼°\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import difflib\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class ExtractionEvaluator:\n",
    "    \"\"\"Evaluate extraction quality against ground truth\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.evaluation_results = []\n",
    "\n",
    "    def evaluate_extraction(self, extracted: Dict, ground_truth: Dict) -> Dict:\n",
    "        \"\"\"Evaluate extraction against ground truth data\"\"\"\n",
    "\n",
    "        results = {\n",
    "            \"overall_accuracy\": 0.0,\n",
    "            \"field_accuracy\": {},\n",
    "            \"entity_metrics\": {},\n",
    "            \"confidence_correlation\": 0.0,\n",
    "        }\n",
    "\n",
    "        # Field-level accuracy\n",
    "        field_scores = []\n",
    "        for field in ground_truth.keys():\n",
    "            if field in extracted:\n",
    "                if isinstance(ground_truth[field], str):\n",
    "                    # Text similarity for string fields\n",
    "                    similarity = difflib.SequenceMatcher(\n",
    "                        None,\n",
    "                        str(extracted[field]).lower(),\n",
    "                        str(ground_truth[field]).lower(),\n",
    "                    ).ratio()\n",
    "                    field_scores.append(similarity)\n",
    "                    results[\"field_accuracy\"][field] = similarity\n",
    "                elif isinstance(ground_truth[field], list):\n",
    "                    # List overlap for list fields\n",
    "                    if len(ground_truth[field]) == 0:\n",
    "                        overlap = 1.0 if len(extracted[field]) == 0 else 0.0\n",
    "                    else:\n",
    "                        extracted_set = set(str(x).lower() for x in extracted[field])\n",
    "                        ground_truth_set = set(\n",
    "                            str(x).lower() for x in ground_truth[field]\n",
    "                        )\n",
    "                        overlap = len(extracted_set & ground_truth_set) / len(\n",
    "                            ground_truth_set\n",
    "                        )\n",
    "                    field_scores.append(overlap)\n",
    "                    results[\"field_accuracy\"][field] = overlap\n",
    "                else:\n",
    "                    # Exact match for other types\n",
    "                    exact_match = (\n",
    "                        1.0 if extracted[field] == ground_truth[field] else 0.0\n",
    "                    )\n",
    "                    field_scores.append(exact_match)\n",
    "                    results[\"field_accuracy\"][field] = exact_match\n",
    "            else:\n",
    "                field_scores.append(0.0)\n",
    "                results[\"field_accuracy\"][field] = 0.0\n",
    "\n",
    "        results[\"overall_accuracy\"] = (\n",
    "            sum(field_scores) / len(field_scores) if field_scores else 0.0\n",
    "        )\n",
    "\n",
    "        # Entity-level evaluation (contacts, companies)\n",
    "        if \"contacts\" in ground_truth and \"contacts\" in extracted:\n",
    "            results[\"entity_metrics\"][\"contacts\"] = self._evaluate_entities(\n",
    "                extracted[\"contacts\"], ground_truth[\"contacts\"]\n",
    "            )\n",
    "\n",
    "        if \"companies\" in ground_truth and \"companies\" in extracted:\n",
    "            results[\"entity_metrics\"][\"companies\"] = self._evaluate_entities(\n",
    "                extracted[\"companies\"], ground_truth[\"companies\"]\n",
    "            )\n",
    "\n",
    "        # Confidence correlation (if available)\n",
    "        if \"confidence_score\" in extracted:\n",
    "            results[\"confidence_correlation\"] = min(\n",
    "                extracted[\"confidence_score\"] * results[\"overall_accuracy\"], 1.0\n",
    "            )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _evaluate_entities(\n",
    "        self, extracted_entities: List[Dict], ground_truth_entities: List[Dict]\n",
    "    ) -> Dict:\n",
    "        \"\"\"Evaluate entity extraction (precision, recall, F1)\"\"\"\n",
    "        if not ground_truth_entities:\n",
    "            return (\n",
    "                {\"precision\": 1.0, \"recall\": 1.0, \"f1\": 1.0}\n",
    "                if not extracted_entities\n",
    "                else {\"precision\": 0.0, \"recall\": 1.0, \"f1\": 0.0}\n",
    "            )\n",
    "\n",
    "        if not extracted_entities:\n",
    "            return {\"precision\": 1.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "        # Simple name-based matching for demonstration\n",
    "        extracted_names = set()\n",
    "        ground_truth_names = set()\n",
    "\n",
    "        for entity in extracted_entities:\n",
    "            if \"name\" in entity and entity[\"name\"]:\n",
    "                extracted_names.add(entity[\"name\"].lower().strip())\n",
    "\n",
    "        for entity in ground_truth_entities:\n",
    "            if \"name\" in entity and entity[\"name\"]:\n",
    "                ground_truth_names.add(entity[\"name\"].lower().strip())\n",
    "\n",
    "        if not ground_truth_names:\n",
    "            precision = 1.0 if not extracted_names else 0.0\n",
    "            recall = 1.0\n",
    "            f1 = 1.0 if precision == 1.0 else 0.0\n",
    "        else:\n",
    "            true_positives = len(extracted_names & ground_truth_names)\n",
    "            precision = (\n",
    "                true_positives / len(extracted_names) if extracted_names else 0.0\n",
    "            )\n",
    "            recall = true_positives / len(ground_truth_names)\n",
    "            f1 = (\n",
    "                2 * (precision * recall) / (precision + recall)\n",
    "                if (precision + recall) > 0\n",
    "                else 0.0\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"true_positives\": len(extracted_names & ground_truth_names),\n",
    "            \"extracted_count\": len(extracted_names),\n",
    "            \"ground_truth_count\": len(ground_truth_names),\n",
    "        }\n",
    "\n",
    "    def create_evaluation_report(self, evaluations: List[Dict]) -> Dict:\n",
    "        \"\"\"Create comprehensive evaluation report\"\"\"\n",
    "        if not evaluations:\n",
    "            return {\"error\": \"No evaluations provided\"}\n",
    "\n",
    "        # Aggregate metrics\n",
    "        overall_accuracies = [e[\"overall_accuracy\"] for e in evaluations]\n",
    "\n",
    "        report = {\n",
    "            \"summary\": {\n",
    "                \"total_evaluations\": len(evaluations),\n",
    "                \"average_accuracy\": sum(overall_accuracies) / len(overall_accuracies),\n",
    "                \"min_accuracy\": min(overall_accuracies),\n",
    "                \"max_accuracy\": max(overall_accuracies),\n",
    "                \"std_accuracy\": (\n",
    "                    sum(\n",
    "                        (x - sum(overall_accuracies) / len(overall_accuracies)) ** 2\n",
    "                        for x in overall_accuracies\n",
    "                    )\n",
    "                    / len(overall_accuracies)\n",
    "                )\n",
    "                ** 0.5,\n",
    "            },\n",
    "            \"field_performance\": {},\n",
    "            \"entity_performance\": {},\n",
    "            \"recommendations\": [],\n",
    "        }\n",
    "\n",
    "        # Field-level performance\n",
    "        all_fields = set()\n",
    "        for eval_result in evaluations:\n",
    "            all_fields.update(eval_result[\"field_accuracy\"].keys())\n",
    "\n",
    "        for field in all_fields:\n",
    "            field_scores = [e[\"field_accuracy\"].get(field, 0.0) for e in evaluations]\n",
    "            report[\"field_performance\"][field] = {\n",
    "                \"average\": sum(field_scores) / len(field_scores),\n",
    "                \"min\": min(field_scores),\n",
    "                \"max\": max(field_scores),\n",
    "            }\n",
    "\n",
    "        # Entity-level performance\n",
    "        for entity_type in [\"contacts\", \"companies\"]:\n",
    "            entity_metrics = []\n",
    "            for eval_result in evaluations:\n",
    "                if entity_type in eval_result[\"entity_metrics\"]:\n",
    "                    entity_metrics.append(eval_result[\"entity_metrics\"][entity_type])\n",
    "\n",
    "            if entity_metrics:\n",
    "                avg_precision = sum(m[\"precision\"] for m in entity_metrics) / len(\n",
    "                    entity_metrics\n",
    "                )\n",
    "                avg_recall = sum(m[\"recall\"] for m in entity_metrics) / len(\n",
    "                    entity_metrics\n",
    "                )\n",
    "                avg_f1 = sum(m[\"f1\"] for m in entity_metrics) / len(entity_metrics)\n",
    "\n",
    "                report[\"entity_performance\"][entity_type] = {\n",
    "                    \"precision\": avg_precision,\n",
    "                    \"recall\": avg_recall,\n",
    "                    \"f1\": avg_f1,\n",
    "                }\n",
    "\n",
    "        # Generate recommendations\n",
    "        if report[\"summary\"][\"average_accuracy\"] < 0.7:\n",
    "            report[\"recommendations\"].append(\n",
    "                \"Overall accuracy is low. Consider improving prompts or using a larger model.\"\n",
    "            )\n",
    "\n",
    "        for field, performance in report[\"field_performance\"].items():\n",
    "            if performance[\"average\"] < 0.5:\n",
    "                report[\"recommendations\"].append(\n",
    "                    f\"Field '{field}' shows poor extraction accuracy. Review extraction logic.\"\n",
    "                )\n",
    "\n",
    "        return report\n",
    "\n",
    "\n",
    "# Test evaluation with sample data\n",
    "# æ¸¬è©¦è©•ä¼°åŠŸèƒ½\n",
    "evaluator = ExtractionEvaluator()\n",
    "\n",
    "# Create ground truth for sample document\n",
    "ground_truth_sample = {\n",
    "    \"document_type\": \"annual_report\",\n",
    "    \"title\": \"Company Annual Report 2024\",\n",
    "    \"summary\": \"Annual financial performance report showing growth\",\n",
    "    \"contacts\": [\n",
    "        {\"name\": \"John Smith\", \"email\": \"john.smith@company.com\", \"title\": \"CEO\"},\n",
    "        {\"name\": \"Jane Doe\", \"email\": \"jane.doe@company.com\", \"title\": \"CFO\"},\n",
    "        {\"name\": \"Bob Wilson\", \"email\": \"bob.wilson@company.com\", \"title\": \"CTO\"},\n",
    "    ],\n",
    "    \"companies\": [{\"company_name\": \"TechCorp\", \"industry\": \"Technology\"}],\n",
    "    \"financial_metrics\": [\n",
    "        {\"name\": \"Revenue\", \"value\": 150000000, \"unit\": \"USD\", \"period\": \"2024\"},\n",
    "        {\"name\": \"Profit\", \"value\": 22000000, \"unit\": \"USD\", \"period\": \"2024\"},\n",
    "    ],\n",
    "    \"key_dates\": [\"2024\"],\n",
    "    \"key_topics\": [\"growth\", \"revenue\", \"partnership\"],\n",
    "    \"confidence_score\": 0.8,\n",
    "}\n",
    "\n",
    "# Evaluate extraction\n",
    "evaluation_result = evaluator.evaluate_extraction(extracted_info, ground_truth_sample)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Extraction Quality Evaluation:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Overall Accuracy: {evaluation_result['overall_accuracy']:.2f}\")\n",
    "print(\"\\nField-level Accuracy:\")\n",
    "for field, score in evaluation_result[\"field_accuracy\"].items():\n",
    "    print(f\"  {field}: {score:.2f}\")\n",
    "\n",
    "if evaluation_result[\"entity_metrics\"]:\n",
    "    print(\"\\nEntity Extraction Metrics:\")\n",
    "    for entity_type, metrics in evaluation_result[\"entity_metrics\"].items():\n",
    "        print(f\"  {entity_type}:\")\n",
    "        print(f\"    Precision: {metrics['precision']:.2f}\")\n",
    "        print(f\"    Recall: {metrics['recall']:.2f}\")\n",
    "        print(f\"    F1-Score: {metrics['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Real-world Use Cases - Contract, Resume, and Academic Paper Analysis\n",
    "# å¯¦æˆ°æ¡ˆä¾‹ - åˆç´„ã€ç°¡æ­·ã€å­¸è¡“è«–æ–‡åˆ†æž\n",
    "\n",
    "\n",
    "class SpecializedExtractors:\n",
    "    \"\"\"Specialized extractors for different document types\"\"\"\n",
    "\n",
    "    def __init__(self, base_extractor: LLMExtractor):\n",
    "        self.base_extractor = base_extractor\n",
    "\n",
    "    def extract_resume_info(self, text: str) -> Dict:\n",
    "        \"\"\"Extract structured information from resume/CV\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "Extract professional information from this resume/CV in JSON format:\n",
    "\n",
    "{text[:1500]}...\n",
    "\n",
    "Extract:\n",
    "1. Personal information (name, email, phone, location)\n",
    "2. Professional summary/objective\n",
    "3. Work experience (company, role, duration, achievements)\n",
    "4. Education (degree, school, year)\n",
    "5. Skills (technical, soft skills)\n",
    "6. Certifications\n",
    "7. Years of experience (total)\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "    \"personal_info\": {{\n",
    "        \"name\": \"Full Name\",\n",
    "        \"email\": \"email@domain.com\",\n",
    "        \"phone\": \"+1-555-0123\",\n",
    "        \"location\": \"City, State\"\n",
    "    }},\n",
    "    \"professional_summary\": \"Brief summary...\",\n",
    "    \"work_experience\": [\n",
    "        {{\n",
    "            \"company\": \"Company Name\",\n",
    "            \"role\": \"Job Title\",\n",
    "            \"duration\": \"2020-2024\",\n",
    "            \"achievements\": [\"Achievement 1\", \"Achievement 2\"]\n",
    "        }}\n",
    "    ],\n",
    "    \"education\": [\n",
    "        {{\n",
    "            \"degree\": \"Master of Science\",\n",
    "            \"field\": \"Computer Science\",\n",
    "            \"school\": \"University Name\",\n",
    "            \"year\": \"2016\"\n",
    "        }}\n",
    "    ],\n",
    "    \"skills\": {{\n",
    "        \"technical\": [\"Python\", \"JavaScript\"],\n",
    "        \"soft\": [\"Leadership\", \"Communication\"]\n",
    "    }},\n",
    "    \"certifications\": [\"AWS Certified\", \"Google Cloud\"],\n",
    "    \"total_experience_years\": 8\n",
    "}}\n",
    "\n",
    "JSON:\n",
    "\"\"\"\n",
    "        return self.base_extractor._extract_with_prompt(prompt, DocumentSummary)\n",
    "\n",
    "    def extract_academic_paper_info(self, text: str) -> Dict:\n",
    "        \"\"\"Extract information from academic papers\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "Extract academic paper information in JSON format:\n",
    "\n",
    "{text[:1500]}...\n",
    "\n",
    "Extract:\n",
    "1. Title and abstract\n",
    "2. Authors and affiliations\n",
    "3. Keywords\n",
    "4. Methodology/approach\n",
    "5. Key findings/results\n",
    "6. References count (estimate)\n",
    "7. Research field/domain\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "    \"title\": \"Paper Title\",\n",
    "    \"abstract\": \"Abstract text...\",\n",
    "    \"authors\": [\n",
    "        {{\"name\": \"Author Name\", \"affiliation\": \"University Name\"}}\n",
    "    ],\n",
    "    \"keywords\": [\"keyword1\", \"keyword2\"],\n",
    "    \"methodology\": \"Research approach description\",\n",
    "    \"key_findings\": [\"Finding 1\", \"Finding 2\"],\n",
    "    \"research_field\": \"Computer Science\",\n",
    "    \"estimated_references\": 25,\n",
    "    \"publication_info\": {{\n",
    "        \"journal\": \"Journal Name\",\n",
    "        \"year\": \"2024\",\n",
    "        \"volume\": \"12\",\n",
    "        \"pages\": \"1-15\"\n",
    "    }}\n",
    "}}\n",
    "\n",
    "JSON:\n",
    "\"\"\"\n",
    "        return self.base_extractor._extract_with_prompt(prompt, DocumentSummary)\n",
    "\n",
    "\n",
    "# Initialize specialized extractors\n",
    "# åˆå§‹åŒ–å°ˆé–€æŠ½å–å™¨\n",
    "specialized = SpecializedExtractors(extractor)\n",
    "\n",
    "# Test resume extraction\n",
    "print(\"\\nðŸ‘¤ Testing Resume Information Extraction:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "resume_info = specialized.extract_resume_info(resume_text)\n",
    "print(\"ðŸ“„ Resume Extraction Results:\")\n",
    "print(json.dumps(resume_info, indent=2, ensure_ascii=False)[:1000] + \"...\")\n",
    "\n",
    "# Create sample academic paper for testing\n",
    "academic_paper_text = \"\"\"\n",
    "Deep Learning for Natural Language Processing: A Comprehensive Survey\n",
    "\n",
    "Abstract:\n",
    "This paper presents a comprehensive survey of deep learning techniques applied to natural language processing (NLP) tasks. We review the evolution from traditional machine learning approaches to modern transformer-based architectures, analyzing their strengths and limitations across various NLP applications.\n",
    "\n",
    "Authors:\n",
    "- Dr. Sarah Johnson, Stanford University, Computer Science Department\n",
    "- Prof. Michael Chen, MIT, Artificial Intelligence Lab\n",
    "- Dr. Elena Rodriguez, Google Research, NLP Team\n",
    "\n",
    "Keywords: deep learning, natural language processing, transformers, BERT, GPT, neural networks, language models\n",
    "\n",
    "1. Introduction\n",
    "Natural Language Processing has undergone significant transformation with the advent of deep learning techniques. Traditional approaches based on rule-based systems and statistical methods have largely been superseded by neural network architectures.\n",
    "\n",
    "2. Methodology\n",
    "We conducted a systematic literature review of 150+ papers published between 2018-2024, focusing on transformer architectures and their applications. Our analysis covers supervised, unsupervised, and semi-supervised learning paradigms.\n",
    "\n",
    "3. Key Findings\n",
    "- Transformer models achieve state-of-the-art performance across multiple NLP benchmarks\n",
    "- Pre-training on large corpora followed by fine-tuning shows consistent improvements\n",
    "- Attention mechanisms enable better handling of long-range dependencies\n",
    "- Model size correlates with performance up to a saturation point\n",
    "\n",
    "4. Applications\n",
    "The reviewed techniques show effectiveness in:\n",
    "- Machine Translation (BLEU scores improved by 15-20%)\n",
    "- Sentiment Analysis (accuracy gains of 10-12%)\n",
    "- Question Answering (F1 scores increased by 8-15%)\n",
    "- Text Summarization (ROUGE scores improved by 12-18%)\n",
    "\n",
    "5. Conclusion\n",
    "Deep learning has revolutionized NLP, with transformer architectures leading current advances. Future research should focus on efficiency improvements and multi-modal integration.\n",
    "\n",
    "References: [1-47 academic citations listed]\n",
    "\n",
    "Published in: Journal of Artificial Intelligence Research, Vol. 28, 2024, pp. 1-25\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nðŸ“š Testing Academic Paper Information Extraction:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "academic_info = specialized.extract_academic_paper_info(academic_paper_text)\n",
    "print(\"ðŸ“„ Academic Paper Extraction Results:\")\n",
    "print(json.dumps(academic_info, indent=2, ensure_ascii=False)[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79383dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Validation and Schema Compliance\n",
    "# é©—è­‰èˆ‡ Schema ç¬¦åˆæ€§æª¢æŸ¥\n",
    "\n",
    "from pydantic import ValidationError\n",
    "import jsonschema\n",
    "\n",
    "\n",
    "class ExtractionValidator:\n",
    "    \"\"\"Validate extraction results against predefined schemas\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.validation_results = []\n",
    "\n",
    "    def validate_against_schema(self, data: Dict, schema_class) -> Dict:\n",
    "        \"\"\"Validate extracted data against Pydantic schema\"\"\"\n",
    "        try:\n",
    "            # Attempt to create model instance\n",
    "            validated_model = schema_class(**data)\n",
    "            return {\n",
    "                \"is_valid\": True,\n",
    "                \"validated_data\": validated_model.dict(),\n",
    "                \"errors\": [],\n",
    "            }\n",
    "        except ValidationError as e:\n",
    "            return {\n",
    "                \"is_valid\": False,\n",
    "                \"validated_data\": None,\n",
    "                \"errors\": [\n",
    "                    {\"field\": err[\"loc\"], \"message\": err[\"msg\"]} for err in e.errors\n",
    "                ],\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"is_valid\": False,\n",
    "                \"validated_data\": None,\n",
    "                \"errors\": [{\"field\": \"general\", \"message\": str(e)}],\n",
    "            }\n",
    "\n",
    "    def clean_and_retry_validation(self, data: Dict, schema_class) -> Dict:\n",
    "        \"\"\"Attempt to clean data and retry validation\"\"\"\n",
    "        cleaned_data = data.copy()\n",
    "\n",
    "        # Common cleaning operations\n",
    "        for key, value in cleaned_data.items():\n",
    "            if isinstance(value, str):\n",
    "                # Clean strings\n",
    "                cleaned_data[key] = value.strip()\n",
    "            elif isinstance(value, list):\n",
    "                # Clean lists - remove None values\n",
    "                cleaned_data[key] = [item for item in value if item is not None]\n",
    "\n",
    "        # Set default values for required fields if missing\n",
    "        if hasattr(schema_class, \"__fields__\"):\n",
    "            for field_name, field_info in schema_class.__fields__.items():\n",
    "                if field_name not in cleaned_data:\n",
    "                    if (\n",
    "                        hasattr(field_info, \"default\")\n",
    "                        and field_info.default is not None\n",
    "                    ):\n",
    "                        cleaned_data[field_name] = field_info.default\n",
    "                    elif field_info.type_ == str:\n",
    "                        cleaned_data[field_name] = \"\"\n",
    "                    elif field_info.type_ == list:\n",
    "                        cleaned_data[field_name] = []\n",
    "                    elif field_info.type_ == float:\n",
    "                        cleaned_data[field_name] = 0.0\n",
    "\n",
    "        return self.validate_against_schema(cleaned_data, schema_class)\n",
    "\n",
    "\n",
    "# Test validation\n",
    "# æ¸¬è©¦é©—è­‰åŠŸèƒ½\n",
    "validator = ExtractionValidator()\n",
    "\n",
    "print(\"\\nâœ… Testing Schema Validation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Validate sample extraction result\n",
    "validation_result = validator.validate_against_schema(extracted_info, DocumentSummary)\n",
    "print(\n",
    "    f\"Validation Result: {'âœ… VALID' if validation_result['is_valid'] else 'âŒ INVALID'}\"\n",
    ")\n",
    "\n",
    "if not validation_result[\"is_valid\"]:\n",
    "    print(\"Validation Errors:\")\n",
    "    for error in validation_result[\"errors\"]:\n",
    "        print(f\"  - {error['field']}: {error['message']}\")\n",
    "\n",
    "    # Try cleaning and re-validation\n",
    "    print(\"\\nðŸ”§ Attempting to clean and re-validate...\")\n",
    "    cleaned_validation = validator.clean_and_retry_validation(\n",
    "        extracted_info, DocumentSummary\n",
    "    )\n",
    "    print(\n",
    "        f\"Cleaned Validation: {'âœ… VALID' if cleaned_validation['is_valid'] else 'âŒ STILL INVALID'}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Performance Monitoring and Optimization\n",
    "# æ€§èƒ½ç›£æŽ§èˆ‡å„ªåŒ–\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Monitor extraction performance and resource usage\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = []\n",
    "\n",
    "    def measure_extraction_performance(\n",
    "        self, extractor: LLMExtractor, text: str, schema_type: str = \"general\"\n",
    "    ) -> Dict:\n",
    "        \"\"\"Measure extraction performance metrics\"\"\"\n",
    "\n",
    "        # Clear GPU cache before measurement\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            initial_vram = torch.cuda.memory_allocated()\n",
    "\n",
    "        initial_ram = psutil.virtual_memory().used\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Perform extraction\n",
    "        try:\n",
    "            result = extractor.extract_structured_info(text, schema_type)\n",
    "            success = True\n",
    "            error = None\n",
    "        except Exception as e:\n",
    "            result = None\n",
    "            success = False\n",
    "            error = str(e)\n",
    "\n",
    "        end_time = time.time()\n",
    "        final_ram = psutil.virtual_memory().used\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            final_vram = torch.cuda.memory_allocated()\n",
    "            vram_used = (final_vram - initial_vram) / 1e6  # MB\n",
    "        else:\n",
    "            vram_used = 0\n",
    "\n",
    "        ram_used = (final_ram - initial_ram) / 1e6  # MB\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        metrics = {\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"ram_used_mb\": ram_used,\n",
    "            \"vram_used_mb\": vram_used,\n",
    "            \"text_length\": len(text),\n",
    "            \"tokens_per_second\": (\n",
    "                len(text.split()) / processing_time if processing_time > 0 else 0\n",
    "            ),\n",
    "            \"success\": success,\n",
    "            \"error\": error,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "        self.metrics.append(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def get_performance_summary(self) -> Dict:\n",
    "        \"\"\"Generate performance summary statistics\"\"\"\n",
    "        if not self.metrics:\n",
    "            return {\"message\": \"No performance data available\"}\n",
    "\n",
    "        successful_runs = [m for m in self.metrics if m[\"success\"]]\n",
    "\n",
    "        if not successful_runs:\n",
    "            return {\"message\": \"No successful runs to analyze\"}\n",
    "\n",
    "        processing_times = [m[\"processing_time_seconds\"] for m in successful_runs]\n",
    "        ram_usage = [m[\"ram_used_mb\"] for m in successful_runs]\n",
    "        vram_usage = [m[\"vram_used_mb\"] for m in successful_runs]\n",
    "\n",
    "        return {\n",
    "            \"total_runs\": len(self.metrics),\n",
    "            \"successful_runs\": len(successful_runs),\n",
    "            \"success_rate\": len(successful_runs) / len(self.metrics) * 100,\n",
    "            \"average_processing_time\": sum(processing_times) / len(processing_times),\n",
    "            \"min_processing_time\": min(processing_times),\n",
    "            \"max_processing_time\": max(processing_times),\n",
    "            \"average_ram_usage_mb\": sum(ram_usage) / len(ram_usage),\n",
    "            \"average_vram_usage_mb\": sum(vram_usage) / len(vram_usage),\n",
    "            \"average_tokens_per_second\": sum(\n",
    "                m[\"tokens_per_second\"] for m in successful_runs\n",
    "            )\n",
    "            / len(successful_runs),\n",
    "        }\n",
    "\n",
    "\n",
    "# Test performance monitoring\n",
    "# æ¸¬è©¦æ€§èƒ½ç›£æŽ§\n",
    "monitor = PerformanceMonitor()\n",
    "\n",
    "print(\"\\nâš¡ Performance Testing:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with sample documents\n",
    "test_texts = [\n",
    "    sample_text,\n",
    "    contract_text,\n",
    "    resume_text[:1000],\n",
    "]  # Truncate for faster testing\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    print(f\"\\nTesting document {i+1}...\")\n",
    "    perf_metrics = monitor.measure_extraction_performance(extractor, text)\n",
    "    print(f\"  Processing time: {perf_metrics['processing_time_seconds']:.2f}s\")\n",
    "    print(f\"  RAM used: {perf_metrics['ram_used_mb']:.1f}MB\")\n",
    "    print(f\"  VRAM used: {perf_metrics['vram_used_mb']:.1f}MB\")\n",
    "    print(f\"  Success: {'âœ…' if perf_metrics['success'] else 'âŒ'}\")\n",
    "\n",
    "# Performance summary\n",
    "perf_summary = monitor.get_performance_summary()\n",
    "print(f\"\\nðŸ“Š Performance Summary:\")\n",
    "for key, value in perf_summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Smoke Test & Validation\n",
    "# é©—æ”¶æ¸¬è©¦èˆ‡é©—è­‰\n",
    "\n",
    "\n",
    "def run_smoke_test():\n",
    "    \"\"\"Comprehensive smoke test for document extraction pipeline\"\"\"\n",
    "    print(\"\\nðŸ§ª Running Comprehensive Smoke Test...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    test_results = {\n",
    "        \"parser_test\": False,\n",
    "        \"extractor_test\": False,\n",
    "        \"pipeline_test\": False,\n",
    "        \"validation_test\": False,\n",
    "        \"performance_test\": False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Test 1: Document Parser\n",
    "        print(\"1ï¸âƒ£ Testing Document Parser...\")\n",
    "        test_parser = DocumentParser()\n",
    "        test_doc = test_parser.parse(sample_path)\n",
    "        assert test_doc.text is not None\n",
    "        assert len(test_doc.text) > 0\n",
    "        test_results[\"parser_test\"] = True\n",
    "        print(\"   âœ… Parser works correctly\")\n",
    "\n",
    "        # Test 2: Information Extractor\n",
    "        print(\"2ï¸âƒ£ Testing Information Extractor...\")\n",
    "        test_extractor = LLMExtractor()\n",
    "        test_extraction = test_extractor.extract_structured_info(sample_text[:500])\n",
    "        assert isinstance(test_extraction, dict)\n",
    "        assert \"document_type\" in test_extraction\n",
    "        test_results[\"extractor_test\"] = True\n",
    "        print(\"   âœ… Extractor works correctly\")\n",
    "\n",
    "        # Test 3: Processing Pipeline\n",
    "        print(\"3ï¸âƒ£ Testing Processing Pipeline...\")\n",
    "        test_pipeline = DocumentProcessingPipeline(\n",
    "            test_parser, test_extractor, max_workers=1\n",
    "        )\n",
    "        pipeline_result = test_pipeline.process_single_document(sample_path)\n",
    "        assert pipeline_result[\"status\"] == \"success\"\n",
    "        assert \"extracted_info\" in pipeline_result\n",
    "        test_results[\"pipeline_test\"] = True\n",
    "        print(\"   âœ… Pipeline works correctly\")\n",
    "\n",
    "        # Test 4: Schema Validation\n",
    "        print(\"4ï¸âƒ£ Testing Schema Validation...\")\n",
    "        test_validator = ExtractionValidator()\n",
    "        validation_result = test_validator.validate_against_schema(\n",
    "            test_extraction, DocumentSummary\n",
    "        )\n",
    "        # Should either be valid or cleanable\n",
    "        if not validation_result[\"is_valid\"]:\n",
    "            cleaned_result = test_validator.clean_and_retry_validation(\n",
    "                test_extraction, DocumentSummary\n",
    "            )\n",
    "            assert (\n",
    "                cleaned_result[\"is_valid\"] or len(cleaned_result[\"errors\"]) < 5\n",
    "            )  # Allow some errors\n",
    "        test_results[\"validation_test\"] = True\n",
    "        print(\"   âœ… Validation works correctly\")\n",
    "\n",
    "        # Test 5: Performance Monitoring\n",
    "        print(\"5ï¸âƒ£ Testing Performance Monitoring...\")\n",
    "        test_monitor = PerformanceMonitor()\n",
    "        perf_result = test_monitor.measure_extraction_performance(\n",
    "            test_extractor, sample_text[:300]\n",
    "        )\n",
    "        assert \"processing_time_seconds\" in perf_result\n",
    "        assert perf_result[\"processing_time_seconds\"] > 0\n",
    "        test_results[\"performance_test\"] = True\n",
    "        print(\"   âœ… Performance monitoring works correctly\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Final results\n",
    "    all_passed = all(test_results.values())\n",
    "    print(f\"\\nðŸŽ¯ Smoke Test Results:\")\n",
    "    for test_name, passed in test_results.items():\n",
    "        status = \"âœ… PASS\" if passed else \"âŒ FAIL\"\n",
    "        print(f\"   {test_name}: {status}\")\n",
    "\n",
    "    print(f\"\\n{'ðŸŽ‰ ALL TESTS PASSED!' if all_passed else 'âš ï¸ SOME TESTS FAILED'}\")\n",
    "    return all_passed\n",
    "\n",
    "\n",
    "# Run comprehensive smoke test\n",
    "smoke_test_result = run_smoke_test()\n",
    "\n",
    "# Cleanup test files\n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.remove(sample_path)\n",
    "    os.remove(contract_path)\n",
    "    os.remove(resume_path)\n",
    "    os.remove(\"extraction_results.json\")\n",
    "    print(\"\\nðŸ§¹ Cleanup completed - test files removed\")\n",
    "except:\n",
    "    print(\"\\nâš ï¸ Some test files could not be removed\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ðŸ“‹ NOTEBOOK COMPLETION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "âœ… å®Œæˆé …ç›® (Completed Items):\n",
    "  â€¢ å¤šæ ¼å¼æ–‡ä»¶è§£æžå™¨ (PDF/DOCX/HTML/TXT/MD)\n",
    "  â€¢ Pydantic Schema å®šç¾©èˆ‡é©—è­‰\n",
    "  â€¢ LLM çµæ§‹åŒ–è³‡è¨ŠæŠ½å–å¼•æ“Ž\n",
    "  â€¢ æ‰¹é‡è™•ç†ç®¡ç·šèˆ‡éŒ¯èª¤è™•ç†\n",
    "  â€¢ å°ˆé–€æŠ½å–å™¨ (ç°¡æ­·/åˆç´„/å­¸è¡“è«–æ–‡)\n",
    "  â€¢ å“è³ªè©•ä¼°èˆ‡æº–ç¢ºçŽ‡è¨ˆç®—\n",
    "  â€¢ æ€§èƒ½ç›£æŽ§èˆ‡è³‡æºä½¿ç”¨è¿½è¹¤\n",
    "  â€¢ å®Œæ•´çš„é©—æ”¶æ¸¬è©¦æµç¨‹\n",
    "\n",
    "ðŸ§  æ ¸å¿ƒæ¦‚å¿µ (Core Concepts):\n",
    "  â€¢ Schema-driven Extraction: ä½¿ç”¨ Pydantic å®šç¾©çµæ§‹åŒ–è¼¸å‡ºæ ¼å¼\n",
    "  â€¢ Multi-format Parsing: çµ±ä¸€ä»‹é¢è™•ç†ä¸åŒæ–‡ä»¶æ ¼å¼\n",
    "  â€¢ LLM Prompting for IE: çµæ§‹åŒ–æç¤ºå·¥ç¨‹é€²è¡Œè³‡è¨ŠæŠ½å–\n",
    "  â€¢ Quality Assessment: æº–ç¢ºçŽ‡ã€å¬å›žçŽ‡ã€F1-score è©•ä¼°\n",
    "  â€¢ Performance Optimization: ä½Ž VRAM é…ç½®èˆ‡æ‰¹é‡è™•ç†\n",
    "  â€¢ Error Handling: å®¹éŒ¯æ©Ÿåˆ¶èˆ‡é™ç´šç­–ç•¥\n",
    "\n",
    "âš ï¸ å¸¸è¦‹å•é¡Œ (Common Issues):\n",
    "  â€¢ æ¨¡åž‹è¼‰å…¥å¤±æ•— â†’ ä½¿ç”¨ 4-bit é‡åŒ–æˆ–é™ç´šè‡³ CPU\n",
    "  â€¢ JSON è§£æžéŒ¯èª¤ â†’ å¯¦ä½œæ­£è¦è¡¨é”å¼å¾Œå‚™æ–¹æ¡ˆ\n",
    "  â€¢ è¨˜æ†¶é«”ä¸è¶³ â†’ æ¸›å°‘ batch size æˆ–ä½¿ç”¨åºåˆ—è™•ç†\n",
    "  â€¢ æŠ½å–æº–ç¢ºçŽ‡ä½Ž â†’ èª¿æ•´æç¤ºè©žæˆ–ä½¿ç”¨æ›´å¤§æ¨¡åž‹\n",
    "  â€¢ Schema é©—è­‰å¤±æ•— â†’ å¯¦ä½œè³‡æ–™æ¸…ç†èˆ‡é è¨­å€¼\n",
    "\n",
    "ðŸš€ ä¸‹ä¸€æ­¥å»ºè­° (Next Steps):\n",
    "  1. æ•´åˆå‘é‡è³‡æ–™åº«é€²è¡Œèªžæ„æœå°‹\n",
    "  2. åŠ å…¥å¤šèªžè¨€æ”¯æ´ (ä¸­æ–‡/è‹±æ–‡æ··åˆ)\n",
    "  3. å¯¦ä½œå¢žé‡å­¸ç¿’æ©Ÿåˆ¶æ”¹å–„æŠ½å–å“è³ª\n",
    "  4. é–‹ç™¼ Web UI é€²è¡Œäº’å‹•å¼æ–‡ä»¶åˆ†æž\n",
    "  5. æ•´åˆ OCR åŠŸèƒ½è™•ç†æŽƒææ–‡ä»¶\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63acd009",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Advanced Features & Extensions (Optional)\n",
    "# é€²éšŽåŠŸèƒ½èˆ‡æ“´å±• (å¯é¸)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ðŸ”¬ ADVANCED FEATURES PREVIEW\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "ðŸ’¡ å¯é¸æ“´å±•åŠŸèƒ½ (Optional Extensions):\n",
    "\n",
    "1. ðŸ“¸ OCR Integration (å…‰å­¸å­—ç¬¦è­˜åˆ¥)\n",
    "   - ä½¿ç”¨ pytesseract æˆ– EasyOCR è™•ç†æŽƒææ–‡ä»¶\n",
    "   - æ”¯æ´ä¸­è‹±æ–‡æ··åˆè­˜åˆ¥\n",
    "   - è¡¨æ ¼çµæ§‹ä¿æŒ\n",
    "\n",
    "2. ðŸŒ Multilingual Support (å¤šèªžè¨€æ”¯æ´)\n",
    "   - ä¸­æ–‡å¯¦é«”è­˜åˆ¥èˆ‡é—œä¿‚æŠ½å–\n",
    "   - ç¹ç°¡è½‰æ› (opencc)\n",
    "   - è·¨èªžè¨€ Schema æ˜ å°„\n",
    "\n",
    "3. ðŸ”„ Active Learning (ä¸»å‹•å­¸ç¿’)\n",
    "   - æ¨™è¨»ä»‹é¢æ•´åˆ\n",
    "   - ä¸ç¢ºå®šæ€§æŽ¡æ¨£\n",
    "   - æ¼¸é€²å¼æ¨¡åž‹æ”¹å–„\n",
    "\n",
    "4. ðŸ“Š Dashboard & Analytics (å„€è¡¨æ¿åˆ†æž)\n",
    "   - Streamlit/Gradio ç¶²é ä»‹é¢\n",
    "   - å³æ™‚æŠ½å–æº–ç¢ºçŽ‡ç›£æŽ§\n",
    "   - æ–‡ä»¶é¡žåž‹åˆ†å¸ƒåˆ†æž\n",
    "\n",
    "5. ðŸ”— API Integration (API æ•´åˆ)\n",
    "   - RESTful API endpoints\n",
    "   - Webhook æ”¯æ´æ‰¹é‡è™•ç†\n",
    "   - é›²ç«¯å„²å­˜æ•´åˆ (S3/GCS)\n",
    "\n",
    "6. ðŸŽ¯ Domain Adaptation (é ˜åŸŸé©æ‡‰)\n",
    "   - é†«ç™‚æ–‡ä»¶å°ˆç”¨æŠ½å–å™¨\n",
    "   - æ³•å¾‹åˆç´„é¢¨éšªè©•ä¼°\n",
    "   - è²¡å‹™å ±è¡¨æ•¸æ“šé©—è­‰\n",
    "\n",
    "ä½¿ç”¨æŒ‡ä»¤æŸ¥çœ‹å…·é«”å¯¦ä½œ:\n",
    "  â€¢ nb17_multilingual_ocr.ipynb\n",
    "  â€¢ nb18_active_learning_pipeline.ipynb\n",
    "  â€¢ nb19_document_analytics_dashboard.ipynb\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ“ å­¸ç¿’å»ºè­° (Learning Recommendations):\")\n",
    "print(\n",
    "    \"\"\"\n",
    "1. å…ˆç†Ÿç·´åŸºæœ¬æŠ½å–ç®¡ç·šï¼Œå†å˜—è©¦é€²éšŽåŠŸèƒ½\n",
    "2. é‡å°ç‰¹å®šé ˜åŸŸæ”¶é›†é«˜å“è³ªæ¨™è¨»è³‡æ–™\n",
    "3. æ¯”è¼ƒä¸åŒ LLM åœ¨çµæ§‹åŒ–æŠ½å–çš„è¡¨ç¾\n",
    "4. å»ºç«‹è©•ä¼°åŸºæº–ä»¥é‡åŒ–æ”¹å–„æ•ˆæžœ\n",
    "5. è€ƒæ…®æˆæœ¬æ•ˆç›Šå¹³è¡¡ (æº–ç¢ºçŽ‡ vs è™•ç†é€Ÿåº¦)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c880b18a",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ“‹ æœ¬ç« å°çµ\n",
    "\n",
    "**âœ… å®Œæˆé …ç›® (Completed Items):**\n",
    "- å¤šæ ¼å¼æ–‡ä»¶è§£æžå™¨ (PDF/DOCX/HTML/TXT/MD çµ±ä¸€ä»‹é¢)\n",
    "- Pydantic Schema å®šç¾©èˆ‡é©—è­‰ (DocumentSummary, ContractInfo)\n",
    "- LLM çµæ§‹åŒ–è³‡è¨ŠæŠ½å–å¼•æ“Ž (æ”¯æ´ä½Ž VRAM é…ç½®)\n",
    "- æ‰¹é‡è™•ç†ç®¡ç·šèˆ‡éŒ¯èª¤è™•ç† (ä¸¦è¡Œè™•ç†èˆ‡å®¹éŒ¯æ©Ÿåˆ¶)\n",
    "- å°ˆé–€æŠ½å–å™¨ (ç°¡æ­·/åˆç´„/å­¸è¡“è«–æ–‡ç‰¹åŒ–ç‰ˆæœ¬)\n",
    "- å“è³ªè©•ä¼°æ¨¡çµ„ (æº–ç¢ºçŽ‡ã€å¬å›žçŽ‡ã€F1-score)\n",
    "- æ€§èƒ½ç›£æŽ§èˆ‡è³‡æºä½¿ç”¨è¿½è¹¤ (RAM/VRAM ç›£æŽ§)\n",
    "- å®Œæ•´çš„é©—æ”¶æ¸¬è©¦æµç¨‹ (ç«¯åˆ°ç«¯åŠŸèƒ½é©—è­‰)\n",
    "\n",
    "**ðŸ§  æ ¸å¿ƒåŽŸç† (Core Concepts):**\n",
    "- **Schema-driven Extraction**: ä½¿ç”¨ Pydantic å®šç¾©çµæ§‹åŒ–è¼¸å‡ºæ ¼å¼ï¼Œç¢ºä¿è³‡æ–™ä¸€è‡´æ€§\n",
    "- **Multi-format Parsing**: çµ±ä¸€ä»‹é¢è™•ç†ä¸åŒæ–‡ä»¶æ ¼å¼ï¼Œé™ä½Žæ•´åˆè¤‡é›œåº¦  \n",
    "- **LLM Prompting for IE**: çµæ§‹åŒ–æç¤ºå·¥ç¨‹é€²è¡Œè³‡è¨ŠæŠ½å–ï¼Œå¹³è¡¡æº–ç¢ºçŽ‡èˆ‡å¯æŽ§æ€§\n",
    "- **Quality Assessment**: å¤šç¶­åº¦è©•ä¼°æŒ‡æ¨™ (æ¬„ä½æº–ç¢ºçŽ‡ã€å¯¦é«” F1ã€ä¿¡å¿ƒåº¦ç›¸é—œæ€§)\n",
    "- **Performance Optimization**: ä½Ž VRAM é…ç½®ç­–ç•¥èˆ‡æ‰¹é‡è™•ç†æœ€ä½³åŒ–\n",
    "\n",
    "**âš ï¸ å¸¸è¦‹é™·é˜± (Common Pitfalls):**\n",
    "- æ¨¡åž‹è¼‰å…¥å¤±æ•— â†’ ä½¿ç”¨ 4-bit é‡åŒ–æˆ–é™ç´šè‡³ CPU æ¨¡å¼\n",
    "- JSON è§£æžéŒ¯èª¤ â†’ å¯¦ä½œæ­£è¦è¡¨é”å¼å¾Œå‚™æŠ½å–æ–¹æ¡ˆ  \n",
    "- è¨˜æ†¶é«”ä¸è¶³ â†’ æ¸›å°‘ batch size æˆ–æ”¹ç”¨åºåˆ—è™•ç†\n",
    "- æŠ½å–æº–ç¢ºçŽ‡ä½Ž â†’ èª¿æ•´æç¤ºè©žæ¨¡æ¿æˆ–ä½¿ç”¨æ›´å¤§æ¨¡åž‹\n",
    "- Schema é©—è­‰å¤±æ•— â†’ å¯¦ä½œè³‡æ–™æ¸…ç†èˆ‡é è¨­å€¼æ©Ÿåˆ¶\n",
    "\n",
    "**ðŸš€ ä¸‹ä¸€æ­¥è¡Œå‹• (Next Actions):**\n",
    "1. **æ•´åˆå‘é‡æª¢ç´¢**: çµåˆ `nb26_rag_basic_faiss.ipynb` é€²è¡Œèªžæ„æœå°‹\n",
    "2. **å¤šèªžè¨€æ”¯æ´**: åŠ å…¥ä¸­æ–‡å¯¦é«”è­˜åˆ¥èˆ‡ç¹ç°¡è½‰æ›\n",
    "3. **å¢žé‡å­¸ç¿’**: å¯¦ä½œä¸»å‹•å­¸ç¿’æ©Ÿåˆ¶æŒçºŒæ”¹å–„æŠ½å–å“è³ª  \n",
    "4. **Web UI é–‹ç™¼**: å»ºç«‹äº’å‹•å¼æ–‡ä»¶åˆ†æžä»‹é¢\n",
    "5. **é ˜åŸŸç‰¹åŒ–**: é‡å°é†«ç™‚/æ³•å¾‹/è²¡å‹™ç­‰ç‰¹å®šé ˜åŸŸå®¢è£½åŒ–\n",
    "\n",
    "**ðŸ’¡ å¯¦å‹™å»ºè­°:**\n",
    "- å„ªå…ˆå»ºç«‹é«˜å“è³ªçš„æ¨™è¨»è³‡æ–™é›†ä½œç‚ºè©•ä¼°åŸºæº–\n",
    "- æ¯”è¼ƒä¸åŒ LLM (GPT/Qwen/DeepSeek) åœ¨çµæ§‹åŒ–æŠ½å–çš„è¡¨ç¾å·®ç•°\n",
    "- è€ƒæ…®æˆæœ¬æ•ˆç›Šå¹³è¡¡ï¼šæº–ç¢ºçŽ‡æå‡ vs è™•ç†é€Ÿåº¦èˆ‡è³‡æºæ¶ˆè€—\n",
    "- å»ºç«‹ç›£æŽ§å„€è¡¨æ¿è¿½è¹¤ç”Ÿç”¢ç’°å¢ƒçš„æŠ½å–å“è³ªè®ŠåŒ–\n",
    "\n",
    "é€™å€‹ notebook ç‚ºå¾ŒçºŒçš„ RAG æ‡‰ç”¨å’Œ Agent ç³»çµ±æä¾›äº†å¼·å¤§çš„æ–‡ä»¶ç†è§£åŸºç¤Žï¼Œæ˜¯æ§‹å»ºæ™ºèƒ½æ–‡ä»¶è™•ç†ç³»çµ±çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
