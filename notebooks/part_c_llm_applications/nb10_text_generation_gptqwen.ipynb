{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f83d96",
   "metadata": {},
   "source": [
    "\n",
    " # 📝 Notebook 10: GPT/Qwen/DeepSeek 文本生成比較實戰\n",
    " \n",
    " **學習目標 (Learning Objectives)**\n",
    " - 掌握主流開源 LLM 的載入與推理技巧\n",
    " - 理解記憶體優化策略 (4bit/8bit quantization, device mapping)\n",
    " - 探索生成參數對輸出品質的影響\n",
    " - 建立標準化的性能評估流程\n",
    "\n",
    " **涵蓋模型系列 (Model Families)**\n",
    " - GPT-2: 英文基礎生成模型\n",
    " - Qwen2.5-Instruct: 中英雙語指令模型  \n",
    " - DeepSeek-R1-Distill: 推理增強模型\n",
    "\n",
    " **適用場景 (Use Cases)**\n",
    " - 創意寫作、問答對話、程式生成、文件翻譯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d41ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Shared Cache Bootstrap (English comments only) ===\n",
    "import os, torch, platform, pathlib, time, psutil\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "import json\n",
    "\n",
    "# Set up shared cache paths\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "\n",
    "for k, v in paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\"\n",
    "    )\n",
    "else:\n",
    "    print(\"[GPU] Using CPU mode\")\n",
    "\n",
    "# System info\n",
    "print(f\"[System] RAM: {psutil.virtual_memory().total // 1024**3} GB\")\n",
    "print(f\"[Python] Version: {platform.python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fc2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Essential imports for LLM text generation ===\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import tracemalloc\n",
    "\n",
    "# Set reproducible seed\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"[Setup] Essential imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eafade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === UnifiedModelLoader: Memory-efficient model loading ===\n",
    "class UnifiedModelLoader:\n",
    "    \"\"\"\n",
    "    Unified loader for GPT-2, Qwen, DeepSeek models with automatic VRAM optimization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, low_vram_mode: bool = None):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Auto-detect low VRAM mode based on available memory\n",
    "        if low_vram_mode is None:\n",
    "            if torch.cuda.is_available():\n",
    "                total_vram = torch.cuda.get_device_properties(0).total_memory\n",
    "                self.low_vram_mode = total_vram < 12 * 1024**3  # Less than 12GB\n",
    "            else:\n",
    "                self.low_vram_mode = True\n",
    "        else:\n",
    "            self.low_vram_mode = low_vram_mode\n",
    "\n",
    "        print(f\"[Loader] Device: {self.device}, Low-VRAM mode: {self.low_vram_mode}\")\n",
    "\n",
    "    def get_quantization_config(self) -> Optional[BitsAndBytesConfig]:\n",
    "        \"\"\"Get 4bit quantization config for memory efficiency\"\"\"\n",
    "        if not self.low_vram_mode or self.device == \"cpu\":\n",
    "            return None\n",
    "\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "    def load_model_and_tokenizer(self, model_id: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Load model and tokenizer with memory optimization\n",
    "\n",
    "        Args:\n",
    "            model_id: HuggingFace model identifier\n",
    "\n",
    "        Returns:\n",
    "            tuple: (model, tokenizer)\n",
    "        \"\"\"\n",
    "        print(f\"[Loading] {model_id}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=os.environ[\"TRANSFORMERS_CACHE\"],\n",
    "            )\n",
    "\n",
    "            # Ensure pad token exists\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            # Prepare model loading kwargs\n",
    "            model_kwargs = {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"cache_dir\": os.environ[\"TRANSFORMERS_CACHE\"],\n",
    "                \"torch_dtype\": (\n",
    "                    torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            # Add quantization config if low VRAM mode\n",
    "            quantization_config = self.get_quantization_config()\n",
    "            if quantization_config:\n",
    "                model_kwargs[\"quantization_config\"] = quantization_config\n",
    "                print(f\"[Loading] Using 4bit quantization\")\n",
    "            else:\n",
    "                model_kwargs[\"device_map\"] = \"auto\" if self.device == \"cuda\" else None\n",
    "\n",
    "            # Load model\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "\n",
    "            # Move to device if not using device_map\n",
    "            if not quantization_config and self.device == \"cuda\":\n",
    "                model = model.to(self.device)\n",
    "\n",
    "            load_time = time.time() - start_time\n",
    "\n",
    "            # Calculate model size\n",
    "            param_count = sum(p.numel() for p in model.parameters())\n",
    "            param_size_mb = param_count * 4 / 1024**2  # Assuming float32\n",
    "\n",
    "            print(f\"[Loaded] {model_id}\")\n",
    "            print(f\"[Stats] Parameters: {param_count:,} ({param_size_mb:.1f} MB)\")\n",
    "            print(f\"[Stats] Load time: {load_time:.2f}s\")\n",
    "\n",
    "            return model, tokenizer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to load {model_id}: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.8,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 0.9,\n",
    "        repetition_penalty: float = 1.1,\n",
    "        do_sample: bool = True,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate text with detailed statistics\n",
    "\n",
    "        Returns:\n",
    "            dict: Generated text, stats, and metadata\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Encode input\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        if self.device == \"cuda\" and inputs.device != model.device:\n",
    "            inputs = inputs.to(model.device)\n",
    "\n",
    "        input_length = inputs.shape[1]\n",
    "\n",
    "        # Generation parameters\n",
    "        gen_kwargs = {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_k\": top_k,\n",
    "            \"top_p\": top_p,\n",
    "            \"repetition_penalty\": repetition_penalty,\n",
    "            \"do_sample\": do_sample,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        }\n",
    "\n",
    "        # Track memory before generation\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            mem_before = torch.cuda.memory_allocated()\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(inputs, **gen_kwargs)\n",
    "\n",
    "        # Decode output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        new_text = generated_text[len(prompt) :]\n",
    "\n",
    "        # Calculate stats\n",
    "        generation_time = time.time() - start_time\n",
    "        output_length = outputs.shape[1] - input_length\n",
    "        tokens_per_sec = output_length / generation_time if generation_time > 0 else 0\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            mem_after = torch.cuda.memory_allocated()\n",
    "            peak_memory_mb = (mem_after - mem_before) / 1024**2\n",
    "        else:\n",
    "            peak_memory_mb = 0\n",
    "\n",
    "        return {\n",
    "            \"generated_text\": new_text,\n",
    "            \"full_text\": generated_text,\n",
    "            \"stats\": {\n",
    "                \"input_tokens\": input_length,\n",
    "                \"output_tokens\": output_length,\n",
    "                \"total_tokens\": outputs.shape[1],\n",
    "                \"generation_time\": generation_time,\n",
    "                \"tokens_per_sec\": tokens_per_sec,\n",
    "                \"peak_memory_mb\": peak_memory_mb,\n",
    "            },\n",
    "            \"parameters\": gen_kwargs,\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize loader\n",
    "loader = UnifiedModelLoader()\n",
    "print(\"[Setup] UnifiedModelLoader ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627695cf",
   "metadata": {},
   "source": [
    "\n",
    " ## 🤖 模型定義與載入策略\n",
    " \n",
    " **記憶體需求評估 (VRAM Requirements)**\n",
    " - GPT-2 Small (124M): ~0.5GB\n",
    " - GPT-2 Medium (355M): ~1.4GB  \n",
    " - GPT-2 Large (774M): ~3.1GB\n",
    " - Qwen2.5-7B-Instruct (4bit): ~4.5GB\n",
    " - DeepSeek-R1-Distill-Qwen-7B (4bit): ~4.5GB\n",
    "\n",
    " **量化策略 (Quantization Strategy)**\n",
    " - <8GB VRAM: 強制 4bit 量化\n",
    " - 8-16GB VRAM: 可選 4bit/8bit\n",
    " - >16GB VRAM: 原生 float16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Model configurations and loading ===\n",
    "MODEL_CONFIGS = {\n",
    "    \"gpt2-small\": {\n",
    "        \"model_id\": \"gpt2\",\n",
    "        \"description\": \"GPT-2 Small (124M) - 英文基礎生成\",\n",
    "        \"vram_requirement\": 0.5,\n",
    "        \"strengths\": [\"輕量快速\", \"英文流暢\", \"創意寫作\"],\n",
    "    },\n",
    "    \"gpt2-medium\": {\n",
    "        \"model_id\": \"gpt2-medium\",\n",
    "        \"description\": \"GPT-2 Medium (355M) - 平衡性能與品質\",\n",
    "        \"vram_requirement\": 1.4,\n",
    "        \"strengths\": [\"中等規模\", \"較佳連貫性\", \"詩詞創作\"],\n",
    "    },\n",
    "    \"qwen2.5-7b-instruct\": {\n",
    "        \"model_id\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"description\": \"Qwen2.5-7B-Instruct - 中英雙語指令模型\",\n",
    "        \"vram_requirement\": 4.5,  # with 4bit quantization\n",
    "        \"strengths\": [\"中文優秀\", \"指令遵循\", \"多語支持\"],\n",
    "    },\n",
    "    \"deepseek-r1-distill\": {\n",
    "        \"model_id\": \"deepseek-ai/deepseek-r1-distill-qwen-7b\",\n",
    "        \"description\": \"DeepSeek-R1-Distill-Qwen-7B - 推理增強模型\",\n",
    "        \"vram_requirement\": 4.5,  # with 4bit quantization\n",
    "        \"strengths\": [\"邏輯推理\", \"數學問題\", \"思維鏈\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def display_model_info():\n",
    "    \"\"\"Display available models and their characteristics\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🤖 可用模型總覽 (Available Models)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for key, config in MODEL_CONFIGS.items():\n",
    "        print(f\"\\n📋 {key.upper()}\")\n",
    "        print(f\"   ID: {config['model_id']}\")\n",
    "        print(f\"   描述: {config['description']}\")\n",
    "        print(f\"   VRAM: ~{config['vram_requirement']}GB\")\n",
    "        print(f\"   優勢: {', '.join(config['strengths'])}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "display_model_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Load and test GPT-2 models ===\n",
    "print(\"🔄 正在載入 GPT-2 系列模型 (Loading GPT-2 models)\")\n",
    "\n",
    "# Load GPT-2 Small for quick testing\n",
    "gpt2_model, gpt2_tokenizer = loader.load_model_and_tokenizer(\"gpt2\")\n",
    "\n",
    "if gpt2_model is not None:\n",
    "    # Test English creative writing\n",
    "    english_prompts = [\n",
    "        \"Once upon a time in a magical forest,\",\n",
    "        \"The future of artificial intelligence will be\",\n",
    "        \"In the year 2030, technology will have\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\n📝 GPT-2 英文創意生成測試\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for i, prompt in enumerate(english_prompts, 1):\n",
    "        print(f\"\\n💭 Prompt {i}: {prompt}\")\n",
    "\n",
    "        result = loader.generate_text(\n",
    "            gpt2_model,\n",
    "            gpt2_tokenizer,\n",
    "            prompt,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "        print(f\"📄 Generated: {result['generated_text'][:200]}...\")\n",
    "        print(f\"⚡ Speed: {result['stats']['tokens_per_sec']:.1f} tokens/sec\")\n",
    "        print(f\"💾 Memory: {result['stats']['peak_memory_mb']:.1f} MB\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ GPT-2 載入失敗\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255cc351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Load and test Qwen2.5-7B-Instruct ===\n",
    "print(\"\\n🔄 正在載入 Qwen2.5-7B-Instruct 模型\")\n",
    "\n",
    "# Free up memory from previous model if needed\n",
    "if \"gpt2_model\" in locals() and gpt2_model is not None:\n",
    "    del gpt2_model, gpt2_tokenizer\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "qwen_model, qwen_tokenizer = loader.load_model_and_tokenizer(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "if qwen_model is not None:\n",
    "    # Test Chinese and English instruction following\n",
    "    instruction_prompts = [\n",
    "        \"請用繁體中文寫一首關於春天的短詩。\",\n",
    "        \"解釋什麼是機器學習，用簡單易懂的方式。\",\n",
    "        \"Write a Python function to calculate fibonacci numbers.\",\n",
    "        \"Translate this to Chinese: 'Artificial intelligence is transforming our world.'\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\n📝 Qwen2.5 中英雙語指令測試\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for i, prompt in enumerate(instruction_prompts, 1):\n",
    "        print(f\"\\n💭 Instruction {i}: {prompt}\")\n",
    "\n",
    "        result = loader.generate_text(\n",
    "            qwen_model,\n",
    "            qwen_tokenizer,\n",
    "            prompt,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            top_p=0.8,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "\n",
    "        print(f\"📄 Response: {result['generated_text']}\")\n",
    "        print(f\"⚡ Speed: {result['stats']['tokens_per_sec']:.1f} tokens/sec\")\n",
    "        print(f\"💾 Memory: {result['stats']['peak_memory_mb']:.1f} MB\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "else:\n",
    "    print(\"❌ Qwen2.5 載入失敗\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ae921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Load and test DeepSeek-R1-Distill ===\n",
    "print(\"\\n🔄 正在載入 DeepSeek-R1-Distill 模型\")\n",
    "\n",
    "# Free up memory from Qwen if needed\n",
    "if \"qwen_model\" in locals() and qwen_model is not None:\n",
    "    del qwen_model, qwen_tokenizer\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "deepseek_model, deepseek_tokenizer = loader.load_model_and_tokenizer(\n",
    "    \"deepseek-ai/deepseek-r1-distill-qwen-7b\"\n",
    ")\n",
    "\n",
    "if deepseek_model is not None:\n",
    "    # Test reasoning and math problems\n",
    "    reasoning_prompts = [\n",
    "        \"Solve this step by step: If a train travels 120 km in 1.5 hours, what is its average speed?\",\n",
    "        \"邏輯推理：如果所有的貓都有尾巴，而小花是一隻貓，那麼小花有尾巴嗎？請說明原因。\",\n",
    "        \"Write Python code to find the prime factors of 84, with explanations.\",\n",
    "        \"如果今天是星期三，那麼100天後是星期幾？請詳細計算。\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\n📝 DeepSeek-R1-Distill 推理增強測試\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for i, prompt in enumerate(reasoning_prompts, 1):\n",
    "        print(f\"\\n💭 Reasoning Task {i}: {prompt}\")\n",
    "\n",
    "        result = loader.generate_text(\n",
    "            deepseek_model,\n",
    "            deepseek_tokenizer,\n",
    "            prompt,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.3,  # Lower temperature for reasoning\n",
    "            top_p=0.8,\n",
    "            repetition_penalty=1.05,\n",
    "        )\n",
    "\n",
    "        print(f\"📄 Solution: {result['generated_text']}\")\n",
    "        print(f\"⚡ Speed: {result['stats']['tokens_per_sec']:.1f} tokens/sec\")\n",
    "        print(f\"💾 Memory: {result['stats']['peak_memory_mb']:.1f} MB\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "else:\n",
    "    print(\"❌ DeepSeek-R1-Distill 載入失敗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0c746",
   "metadata": {},
   "source": [
    "\n",
    " ## 🎛️ 生成參數調優實驗\n",
    " \n",
    " **核心參數說明 (Parameter Explanation)**\n",
    " - **Temperature**: 控制隨機性 (0.1=保守, 1.0=創意, 2.0=極富創意)\n",
    " - **Top-k**: 候選詞數量限制 (10=嚴格, 50=平衡, 100=寬鬆)\n",
    " - **Top-p**: 累積機率閾值 (0.8=保守, 0.9=平衡, 0.95=創意)\n",
    " - **Repetition Penalty**: 重複懲罰 (1.0=無懲罰, 1.1=輕微, 1.3=嚴格)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Parameter tuning experiments ===\n",
    "def parameter_experiment(model, tokenizer, prompt: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Conduct systematic parameter tuning experiments\n",
    "    \"\"\"\n",
    "    print(f\"\\n🧪 {model_name} 參數調優實驗\")\n",
    "    print(f\"📝 測試提示: {prompt}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Parameter grid for testing\n",
    "    param_configs = [\n",
    "        {\n",
    "            \"name\": \"保守設定\",\n",
    "            \"temp\": 0.3,\n",
    "            \"top_k\": 20,\n",
    "            \"top_p\": 0.8,\n",
    "            \"rep_penalty\": 1.1,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"平衡設定\",\n",
    "            \"temp\": 0.7,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.9,\n",
    "            \"rep_penalty\": 1.05,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"創意設定\",\n",
    "            \"temp\": 1.0,\n",
    "            \"top_k\": 100,\n",
    "            \"top_p\": 0.95,\n",
    "            \"rep_penalty\": 1.0,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"極端創意\",\n",
    "            \"temp\": 1.5,\n",
    "            \"top_k\": 0,\n",
    "            \"top_p\": 0.98,\n",
    "            \"rep_penalty\": 0.95,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(\n",
    "            f\"\\n🎯 {config['name']} (T={config['temp']}, k={config['top_k']}, p={config['top_p']}, rp={config['rep_penalty']})\"\n",
    "        )\n",
    "\n",
    "        result = loader.generate_text(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            max_new_tokens=120,\n",
    "            temperature=config[\"temp\"],\n",
    "            top_k=config[\"top_k\"] if config[\"top_k\"] > 0 else None,\n",
    "            top_p=config[\"top_p\"],\n",
    "            repetition_penalty=config[\"rep_penalty\"],\n",
    "        )\n",
    "\n",
    "        print(f\"📄 輸出: {result['generated_text'][:150]}...\")\n",
    "        print(f\"⚡ 速度: {result['stats']['tokens_per_sec']:.1f} tokens/sec\")\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"config\": config[\"name\"],\n",
    "                \"output\": result[\"generated_text\"],\n",
    "                \"speed\": result[\"stats\"][\"tokens_per_sec\"],\n",
    "                \"length\": result[\"stats\"][\"output_tokens\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run parameter experiments if model is available\n",
    "if \"deepseek_model\" in locals() and deepseek_model is not None:\n",
    "    test_prompt = \"寫一個關於人工智慧的短故事：\"\n",
    "    param_results = parameter_experiment(\n",
    "        deepseek_model, deepseek_tokenizer, test_prompt, \"DeepSeek-R1-Distill\"\n",
    "    )\n",
    "\n",
    "    # Create comparison DataFrame\n",
    "    param_df = pd.DataFrame(param_results)\n",
    "    print(\"\\n📊 參數調優結果總覽\")\n",
    "    print(param_df[[\"config\", \"speed\", \"length\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Multi-scenario application demonstrations ===\n",
    "print(\"\\n🎭 多場景應用示範 (Multi-scenario Applications)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define application scenarios\n",
    "scenarios = {\n",
    "    \"程式碼生成\": {\n",
    "        \"prompt\": \"Write a Python function to merge two sorted lists:\",\n",
    "        \"params\": {\"temperature\": 0.2, \"top_p\": 0.8, \"max_new_tokens\": 200},\n",
    "    },\n",
    "    \"文件摘要\": {\n",
    "        \"prompt\": \"請摘要以下內容的重點：人工智慧是一門研究如何讓機器模擬人類智慧的學科，包括機器學習、深度學習、自然語言處理等多個分支領域。\",\n",
    "        \"params\": {\"temperature\": 0.3, \"top_p\": 0.8, \"max_new_tokens\": 100},\n",
    "    },\n",
    "    \"創意寫作\": {\n",
    "        \"prompt\": \"在一個科技高度發達的未來城市裡，\",\n",
    "        \"params\": {\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_tokens\": 150},\n",
    "    },\n",
    "    \"翻譯任務\": {\n",
    "        \"prompt\": \"Translate to Traditional Chinese: 'Machine learning algorithms can process vast amounts of data to identify patterns and make predictions.'\",\n",
    "        \"params\": {\"temperature\": 0.1, \"top_p\": 0.8, \"max_new_tokens\": 100},\n",
    "    },\n",
    "}\n",
    "\n",
    "application_results = {}\n",
    "\n",
    "# Test scenarios with available model\n",
    "current_model = None\n",
    "current_tokenizer = None\n",
    "current_model_name = \"Unknown\"\n",
    "\n",
    "# Determine which model is currently loaded\n",
    "if \"deepseek_model\" in locals() and deepseek_model is not None:\n",
    "    current_model = deepseek_model\n",
    "    current_tokenizer = deepseek_tokenizer\n",
    "    current_model_name = \"DeepSeek-R1-Distill\"\n",
    "elif \"qwen_model\" in locals() and qwen_model is not None:\n",
    "    current_model = qwen_model\n",
    "    current_tokenizer = qwen_tokenizer\n",
    "    current_model_name = \"Qwen2.5-7B-Instruct\"\n",
    "elif \"gpt2_model\" in locals() and gpt2_model is not None:\n",
    "    current_model = gpt2_model\n",
    "    current_tokenizer = gpt2_tokenizer\n",
    "    current_model_name = \"GPT-2\"\n",
    "\n",
    "if current_model is not None:\n",
    "    print(f\"🤖 使用模型: {current_model_name}\")\n",
    "\n",
    "    for scenario_name, scenario_config in scenarios.items():\n",
    "        print(f\"\\n🎯 場景: {scenario_name}\")\n",
    "        print(f\"💭 提示: {scenario_config['prompt']}\")\n",
    "\n",
    "        result = loader.generate_text(\n",
    "            current_model,\n",
    "            current_tokenizer,\n",
    "            scenario_config[\"prompt\"],\n",
    "            **scenario_config[\"params\"],\n",
    "        )\n",
    "\n",
    "        print(f\"📄 結果: {result['generated_text']}\")\n",
    "        print(f\"⚡ 性能: {result['stats']['tokens_per_sec']:.1f} tokens/sec\")\n",
    "\n",
    "        application_results[scenario_name] = result\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"❌ 無可用模型進行應用測試\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Performance benchmarking and comparison ===\n",
    "class ModelBenchmark:\n",
    "    \"\"\"\n",
    "    Comprehensive model performance benchmarking\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "\n",
    "    def benchmark_model(self, model, tokenizer, model_name: str, num_runs: int = 3):\n",
    "        \"\"\"\n",
    "        Benchmark a model across multiple metrics\n",
    "        \"\"\"\n",
    "        print(f\"\\n📊 {model_name} 性能基準測試\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Standard test prompts\n",
    "        test_prompts = [\n",
    "            \"The future of artificial intelligence\",\n",
    "            \"解釋機器學習的基本概念\",\n",
    "            \"Write code to calculate factorial\",\n",
    "        ]\n",
    "\n",
    "        # Collect metrics across multiple runs\n",
    "        all_metrics = []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            run_metrics = []\n",
    "\n",
    "            for prompt in test_prompts:\n",
    "                # Warm up run (not counted)\n",
    "                if run == 0:\n",
    "                    _ = loader.generate_text(\n",
    "                        model, tokenizer, prompt, max_new_tokens=50\n",
    "                    )\n",
    "\n",
    "                # Actual benchmark run\n",
    "                start_time = time.time()\n",
    "                result = loader.generate_text(\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    prompt,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                )\n",
    "\n",
    "                run_metrics.append(\n",
    "                    {\n",
    "                        \"prompt\": prompt[:30] + \"...\",\n",
    "                        \"tokens_per_sec\": result[\"stats\"][\"tokens_per_sec\"],\n",
    "                        \"generation_time\": result[\"stats\"][\"generation_time\"],\n",
    "                        \"output_tokens\": result[\"stats\"][\"output_tokens\"],\n",
    "                        \"peak_memory_mb\": result[\"stats\"][\"peak_memory_mb\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            all_metrics.extend(run_metrics)\n",
    "\n",
    "        # Calculate average metrics\n",
    "        avg_metrics = {\n",
    "            \"model_name\": model_name,\n",
    "            \"avg_tokens_per_sec\": np.mean([m[\"tokens_per_sec\"] for m in all_metrics]),\n",
    "            \"avg_generation_time\": np.mean([m[\"generation_time\"] for m in all_metrics]),\n",
    "            \"avg_output_tokens\": np.mean([m[\"output_tokens\"] for m in all_metrics]),\n",
    "            \"avg_peak_memory_mb\": np.mean([m[\"peak_memory_mb\"] for m in all_metrics]),\n",
    "            \"std_tokens_per_sec\": np.std([m[\"tokens_per_sec\"] for m in all_metrics]),\n",
    "        }\n",
    "\n",
    "        # Print results\n",
    "        print(\n",
    "            f\"🏃 平均速度: {avg_metrics['avg_tokens_per_sec']:.1f} ± {avg_metrics['std_tokens_per_sec']:.1f} tokens/sec\"\n",
    "        )\n",
    "        print(f\"⏱️ 平均生成時間: {avg_metrics['avg_generation_time']:.2f}s\")\n",
    "        print(f\"💾 平均記憶體使用: {avg_metrics['avg_peak_memory_mb']:.1f} MB\")\n",
    "        print(f\"📝 平均輸出長度: {avg_metrics['avg_output_tokens']:.0f} tokens\")\n",
    "\n",
    "        self.results.append(avg_metrics)\n",
    "        return avg_metrics\n",
    "\n",
    "    def compare_models(self):\n",
    "        \"\"\"Generate comparison report\"\"\"\n",
    "        if len(self.results) < 2:\n",
    "            print(\"⚠️ 需要至少兩個模型進行比較\")\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.results)\n",
    "        print(\"\\n📈 模型性能比較表\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\n",
    "            df[[\"model_name\", \"avg_tokens_per_sec\", \"avg_peak_memory_mb\"]].to_string(\n",
    "                index=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Find best performers\n",
    "        fastest = df.loc[df[\"avg_tokens_per_sec\"].idxmax()]\n",
    "        most_efficient = df.loc[df[\"avg_peak_memory_mb\"].idxmin()]\n",
    "\n",
    "        print(\n",
    "            f\"\\n🏆 最快模型: {fastest['model_name']} ({fastest['avg_tokens_per_sec']:.1f} tokens/sec)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"💡 最節省記憶體: {most_efficient['model_name']} ({most_efficient['avg_peak_memory_mb']:.1f} MB)\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Initialize benchmark\n",
    "benchmark = ModelBenchmark()\n",
    "\n",
    "# Benchmark currently loaded model\n",
    "if current_model is not None:\n",
    "    benchmark.benchmark_model(current_model, current_tokenizer, current_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68dd5bb",
   "metadata": {},
   "source": [
    " ## ⚡ 記憶體與性能優化技巧\n",
    " \n",
    " **低 VRAM 優化策略 (Low-VRAM Optimization)**\n",
    " 1. **量化 (Quantization)**: 4bit/8bit 減少 50-75% 記憶體使用\n",
    " 2. **CPU Offload**: 將部分層移至 CPU 記憶體\n",
    " 3. **Gradient Checkpointing**: 犧牲計算換取記憶體\n",
    " 4. **Dynamic Batching**: 根據序列長度動態調整批次大小\n",
    "\n",
    " **生成速度優化 (Speed Optimization)**\n",
    " 1. **KV Cache**: 避免重複計算注意力\n",
    " 2. **Beam Search 替代**: 使用 Top-k/Top-p 採樣\n",
    " 3. **Early Stopping**: 設定合理的最大長度\n",
    " 4. **Model Compilation**: 使用 torch.compile (PyTorch 2.0+)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aa0ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Memory and performance optimization demonstrations ===\n",
    "def demonstrate_optimization_techniques():\n",
    "    \"\"\"\n",
    "    Show various optimization techniques for different scenarios\n",
    "    \"\"\"\n",
    "    print(\"\\n🔧 記憶體與性能優化示範\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    optimization_tips = {\n",
    "        \"4bit 量化\": {\n",
    "            \"description\": \"減少 75% 記憶體使用，輕微精度損失\",\n",
    "            \"code\": \"\"\"\n",
    "# 4bit quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=quantization_config\n",
    ")\"\"\",\n",
    "        },\n",
    "        \"CPU Offload\": {\n",
    "            \"description\": \"將未使用的層移至 CPU，節省 VRAM\",\n",
    "            \"code\": \"\"\"\n",
    "# CPU offload with device_map\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\"\n",
    ")\"\"\",\n",
    "        },\n",
    "        \"動態批次\": {\n",
    "            \"description\": \"根據序列長度調整批次大小\",\n",
    "            \"code\": \"\"\"\n",
    "# Dynamic batching based on sequence length\n",
    "def get_batch_size(seq_length):\n",
    "    if seq_length < 512:\n",
    "        return 8\n",
    "    elif seq_length < 1024:\n",
    "        return 4\n",
    "    else:\n",
    "        return 2\"\"\",\n",
    "        },\n",
    "        \"生成優化\": {\n",
    "            \"description\": \"使用 KV cache 和早停策略\",\n",
    "            \"code\": \"\"\"\n",
    "# Optimized generation\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=256,\n",
    "    use_cache=True,  # Enable KV cache\n",
    "    do_sample=True,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\"\"\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for technique, info in optimization_tips.items():\n",
    "        print(f\"\\n💡 {technique}\")\n",
    "        print(f\"   📝 說明: {info['description']}\")\n",
    "        print(f\"   💻 程式碼:\")\n",
    "        print(\"   \" + \"\\n   \".join(info[\"code\"].split(\"\\n\")))\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "demonstrate_optimization_techniques()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdcbe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Practical usage guidelines and best practices ===\n",
    "def generate_usage_guidelines():\n",
    "    \"\"\"\n",
    "    Provide practical guidelines for model selection and usage\n",
    "    \"\"\"\n",
    "    guidelines = {\n",
    "        \"模型選擇建議\": {\n",
    "            \"輕量任務 (<2GB VRAM)\": [\"GPT-2 Small/Medium\", \"適合快速原型和測試\"],\n",
    "            \"中文任務 (4-8GB VRAM)\": [\n",
    "                \"Qwen2.5-7B-Instruct (4bit)\",\n",
    "                \"優秀的中文理解和生成\",\n",
    "            ],\n",
    "            \"推理任務 (4-8GB VRAM)\": [\n",
    "                \"DeepSeek-R1-Distill (4bit)\",\n",
    "                \"邏輯推理和數學問題\",\n",
    "            ],\n",
    "            \"高品質生成 (>12GB VRAM)\": [\"Qwen2.5-14B/32B\", \"最佳生成品質\"],\n",
    "        },\n",
    "        \"參數調優指南\": {\n",
    "            \"創意寫作\": \"temperature=0.8-1.2, top_p=0.9-0.95\",\n",
    "            \"技術文檔\": \"temperature=0.1-0.3, top_p=0.8-0.9\",\n",
    "            \"程式碼生成\": \"temperature=0.1-0.2, top_p=0.8\",\n",
    "            \"對話系統\": \"temperature=0.6-0.8, top_p=0.9\",\n",
    "        },\n",
    "        \"常見問題解決\": {\n",
    "            \"記憶體不足\": \"使用 4bit 量化 + CPU offload\",\n",
    "            \"生成速度慢\": \"減少 max_new_tokens + 使用 top_k 採樣\",\n",
    "            \"重複文本\": \"增加 repetition_penalty (1.1-1.3)\",\n",
    "            \"輸出不穩定\": \"降低 temperature + 設定隨機種子\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(\"\\n📚 實用指南與最佳實踐\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for category, items in guidelines.items():\n",
    "        print(f\"\\n🎯 {category}\")\n",
    "        for key, value in items.items():\n",
    "            if isinstance(value, list):\n",
    "                print(f\"   • {key}: {', '.join(value)}\")\n",
    "            else:\n",
    "                print(f\"   • {key}: {value}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "generate_usage_guidelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c2996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Final smoke test and validation ===\n",
    "def run_smoke_test():\n",
    "    \"\"\"\n",
    "    Comprehensive smoke test to validate all functionality\n",
    "    \"\"\"\n",
    "    print(\"\\n🧪 驗收測試 (Smoke Test)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    test_results = {\n",
    "        \"shared_cache_setup\": False,\n",
    "        \"model_loading\": False,\n",
    "        \"text_generation\": False,\n",
    "        \"parameter_tuning\": False,\n",
    "        \"memory_monitoring\": False,\n",
    "    }\n",
    "\n",
    "    # Test 1: Shared cache setup\n",
    "    try:\n",
    "        cache_exists = all(os.path.exists(path) for path in paths.values())\n",
    "        test_results[\"shared_cache_setup\"] = cache_exists\n",
    "        print(f\"✅ 共享快取設定: {'通過' if cache_exists else '失敗'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 共享快取設定失敗: {e}\")\n",
    "\n",
    "    # Test 2: Model loading\n",
    "    try:\n",
    "        test_model, test_tokenizer = loader.load_model_and_tokenizer(\"gpt2\")\n",
    "        test_results[\"model_loading\"] = test_model is not None\n",
    "        print(f\"✅ 模型載入: {'通過' if test_model is not None else '失敗'}\")\n",
    "\n",
    "        if test_model is not None:\n",
    "            # Test 3: Text generation\n",
    "            try:\n",
    "                result = loader.generate_text(\n",
    "                    test_model, test_tokenizer, \"Hello, world!\", max_new_tokens=10\n",
    "                )\n",
    "                test_results[\"text_generation\"] = len(result[\"generated_text\"]) > 0\n",
    "                print(\n",
    "                    f\"✅ 文本生成: {'通過' if len(result['generated_text']) > 0 else '失敗'}\"\n",
    "                )\n",
    "\n",
    "                # Test 4: Parameter tuning\n",
    "                try:\n",
    "                    result_low_temp = loader.generate_text(\n",
    "                        test_model,\n",
    "                        test_tokenizer,\n",
    "                        \"Hello, world!\",\n",
    "                        max_new_tokens=10,\n",
    "                        temperature=0.1,\n",
    "                    )\n",
    "                    result_high_temp = loader.generate_text(\n",
    "                        test_model,\n",
    "                        test_tokenizer,\n",
    "                        \"Hello, world!\",\n",
    "                        max_new_tokens=10,\n",
    "                        temperature=1.5,\n",
    "                    )\n",
    "                    test_results[\"parameter_tuning\"] = True\n",
    "                    print(\"✅ 參數調優: 通過\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ 參數調優失敗: {e}\")\n",
    "\n",
    "                # Test 5: Memory monitoring\n",
    "                try:\n",
    "                    memory_tracked = result[\"stats\"][\"peak_memory_mb\"] >= 0\n",
    "                    test_results[\"memory_monitoring\"] = memory_tracked\n",
    "                    print(f\"✅ 記憶體監控: {'通過' if memory_tracked else '失敗'}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ 記憶體監控失敗: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 文本生成失敗: {e}\")\n",
    "\n",
    "        # Cleanup test model\n",
    "        if test_model is not None:\n",
    "            del test_model, test_tokenizer\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 模型載入失敗: {e}\")\n",
    "\n",
    "    # Summary\n",
    "    passed_tests = sum(test_results.values())\n",
    "    total_tests = len(test_results)\n",
    "\n",
    "    print(f\"\\n📊 測試結果: {passed_tests}/{total_tests} 通過\")\n",
    "    if passed_tests == total_tests:\n",
    "        print(\"🎉 所有測試通過！系統運行正常\")\n",
    "    else:\n",
    "        print(\"⚠️ 部分測試失敗，請檢查錯誤訊息\")\n",
    "\n",
    "    return test_results\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_results = run_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23085ef",
   "metadata": {},
   "source": [
    " ## 📋 章節總結與下一步建議\n",
    " \n",
    " ### ✅ 完成項目 (Completed Items)\n",
    " 1. **統一模型載入器** - 支援 GPT-2、Qwen2.5、DeepSeek-R1-Distill 三大系列\n",
    " 2. **記憶體優化策略** - 4bit 量化、device mapping、CPU offload\n",
    " 3. **生成參數調優** - temperature、top-k、top-p、repetition penalty 實驗\n",
    " 4. **多場景應用** - 程式碼生成、文件摘要、創意寫作、翻譯任務\n",
    " 5. **性能基準測試** - 速度、記憶體使用量、輸出品質評估\n",
    " \n",
    " ### 🧠 核心概念 (Core Concepts)\n",
    " - **Transformer 解碼策略**: Greedy vs Sampling vs Beam Search\n",
    " - **量化技術**: BitsAndBytesConfig 4bit/8bit 量化原理\n",
    " - **記憶體管理**: VRAM 監控與 CPU offload 策略\n",
    " - **參數調優**: 創意性與一致性的平衡\n",
    " - **模型特性**: 不同模型的優勢與適用場景\n",
    " \n",
    " ### ⚠️ 常見陷阱 (Common Pitfalls)\n",
    " - **VRAM 溢出**: 未正確設定量化或 device mapping\n",
    " - **重複文本**: repetition_penalty 設定過低\n",
    " - **生成速度慢**: max_new_tokens 過大或未使用 KV cache\n",
    " - **中文支援**: tokenizer 不支援中文或分詞錯誤\n",
    " - **記憶體洩漏**: 未正確清理舊模型或快取\n",
    " \n",
    " ### 🚀 下一步建議 (Next Steps)\n",
    " 1. **指令調優 (nb11)**: 學習 Alpaca/Dolly 資料格式與微調流程\n",
    " 2. **LLM 評估 (nb12)**: 實作 perplexity、ROUGE、BLEU 自動評估\n",
    " 3. **Function Calling (nb13)**: 整合工具調用與 Agent 能力\n",
    " 4. **RAG 系統 (nb26)**: 結合檢索與生成的問答系統\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262bd3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Generate next steps and recommendations ===\n",
    "def generate_next_steps():\n",
    "    \"\"\"\n",
    "    Provide specific recommendations for next learning steps\n",
    "    \"\"\"\n",
    "    next_steps = {\n",
    "        \"立即實踐 (Immediate Practice)\": [\n",
    "            \"嘗試不同的中文提示詞，觀察模型的理解能力\",\n",
    "            \"實驗更多參數組合，找到最適合你任務的設定\",\n",
    "            \"測試模型在特定領域（如醫療、法律、金融）的表現\",\n",
    "            \"建立你自己的提示詞模板庫\",\n",
    "        ],\n",
    "        \"深化學習 (Deep Learning)\": [\n",
    "            \"研究 Transformer 架構的內部機制\",\n",
    "            \"了解不同量化方法的精度與速度權衡\",\n",
    "            \"學習模型並行與分散式推理技術\",\n",
    "            \"探索新興的長文本處理技術\",\n",
    "        ],\n",
    "        \"實際應用 (Practical Applications)\": [\n",
    "            \"建構特定領域的聊天機器人\",\n",
    "            \"開發程式碼自動生成與除錯工具\",\n",
    "            \"實作多語言翻譯與在地化系統\",\n",
    "            \"設計創意寫作輔助應用\",\n",
    "        ],\n",
    "        \"技術提升 (Technical Advancement)\": [\n",
    "            \"nb11: 指令調優與資料準備技巧\",\n",
    "            \"nb12: 全面的 LLM 評估方法論\",\n",
    "            \"nb13: Function Calling 與工具整合\",\n",
    "            \"nb26: RAG 檢索增強生成系統\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(\"\\n🎯 學習路徑建議\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for category, recommendations in next_steps.items():\n",
    "        print(f\"\\n📌 {category}\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"   {i}. {rec}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # Priority recommendations\n",
    "    print(\"\\n⭐ 優先建議\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"1. 如果想深入微調: 先完成 nb11 (指令調優)\")\n",
    "    print(\"2. 如果想建構應用: 先完成 nb13 (Function Calling)\")\n",
    "    print(\"3. 如果想評估模型: 先完成 nb12 (LLM 評估)\")\n",
    "    print(\"4. 如果想建構 RAG: 先完成 nb26 (RAG 基礎)\")\n",
    "\n",
    "\n",
    "generate_next_steps()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎓 Notebook 10 完成！\")\n",
    "print(\"📖 你已掌握主流開源 LLM 的載入、調優與應用技巧\")\n",
    "print(\"🚀 準備好進入下一個階段的學習了！\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d741791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quick validation test ===\n",
    "def quick_validation():\n",
    "    \"\"\"5-line smoke test for immediate verification\"\"\"\n",
    "    cache_ok = os.path.exists(os.environ[\"TRANSFORMERS_CACHE\"])\n",
    "    model, tokenizer = loader.load_model_and_tokenizer(\"gpt2\")\n",
    "    if model:\n",
    "        result = loader.generate_text(model, tokenizer, \"Test\", max_new_tokens=5)\n",
    "    print(\n",
    "        f\"✅ Cache: {cache_ok}, Model: {model is not None}, Generation: {'✅' if model and len(result['generated_text']) > 0 else '❌'}\"\n",
    "    )\n",
    "    return cache_ok and model is not None\n",
    "\n",
    "\n",
    "quick_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f04f6",
   "metadata": {},
   "source": [
    "## 本章小結\n",
    "\n",
    "### ✅ 完成項目 (Completed Items)\n",
    "- **統一模型載入系統** - 支援 GPT-2、Qwen2.5、DeepSeek-R1-Distill 與自動記憶體優化\n",
    "- **低 VRAM 友善策略** - 4bit 量化、CPU offload、device mapping 完整實作\n",
    "- **參數調優實驗室** - 系統化測試 temperature、top-k、top-p、repetition penalty 影響\n",
    "- **多場景應用展示** - 程式碼生成、創意寫作、翻譯、摘要等實用案例\n",
    "- **性能基準測試** - 速度、記憶體、品質的標準化評估流程\n",
    "\n",
    "### 🧠 核心原理要點 (Core Concepts)\n",
    "- **Transformer 解碼策略**: 貪婪搜尋 vs 採樣 vs Beam Search 的適用時機\n",
    "- **量化技術原理**: BitsAndBytesConfig 如何實現 75% 記憶體節省\n",
    "- **生成品質控制**: 創意性 (temperature) 與一致性 (repetition penalty) 的平衡\n",
    "- **記憶體管理**: VRAM 監控、CPU offload、梯度檢查點的組合策略\n",
    "- **模型特性分析**: 不同模型系列的優勢與適用場景識別\n",
    "\n",
    "### 🚀 下一步建議 (Next Steps)\n",
    "1. **如果想深入微調技術** → 優先學習 `nb11_instruction_tuning_demo.ipynb`\n",
    "2. **如果想建構實用應用** → 優先學習 `nb13_function_calling_tools.ipynb`  \n",
    "3. **如果想系統性評估** → 優先學習 `nb12_llm_evaluation_metrics.ipynb`\n",
    "4. **如果想整合檢索系統** → 優先學習 `nb26_rag_basic_faiss.ipynb`\n",
    "\n",
    "**技術深化方向**: 探索 KV cache 優化、模型並行推理、長文本處理技術\n",
    "**應用拓展方向**: 特定領域聊天機器人、程式碼助手、多語言翻譯系統\n",
    "\n",
    "---\n",
    "\n",
    "這個 notebook 為你建立了堅實的 LLM 應用基礎，涵蓋了從模型載入到性能優化的完整流程。你現在具備了選擇適合的模型、調優生成參數、處理記憶體限制的實戰能力！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
