{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f83d96",
   "metadata": {},
   "source": [
    "\n",
    " # ğŸ“ Notebook 10: GPT/Qwen/DeepSeek æ–‡æœ¬ç”Ÿæˆæ¯”è¼ƒå¯¦æˆ°\n",
    " \n",
    " **å­¸ç¿’ç›®æ¨™ (Learning Objectives)**\n",
    " - æŒæ¡ä¸»æµé–‹æº LLM çš„è¼‰å…¥èˆ‡æ¨ç†æŠ€å·§\n",
    " - ç†è§£è¨˜æ†¶é«”å„ªåŒ–ç­–ç•¥ (4bit/8bit quantization, device mapping)\n",
    " - æ¢ç´¢ç”Ÿæˆåƒæ•¸å°è¼¸å‡ºå“è³ªçš„å½±éŸ¿\n",
    " - å»ºç«‹æ¨™æº–åŒ–çš„æ€§èƒ½è©•ä¼°æµç¨‹\n",
    "\n",
    " **æ¶µè“‹æ¨¡å‹ç³»åˆ— (Model Families)**\n",
    " - GPT-2: è‹±æ–‡åŸºç¤ç”Ÿæˆæ¨¡å‹\n",
    " - Qwen2.5-Instruct: ä¸­è‹±é›™èªæŒ‡ä»¤æ¨¡å‹  \n",
    " - DeepSeek-R1-Distill: æ¨ç†å¢å¼·æ¨¡å‹\n",
    "\n",
    " **é©ç”¨å ´æ™¯ (Use Cases)**\n",
    " - å‰µæ„å¯«ä½œã€å•ç­”å°è©±ã€ç¨‹å¼ç”Ÿæˆã€æ–‡ä»¶ç¿»è­¯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d41ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Shared Cache Bootstrap (English comments only) ===\n",
    "import os, torch, platform, pathlib, time, psutil\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "import json\n",
    "\n",
    "# Set up shared cache paths\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "\n",
    "for k, v in paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\"\n",
    "    )\n",
    "else:\n",
    "    print(\"[GPU] Using CPU mode\")\n",
    "\n",
    "# System info\n",
    "print(f\"[System] RAM: {psutil.virtual_memory().total // 1024**3} GB\")\n",
    "print(f\"[Python] Version: {platform.python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fc2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Essential imports for LLM text generation ===\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import tracemalloc\n",
    "\n",
    "# Set reproducible seed\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"[Setup] Essential imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eafade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === UnifiedModelLoader: Memory-efficient model loading ===\n",
    "class UnifiedModelLoader:\n",
    "    \"\"\"\n",
    "    Unified loader for GPT-2, Qwen, DeepSeek models with automatic VRAM optimization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, low_vram_mode: bool = None):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Auto-detect low VRAM mode based on available memory\n",
    "        if low_vram_mode is None:\n",
    "            if torch.cuda.is_available():\n",
    "                total_vram = torch.cuda.get_device_properties(0).total_memory\n",
    "                self.low_vram_mode = total_vram < 12 * 1024**3  # Less than 12GB\n",
    "            else:\n",
    "                self.low_vram_mode = True\n",
    "        else:\n",
    "            self.low_vram_mode = low_vram_mode\n",
    "\n",
    "        print(f\"[Loader] Device: {self.device}, Low-VRAM mode: {self.low_vram_mode}\")\n",
    "\n",
    "    def get_quantization_config(self) -> Optional[BitsAndBytesConfig]:\n",
    "        \"\"\"Get 4bit quantization config for memory efficiency\"\"\"\n",
    "        if not self.low_vram_mode or self.device == \"cpu\":\n",
    "            return None\n",
    "\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "    def load_model_and_tokenizer(self, model_id: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Load model and tokenizer with memory optimization\n",
    "\n",
    "        Args:\n",
    "            model_id: HuggingFace model identifier\n",
    "\n",
    "        Returns:\n",
    "            tuple: (model, tokenizer)\n",
    "        \"\"\"\n",
    "        print(f\"[Loading] {model_id}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=os.environ[\"TRANSFORMERS_CACHE\"],\n",
    "            )\n",
    "\n",
    "            # Ensure pad token exists\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            # Prepare model loading kwargs\n",
    "            model_kwargs = {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"cache_dir\": os.environ[\"TRANSFORMERS_CACHE\"],\n",
    "                \"torch_dtype\": (\n",
    "                    torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            # Add quantization config if low VRAM mode\n",
    "            quantization_config = self.get_quantization_config()\n",
    "            if quantization_config:\n",
    "                model_kwargs[\"quantization_config\"] = quantization_config\n",
    "                print(f\"[Loading] Using 4bit quantization\")\n",
    "            else:\n",
    "                model_kwargs[\"device_map\"] = \"auto\" if self.device == \"cuda\" else None\n",
    "\n",
    "            # Load model\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "\n",
    "            # Move to device if not using device_map\n",
    "            if not quantization_config and self.device == \"cuda\":\n",
    "                model = model.to(self.device)\n",
    "\n",
    "            load_time = time.time() - start_time\n",
    "\n",
    "            # Calculate model size\n",
    "            param_count = sum(p.numel() for p in model.parameters())\n",
    "            param_size_mb = param_count * 4 / 1024**2  # Assuming float32\n",
    "\n",
    "            print(f\"[Loaded] {model_id}\")\n",
    "            print(f\"[Stats] Parameters: {param_count:,} ({param_size_mb:.1f} MB)\")\n",
    "            print(f\"[Stats] Load time: {load_time:.2f}s\")\n",
    "\n",
    "            return model, tokenizer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to load {model_id}: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.8,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 0.9,\n",
    "        repetition_penalty: float = 1.1,\n",
    "        do_sample: bool = True,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate text with detailed statistics\n",
    "\n",
    "        Returns:\n",
    "            dict: Generated text, stats, and metadata\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Encode input\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        if self.device == \"cuda\" and inputs.device != model.device:\n",
    "            inputs = inputs.to(model.device)\n",
    "\n",
    "        input_length = inputs.shape[1]\n",
    "\n",
    "        # Generation parameters\n",
    "        gen_kwargs = {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_k\": top_k,\n",
    "            \"top_p\": top_p,\n",
    "            \"repetition_penalty\": repetition_penalty,\n",
    "            \"do_sample\": do_sample,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        }\n",
    "\n",
    "        # Track memory before generation\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            mem_before = torch.cuda.memory_allocated()\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(inputs, **gen_kwargs)\n",
    "\n",
    "        # Decode output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        new_text = generated_text[len(prompt) :]\n",
    "\n",
    "        # Calculate stats\n",
    "        generation_time = time.time() - start_time\n",
    "        output_length = outputs.shape[1] - input_length\n",
    "        tokens_per_sec = output_length / generation_time if generation_time > 0 else 0\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            mem_after = torch.cuda.memory_allocated()\n",
    "            peak_memory_mb = (mem_after - mem_before) / 1024**2\n",
    "        else:\n",
    "            peak_memory_mb = 0\n",
    "\n",
    "        return {\n",
    "            \"generated_text\": new_text,\n",
    "            \"full_text\": generated_text,\n",
    "            \"stats\": {\n",
    "                \"input_tokens\": input_length,\n",
    "                \"output_tokens\": output_length,\n",
    "                \"total_tokens\": outputs.shape[1],\n",
    "                \"generation_time\": generation_time,\n",
    "                \"tokens_per_sec\": tokens_per_sec,\n",
    "                \"peak_memory_mb\": peak_memory_mb,\n",
    "            },\n",
    "            \"parameters\": gen_kwargs,\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize loader\n",
    "loader = UnifiedModelLoader()\n",
    "print(\"[Setup] UnifiedModelLoader ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627695cf",
   "metadata": {},
   "source": [
    "\n",
    " ## ğŸ¤– æ¨¡å‹å®šç¾©èˆ‡è¼‰å…¥ç­–ç•¥\n",
    " \n",
    " **è¨˜æ†¶é«”éœ€æ±‚è©•ä¼° (VRAM Requirements)**\n",
    " - GPT-2 Small (124M): ~0.5GB\n",
    " - GPT-2 Medium (355M): ~1.4GB  \n",
    " - GPT-2 Large (774M): ~3.1GB\n",
    " - Qwen2.5-7B-Instruct (4bit): ~4.5GB\n",
    " - DeepSeek-R1-Distill-Qwen-7B (4bit): ~4.5GB\n",
    "\n",
    " **é‡åŒ–ç­–ç•¥ (Quantization Strategy)**\n",
    " - <8GB VRAM: å¼·åˆ¶ 4bit é‡åŒ–\n",
    " - 8-16GB VRAM: å¯é¸ 4bit/8bit\n",
    " - >16GB VRAM: åŸç”Ÿ float16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Model configurations and loading ===\n",
    "MODEL_CONFIGS = {\n",
    "    \"gpt2-small\": {\n",
    "        \"model_id\": \"gpt2\",\n",
    "        \"description\": \"GPT-2 Small (124M) - è‹±æ–‡åŸºç¤ç”Ÿæˆ\",\n",
    "        \"vram_requirement\": 0.5,\n",
    "        \"strengths\": [\"è¼•é‡å¿«é€Ÿ\", \"è‹±æ–‡æµæš¢\", \"å‰µæ„å¯«ä½œ\"],\n",
    "    },\n",
    "    \"gpt2-medium\": {\n",
    "        \"model_id\": \"gpt2-medium\",\n",
    "        \"description\": \"GPT-2 Medium (355M) - å¹³è¡¡æ€§èƒ½èˆ‡å“è³ª\",\n",
    "        \"vram_requirement\": 1.4,\n",
    "        \"strengths\": [\"ä¸­ç­‰è¦æ¨¡\", \"è¼ƒä½³é€£è²«æ€§\", \"è©©è©å‰µä½œ\"],\n",
    "    },\n",
    "    \"qwen2.5-7b-instruct\": {\n",
    "        \"model_id\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"description\": \"Qwen2.5-7B-Instruct - ä¸­è‹±é›™èªæŒ‡ä»¤æ¨¡å‹\",\n",
    "        \"vram_requirement\": 4.5,  # with 4bit quantization\n",
    "        \"strengths\": [\"ä¸­æ–‡å„ªç§€\", \"æŒ‡ä»¤éµå¾ª\", \"å¤šèªæ”¯æŒ\"],\n",
    "    },\n",
    "    \"deepseek-r1-distill\": {\n",
    "        \"model_id\": \"deepseek-ai/deepseek-r1-distill-qwen-7b\",\n",
    "        \"description\": \"DeepSeek-R1-Distill-Qwen-7B - æ¨ç†å¢å¼·æ¨¡å‹\",\n",
    "        \"vram_requirement\": 4.5,  # with 4bit quantization\n",
    "        \"strengths\": [\"é‚è¼¯æ¨ç†\", \"æ•¸å­¸å•é¡Œ\", \"æ€ç¶­éˆ\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def display_model_info():\n",
    "    \"\"\"Display available models and their characteristics\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ¤– å¯ç”¨æ¨¡å‹ç¸½è¦½ (Available Models)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for key, config in MODEL_CONFIGS.items():\n",
    "        print(f\"\\nğŸ“‹ {key.upper()}\")\n",
    "        print(f\"   ID: {config['model_id']}\")\n",
    "        print(f\"   æè¿°: {config['description']}\")\n",
    "        print(f\"   VRAM: ~{config['vram_requirement']}GB\")\n",
    "        print(f\"   å„ªå‹¢: {', '.join(config['strengths'])}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "display_model_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Load and test GPT-2 models ===\n",
    "print(\"ğŸ”„ æ­£åœ¨è¼‰å…¥ GPT-2 ç³»åˆ—æ¨¡å‹ (Loading GPT-2 models)\")\n",
    "\n",
    "# Load GPT-2 Small for quick testing\n",
    "gpt2_model, gpt2_tokenizer = loader.load_model_and_tokenizer(\"gpt2\")\n",
    "\n",
    "if gpt2_model is not None:\n",
    "    # Test English creative writing\n",
    "    english_prompts = [\n",
    "        \"Once upon a time in a magical forest,\",\n",
    "        \"The future of artificial intelligence will be\",\n",
    "        \"In the year 2030, technology will have\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\nğŸ“ GPT-2 è‹±æ–‡å‰µæ„ç”Ÿæˆæ¸¬è©¦\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for i, prompt in enumerate(english_prompts, 1):\n",
    "        print(f\"\\nğŸ’­ Prompt {i}: {prompt}\")\n",
    "\n",
    "        result = loader.generate_text(\n",
    "            gpt2_model,\n",
    "            gpt2_tokenizer,\n",
    "            prompt,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "        print(f\"ğŸ“„ Generated: {result['generated_text'][:200]}...\")\n",
    "        print(f\"âš¡ Speed: {result['stats']['tokens_per_sec']:.1f} tokens/sec\")\n",
    "        print(f\"ğŸ’¾ Memory: {result['stats']['peak_memory_mb']:.1f} MB\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ GPT-2 è¼‰å…¥å¤±æ•—\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255cc351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Load and test Qwen2.5-7B-Instruct ===\n",
    "print(\"\\nğŸ”„ æ­£åœ¨è¼‰å…¥ Qwen2.5-7B-Instruct æ¨¡å‹\")\n",
    "\n",
    "# Free up memory from previous model if needed\n",
    "if \"gpt2_model\" in locals() and gpt2_model is not None:\n",
    "    del gpt2_model, gpt2_tokenizer\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "qwen_model, qwen_tokenizer = loader.load_model_and_tokenizer(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "if qwen_model is not None:\n",
    "    # Test Chinese and English instruction following\n",
    "    instruction_prompts = [\n",
    "        \"è«‹ç”¨ç¹é«”ä¸­æ–‡å¯«ä¸€é¦–é—œæ–¼æ˜¥å¤©çš„çŸ­è©©ã€‚\",\n",
    "        \"è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Œç”¨ç°¡å–®æ˜“æ‡‚çš„æ–¹å¼ã€‚\",\n",
    "        \"Write a Python function to calculate fibonacci numbers.\",\n",
    "        \"Translate this to Chinese: 'Artificial intelligence is transforming our world.'\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\nğŸ“ Qwen2.5 ä¸­è‹±é›™èªæŒ‡ä»¤æ¸¬è©¦\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for i, prompt in enumerate(instruction_prompts, 1):\n",
    "        print(f\"\\nğŸ’­ Instruction {i}: {prompt}\")\n",
    "\n",
    "        result = loader.generate_text(\n",
    "            qwen_model,\n",
    "            qwen_tokenizer,\n",
    "            prompt,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            top_p=0.8,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "\n",
    "        print(f\"ğŸ“„ Response: {result['generated_text']}\")\n",
    "        print(f\"âš¡ Speed: {result['stats']['tokens_per_sec']:.1f} tokens/sec\")\n",
    "        print(f\"ğŸ’¾ Memory: {result['stats']['peak_memory_mb']:.1f} MB\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Qwen2.5 è¼‰å…¥å¤±æ•—\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ae921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Load and test DeepSeek-R1-Distill ===\n",
    "print(\"\\nğŸ”„ æ­£åœ¨è¼‰å…¥ DeepSeek-R1-Distill æ¨¡å‹\")\n",
    "\n",
    "# Free up memory from Qwen if needed\n",
    "if \"qwen_model\" in locals() and qwen_model is not None:\n",
    "    del qwen_model, qwen_tokenizer\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "deepseek_model, deepseek_tokenizer = loader.load_model_and_tokenizer(\n",
    "    \"deepseek-ai/deepseek-r1-distill-qwen-7b\"\n",
    ")\n",
    "\n",
    "if deepseek_model is not None:\n",
    "    # Test reasoning and math problems\n",
    "    reasoning_prompts = [\n",
    "        \"Solve this step by step: If a train travels 120 km in 1.5 hours, what is its average speed?\",\n",
    "        \"é‚è¼¯æ¨ç†ï¼šå¦‚æœæ‰€æœ‰çš„è²“éƒ½æœ‰å°¾å·´ï¼Œè€Œå°èŠ±æ˜¯ä¸€éš»è²“ï¼Œé‚£éº¼å°èŠ±æœ‰å°¾å·´å—ï¼Ÿè«‹èªªæ˜åŸå› ã€‚\",\n",
    "        \"Write Python code to find the prime factors of 84, with explanations.\",\n",
    "        \"å¦‚æœä»Šå¤©æ˜¯æ˜ŸæœŸä¸‰ï¼Œé‚£éº¼100å¤©å¾Œæ˜¯æ˜ŸæœŸå¹¾ï¼Ÿè«‹è©³ç´°è¨ˆç®—ã€‚\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\nğŸ“ DeepSeek-R1-Distill æ¨ç†å¢å¼·æ¸¬è©¦\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for i, prompt in enumerate(reasoning_prompts, 1):\n",
    "        print(f\"\\nğŸ’­ Reasoning Task {i}: {prompt}\")\n",
    "\n",
    "        result = loader.generate_text(\n",
    "            deepseek_model,\n",
    "            deepseek_tokenizer,\n",
    "            prompt,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.3,  # Lower temperature for reasoning\n",
    "            top_p=0.8,\n",
    "            repetition_penalty=1.05,\n",
    "        )\n",
    "\n",
    "        print(f\"ğŸ“„ Solution: {result['generated_text']}\")\n",
    "        print(f\"âš¡ Speed: {result['stats']['tokens_per_sec']:.1f} tokens/sec\")\n",
    "        print(f\"ğŸ’¾ Memory: {result['stats']['peak_memory_mb']:.1f} MB\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "else:\n",
    "    print(\"âŒ DeepSeek-R1-Distill è¼‰å…¥å¤±æ•—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0c746",
   "metadata": {},
   "source": [
    "\n",
    " ## ğŸ›ï¸ ç”Ÿæˆåƒæ•¸èª¿å„ªå¯¦é©—\n",
    " \n",
    " **æ ¸å¿ƒåƒæ•¸èªªæ˜ (Parameter Explanation)**\n",
    " - **Temperature**: æ§åˆ¶éš¨æ©Ÿæ€§ (0.1=ä¿å®ˆ, 1.0=å‰µæ„, 2.0=æ¥µå¯Œå‰µæ„)\n",
    " - **Top-k**: å€™é¸è©æ•¸é‡é™åˆ¶ (10=åš´æ ¼, 50=å¹³è¡¡, 100=å¯¬é¬†)\n",
    " - **Top-p**: ç´¯ç©æ©Ÿç‡é–¾å€¼ (0.8=ä¿å®ˆ, 0.9=å¹³è¡¡, 0.95=å‰µæ„)\n",
    " - **Repetition Penalty**: é‡è¤‡æ‡²ç½° (1.0=ç„¡æ‡²ç½°, 1.1=è¼•å¾®, 1.3=åš´æ ¼)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Parameter tuning experiments ===\n",
    "def parameter_experiment(model, tokenizer, prompt: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Conduct systematic parameter tuning experiments\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ§ª {model_name} åƒæ•¸èª¿å„ªå¯¦é©—\")\n",
    "    print(f\"ğŸ“ æ¸¬è©¦æç¤º: {prompt}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Parameter grid for testing\n",
    "    param_configs = [\n",
    "        {\n",
    "            \"name\": \"ä¿å®ˆè¨­å®š\",\n",
    "            \"temp\": 0.3,\n",
    "            \"top_k\": 20,\n",
    "            \"top_p\": 0.8,\n",
    "            \"rep_penalty\": 1.1,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"å¹³è¡¡è¨­å®š\",\n",
    "            \"temp\": 0.7,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.9,\n",
    "            \"rep_penalty\": 1.05,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"å‰µæ„è¨­å®š\",\n",
    "            \"temp\": 1.0,\n",
    "            \"top_k\": 100,\n",
    "            \"top_p\": 0.95,\n",
    "            \"rep_penalty\": 1.0,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"æ¥µç«¯å‰µæ„\",\n",
    "            \"temp\": 1.5,\n",
    "            \"top_k\": 0,\n",
    "            \"top_p\": 0.98,\n",
    "            \"rep_penalty\": 0.95,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for config in param_configs:\n",
    "        print(\n",
    "            f\"\\nğŸ¯ {config['name']} (T={config['temp']}, k={config['top_k']}, p={config['top_p']}, rp={config['rep_penalty']})\"\n",
    "        )\n",
    "\n",
    "        result = loader.generate_text(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            max_new_tokens=120,\n",
    "            temperature=config[\"temp\"],\n",
    "            top_k=config[\"top_k\"] if config[\"top_k\"] > 0 else None,\n",
    "            top_p=config[\"top_p\"],\n",
    "            repetition_penalty=config[\"rep_penalty\"],\n",
    "        )\n",
    "\n",
    "        print(f\"ğŸ“„ è¼¸å‡º: {result['generated_text'][:150]}...\")\n",
    "        print(f\"âš¡ é€Ÿåº¦: {result['stats']['tokens_per_sec']:.1f} tokens/sec\")\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"config\": config[\"name\"],\n",
    "                \"output\": result[\"generated_text\"],\n",
    "                \"speed\": result[\"stats\"][\"tokens_per_sec\"],\n",
    "                \"length\": result[\"stats\"][\"output_tokens\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run parameter experiments if model is available\n",
    "if \"deepseek_model\" in locals() and deepseek_model is not None:\n",
    "    test_prompt = \"å¯«ä¸€å€‹é—œæ–¼äººå·¥æ™ºæ…§çš„çŸ­æ•…äº‹ï¼š\"\n",
    "    param_results = parameter_experiment(\n",
    "        deepseek_model, deepseek_tokenizer, test_prompt, \"DeepSeek-R1-Distill\"\n",
    "    )\n",
    "\n",
    "    # Create comparison DataFrame\n",
    "    param_df = pd.DataFrame(param_results)\n",
    "    print(\"\\nğŸ“Š åƒæ•¸èª¿å„ªçµæœç¸½è¦½\")\n",
    "    print(param_df[[\"config\", \"speed\", \"length\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Multi-scenario application demonstrations ===\n",
    "print(\"\\nğŸ­ å¤šå ´æ™¯æ‡‰ç”¨ç¤ºç¯„ (Multi-scenario Applications)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define application scenarios\n",
    "scenarios = {\n",
    "    \"ç¨‹å¼ç¢¼ç”Ÿæˆ\": {\n",
    "        \"prompt\": \"Write a Python function to merge two sorted lists:\",\n",
    "        \"params\": {\"temperature\": 0.2, \"top_p\": 0.8, \"max_new_tokens\": 200},\n",
    "    },\n",
    "    \"æ–‡ä»¶æ‘˜è¦\": {\n",
    "        \"prompt\": \"è«‹æ‘˜è¦ä»¥ä¸‹å…§å®¹çš„é‡é»ï¼šäººå·¥æ™ºæ…§æ˜¯ä¸€é–€ç ”ç©¶å¦‚ä½•è®“æ©Ÿå™¨æ¨¡æ“¬äººé¡æ™ºæ…§çš„å­¸ç§‘ï¼ŒåŒ…æ‹¬æ©Ÿå™¨å­¸ç¿’ã€æ·±åº¦å­¸ç¿’ã€è‡ªç„¶èªè¨€è™•ç†ç­‰å¤šå€‹åˆ†æ”¯é ˜åŸŸã€‚\",\n",
    "        \"params\": {\"temperature\": 0.3, \"top_p\": 0.8, \"max_new_tokens\": 100},\n",
    "    },\n",
    "    \"å‰µæ„å¯«ä½œ\": {\n",
    "        \"prompt\": \"åœ¨ä¸€å€‹ç§‘æŠ€é«˜åº¦ç™¼é”çš„æœªä¾†åŸå¸‚è£¡ï¼Œ\",\n",
    "        \"params\": {\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_tokens\": 150},\n",
    "    },\n",
    "    \"ç¿»è­¯ä»»å‹™\": {\n",
    "        \"prompt\": \"Translate to Traditional Chinese: 'Machine learning algorithms can process vast amounts of data to identify patterns and make predictions.'\",\n",
    "        \"params\": {\"temperature\": 0.1, \"top_p\": 0.8, \"max_new_tokens\": 100},\n",
    "    },\n",
    "}\n",
    "\n",
    "application_results = {}\n",
    "\n",
    "# Test scenarios with available model\n",
    "current_model = None\n",
    "current_tokenizer = None\n",
    "current_model_name = \"Unknown\"\n",
    "\n",
    "# Determine which model is currently loaded\n",
    "if \"deepseek_model\" in locals() and deepseek_model is not None:\n",
    "    current_model = deepseek_model\n",
    "    current_tokenizer = deepseek_tokenizer\n",
    "    current_model_name = \"DeepSeek-R1-Distill\"\n",
    "elif \"qwen_model\" in locals() and qwen_model is not None:\n",
    "    current_model = qwen_model\n",
    "    current_tokenizer = qwen_tokenizer\n",
    "    current_model_name = \"Qwen2.5-7B-Instruct\"\n",
    "elif \"gpt2_model\" in locals() and gpt2_model is not None:\n",
    "    current_model = gpt2_model\n",
    "    current_tokenizer = gpt2_tokenizer\n",
    "    current_model_name = \"GPT-2\"\n",
    "\n",
    "if current_model is not None:\n",
    "    print(f\"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {current_model_name}\")\n",
    "\n",
    "    for scenario_name, scenario_config in scenarios.items():\n",
    "        print(f\"\\nğŸ¯ å ´æ™¯: {scenario_name}\")\n",
    "        print(f\"ğŸ’­ æç¤º: {scenario_config['prompt']}\")\n",
    "\n",
    "        result = loader.generate_text(\n",
    "            current_model,\n",
    "            current_tokenizer,\n",
    "            scenario_config[\"prompt\"],\n",
    "            **scenario_config[\"params\"],\n",
    "        )\n",
    "\n",
    "        print(f\"ğŸ“„ çµæœ: {result['generated_text']}\")\n",
    "        print(f\"âš¡ æ€§èƒ½: {result['stats']['tokens_per_sec']:.1f} tokens/sec\")\n",
    "\n",
    "        application_results[scenario_name] = result\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"âŒ ç„¡å¯ç”¨æ¨¡å‹é€²è¡Œæ‡‰ç”¨æ¸¬è©¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Performance benchmarking and comparison ===\n",
    "class ModelBenchmark:\n",
    "    \"\"\"\n",
    "    Comprehensive model performance benchmarking\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "\n",
    "    def benchmark_model(self, model, tokenizer, model_name: str, num_runs: int = 3):\n",
    "        \"\"\"\n",
    "        Benchmark a model across multiple metrics\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“Š {model_name} æ€§èƒ½åŸºæº–æ¸¬è©¦\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Standard test prompts\n",
    "        test_prompts = [\n",
    "            \"The future of artificial intelligence\",\n",
    "            \"è§£é‡‹æ©Ÿå™¨å­¸ç¿’çš„åŸºæœ¬æ¦‚å¿µ\",\n",
    "            \"Write code to calculate factorial\",\n",
    "        ]\n",
    "\n",
    "        # Collect metrics across multiple runs\n",
    "        all_metrics = []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            run_metrics = []\n",
    "\n",
    "            for prompt in test_prompts:\n",
    "                # Warm up run (not counted)\n",
    "                if run == 0:\n",
    "                    _ = loader.generate_text(\n",
    "                        model, tokenizer, prompt, max_new_tokens=50\n",
    "                    )\n",
    "\n",
    "                # Actual benchmark run\n",
    "                start_time = time.time()\n",
    "                result = loader.generate_text(\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    prompt,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                )\n",
    "\n",
    "                run_metrics.append(\n",
    "                    {\n",
    "                        \"prompt\": prompt[:30] + \"...\",\n",
    "                        \"tokens_per_sec\": result[\"stats\"][\"tokens_per_sec\"],\n",
    "                        \"generation_time\": result[\"stats\"][\"generation_time\"],\n",
    "                        \"output_tokens\": result[\"stats\"][\"output_tokens\"],\n",
    "                        \"peak_memory_mb\": result[\"stats\"][\"peak_memory_mb\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            all_metrics.extend(run_metrics)\n",
    "\n",
    "        # Calculate average metrics\n",
    "        avg_metrics = {\n",
    "            \"model_name\": model_name,\n",
    "            \"avg_tokens_per_sec\": np.mean([m[\"tokens_per_sec\"] for m in all_metrics]),\n",
    "            \"avg_generation_time\": np.mean([m[\"generation_time\"] for m in all_metrics]),\n",
    "            \"avg_output_tokens\": np.mean([m[\"output_tokens\"] for m in all_metrics]),\n",
    "            \"avg_peak_memory_mb\": np.mean([m[\"peak_memory_mb\"] for m in all_metrics]),\n",
    "            \"std_tokens_per_sec\": np.std([m[\"tokens_per_sec\"] for m in all_metrics]),\n",
    "        }\n",
    "\n",
    "        # Print results\n",
    "        print(\n",
    "            f\"ğŸƒ å¹³å‡é€Ÿåº¦: {avg_metrics['avg_tokens_per_sec']:.1f} Â± {avg_metrics['std_tokens_per_sec']:.1f} tokens/sec\"\n",
    "        )\n",
    "        print(f\"â±ï¸ å¹³å‡ç”Ÿæˆæ™‚é–“: {avg_metrics['avg_generation_time']:.2f}s\")\n",
    "        print(f\"ğŸ’¾ å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨: {avg_metrics['avg_peak_memory_mb']:.1f} MB\")\n",
    "        print(f\"ğŸ“ å¹³å‡è¼¸å‡ºé•·åº¦: {avg_metrics['avg_output_tokens']:.0f} tokens\")\n",
    "\n",
    "        self.results.append(avg_metrics)\n",
    "        return avg_metrics\n",
    "\n",
    "    def compare_models(self):\n",
    "        \"\"\"Generate comparison report\"\"\"\n",
    "        if len(self.results) < 2:\n",
    "            print(\"âš ï¸ éœ€è¦è‡³å°‘å…©å€‹æ¨¡å‹é€²è¡Œæ¯”è¼ƒ\")\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.results)\n",
    "        print(\"\\nğŸ“ˆ æ¨¡å‹æ€§èƒ½æ¯”è¼ƒè¡¨\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\n",
    "            df[[\"model_name\", \"avg_tokens_per_sec\", \"avg_peak_memory_mb\"]].to_string(\n",
    "                index=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Find best performers\n",
    "        fastest = df.loc[df[\"avg_tokens_per_sec\"].idxmax()]\n",
    "        most_efficient = df.loc[df[\"avg_peak_memory_mb\"].idxmin()]\n",
    "\n",
    "        print(\n",
    "            f\"\\nğŸ† æœ€å¿«æ¨¡å‹: {fastest['model_name']} ({fastest['avg_tokens_per_sec']:.1f} tokens/sec)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"ğŸ’¡ æœ€ç¯€çœè¨˜æ†¶é«”: {most_efficient['model_name']} ({most_efficient['avg_peak_memory_mb']:.1f} MB)\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Initialize benchmark\n",
    "benchmark = ModelBenchmark()\n",
    "\n",
    "# Benchmark currently loaded model\n",
    "if current_model is not None:\n",
    "    benchmark.benchmark_model(current_model, current_tokenizer, current_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68dd5bb",
   "metadata": {},
   "source": [
    " ## âš¡ è¨˜æ†¶é«”èˆ‡æ€§èƒ½å„ªåŒ–æŠ€å·§\n",
    " \n",
    " **ä½ VRAM å„ªåŒ–ç­–ç•¥ (Low-VRAM Optimization)**\n",
    " 1. **é‡åŒ– (Quantization)**: 4bit/8bit æ¸›å°‘ 50-75% è¨˜æ†¶é«”ä½¿ç”¨\n",
    " 2. **CPU Offload**: å°‡éƒ¨åˆ†å±¤ç§»è‡³ CPU è¨˜æ†¶é«”\n",
    " 3. **Gradient Checkpointing**: çŠ§ç‰²è¨ˆç®—æ›å–è¨˜æ†¶é«”\n",
    " 4. **Dynamic Batching**: æ ¹æ“šåºåˆ—é•·åº¦å‹•æ…‹èª¿æ•´æ‰¹æ¬¡å¤§å°\n",
    "\n",
    " **ç”Ÿæˆé€Ÿåº¦å„ªåŒ– (Speed Optimization)**\n",
    " 1. **KV Cache**: é¿å…é‡è¤‡è¨ˆç®—æ³¨æ„åŠ›\n",
    " 2. **Beam Search æ›¿ä»£**: ä½¿ç”¨ Top-k/Top-p æ¡æ¨£\n",
    " 3. **Early Stopping**: è¨­å®šåˆç†çš„æœ€å¤§é•·åº¦\n",
    " 4. **Model Compilation**: ä½¿ç”¨ torch.compile (PyTorch 2.0+)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aa0ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Memory and performance optimization demonstrations ===\n",
    "def demonstrate_optimization_techniques():\n",
    "    \"\"\"\n",
    "    Show various optimization techniques for different scenarios\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ”§ è¨˜æ†¶é«”èˆ‡æ€§èƒ½å„ªåŒ–ç¤ºç¯„\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    optimization_tips = {\n",
    "        \"4bit é‡åŒ–\": {\n",
    "            \"description\": \"æ¸›å°‘ 75% è¨˜æ†¶é«”ä½¿ç”¨ï¼Œè¼•å¾®ç²¾åº¦æå¤±\",\n",
    "            \"code\": \"\"\"\n",
    "# 4bit quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=quantization_config\n",
    ")\"\"\",\n",
    "        },\n",
    "        \"CPU Offload\": {\n",
    "            \"description\": \"å°‡æœªä½¿ç”¨çš„å±¤ç§»è‡³ CPUï¼Œç¯€çœ VRAM\",\n",
    "            \"code\": \"\"\"\n",
    "# CPU offload with device_map\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\"\n",
    ")\"\"\",\n",
    "        },\n",
    "        \"å‹•æ…‹æ‰¹æ¬¡\": {\n",
    "            \"description\": \"æ ¹æ“šåºåˆ—é•·åº¦èª¿æ•´æ‰¹æ¬¡å¤§å°\",\n",
    "            \"code\": \"\"\"\n",
    "# Dynamic batching based on sequence length\n",
    "def get_batch_size(seq_length):\n",
    "    if seq_length < 512:\n",
    "        return 8\n",
    "    elif seq_length < 1024:\n",
    "        return 4\n",
    "    else:\n",
    "        return 2\"\"\",\n",
    "        },\n",
    "        \"ç”Ÿæˆå„ªåŒ–\": {\n",
    "            \"description\": \"ä½¿ç”¨ KV cache å’Œæ—©åœç­–ç•¥\",\n",
    "            \"code\": \"\"\"\n",
    "# Optimized generation\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=256,\n",
    "    use_cache=True,  # Enable KV cache\n",
    "    do_sample=True,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\"\"\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for technique, info in optimization_tips.items():\n",
    "        print(f\"\\nğŸ’¡ {technique}\")\n",
    "        print(f\"   ğŸ“ èªªæ˜: {info['description']}\")\n",
    "        print(f\"   ğŸ’» ç¨‹å¼ç¢¼:\")\n",
    "        print(\"   \" + \"\\n   \".join(info[\"code\"].split(\"\\n\")))\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "demonstrate_optimization_techniques()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdcbe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Practical usage guidelines and best practices ===\n",
    "def generate_usage_guidelines():\n",
    "    \"\"\"\n",
    "    Provide practical guidelines for model selection and usage\n",
    "    \"\"\"\n",
    "    guidelines = {\n",
    "        \"æ¨¡å‹é¸æ“‡å»ºè­°\": {\n",
    "            \"è¼•é‡ä»»å‹™ (<2GB VRAM)\": [\"GPT-2 Small/Medium\", \"é©åˆå¿«é€ŸåŸå‹å’Œæ¸¬è©¦\"],\n",
    "            \"ä¸­æ–‡ä»»å‹™ (4-8GB VRAM)\": [\n",
    "                \"Qwen2.5-7B-Instruct (4bit)\",\n",
    "                \"å„ªç§€çš„ä¸­æ–‡ç†è§£å’Œç”Ÿæˆ\",\n",
    "            ],\n",
    "            \"æ¨ç†ä»»å‹™ (4-8GB VRAM)\": [\n",
    "                \"DeepSeek-R1-Distill (4bit)\",\n",
    "                \"é‚è¼¯æ¨ç†å’Œæ•¸å­¸å•é¡Œ\",\n",
    "            ],\n",
    "            \"é«˜å“è³ªç”Ÿæˆ (>12GB VRAM)\": [\"Qwen2.5-14B/32B\", \"æœ€ä½³ç”Ÿæˆå“è³ª\"],\n",
    "        },\n",
    "        \"åƒæ•¸èª¿å„ªæŒ‡å—\": {\n",
    "            \"å‰µæ„å¯«ä½œ\": \"temperature=0.8-1.2, top_p=0.9-0.95\",\n",
    "            \"æŠ€è¡“æ–‡æª”\": \"temperature=0.1-0.3, top_p=0.8-0.9\",\n",
    "            \"ç¨‹å¼ç¢¼ç”Ÿæˆ\": \"temperature=0.1-0.2, top_p=0.8\",\n",
    "            \"å°è©±ç³»çµ±\": \"temperature=0.6-0.8, top_p=0.9\",\n",
    "        },\n",
    "        \"å¸¸è¦‹å•é¡Œè§£æ±º\": {\n",
    "            \"è¨˜æ†¶é«”ä¸è¶³\": \"ä½¿ç”¨ 4bit é‡åŒ– + CPU offload\",\n",
    "            \"ç”Ÿæˆé€Ÿåº¦æ…¢\": \"æ¸›å°‘ max_new_tokens + ä½¿ç”¨ top_k æ¡æ¨£\",\n",
    "            \"é‡è¤‡æ–‡æœ¬\": \"å¢åŠ  repetition_penalty (1.1-1.3)\",\n",
    "            \"è¼¸å‡ºä¸ç©©å®š\": \"é™ä½ temperature + è¨­å®šéš¨æ©Ÿç¨®å­\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(\"\\nğŸ“š å¯¦ç”¨æŒ‡å—èˆ‡æœ€ä½³å¯¦è¸\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for category, items in guidelines.items():\n",
    "        print(f\"\\nğŸ¯ {category}\")\n",
    "        for key, value in items.items():\n",
    "            if isinstance(value, list):\n",
    "                print(f\"   â€¢ {key}: {', '.join(value)}\")\n",
    "            else:\n",
    "                print(f\"   â€¢ {key}: {value}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "generate_usage_guidelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c2996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Final smoke test and validation ===\n",
    "def run_smoke_test():\n",
    "    \"\"\"\n",
    "    Comprehensive smoke test to validate all functionality\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ§ª é©—æ”¶æ¸¬è©¦ (Smoke Test)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    test_results = {\n",
    "        \"shared_cache_setup\": False,\n",
    "        \"model_loading\": False,\n",
    "        \"text_generation\": False,\n",
    "        \"parameter_tuning\": False,\n",
    "        \"memory_monitoring\": False,\n",
    "    }\n",
    "\n",
    "    # Test 1: Shared cache setup\n",
    "    try:\n",
    "        cache_exists = all(os.path.exists(path) for path in paths.values())\n",
    "        test_results[\"shared_cache_setup\"] = cache_exists\n",
    "        print(f\"âœ… å…±äº«å¿«å–è¨­å®š: {'é€šé' if cache_exists else 'å¤±æ•—'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å…±äº«å¿«å–è¨­å®šå¤±æ•—: {e}\")\n",
    "\n",
    "    # Test 2: Model loading\n",
    "    try:\n",
    "        test_model, test_tokenizer = loader.load_model_and_tokenizer(\"gpt2\")\n",
    "        test_results[\"model_loading\"] = test_model is not None\n",
    "        print(f\"âœ… æ¨¡å‹è¼‰å…¥: {'é€šé' if test_model is not None else 'å¤±æ•—'}\")\n",
    "\n",
    "        if test_model is not None:\n",
    "            # Test 3: Text generation\n",
    "            try:\n",
    "                result = loader.generate_text(\n",
    "                    test_model, test_tokenizer, \"Hello, world!\", max_new_tokens=10\n",
    "                )\n",
    "                test_results[\"text_generation\"] = len(result[\"generated_text\"]) > 0\n",
    "                print(\n",
    "                    f\"âœ… æ–‡æœ¬ç”Ÿæˆ: {'é€šé' if len(result['generated_text']) > 0 else 'å¤±æ•—'}\"\n",
    "                )\n",
    "\n",
    "                # Test 4: Parameter tuning\n",
    "                try:\n",
    "                    result_low_temp = loader.generate_text(\n",
    "                        test_model,\n",
    "                        test_tokenizer,\n",
    "                        \"Hello, world!\",\n",
    "                        max_new_tokens=10,\n",
    "                        temperature=0.1,\n",
    "                    )\n",
    "                    result_high_temp = loader.generate_text(\n",
    "                        test_model,\n",
    "                        test_tokenizer,\n",
    "                        \"Hello, world!\",\n",
    "                        max_new_tokens=10,\n",
    "                        temperature=1.5,\n",
    "                    )\n",
    "                    test_results[\"parameter_tuning\"] = True\n",
    "                    print(\"âœ… åƒæ•¸èª¿å„ª: é€šé\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ åƒæ•¸èª¿å„ªå¤±æ•—: {e}\")\n",
    "\n",
    "                # Test 5: Memory monitoring\n",
    "                try:\n",
    "                    memory_tracked = result[\"stats\"][\"peak_memory_mb\"] >= 0\n",
    "                    test_results[\"memory_monitoring\"] = memory_tracked\n",
    "                    print(f\"âœ… è¨˜æ†¶é«”ç›£æ§: {'é€šé' if memory_tracked else 'å¤±æ•—'}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ è¨˜æ†¶é«”ç›£æ§å¤±æ•—: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ æ–‡æœ¬ç”Ÿæˆå¤±æ•—: {e}\")\n",
    "\n",
    "        # Cleanup test model\n",
    "        if test_model is not None:\n",
    "            del test_model, test_tokenizer\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\")\n",
    "\n",
    "    # Summary\n",
    "    passed_tests = sum(test_results.values())\n",
    "    total_tests = len(test_results)\n",
    "\n",
    "    print(f\"\\nğŸ“Š æ¸¬è©¦çµæœ: {passed_tests}/{total_tests} é€šé\")\n",
    "    if passed_tests == total_tests:\n",
    "        print(\"ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼ç³»çµ±é‹è¡Œæ­£å¸¸\")\n",
    "    else:\n",
    "        print(\"âš ï¸ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯\")\n",
    "\n",
    "    return test_results\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_results = run_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23085ef",
   "metadata": {},
   "source": [
    " ## ğŸ“‹ ç« ç¯€ç¸½çµèˆ‡ä¸‹ä¸€æ­¥å»ºè­°\n",
    " \n",
    " ### âœ… å®Œæˆé …ç›® (Completed Items)\n",
    " 1. **çµ±ä¸€æ¨¡å‹è¼‰å…¥å™¨** - æ”¯æ´ GPT-2ã€Qwen2.5ã€DeepSeek-R1-Distill ä¸‰å¤§ç³»åˆ—\n",
    " 2. **è¨˜æ†¶é«”å„ªåŒ–ç­–ç•¥** - 4bit é‡åŒ–ã€device mappingã€CPU offload\n",
    " 3. **ç”Ÿæˆåƒæ•¸èª¿å„ª** - temperatureã€top-kã€top-pã€repetition penalty å¯¦é©—\n",
    " 4. **å¤šå ´æ™¯æ‡‰ç”¨** - ç¨‹å¼ç¢¼ç”Ÿæˆã€æ–‡ä»¶æ‘˜è¦ã€å‰µæ„å¯«ä½œã€ç¿»è­¯ä»»å‹™\n",
    " 5. **æ€§èƒ½åŸºæº–æ¸¬è©¦** - é€Ÿåº¦ã€è¨˜æ†¶é«”ä½¿ç”¨é‡ã€è¼¸å‡ºå“è³ªè©•ä¼°\n",
    " \n",
    " ### ğŸ§  æ ¸å¿ƒæ¦‚å¿µ (Core Concepts)\n",
    " - **Transformer è§£ç¢¼ç­–ç•¥**: Greedy vs Sampling vs Beam Search\n",
    " - **é‡åŒ–æŠ€è¡“**: BitsAndBytesConfig 4bit/8bit é‡åŒ–åŸç†\n",
    " - **è¨˜æ†¶é«”ç®¡ç†**: VRAM ç›£æ§èˆ‡ CPU offload ç­–ç•¥\n",
    " - **åƒæ•¸èª¿å„ª**: å‰µæ„æ€§èˆ‡ä¸€è‡´æ€§çš„å¹³è¡¡\n",
    " - **æ¨¡å‹ç‰¹æ€§**: ä¸åŒæ¨¡å‹çš„å„ªå‹¢èˆ‡é©ç”¨å ´æ™¯\n",
    " \n",
    " ### âš ï¸ å¸¸è¦‹é™·é˜± (Common Pitfalls)\n",
    " - **VRAM æº¢å‡º**: æœªæ­£ç¢ºè¨­å®šé‡åŒ–æˆ– device mapping\n",
    " - **é‡è¤‡æ–‡æœ¬**: repetition_penalty è¨­å®šéä½\n",
    " - **ç”Ÿæˆé€Ÿåº¦æ…¢**: max_new_tokens éå¤§æˆ–æœªä½¿ç”¨ KV cache\n",
    " - **ä¸­æ–‡æ”¯æ´**: tokenizer ä¸æ”¯æ´ä¸­æ–‡æˆ–åˆ†è©éŒ¯èª¤\n",
    " - **è¨˜æ†¶é«”æ´©æ¼**: æœªæ­£ç¢ºæ¸…ç†èˆŠæ¨¡å‹æˆ–å¿«å–\n",
    " \n",
    " ### ğŸš€ ä¸‹ä¸€æ­¥å»ºè­° (Next Steps)\n",
    " 1. **æŒ‡ä»¤èª¿å„ª (nb11)**: å­¸ç¿’ Alpaca/Dolly è³‡æ–™æ ¼å¼èˆ‡å¾®èª¿æµç¨‹\n",
    " 2. **LLM è©•ä¼° (nb12)**: å¯¦ä½œ perplexityã€ROUGEã€BLEU è‡ªå‹•è©•ä¼°\n",
    " 3. **Function Calling (nb13)**: æ•´åˆå·¥å…·èª¿ç”¨èˆ‡ Agent èƒ½åŠ›\n",
    " 4. **RAG ç³»çµ± (nb26)**: çµåˆæª¢ç´¢èˆ‡ç”Ÿæˆçš„å•ç­”ç³»çµ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262bd3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# === Generate next steps and recommendations ===\n",
    "def generate_next_steps():\n",
    "    \"\"\"\n",
    "    Provide specific recommendations for next learning steps\n",
    "    \"\"\"\n",
    "    next_steps = {\n",
    "        \"ç«‹å³å¯¦è¸ (Immediate Practice)\": [\n",
    "            \"å˜—è©¦ä¸åŒçš„ä¸­æ–‡æç¤ºè©ï¼Œè§€å¯Ÿæ¨¡å‹çš„ç†è§£èƒ½åŠ›\",\n",
    "            \"å¯¦é©—æ›´å¤šåƒæ•¸çµ„åˆï¼Œæ‰¾åˆ°æœ€é©åˆä½ ä»»å‹™çš„è¨­å®š\",\n",
    "            \"æ¸¬è©¦æ¨¡å‹åœ¨ç‰¹å®šé ˜åŸŸï¼ˆå¦‚é†«ç™‚ã€æ³•å¾‹ã€é‡‘èï¼‰çš„è¡¨ç¾\",\n",
    "            \"å»ºç«‹ä½ è‡ªå·±çš„æç¤ºè©æ¨¡æ¿åº«\",\n",
    "        ],\n",
    "        \"æ·±åŒ–å­¸ç¿’ (Deep Learning)\": [\n",
    "            \"ç ”ç©¶ Transformer æ¶æ§‹çš„å…§éƒ¨æ©Ÿåˆ¶\",\n",
    "            \"äº†è§£ä¸åŒé‡åŒ–æ–¹æ³•çš„ç²¾åº¦èˆ‡é€Ÿåº¦æ¬Šè¡¡\",\n",
    "            \"å­¸ç¿’æ¨¡å‹ä¸¦è¡Œèˆ‡åˆ†æ•£å¼æ¨ç†æŠ€è¡“\",\n",
    "            \"æ¢ç´¢æ–°èˆˆçš„é•·æ–‡æœ¬è™•ç†æŠ€è¡“\",\n",
    "        ],\n",
    "        \"å¯¦éš›æ‡‰ç”¨ (Practical Applications)\": [\n",
    "            \"å»ºæ§‹ç‰¹å®šé ˜åŸŸçš„èŠå¤©æ©Ÿå™¨äºº\",\n",
    "            \"é–‹ç™¼ç¨‹å¼ç¢¼è‡ªå‹•ç”Ÿæˆèˆ‡é™¤éŒ¯å·¥å…·\",\n",
    "            \"å¯¦ä½œå¤šèªè¨€ç¿»è­¯èˆ‡åœ¨åœ°åŒ–ç³»çµ±\",\n",
    "            \"è¨­è¨ˆå‰µæ„å¯«ä½œè¼”åŠ©æ‡‰ç”¨\",\n",
    "        ],\n",
    "        \"æŠ€è¡“æå‡ (Technical Advancement)\": [\n",
    "            \"nb11: æŒ‡ä»¤èª¿å„ªèˆ‡è³‡æ–™æº–å‚™æŠ€å·§\",\n",
    "            \"nb12: å…¨é¢çš„ LLM è©•ä¼°æ–¹æ³•è«–\",\n",
    "            \"nb13: Function Calling èˆ‡å·¥å…·æ•´åˆ\",\n",
    "            \"nb26: RAG æª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(\"\\nğŸ¯ å­¸ç¿’è·¯å¾‘å»ºè­°\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for category, recommendations in next_steps.items():\n",
    "        print(f\"\\nğŸ“Œ {category}\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"   {i}. {rec}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # Priority recommendations\n",
    "    print(\"\\nâ­ å„ªå…ˆå»ºè­°\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"1. å¦‚æœæƒ³æ·±å…¥å¾®èª¿: å…ˆå®Œæˆ nb11 (æŒ‡ä»¤èª¿å„ª)\")\n",
    "    print(\"2. å¦‚æœæƒ³å»ºæ§‹æ‡‰ç”¨: å…ˆå®Œæˆ nb13 (Function Calling)\")\n",
    "    print(\"3. å¦‚æœæƒ³è©•ä¼°æ¨¡å‹: å…ˆå®Œæˆ nb12 (LLM è©•ä¼°)\")\n",
    "    print(\"4. å¦‚æœæƒ³å»ºæ§‹ RAG: å…ˆå®Œæˆ nb26 (RAG åŸºç¤)\")\n",
    "\n",
    "\n",
    "generate_next_steps()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ Notebook 10 å®Œæˆï¼\")\n",
    "print(\"ğŸ“– ä½ å·²æŒæ¡ä¸»æµé–‹æº LLM çš„è¼‰å…¥ã€èª¿å„ªèˆ‡æ‡‰ç”¨æŠ€å·§\")\n",
    "print(\"ğŸš€ æº–å‚™å¥½é€²å…¥ä¸‹ä¸€å€‹éšæ®µçš„å­¸ç¿’äº†ï¼\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d741791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quick validation test ===\n",
    "def quick_validation():\n",
    "    \"\"\"5-line smoke test for immediate verification\"\"\"\n",
    "    cache_ok = os.path.exists(os.environ[\"TRANSFORMERS_CACHE\"])\n",
    "    model, tokenizer = loader.load_model_and_tokenizer(\"gpt2\")\n",
    "    if model:\n",
    "        result = loader.generate_text(model, tokenizer, \"Test\", max_new_tokens=5)\n",
    "    print(\n",
    "        f\"âœ… Cache: {cache_ok}, Model: {model is not None}, Generation: {'âœ…' if model and len(result['generated_text']) > 0 else 'âŒ'}\"\n",
    "    )\n",
    "    return cache_ok and model is not None\n",
    "\n",
    "\n",
    "quick_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f04f6",
   "metadata": {},
   "source": [
    "## æœ¬ç« å°çµ\n",
    "\n",
    "### âœ… å®Œæˆé …ç›® (Completed Items)\n",
    "- **çµ±ä¸€æ¨¡å‹è¼‰å…¥ç³»çµ±** - æ”¯æ´ GPT-2ã€Qwen2.5ã€DeepSeek-R1-Distill èˆ‡è‡ªå‹•è¨˜æ†¶é«”å„ªåŒ–\n",
    "- **ä½ VRAM å‹å–„ç­–ç•¥** - 4bit é‡åŒ–ã€CPU offloadã€device mapping å®Œæ•´å¯¦ä½œ\n",
    "- **åƒæ•¸èª¿å„ªå¯¦é©—å®¤** - ç³»çµ±åŒ–æ¸¬è©¦ temperatureã€top-kã€top-pã€repetition penalty å½±éŸ¿\n",
    "- **å¤šå ´æ™¯æ‡‰ç”¨å±•ç¤º** - ç¨‹å¼ç¢¼ç”Ÿæˆã€å‰µæ„å¯«ä½œã€ç¿»è­¯ã€æ‘˜è¦ç­‰å¯¦ç”¨æ¡ˆä¾‹\n",
    "- **æ€§èƒ½åŸºæº–æ¸¬è©¦** - é€Ÿåº¦ã€è¨˜æ†¶é«”ã€å“è³ªçš„æ¨™æº–åŒ–è©•ä¼°æµç¨‹\n",
    "\n",
    "### ğŸ§  æ ¸å¿ƒåŸç†è¦é» (Core Concepts)\n",
    "- **Transformer è§£ç¢¼ç­–ç•¥**: è²ªå©ªæœå°‹ vs æ¡æ¨£ vs Beam Search çš„é©ç”¨æ™‚æ©Ÿ\n",
    "- **é‡åŒ–æŠ€è¡“åŸç†**: BitsAndBytesConfig å¦‚ä½•å¯¦ç¾ 75% è¨˜æ†¶é«”ç¯€çœ\n",
    "- **ç”Ÿæˆå“è³ªæ§åˆ¶**: å‰µæ„æ€§ (temperature) èˆ‡ä¸€è‡´æ€§ (repetition penalty) çš„å¹³è¡¡\n",
    "- **è¨˜æ†¶é«”ç®¡ç†**: VRAM ç›£æ§ã€CPU offloadã€æ¢¯åº¦æª¢æŸ¥é»çš„çµ„åˆç­–ç•¥\n",
    "- **æ¨¡å‹ç‰¹æ€§åˆ†æ**: ä¸åŒæ¨¡å‹ç³»åˆ—çš„å„ªå‹¢èˆ‡é©ç”¨å ´æ™¯è­˜åˆ¥\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å»ºè­° (Next Steps)\n",
    "1. **å¦‚æœæƒ³æ·±å…¥å¾®èª¿æŠ€è¡“** â†’ å„ªå…ˆå­¸ç¿’ `nb11_instruction_tuning_demo.ipynb`\n",
    "2. **å¦‚æœæƒ³å»ºæ§‹å¯¦ç”¨æ‡‰ç”¨** â†’ å„ªå…ˆå­¸ç¿’ `nb13_function_calling_tools.ipynb`  \n",
    "3. **å¦‚æœæƒ³ç³»çµ±æ€§è©•ä¼°** â†’ å„ªå…ˆå­¸ç¿’ `nb12_llm_evaluation_metrics.ipynb`\n",
    "4. **å¦‚æœæƒ³æ•´åˆæª¢ç´¢ç³»çµ±** â†’ å„ªå…ˆå­¸ç¿’ `nb26_rag_basic_faiss.ipynb`\n",
    "\n",
    "**æŠ€è¡“æ·±åŒ–æ–¹å‘**: æ¢ç´¢ KV cache å„ªåŒ–ã€æ¨¡å‹ä¸¦è¡Œæ¨ç†ã€é•·æ–‡æœ¬è™•ç†æŠ€è¡“\n",
    "**æ‡‰ç”¨æ‹“å±•æ–¹å‘**: ç‰¹å®šé ˜åŸŸèŠå¤©æ©Ÿå™¨äººã€ç¨‹å¼ç¢¼åŠ©æ‰‹ã€å¤šèªè¨€ç¿»è­¯ç³»çµ±\n",
    "\n",
    "---\n",
    "\n",
    "é€™å€‹ notebook ç‚ºä½ å»ºç«‹äº†å …å¯¦çš„ LLM æ‡‰ç”¨åŸºç¤ï¼Œæ¶µè“‹äº†å¾æ¨¡å‹è¼‰å…¥åˆ°æ€§èƒ½å„ªåŒ–çš„å®Œæ•´æµç¨‹ã€‚ä½ ç¾åœ¨å…·å‚™äº†é¸æ“‡é©åˆçš„æ¨¡å‹ã€èª¿å„ªç”Ÿæˆåƒæ•¸ã€è™•ç†è¨˜æ†¶é«”é™åˆ¶çš„å¯¦æˆ°èƒ½åŠ›ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
