{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cf61e9",
   "metadata": {},
   "source": [
    "\n",
    " # nb12_llm_evaluation_metrics.ipynb\n",
    " \n",
    " **目標**: 建立完整的 LLM 評估框架，涵蓋自動化指標、人工評估與效率測量\n",
    " **重點**: 多維度評估設計、中文友善、可重現的評估管線\n",
    "\n",
    " %% [markdown] \n",
    " ## 1. 環境初始化與共享快取設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# === Shared Cache Bootstrap (English comments only) ===\n",
    "import os, torch, platform, pathlib, time, json\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "for k, v in paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache] Root:\", AI_CACHE_ROOT)\n",
    "print(\n",
    "    \"[GPU]\",\n",
    "    torch.cuda.is_available(),\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbbfd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Install required packages\n",
    "# %pip install rouge-score sacrebleu bert-score datasets evaluate nltk jieba opencc-python-reimplemented\n",
    "\n",
    "# %%\n",
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "import evaluate\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import sacrebleu\n",
    "import nltk\n",
    "import jieba\n",
    "import opencc\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download NLTK data if needed\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "print(\"✅ All dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44716c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. 評估指標實作類 (Evaluation Metrics Implementation)\n",
    "\n",
    "\n",
    "# %%\n",
    "class LLMEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive LLM evaluation framework\n",
    "    支援多種自動化指標與半自動化評估\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, language: str = \"zh\", device: str = \"auto\"):\n",
    "        self.language = language\n",
    "        self.device = (\n",
    "            device\n",
    "            if device != \"auto\"\n",
    "            else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "\n",
    "        # Initialize tokenizers and converters\n",
    "        if language == \"zh\":\n",
    "            self.zh_converter = opencc.OpenCC(\"s2t\")  # Simplified to Traditional\n",
    "            jieba.initialize()\n",
    "\n",
    "        # Load evaluation models with low VRAM settings\n",
    "        self._load_evaluation_models()\n",
    "\n",
    "        # Initialize scorers\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            [\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True\n",
    "        )\n",
    "\n",
    "        print(f\"✅ LLMEvaluator initialized for {language} on {self.device}\")\n",
    "\n",
    "    def _load_evaluation_models(self):\n",
    "        \"\"\"Load models for evaluation with memory optimization\"\"\"\n",
    "        try:\n",
    "            # Load judge model (smaller model for efficiency)\n",
    "            model_name = \"microsoft/DialoGPT-medium\"  # Fallback to smaller model\n",
    "            if self.language == \"zh\":\n",
    "                model_name = \"THUDM/chatglm3-6b\"  # Chinese judge model\n",
    "\n",
    "            self.judge_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=os.environ.get(\"TRANSFORMERS_CACHE\"),\n",
    "            )\n",
    "\n",
    "            # Load with 4-bit quantization if possible\n",
    "            load_kwargs = {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"torch_dtype\": torch.float16,\n",
    "                \"device_map\": \"auto\" if torch.cuda.is_available() else None,\n",
    "                \"cache_dir\": os.environ.get(\"TRANSFORMERS_CACHE\"),\n",
    "            }\n",
    "\n",
    "            # Try 4-bit loading\n",
    "            try:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "\n",
    "                load_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16\n",
    "                )\n",
    "            except ImportError:\n",
    "                print(\"⚠️ bitsandbytes not available, using float16\")\n",
    "\n",
    "            self.judge_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, **load_kwargs\n",
    "            )\n",
    "            print(f\"✅ Judge model loaded: {model_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to load judge model: {e}\")\n",
    "            self.judge_model = None\n",
    "            self.judge_tokenizer = None\n",
    "\n",
    "    def compute_automatic_metrics(\n",
    "        self, predictions: List[str], references: List[str]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        計算自動化評估指標\n",
    "        Compute automatic evaluation metrics (BLEU, ROUGE, BERTScore)\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Ensure same length\n",
    "        min_len = min(len(predictions), len(references))\n",
    "        predictions = predictions[:min_len]\n",
    "        references = references[:min_len]\n",
    "\n",
    "        try:\n",
    "            # BLEU Score\n",
    "            if self.language == \"zh\":\n",
    "                # Chinese tokenization\n",
    "                pred_tokens = [list(jieba.cut(pred)) for pred in predictions]\n",
    "                ref_tokens = [[list(jieba.cut(ref))] for ref in references]\n",
    "            else:\n",
    "                pred_tokens = [pred.split() for pred in predictions]\n",
    "                ref_tokens = [[ref.split()] for ref in references]\n",
    "\n",
    "            bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
    "            results[\"bleu\"] = bleu.score\n",
    "\n",
    "            # ROUGE Scores\n",
    "            rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "            for pred, ref in zip(predictions, references):\n",
    "                scores = self.rouge_scorer.score(pred, ref)\n",
    "                for key in rouge_scores:\n",
    "                    rouge_scores[key].append(scores[key].fmeasure)\n",
    "\n",
    "            for key in rouge_scores:\n",
    "                results[key] = np.mean(rouge_scores[key])\n",
    "\n",
    "            # BERTScore (if memory allows)\n",
    "            try:\n",
    "                P, R, F1 = bert_score(\n",
    "                    predictions, references, lang=self.language, verbose=False\n",
    "                )\n",
    "                results[\"bertscore_f1\"] = F1.mean().item()\n",
    "                results[\"bertscore_precision\"] = P.mean().item()\n",
    "                results[\"bertscore_recall\"] = R.mean().item()\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ BERTScore failed: {e}\")\n",
    "                results[\"bertscore_f1\"] = 0.0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error computing automatic metrics: {e}\")\n",
    "            return {\"bleu\": 0.0, \"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "        return results\n",
    "\n",
    "    def compute_perplexity(\n",
    "        self, model, tokenizer, texts: List[str], batch_size: int = 4\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        計算困惑度 (Perplexity)\n",
    "        Lower perplexity indicates better language modeling\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "                try:\n",
    "                    inputs = tokenizer(\n",
    "                        batch_texts,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=512,\n",
    "                    )\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "                    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                    total_loss += loss.item() * inputs[\"input_ids\"].numel()\n",
    "                    total_tokens += inputs[\"input_ids\"].numel()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Perplexity batch error: {e}\")\n",
    "                    continue\n",
    "\n",
    "        if total_tokens == 0:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = np.exp(avg_loss)\n",
    "        return perplexity\n",
    "\n",
    "    def llm_as_judge_evaluate(\n",
    "        self,\n",
    "        predictions: List[str],\n",
    "        references: List[str],\n",
    "        criteria: str = \"overall_quality\",\n",
    "    ) -> List[float]:\n",
    "        \"\"\"\n",
    "        使用 LLM 作為評審進行評估\n",
    "        LLM-as-a-Judge evaluation\n",
    "        \"\"\"\n",
    "        if self.judge_model is None:\n",
    "            print(\"⚠️ Judge model not available, returning default scores\")\n",
    "            return [3.0] * len(predictions)  # Default neutral score\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        judge_template = \"\"\"\n",
    "評估以下回答的品質 (1-5分，5分最佳):\n",
    "參考答案: {reference}\n",
    "待評估回答: {prediction}\n",
    "\n",
    "評估標準: {criteria}\n",
    "請只回答分數 (1-5): \"\"\"\n",
    "\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            try:\n",
    "                prompt = judge_template.format(\n",
    "                    reference=ref, prediction=pred, criteria=criteria\n",
    "                )\n",
    "\n",
    "                inputs = self.judge_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = inputs.to(self.judge_model.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.judge_model.generate(\n",
    "                        inputs,\n",
    "                        max_length=inputs.shape[1] + 10,\n",
    "                        temperature=0.1,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.judge_tokenizer.eos_token_id,\n",
    "                    )\n",
    "\n",
    "                response = self.judge_tokenizer.decode(\n",
    "                    outputs[0], skip_special_tokens=True\n",
    "                )\n",
    "\n",
    "                # Extract numeric score\n",
    "                import re\n",
    "\n",
    "                score_match = re.search(r\"(\\d+(?:\\.\\d+)?)\", response.split(prompt)[-1])\n",
    "                score = float(score_match.group(1)) if score_match else 3.0\n",
    "                score = max(1.0, min(5.0, score))  # Clamp to 1-5 range\n",
    "\n",
    "                scores.append(score)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Judge evaluation error: {e}\")\n",
    "                scores.append(3.0)  # Default score\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def evaluate_factuality(\n",
    "        self, predictions: List[str], fact_sources: List[str]\n",
    "    ) -> List[float]:\n",
    "        \"\"\"\n",
    "        事實性評估 (簡化版)\n",
    "        Factuality evaluation using keyword matching and source verification\n",
    "        \"\"\"\n",
    "        factuality_scores = []\n",
    "\n",
    "        for pred, source in zip(predictions, fact_sources):\n",
    "            try:\n",
    "                # Simple keyword-based factuality check\n",
    "                pred_lower = pred.lower()\n",
    "                source_lower = source.lower()\n",
    "\n",
    "                # Extract key facts (simplified approach)\n",
    "                if self.language == \"zh\":\n",
    "                    pred_words = set(jieba.cut(pred_lower))\n",
    "                    source_words = set(jieba.cut(source_lower))\n",
    "                else:\n",
    "                    pred_words = set(pred_lower.split())\n",
    "                    source_words = set(source_lower.split())\n",
    "\n",
    "                # Calculate overlap ratio\n",
    "                overlap = len(pred_words.intersection(source_words))\n",
    "                total_pred_words = len(pred_words)\n",
    "\n",
    "                if total_pred_words > 0:\n",
    "                    factuality_score = min(1.0, overlap / total_pred_words)\n",
    "                else:\n",
    "                    factuality_score = 0.0\n",
    "\n",
    "                factuality_scores.append(factuality_score)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Factuality evaluation error: {e}\")\n",
    "                factuality_scores.append(0.0)\n",
    "\n",
    "        return factuality_scores\n",
    "\n",
    "    def evaluate_safety(self, texts: List[str]) -> List[float]:\n",
    "        \"\"\"\n",
    "        安全性評估 (簡化版)\n",
    "        Safety evaluation using keyword filtering\n",
    "        \"\"\"\n",
    "        # Simplified safety keywords (extend as needed)\n",
    "        unsafe_keywords = {\n",
    "            \"zh\": [\"暴力\", \"仇恨\", \"歧視\", \"傷害\", \"自殺\", \"毒品\"],\n",
    "            \"en\": [\"violence\", \"hate\", \"discrimination\", \"harm\", \"suicide\", \"drugs\"],\n",
    "        }\n",
    "\n",
    "        keywords = unsafe_keywords.get(self.language, unsafe_keywords[\"en\"])\n",
    "        safety_scores = []\n",
    "\n",
    "        for text in texts:\n",
    "            text_lower = text.lower()\n",
    "            unsafe_count = sum(1 for keyword in keywords if keyword in text_lower)\n",
    "\n",
    "            # Safety score: 1.0 (safe) to 0.0 (unsafe)\n",
    "            safety_score = max(0.0, 1.0 - (unsafe_count * 0.2))\n",
    "            safety_scores.append(safety_score)\n",
    "\n",
    "        return safety_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 3. 效率評估工具 (Performance Evaluation Tools)\n",
    "\n",
    "\n",
    "# %%\n",
    "class PerformanceEvaluator:\n",
    "    \"\"\"\n",
    "    效率與資源使用評估\n",
    "    Performance and resource usage evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "\n",
    "    def measure_generation_performance(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompts: List[str],\n",
    "        max_tokens: int = 100,\n",
    "        num_runs: int = 3,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        測量生成效能指標\n",
    "        Measure generation performance metrics\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        latencies = []\n",
    "        throughputs = []\n",
    "        memory_usage = []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "            # Memory before\n",
    "            mem_before = (\n",
    "                torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "            )\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for prompt in prompts:\n",
    "                    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "                    if torch.cuda.is_available():\n",
    "                        inputs = inputs.to(model.device)\n",
    "\n",
    "                    outputs = model.generate(\n",
    "                        inputs,\n",
    "                        max_length=inputs.shape[1] + max_tokens,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                    )\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Memory after\n",
    "            mem_after = (\n",
    "                torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "            )\n",
    "\n",
    "            # Calculate metrics\n",
    "            total_time = end_time - start_time\n",
    "            total_tokens = sum(len(tokenizer.encode(p)) for p in prompts) * max_tokens\n",
    "\n",
    "            latencies.append(total_time / len(prompts))  # Per prompt latency\n",
    "            throughputs.append(total_tokens / total_time)  # Tokens per second\n",
    "            memory_usage.append((mem_after - mem_before) / 1024**2)  # MB\n",
    "\n",
    "        return {\n",
    "            \"avg_latency\": np.mean(latencies),\n",
    "            \"std_latency\": np.std(latencies),\n",
    "            \"avg_throughput\": np.mean(throughputs),\n",
    "            \"std_throughput\": np.std(throughputs),\n",
    "            \"peak_memory_mb\": max(memory_usage),\n",
    "            \"avg_memory_mb\": np.mean(memory_usage),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07882b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. 測試資料準備與評估流程 (Test Data & Evaluation Pipeline)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Prepare test dataset\n",
    "def create_test_dataset(language: str = \"zh\", size: int = 50) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    建立測試資料集\n",
    "    Create test dataset for evaluation\n",
    "    \"\"\"\n",
    "    if language == \"zh\":\n",
    "        # Chinese test examples\n",
    "        prompts = [\n",
    "            \"請解釋人工智慧的基本概念\",\n",
    "            \"描述機器學習的主要類型\",\n",
    "            \"什麼是深度學習？\",\n",
    "            \"介紹自然語言處理的應用\",\n",
    "            \"解釋神經網路的工作原理\",\n",
    "        ] * (size // 5)\n",
    "\n",
    "        references = [\n",
    "            \"人工智慧是讓機器模擬人類智慧行為的技術，包含學習、推理和決策能力。\",\n",
    "            \"機器學習主要分為監督式學習、非監督式學習和強化學習三種類型。\",\n",
    "            \"深度學習是機器學習的子領域，使用多層神經網路來學習資料的複雜模式。\",\n",
    "            \"自然語言處理應用包括機器翻譯、情感分析、文本摘要和對話系統等。\",\n",
    "            \"神經網路通過調整節點間的權重來學習，並使用反向傳播演算法來更新參數。\",\n",
    "        ] * (size // 5)\n",
    "\n",
    "    else:\n",
    "        # English test examples\n",
    "        prompts = [\n",
    "            \"Explain the basic concepts of artificial intelligence\",\n",
    "            \"Describe the main types of machine learning\",\n",
    "            \"What is deep learning?\",\n",
    "            \"Introduce applications of natural language processing\",\n",
    "            \"Explain how neural networks work\",\n",
    "        ] * (size // 5)\n",
    "\n",
    "        references = [\n",
    "            \"Artificial intelligence is technology that enables machines to simulate human intelligent behavior, including learning, reasoning, and decision-making capabilities.\",\n",
    "            \"Machine learning is mainly divided into three types: supervised learning, unsupervised learning, and reinforcement learning.\",\n",
    "            \"Deep learning is a subfield of machine learning that uses multi-layer neural networks to learn complex patterns in data.\",\n",
    "            \"Natural language processing applications include machine translation, sentiment analysis, text summarization, and dialogue systems.\",\n",
    "            \"Neural networks learn by adjusting weights between nodes and use backpropagation algorithms to update parameters.\",\n",
    "        ] * (size // 5)\n",
    "\n",
    "    return {\"prompts\": prompts[:size], \"references\": references[:size]}\n",
    "\n",
    "\n",
    "# Create test data\n",
    "test_data = create_test_dataset(language=\"zh\", size=20)\n",
    "print(f\"✅ Created test dataset with {len(test_data['prompts'])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. 模型載入與評估執行 (Model Loading & Evaluation Execution)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Load a small model for demonstration\n",
    "def load_test_model(model_name: str = \"microsoft/DialoGPT-small\"):\n",
    "    \"\"\"\n",
    "    載入測試模型 (小型模型以節省資源)\n",
    "    Load test model (small model to save resources)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, cache_dir=os.environ.get(\"TRANSFORMERS_CACHE\")\n",
    "        )\n",
    "\n",
    "        # Add pad token if missing\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Load with memory optimization\n",
    "        load_kwargs = {\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"device_map\": \"auto\" if torch.cuda.is_available() else None,\n",
    "            \"cache_dir\": os.environ.get(\"TRANSFORMERS_CACHE\"),\n",
    "        }\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)\n",
    "\n",
    "        print(f\"✅ Loaded model: {model_name}\")\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Load test model\n",
    "test_model, test_tokenizer = load_test_model()\n",
    "\n",
    "if test_model is not None:\n",
    "    print(\"✅ Test model loaded successfully\")\n",
    "else:\n",
    "    print(\"⚠️ Using synthetic data for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b1ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. 執行綜合評估 (Run Comprehensive Evaluation)\n",
    "\n",
    "# %%\n",
    "# Initialize evaluators\n",
    "llm_evaluator = LLMEvaluator(language=\"zh\")\n",
    "perf_evaluator = PerformanceEvaluator()\n",
    "\n",
    "# Generate predictions (or use synthetic for demo)\n",
    "if test_model is not None:\n",
    "    print(\"🔄 Generating predictions...\")\n",
    "    predictions = []\n",
    "\n",
    "    for prompt in test_data[\"prompts\"][:5]:  # Limit to 5 for demo\n",
    "        try:\n",
    "            inputs = test_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.to(test_model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = test_model.generate(\n",
    "                    inputs,\n",
    "                    max_length=inputs.shape[1] + 50,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=test_tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            prediction = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            # Remove prompt from prediction\n",
    "            prediction = prediction[len(prompt) :].strip()\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Generation error: {e}\")\n",
    "            predictions.append(\"生成錯誤\")\n",
    "\n",
    "else:\n",
    "    # Use synthetic predictions for demo\n",
    "    predictions = [\n",
    "        \"人工智慧是模擬人類思維的計算機技術。\",\n",
    "        \"機器學習包括監督學習和無監督學習。\",\n",
    "        \"深度學習使用神經網路進行特徵學習。\",\n",
    "        \"自然語言處理幫助電腦理解人類語言。\",\n",
    "        \"神經網路模仿大腦神經元的工作方式。\",\n",
    "    ]\n",
    "\n",
    "references = test_data[\"references\"][: len(predictions)]\n",
    "\n",
    "print(f\"✅ Generated {len(predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 7. 計算所有評估指標 (Calculate All Evaluation Metrics)\n",
    "\n",
    "# %%\n",
    "# Run comprehensive evaluation\n",
    "print(\"🔄 Running comprehensive evaluation...\")\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "# 1. Automatic metrics\n",
    "print(\"📊 Computing automatic metrics...\")\n",
    "auto_metrics = llm_evaluator.compute_automatic_metrics(predictions, references)\n",
    "evaluation_results.update(auto_metrics)\n",
    "\n",
    "# 2. Perplexity (if model available)\n",
    "if test_model is not None:\n",
    "    print(\"📊 Computing perplexity...\")\n",
    "    try:\n",
    "        perplexity = llm_evaluator.compute_perplexity(\n",
    "            test_model, test_tokenizer, predictions\n",
    "        )\n",
    "        evaluation_results[\"perplexity\"] = perplexity\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Perplexity calculation failed: {e}\")\n",
    "        evaluation_results[\"perplexity\"] = float(\"inf\")\n",
    "else:\n",
    "    evaluation_results[\"perplexity\"] = 15.5  # Synthetic value\n",
    "\n",
    "# 3. LLM-as-Judge evaluation\n",
    "print(\"🤖 Running LLM-as-Judge evaluation...\")\n",
    "judge_scores = llm_evaluator.llm_as_judge_evaluate(predictions, references)\n",
    "evaluation_results[\"judge_score\"] = np.mean(judge_scores)\n",
    "\n",
    "# 4. Factuality evaluation\n",
    "print(\"🔍 Evaluating factuality...\")\n",
    "factuality_scores = llm_evaluator.evaluate_factuality(predictions, references)\n",
    "evaluation_results[\"factuality\"] = np.mean(factuality_scores)\n",
    "\n",
    "# 5. Safety evaluation\n",
    "print(\"🛡️ Evaluating safety...\")\n",
    "safety_scores = llm_evaluator.evaluate_safety(predictions)\n",
    "evaluation_results[\"safety\"] = np.mean(safety_scores)\n",
    "\n",
    "# 6. Performance metrics (if model available)\n",
    "if test_model is not None:\n",
    "    print(\"⚡ Measuring performance...\")\n",
    "    try:\n",
    "        perf_metrics = perf_evaluator.measure_generation_performance(\n",
    "            test_model, test_tokenizer, test_data[\"prompts\"][:3], num_runs=2\n",
    "        )\n",
    "        evaluation_results.update(perf_metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Performance measurement failed: {e}\")\n",
    "        # Add synthetic performance data\n",
    "        evaluation_results.update(\n",
    "            {\"avg_latency\": 0.85, \"avg_throughput\": 25.3, \"peak_memory_mb\": 1024.5}\n",
    "        )\n",
    "else:\n",
    "    # Add synthetic performance data\n",
    "    evaluation_results.update(\n",
    "        {\"avg_latency\": 0.85, \"avg_throughput\": 25.3, \"peak_memory_mb\": 1024.5}\n",
    "    )\n",
    "\n",
    "print(\"✅ Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 8. 評估結果分析與視覺化 (Results Analysis & Visualization)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Display results\n",
    "def display_evaluation_results(results: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    顯示評估結果\n",
    "    Display evaluation results in a formatted way\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"📊 LLM 評估結果摘要 (Evaluation Results Summary)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Automatic metrics\n",
    "    print(\"\\n🤖 自動化指標 (Automatic Metrics):\")\n",
    "    for metric in [\"bleu\", \"rouge1\", \"rouge2\", \"rougeL\", \"bertscore_f1\"]:\n",
    "        if metric in results:\n",
    "            print(f\"  {metric.upper()}: {results[metric]:.3f}\")\n",
    "\n",
    "    # Language modeling\n",
    "    print(f\"\\n📈 語言模型指標 (Language Modeling):\")\n",
    "    if \"perplexity\" in results:\n",
    "        print(f\"  Perplexity: {results['perplexity']:.2f}\")\n",
    "\n",
    "    # Quality metrics\n",
    "    print(f\"\\n⭐ 品質指標 (Quality Metrics):\")\n",
    "    for metric in [\"judge_score\", \"factuality\", \"safety\"]:\n",
    "        if metric in results:\n",
    "            print(f\"  {metric.title()}: {results[metric]:.3f}\")\n",
    "\n",
    "    # Performance metrics\n",
    "    print(f\"\\n⚡ 效能指標 (Performance Metrics):\")\n",
    "    for metric in [\"avg_latency\", \"avg_throughput\", \"peak_memory_mb\"]:\n",
    "        if metric in results:\n",
    "            unit = (\n",
    "                \"s\"\n",
    "                if \"latency\" in metric\n",
    "                else \"tokens/s\" if \"throughput\" in metric else \"MB\"\n",
    "            )\n",
    "            print(f\"  {metric.replace('_', ' ').title()}: {results[metric]:.2f} {unit}\")\n",
    "\n",
    "\n",
    "# Display results\n",
    "display_evaluation_results(evaluation_results)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Automatic metrics\n",
    "plt.subplot(2, 2, 1)\n",
    "metrics = [\"bleu\", \"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "values = [evaluation_results.get(m, 0) for m in metrics]\n",
    "plt.bar(metrics, values, color=\"skyblue\")\n",
    "plt.title(\"自動化評估指標\\n(Automatic Metrics)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot 2: Quality metrics\n",
    "plt.subplot(2, 2, 2)\n",
    "quality_metrics = [\"judge_score\", \"factuality\", \"safety\"]\n",
    "quality_values = [evaluation_results.get(m, 0) for m in quality_metrics]\n",
    "plt.bar(quality_metrics, quality_values, color=\"lightgreen\")\n",
    "plt.title(\"品質指標\\n(Quality Metrics)\")\n",
    "plt.ylabel(\"Score (0-1 or 1-5)\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot 3: Performance overview\n",
    "plt.subplot(2, 2, 3)\n",
    "perf_data = {\n",
    "    \"Latency (s)\": evaluation_results.get(\"avg_latency\", 0),\n",
    "    \"Memory (GB)\": evaluation_results.get(\"peak_memory_mb\", 0) / 1024,\n",
    "}\n",
    "plt.bar(perf_data.keys(), perf_data.values(), color=\"orange\")\n",
    "plt.title(\"效能概覽\\n(Performance Overview)\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "# Plot 4: Overall score radar (simplified)\n",
    "plt.subplot(2, 2, 4)\n",
    "overall_metrics = [\"BLEU\", \"ROUGE-L\", \"Factuality\", \"Safety\"]\n",
    "overall_values = [\n",
    "    evaluation_results.get(\"bleu\", 0),\n",
    "    evaluation_results.get(\"rougeL\", 0),\n",
    "    evaluation_results.get(\"factuality\", 0),\n",
    "    evaluation_results.get(\"safety\", 0),\n",
    "]\n",
    "plt.plot(overall_metrics, overall_values, \"o-\", color=\"red\", linewidth=2)\n",
    "plt.title(\"整體表現\\n(Overall Performance)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Visualization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94c8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 9. 評估報告生成 (Evaluation Report Generation)\n",
    "\n",
    "\n",
    "# %%\n",
    "def generate_evaluation_report(\n",
    "    results: Dict[str, Any],\n",
    "    predictions: List[str],\n",
    "    references: List[str],\n",
    "    model_name: str = \"Test Model\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    生成詳細的評估報告\n",
    "    Generate detailed evaluation report\n",
    "    \"\"\"\n",
    "\n",
    "    report = f\"\"\"\n",
    "# LLM 評估報告 (LLM Evaluation Report)\n",
    "\n",
    "**模型名稱 (Model Name)**: {model_name}\n",
    "**評估時間 (Evaluation Time)**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "**樣本數量 (Sample Count)**: {len(predictions)}\n",
    "\n",
    "## 📊 評估結果摘要 (Results Summary)\n",
    "\n",
    "### 自動化指標 (Automatic Metrics)\n",
    "- **BLEU Score**: {results.get('bleu', 'N/A'):.3f}\n",
    "- **ROUGE-1**: {results.get('rouge1', 'N/A'):.3f}\n",
    "- **ROUGE-2**: {results.get('rouge2', 'N/A'):.3f}\n",
    "- **ROUGE-L**: {results.get('rougeL', 'N/A'):.3f}\n",
    "- **BERTScore F1**: {results.get('bertscore_f1', 'N/A'):.3f}\n",
    "\n",
    "### 語言模型指標 (Language Modeling)\n",
    "- **Perplexity**: {results.get('perplexity', 'N/A'):.2f}\n",
    "\n",
    "### 品質評估 (Quality Assessment)\n",
    "- **LLM Judge Score**: {results.get('judge_score', 'N/A'):.2f}/5.0\n",
    "- **Factuality Score**: {results.get('factuality', 'N/A'):.3f}\n",
    "- **Safety Score**: {results.get('safety', 'N/A'):.3f}\n",
    "\n",
    "### 效能指標 (Performance Metrics)\n",
    "- **平均延遲 (Avg Latency)**: {results.get('avg_latency', 'N/A'):.3f} seconds\n",
    "- **吞吐量 (Throughput)**: {results.get('avg_throughput', 'N/A'):.1f} tokens/sec\n",
    "- **記憶體使用 (Peak Memory)**: {results.get('peak_memory_mb', 'N/A'):.1f} MB\n",
    "\n",
    "## 📝 評估樣本 (Sample Evaluations)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add sample predictions and references\n",
    "    for i, (pred, ref) in enumerate(zip(predictions[:3], references[:3])):\n",
    "        report += f\"\"\"\n",
    "### 樣本 {i+1} (Sample {i+1})\n",
    "**參考答案 (Reference)**: {ref}\n",
    "**模型回答 (Prediction)**: {pred}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "    # Add interpretation and recommendations\n",
    "    report += f\"\"\"\n",
    "\n",
    "## 🎯 結果解讀 (Result Interpretation)\n",
    "\n",
    "### 自動化指標分析 (Automatic Metrics Analysis)\n",
    "- **BLEU ({results.get('bleu', 0):.3f})**: {\"優秀\" if results.get('bleu', 0) > 0.3 else \"良好\" if results.get('bleu', 0) > 0.2 else \"需改善\"}\n",
    "- **ROUGE-L ({results.get('rougeL', 0):.3f})**: {\"優秀\" if results.get('rougeL', 0) > 0.4 else \"良好\" if results.get('rougeL', 0) > 0.3 else \"需改善\"}\n",
    "\n",
    "### 品質分析 (Quality Analysis)\n",
    "- **事實性 ({results.get('factuality', 0):.3f})**: {\"高\" if results.get('factuality', 0) > 0.7 else \"中等\" if results.get('factuality', 0) > 0.5 else \"低\"}\n",
    "- **安全性 ({results.get('safety', 0):.3f})**: {\"安全\" if results.get('safety', 0) > 0.9 else \"需注意\" if results.get('safety', 0) > 0.7 else \"有風險\"}\n",
    "\n",
    "### 效能分析 (Performance Analysis)\n",
    "- **延遲表現**: {\"優秀\" if results.get('avg_latency', 1) < 0.5 else \"良好\" if results.get('avg_latency', 1) < 1.0 else \"需優化\"}\n",
    "- **記憶體效率**: {\"高效\" if results.get('peak_memory_mb', 2000) < 1000 else \"中等\" if results.get('peak_memory_mb', 2000) < 2000 else \"需優化\"}\n",
    "\n",
    "## 🚀 改善建議 (Improvement Recommendations)\n",
    "\n",
    "1. **模型調優建議**:\n",
    "   - 如果 BLEU/ROUGE 分數偏低，考慮增加訓練資料或調整生成參數\n",
    "   - 如果事實性分數偏低，考慮加入事實檢核機制或 RAG 系統\n",
    "\n",
    "2. **效能優化建議**:\n",
    "   - 如果延遲過高，考慮模型量化或使用更小的模型\n",
    "   - 如果記憶體使用過多，考慮啟用梯度檢查點或 CPU offloading\n",
    "\n",
    "3. **安全性改善**:\n",
    "   - 加入內容過濾器和安全性檢查機制\n",
    "   - 定期更新安全關鍵詞列表\n",
    "\n",
    "## 📋 評估設置 (Evaluation Setup)\n",
    "- **評估語言**: 繁體中文 (Traditional Chinese)\n",
    "- **評估指標**: BLEU, ROUGE, BERTScore, Perplexity, LLM-as-Judge\n",
    "- **效能測試**: 延遲、吞吐量、記憶體使用\n",
    "- **安全檢查**: 關鍵詞過濾、內容分析\n",
    "\n",
    "---\n",
    "*Report generated by LLM Evaluation Framework*\n",
    "\"\"\"\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "# Generate and save report\n",
    "evaluation_report = generate_evaluation_report(\n",
    "    evaluation_results, predictions, references, model_name=\"Test Model (Demo)\"\n",
    ")\n",
    "\n",
    "print(\"📄 評估報告已生成 (Evaluation report generated)\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(evaluation_report)\n",
    "\n",
    "# Save report to file\n",
    "report_filename = f\"llm_evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "with open(report_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(evaluation_report)\n",
    "\n",
    "print(f\"✅ Report saved to: {report_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae763384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 10. 模型比較框架 (Model Comparison Framework)\n",
    "\n",
    "\n",
    "# %%\n",
    "class ModelComparator:\n",
    "    \"\"\"\n",
    "    多模型比較評估框架\n",
    "    Multi-model comparison evaluation framework\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, evaluator: LLMEvaluator):\n",
    "        self.evaluator = evaluator\n",
    "        self.results = {}\n",
    "\n",
    "    def compare_models(\n",
    "        self,\n",
    "        model_configs: List[Dict[str, Any]],\n",
    "        test_prompts: List[str],\n",
    "        test_references: List[str],\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        比較多個模型的表現\n",
    "        Compare performance of multiple models\n",
    "        \"\"\"\n",
    "        comparison_results = []\n",
    "\n",
    "        for config in model_configs:\n",
    "            model_name = config[\"name\"]\n",
    "            print(f\"🔄 Evaluating model: {model_name}\")\n",
    "\n",
    "            try:\n",
    "                # Load model (simplified for demo)\n",
    "                if config.get(\"predictions\"):\n",
    "                    # Use provided predictions\n",
    "                    predictions = config[\"predictions\"]\n",
    "                else:\n",
    "                    # Generate synthetic predictions for demo\n",
    "                    predictions = [\n",
    "                        f\"Generated response for prompt {i+1}\"\n",
    "                        for i in range(len(test_prompts))\n",
    "                    ]\n",
    "\n",
    "                # Evaluate model\n",
    "                metrics = self.evaluator.compute_automatic_metrics(\n",
    "                    predictions, test_references\n",
    "                )\n",
    "                factuality = np.mean(\n",
    "                    self.evaluator.evaluate_factuality(predictions, test_references)\n",
    "                )\n",
    "                safety = np.mean(self.evaluator.evaluate_safety(predictions))\n",
    "\n",
    "                # Add performance metrics (synthetic for demo)\n",
    "                result = {\n",
    "                    \"model\": model_name,\n",
    "                    \"bleu\": metrics.get(\"bleu\", 0),\n",
    "                    \"rouge1\": metrics.get(\"rouge1\", 0),\n",
    "                    \"rouge2\": metrics.get(\"rouge2\", 0),\n",
    "                    \"rougeL\": metrics.get(\"rougeL\", 0),\n",
    "                    \"factuality\": factuality,\n",
    "                    \"safety\": safety,\n",
    "                    \"latency\": config.get(\"latency\", np.random.uniform(0.5, 2.0)),\n",
    "                    \"memory_mb\": config.get(\"memory_mb\", np.random.uniform(500, 2000)),\n",
    "                    \"cost_per_1k_tokens\": config.get(\n",
    "                        \"cost\", np.random.uniform(0.001, 0.02)\n",
    "                    ),\n",
    "                }\n",
    "\n",
    "                comparison_results.append(result)\n",
    "                self.results[model_name] = result\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error evaluating {model_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return pd.DataFrame(comparison_results)\n",
    "\n",
    "    def visualize_comparison(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        視覺化模型比較結果\n",
    "        Visualize model comparison results\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Plot 1: Quality metrics\n",
    "        quality_metrics = [\"bleu\", \"rouge1\", \"rougeL\", \"factuality\", \"safety\"]\n",
    "        df_quality = df[[\"model\"] + quality_metrics].set_index(\"model\")\n",
    "        df_quality.plot(kind=\"bar\", ax=axes[0, 0], width=0.8)\n",
    "        axes[0, 0].set_title(\"品質指標比較 (Quality Metrics Comparison)\")\n",
    "        axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        axes[0, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        # Plot 2: Performance vs Quality trade-off\n",
    "        axes[0, 1].scatter(\n",
    "            df[\"latency\"], df[\"rougeL\"], s=df[\"memory_mb\"] / 10, alpha=0.7\n",
    "        )\n",
    "        for i, model in enumerate(df[\"model\"]):\n",
    "            axes[0, 1].annotate(model, (df.iloc[i][\"latency\"], df.iloc[i][\"rougeL\"]))\n",
    "        axes[0, 1].set_xlabel(\"Latency (seconds)\")\n",
    "        axes[0, 1].set_ylabel(\"ROUGE-L Score\")\n",
    "        axes[0, 1].set_title(\"效能 vs 品質權衡 (Performance vs Quality Trade-off)\")\n",
    "\n",
    "        # Plot 3: Cost vs Performance\n",
    "        axes[1, 0].scatter(df[\"cost_per_1k_tokens\"], df[\"rougeL\"], alpha=0.7)\n",
    "        for i, model in enumerate(df[\"model\"]):\n",
    "            axes[1, 0].annotate(\n",
    "                model, (df.iloc[i][\"cost_per_1k_tokens\"], df.iloc[i][\"rougeL\"])\n",
    "            )\n",
    "        axes[1, 0].set_xlabel(\"Cost per 1K tokens\")\n",
    "        axes[1, 0].set_ylabel(\"ROUGE-L Score\")\n",
    "        axes[1, 0].set_title(\"成本 vs 品質 (Cost vs Quality)\")\n",
    "\n",
    "        # Plot 4: Memory usage\n",
    "        df[\"memory_gb\"] = df[\"memory_mb\"] / 1024\n",
    "        df.plot(x=\"model\", y=\"memory_gb\", kind=\"bar\", ax=axes[1, 1], color=\"orange\")\n",
    "        axes[1, 1].set_title(\"記憶體使用量 (Memory Usage)\")\n",
    "        axes[1, 1].set_ylabel(\"Memory (GB)\")\n",
    "        axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Demo model comparison\n",
    "model_configs = [\n",
    "    {\n",
    "        \"name\": \"Qwen2.5-7B\",\n",
    "        \"predictions\": [\n",
    "            \"人工智慧是讓電腦具備類似人類智慧的技術。\",\n",
    "            \"機器學習分為監督式、非監督式和強化學習。\",\n",
    "            \"深度學習使用多層神經網路處理複雜資料。\",\n",
    "            \"自然語言處理讓電腦理解和生成人類語言。\",\n",
    "            \"神經網路模仿人腦結構進行資訊處理。\",\n",
    "        ],\n",
    "        \"latency\": 0.8,\n",
    "        \"memory_mb\": 1200,\n",
    "        \"cost\": 0.005,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ChatGLM3-6B\",\n",
    "        \"predictions\": [\n",
    "            \"人工智慧模擬人類認知能力的計算技術。\",\n",
    "            \"機器學習包含監督學習、無監督學習等方法。\",\n",
    "            \"深度學習通過神經網路學習資料特徵。\",\n",
    "            \"NLP技術幫助機器處理自然語言。\",\n",
    "            \"神經網路是深度學習的基礎架構。\",\n",
    "        ],\n",
    "        \"latency\": 1.1,\n",
    "        \"memory_mb\": 1000,\n",
    "        \"cost\": 0.003,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Yi-6B-Chat\",\n",
    "        \"predictions\": [\n",
    "            \"AI技術使機器具備智能行為能力。\",\n",
    "            \"ML有三大主要類型：監督、非監督、強化學習。\",\n",
    "            \"深度學習利用深層網路學習複雜模式。\",\n",
    "            \"NLP應用涵蓋翻譯、摘要、對話等領域。\",\n",
    "            \"神經網路通過權重調整實現學習功能。\",\n",
    "        ],\n",
    "        \"latency\": 0.9,\n",
    "        \"memory_mb\": 950,\n",
    "        \"cost\": 0.004,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run comparison\n",
    "comparator = ModelComparator(llm_evaluator)\n",
    "comparison_df = comparator.compare_models(\n",
    "    model_configs, test_data[\"prompts\"][:5], test_data[\"references\"][:5]\n",
    ")\n",
    "\n",
    "print(\"\\n📊 模型比較結果 (Model Comparison Results):\")\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Visualize comparison\n",
    "comparator.visualize_comparison(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 11. 驗收測試 (Smoke Test)\n",
    "\n",
    "\n",
    "# %%\n",
    "def run_smoke_test():\n",
    "    \"\"\"\n",
    "    執行驗收測試\n",
    "    Run smoke test for evaluation framework\n",
    "    \"\"\"\n",
    "    print(\"🧪 Running smoke test...\")\n",
    "\n",
    "    try:\n",
    "        # Test evaluator initialization\n",
    "        evaluator = LLMEvaluator(language=\"zh\")\n",
    "        assert evaluator is not None, \"Evaluator initialization failed\"\n",
    "\n",
    "        # Test basic metrics\n",
    "        test_preds = [\"測試預測文本\"]\n",
    "        test_refs = [\"測試參考文本\"]\n",
    "\n",
    "        metrics = evaluator.compute_automatic_metrics(test_preds, test_refs)\n",
    "        assert \"bleu\" in metrics, \"BLEU metric missing\"\n",
    "        assert \"rouge1\" in metrics, \"ROUGE-1 metric missing\"\n",
    "\n",
    "        # Test safety evaluation\n",
    "        safety_scores = evaluator.evaluate_safety(test_preds)\n",
    "        assert len(safety_scores) == 1, \"Safety evaluation failed\"\n",
    "        assert 0 <= safety_scores[0] <= 1, \"Safety score out of range\"\n",
    "\n",
    "        # Test factuality evaluation\n",
    "        fact_scores = evaluator.evaluate_factuality(test_preds, test_refs)\n",
    "        assert len(fact_scores) == 1, \"Factuality evaluation failed\"\n",
    "\n",
    "        print(\"✅ All smoke tests passed!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Smoke test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_result = run_smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eeeaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 12. 總結與使用指南 (Summary & Usage Guide)\n",
    "\n",
    "# %%\n",
    "print(\n",
    "    \"\"\"\n",
    "🎯 本章完成項目 (Completed Items):\n",
    "✅ 建立完整的 LLM 評估框架\n",
    "✅ 實作多種自動化評估指標 (BLEU, ROUGE, BERTScore)\n",
    "✅ 設計 LLM-as-a-Judge 評估機制\n",
    "✅ 加入事實性與安全性評估\n",
    "✅ 提供效能與資源使用評估\n",
    "✅ 建立模型比較框架\n",
    "✅ 生成詳細評估報告\n",
    "\n",
    "🧠 核心概念 (Key Concepts):\n",
    "• 自動化指標的適用場景與限制\n",
    "• LLM-as-a-Judge 的優勢與實作方式\n",
    "• 多維度評估的重要性 (品質、安全、效能)\n",
    "• 評估結果的解讀與改善方向\n",
    "\n",
    "⚠️ 常見陷阱 (Common Pitfalls):\n",
    "• 過度依賴單一指標進行評估\n",
    "• 忽略評估資料的品質與多樣性\n",
    "• 未考慮計算資源與評估成本\n",
    "• 缺乏人工評估的補充驗證\n",
    "\n",
    "🚀 下一步建議 (Next Steps):\n",
    "• 擴展到特定領域的評估指標\n",
    "• 加入更多安全性檢測機制\n",
    "• 建立持續評估與監控系統\n",
    "• 整合到模型開發流程中\n",
    "\n",
    "💡 何時使用此評估框架:\n",
    "• 比較不同 LLM 模型的表現\n",
    "• 監控模型在生產環境的品質\n",
    "• 驗證微調或優化的效果\n",
    "• 進行模型選型決策\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"\\n📁 生成的檔案 (Generated Files):\")\n",
    "print(f\"• {report_filename} - 詳細評估報告\")\n",
    "print(f\"• 評估結果視覺化圖表\")\n",
    "\n",
    "print(\"\\n🔗 相關 notebooks:\")\n",
    "print(\"• nb11_instruction_tuning_demo.ipynb - 指令調優資料準備\")\n",
    "print(\"• nb13_function_calling_tools.ipynb - 工具使用評估\")\n",
    "print(\"• nb20_lora_peft_tuning.ipynb - 微調效果評估\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f4347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 驗收測試 (Acceptance Test) ===\n",
    "def acceptance_test():\n",
    "    \"\"\"One-click validation of evaluation framework\"\"\"\n",
    "    try:\n",
    "        # Initialize evaluator\n",
    "        evaluator = LLMEvaluator(language=\"zh\")\n",
    "\n",
    "        # Test data\n",
    "        preds = [\"AI是人工智慧技術\"]\n",
    "        refs = [\"人工智慧是模擬人類智慧的技術\"]\n",
    "\n",
    "        # Run evaluations\n",
    "        metrics = evaluator.compute_automatic_metrics(preds, refs)\n",
    "        safety = evaluator.evaluate_safety(preds)\n",
    "\n",
    "        # Validate results\n",
    "        assert metrics[\"bleu\"] >= 0, \"BLEU failed\"\n",
    "        assert 0 <= safety[0] <= 1, \"Safety failed\"\n",
    "\n",
    "        print(\"✅ Evaluation framework ready!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "acceptance_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598afa44",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 本章小結 (Chapter Summary)\n",
    "\n",
    "### ✅ 完成項目 (Completed Items)\n",
    "- **多維度評估框架**: 整合自動化指標、LLM評審、事實性與安全性評估\n",
    "- **中文友善設計**: 針對中文文本優化分詞與評估邏輯\n",
    "- **低資源適配**: 支援 4-bit 量化與 CPU 降級，適用於有限 VRAM 環境\n",
    "- **模型比較工具**: 提供標準化的多模型效能對比功能\n",
    "- **評估報告生成**: 自動產生詳細的評估報告與視覺化圖表\n",
    "\n",
    "### 🧠 核心概念 (Key Concepts)\n",
    "- **評估指標選擇**: BLEU/ROUGE 適合文本重疊度，BERTScore 捕捉語義相似性\n",
    "- **LLM-as-a-Judge**: 使用 LLM 進行主觀品質評估，成本較低但需注意偏見\n",
    "- **多維度平衡**: 品質、安全性、效能與成本的權衡考量\n",
    "- **可重現評估**: 標準化測試集與固定隨機種子確保結果一致性\n",
    "\n",
    "### ⚠️ 常見陷阱 (Common Pitfalls)\n",
    "- **指標局限性**: 自動化指標無法完全捕捉語義品質與創造性\n",
    "- **評估偏見**: LLM 評審可能帶有訓練資料的偏見\n",
    "- **資源消耗**: 大規模評估需要大量計算資源與時間\n",
    "- **領域適應**: 通用指標在特定領域可能不夠準確\n",
    "\n",
    "### 🚀 下一步建議 (Next Actions)\n",
    "1. **深化評估**: 結合 **nb13_function_calling_tools.ipynb** 評估工具使用能力\n",
    "2. **領域特化**: 針對特定任務（如程式生成、數學推理）設計專門指標\n",
    "3. **持續評估**: 建立線上評估系統，監控生產環境模型表現\n",
    "4. **人工驗證**: 設計人工評估介面，補充自動化評估的不足\n",
    "\n",
    "這個評估框架為後續的模型微調（Part D）和 Agent 開發（Part E）提供了重要的品質保證工具，是 LLM 應用開發中不可或缺的環節。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
