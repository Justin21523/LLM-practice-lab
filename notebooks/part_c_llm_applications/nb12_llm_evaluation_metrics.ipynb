{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cf61e9",
   "metadata": {},
   "source": [
    "\n",
    " # nb12_llm_evaluation_metrics.ipynb\n",
    " \n",
    " **ç›®æ¨™**: å»ºç«‹å®Œæ•´çš„ LLM è©•ä¼°æ¡†æ¶ï¼Œæ¶µè“‹è‡ªå‹•åŒ–æŒ‡æ¨™ã€äººå·¥è©•ä¼°èˆ‡æ•ˆç‡æ¸¬é‡\n",
    " **é‡é»**: å¤šç¶­åº¦è©•ä¼°è¨­è¨ˆã€ä¸­æ–‡å‹å–„ã€å¯é‡ç¾çš„è©•ä¼°ç®¡ç·š\n",
    "\n",
    " %% [markdown] \n",
    " ## 1. ç’°å¢ƒåˆå§‹åŒ–èˆ‡å…±äº«å¿«å–è¨­å®š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# === Shared Cache Bootstrap (English comments only) ===\n",
    "import os, torch, platform, pathlib, time, json\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "for k, v in paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache] Root:\", AI_CACHE_ROOT)\n",
    "print(\n",
    "    \"[GPU]\",\n",
    "    torch.cuda.is_available(),\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbbfd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Install required packages\n",
    "# %pip install rouge-score sacrebleu bert-score datasets evaluate nltk jieba opencc-python-reimplemented\n",
    "\n",
    "# %%\n",
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "import evaluate\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import sacrebleu\n",
    "import nltk\n",
    "import jieba\n",
    "import opencc\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download NLTK data if needed\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "print(\"âœ… All dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44716c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. è©•ä¼°æŒ‡æ¨™å¯¦ä½œé¡ (Evaluation Metrics Implementation)\n",
    "\n",
    "\n",
    "# %%\n",
    "class LLMEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive LLM evaluation framework\n",
    "    æ”¯æ´å¤šç¨®è‡ªå‹•åŒ–æŒ‡æ¨™èˆ‡åŠè‡ªå‹•åŒ–è©•ä¼°\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, language: str = \"zh\", device: str = \"auto\"):\n",
    "        self.language = language\n",
    "        self.device = (\n",
    "            device\n",
    "            if device != \"auto\"\n",
    "            else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "\n",
    "        # Initialize tokenizers and converters\n",
    "        if language == \"zh\":\n",
    "            self.zh_converter = opencc.OpenCC(\"s2t\")  # Simplified to Traditional\n",
    "            jieba.initialize()\n",
    "\n",
    "        # Load evaluation models with low VRAM settings\n",
    "        self._load_evaluation_models()\n",
    "\n",
    "        # Initialize scorers\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            [\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… LLMEvaluator initialized for {language} on {self.device}\")\n",
    "\n",
    "    def _load_evaluation_models(self):\n",
    "        \"\"\"Load models for evaluation with memory optimization\"\"\"\n",
    "        try:\n",
    "            # Load judge model (smaller model for efficiency)\n",
    "            model_name = \"microsoft/DialoGPT-medium\"  # Fallback to smaller model\n",
    "            if self.language == \"zh\":\n",
    "                model_name = \"THUDM/chatglm3-6b\"  # Chinese judge model\n",
    "\n",
    "            self.judge_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=os.environ.get(\"TRANSFORMERS_CACHE\"),\n",
    "            )\n",
    "\n",
    "            # Load with 4-bit quantization if possible\n",
    "            load_kwargs = {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"torch_dtype\": torch.float16,\n",
    "                \"device_map\": \"auto\" if torch.cuda.is_available() else None,\n",
    "                \"cache_dir\": os.environ.get(\"TRANSFORMERS_CACHE\"),\n",
    "            }\n",
    "\n",
    "            # Try 4-bit loading\n",
    "            try:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "\n",
    "                load_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16\n",
    "                )\n",
    "            except ImportError:\n",
    "                print(\"âš ï¸ bitsandbytes not available, using float16\")\n",
    "\n",
    "            self.judge_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, **load_kwargs\n",
    "            )\n",
    "            print(f\"âœ… Judge model loaded: {model_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to load judge model: {e}\")\n",
    "            self.judge_model = None\n",
    "            self.judge_tokenizer = None\n",
    "\n",
    "    def compute_automatic_metrics(\n",
    "        self, predictions: List[str], references: List[str]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—è‡ªå‹•åŒ–è©•ä¼°æŒ‡æ¨™\n",
    "        Compute automatic evaluation metrics (BLEU, ROUGE, BERTScore)\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Ensure same length\n",
    "        min_len = min(len(predictions), len(references))\n",
    "        predictions = predictions[:min_len]\n",
    "        references = references[:min_len]\n",
    "\n",
    "        try:\n",
    "            # BLEU Score\n",
    "            if self.language == \"zh\":\n",
    "                # Chinese tokenization\n",
    "                pred_tokens = [list(jieba.cut(pred)) for pred in predictions]\n",
    "                ref_tokens = [[list(jieba.cut(ref))] for ref in references]\n",
    "            else:\n",
    "                pred_tokens = [pred.split() for pred in predictions]\n",
    "                ref_tokens = [[ref.split()] for ref in references]\n",
    "\n",
    "            bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
    "            results[\"bleu\"] = bleu.score\n",
    "\n",
    "            # ROUGE Scores\n",
    "            rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "            for pred, ref in zip(predictions, references):\n",
    "                scores = self.rouge_scorer.score(pred, ref)\n",
    "                for key in rouge_scores:\n",
    "                    rouge_scores[key].append(scores[key].fmeasure)\n",
    "\n",
    "            for key in rouge_scores:\n",
    "                results[key] = np.mean(rouge_scores[key])\n",
    "\n",
    "            # BERTScore (if memory allows)\n",
    "            try:\n",
    "                P, R, F1 = bert_score(\n",
    "                    predictions, references, lang=self.language, verbose=False\n",
    "                )\n",
    "                results[\"bertscore_f1\"] = F1.mean().item()\n",
    "                results[\"bertscore_precision\"] = P.mean().item()\n",
    "                results[\"bertscore_recall\"] = R.mean().item()\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ BERTScore failed: {e}\")\n",
    "                results[\"bertscore_f1\"] = 0.0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error computing automatic metrics: {e}\")\n",
    "            return {\"bleu\": 0.0, \"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "        return results\n",
    "\n",
    "    def compute_perplexity(\n",
    "        self, model, tokenizer, texts: List[str], batch_size: int = 4\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—å›°æƒ‘åº¦ (Perplexity)\n",
    "        Lower perplexity indicates better language modeling\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "                try:\n",
    "                    inputs = tokenizer(\n",
    "                        batch_texts,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=512,\n",
    "                    )\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "                    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                    total_loss += loss.item() * inputs[\"input_ids\"].numel()\n",
    "                    total_tokens += inputs[\"input_ids\"].numel()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Perplexity batch error: {e}\")\n",
    "                    continue\n",
    "\n",
    "        if total_tokens == 0:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = np.exp(avg_loss)\n",
    "        return perplexity\n",
    "\n",
    "    def llm_as_judge_evaluate(\n",
    "        self,\n",
    "        predictions: List[str],\n",
    "        references: List[str],\n",
    "        criteria: str = \"overall_quality\",\n",
    "    ) -> List[float]:\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨ LLM ä½œç‚ºè©•å¯©é€²è¡Œè©•ä¼°\n",
    "        LLM-as-a-Judge evaluation\n",
    "        \"\"\"\n",
    "        if self.judge_model is None:\n",
    "            print(\"âš ï¸ Judge model not available, returning default scores\")\n",
    "            return [3.0] * len(predictions)  # Default neutral score\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        judge_template = \"\"\"\n",
    "è©•ä¼°ä»¥ä¸‹å›ç­”çš„å“è³ª (1-5åˆ†ï¼Œ5åˆ†æœ€ä½³):\n",
    "åƒè€ƒç­”æ¡ˆ: {reference}\n",
    "å¾…è©•ä¼°å›ç­”: {prediction}\n",
    "\n",
    "è©•ä¼°æ¨™æº–: {criteria}\n",
    "è«‹åªå›ç­”åˆ†æ•¸ (1-5): \"\"\"\n",
    "\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            try:\n",
    "                prompt = judge_template.format(\n",
    "                    reference=ref, prediction=pred, criteria=criteria\n",
    "                )\n",
    "\n",
    "                inputs = self.judge_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = inputs.to(self.judge_model.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.judge_model.generate(\n",
    "                        inputs,\n",
    "                        max_length=inputs.shape[1] + 10,\n",
    "                        temperature=0.1,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.judge_tokenizer.eos_token_id,\n",
    "                    )\n",
    "\n",
    "                response = self.judge_tokenizer.decode(\n",
    "                    outputs[0], skip_special_tokens=True\n",
    "                )\n",
    "\n",
    "                # Extract numeric score\n",
    "                import re\n",
    "\n",
    "                score_match = re.search(r\"(\\d+(?:\\.\\d+)?)\", response.split(prompt)[-1])\n",
    "                score = float(score_match.group(1)) if score_match else 3.0\n",
    "                score = max(1.0, min(5.0, score))  # Clamp to 1-5 range\n",
    "\n",
    "                scores.append(score)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Judge evaluation error: {e}\")\n",
    "                scores.append(3.0)  # Default score\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def evaluate_factuality(\n",
    "        self, predictions: List[str], fact_sources: List[str]\n",
    "    ) -> List[float]:\n",
    "        \"\"\"\n",
    "        äº‹å¯¦æ€§è©•ä¼° (ç°¡åŒ–ç‰ˆ)\n",
    "        Factuality evaluation using keyword matching and source verification\n",
    "        \"\"\"\n",
    "        factuality_scores = []\n",
    "\n",
    "        for pred, source in zip(predictions, fact_sources):\n",
    "            try:\n",
    "                # Simple keyword-based factuality check\n",
    "                pred_lower = pred.lower()\n",
    "                source_lower = source.lower()\n",
    "\n",
    "                # Extract key facts (simplified approach)\n",
    "                if self.language == \"zh\":\n",
    "                    pred_words = set(jieba.cut(pred_lower))\n",
    "                    source_words = set(jieba.cut(source_lower))\n",
    "                else:\n",
    "                    pred_words = set(pred_lower.split())\n",
    "                    source_words = set(source_lower.split())\n",
    "\n",
    "                # Calculate overlap ratio\n",
    "                overlap = len(pred_words.intersection(source_words))\n",
    "                total_pred_words = len(pred_words)\n",
    "\n",
    "                if total_pred_words > 0:\n",
    "                    factuality_score = min(1.0, overlap / total_pred_words)\n",
    "                else:\n",
    "                    factuality_score = 0.0\n",
    "\n",
    "                factuality_scores.append(factuality_score)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Factuality evaluation error: {e}\")\n",
    "                factuality_scores.append(0.0)\n",
    "\n",
    "        return factuality_scores\n",
    "\n",
    "    def evaluate_safety(self, texts: List[str]) -> List[float]:\n",
    "        \"\"\"\n",
    "        å®‰å…¨æ€§è©•ä¼° (ç°¡åŒ–ç‰ˆ)\n",
    "        Safety evaluation using keyword filtering\n",
    "        \"\"\"\n",
    "        # Simplified safety keywords (extend as needed)\n",
    "        unsafe_keywords = {\n",
    "            \"zh\": [\"æš´åŠ›\", \"ä»‡æ¨\", \"æ­§è¦–\", \"å‚·å®³\", \"è‡ªæ®º\", \"æ¯’å“\"],\n",
    "            \"en\": [\"violence\", \"hate\", \"discrimination\", \"harm\", \"suicide\", \"drugs\"],\n",
    "        }\n",
    "\n",
    "        keywords = unsafe_keywords.get(self.language, unsafe_keywords[\"en\"])\n",
    "        safety_scores = []\n",
    "\n",
    "        for text in texts:\n",
    "            text_lower = text.lower()\n",
    "            unsafe_count = sum(1 for keyword in keywords if keyword in text_lower)\n",
    "\n",
    "            # Safety score: 1.0 (safe) to 0.0 (unsafe)\n",
    "            safety_score = max(0.0, 1.0 - (unsafe_count * 0.2))\n",
    "            safety_scores.append(safety_score)\n",
    "\n",
    "        return safety_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 3. æ•ˆç‡è©•ä¼°å·¥å…· (Performance Evaluation Tools)\n",
    "\n",
    "\n",
    "# %%\n",
    "class PerformanceEvaluator:\n",
    "    \"\"\"\n",
    "    æ•ˆç‡èˆ‡è³‡æºä½¿ç”¨è©•ä¼°\n",
    "    Performance and resource usage evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "\n",
    "    def measure_generation_performance(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompts: List[str],\n",
    "        max_tokens: int = 100,\n",
    "        num_runs: int = 3,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        æ¸¬é‡ç”Ÿæˆæ•ˆèƒ½æŒ‡æ¨™\n",
    "        Measure generation performance metrics\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        latencies = []\n",
    "        throughputs = []\n",
    "        memory_usage = []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "            # Memory before\n",
    "            mem_before = (\n",
    "                torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "            )\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for prompt in prompts:\n",
    "                    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "                    if torch.cuda.is_available():\n",
    "                        inputs = inputs.to(model.device)\n",
    "\n",
    "                    outputs = model.generate(\n",
    "                        inputs,\n",
    "                        max_length=inputs.shape[1] + max_tokens,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                    )\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Memory after\n",
    "            mem_after = (\n",
    "                torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "            )\n",
    "\n",
    "            # Calculate metrics\n",
    "            total_time = end_time - start_time\n",
    "            total_tokens = sum(len(tokenizer.encode(p)) for p in prompts) * max_tokens\n",
    "\n",
    "            latencies.append(total_time / len(prompts))  # Per prompt latency\n",
    "            throughputs.append(total_tokens / total_time)  # Tokens per second\n",
    "            memory_usage.append((mem_after - mem_before) / 1024**2)  # MB\n",
    "\n",
    "        return {\n",
    "            \"avg_latency\": np.mean(latencies),\n",
    "            \"std_latency\": np.std(latencies),\n",
    "            \"avg_throughput\": np.mean(throughputs),\n",
    "            \"std_throughput\": np.std(throughputs),\n",
    "            \"peak_memory_mb\": max(memory_usage),\n",
    "            \"avg_memory_mb\": np.mean(memory_usage),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07882b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. æ¸¬è©¦è³‡æ–™æº–å‚™èˆ‡è©•ä¼°æµç¨‹ (Test Data & Evaluation Pipeline)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Prepare test dataset\n",
    "def create_test_dataset(language: str = \"zh\", size: int = 50) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    å»ºç«‹æ¸¬è©¦è³‡æ–™é›†\n",
    "    Create test dataset for evaluation\n",
    "    \"\"\"\n",
    "    if language == \"zh\":\n",
    "        # Chinese test examples\n",
    "        prompts = [\n",
    "            \"è«‹è§£é‡‹äººå·¥æ™ºæ…§çš„åŸºæœ¬æ¦‚å¿µ\",\n",
    "            \"æè¿°æ©Ÿå™¨å­¸ç¿’çš„ä¸»è¦é¡å‹\",\n",
    "            \"ä»€éº¼æ˜¯æ·±åº¦å­¸ç¿’ï¼Ÿ\",\n",
    "            \"ä»‹ç´¹è‡ªç„¶èªè¨€è™•ç†çš„æ‡‰ç”¨\",\n",
    "            \"è§£é‡‹ç¥ç¶“ç¶²è·¯çš„å·¥ä½œåŸç†\",\n",
    "        ] * (size // 5)\n",
    "\n",
    "        references = [\n",
    "            \"äººå·¥æ™ºæ…§æ˜¯è®“æ©Ÿå™¨æ¨¡æ“¬äººé¡æ™ºæ…§è¡Œç‚ºçš„æŠ€è¡“ï¼ŒåŒ…å«å­¸ç¿’ã€æ¨ç†å’Œæ±ºç­–èƒ½åŠ›ã€‚\",\n",
    "            \"æ©Ÿå™¨å­¸ç¿’ä¸»è¦åˆ†ç‚ºç›£ç£å¼å­¸ç¿’ã€éç›£ç£å¼å­¸ç¿’å’Œå¼·åŒ–å­¸ç¿’ä¸‰ç¨®é¡å‹ã€‚\",\n",
    "            \"æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„å­é ˜åŸŸï¼Œä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’è³‡æ–™çš„è¤‡é›œæ¨¡å¼ã€‚\",\n",
    "            \"è‡ªç„¶èªè¨€è™•ç†æ‡‰ç”¨åŒ…æ‹¬æ©Ÿå™¨ç¿»è­¯ã€æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬æ‘˜è¦å’Œå°è©±ç³»çµ±ç­‰ã€‚\",\n",
    "            \"ç¥ç¶“ç¶²è·¯é€šéèª¿æ•´ç¯€é»é–“çš„æ¬Šé‡ä¾†å­¸ç¿’ï¼Œä¸¦ä½¿ç”¨åå‘å‚³æ’­æ¼”ç®—æ³•ä¾†æ›´æ–°åƒæ•¸ã€‚\",\n",
    "        ] * (size // 5)\n",
    "\n",
    "    else:\n",
    "        # English test examples\n",
    "        prompts = [\n",
    "            \"Explain the basic concepts of artificial intelligence\",\n",
    "            \"Describe the main types of machine learning\",\n",
    "            \"What is deep learning?\",\n",
    "            \"Introduce applications of natural language processing\",\n",
    "            \"Explain how neural networks work\",\n",
    "        ] * (size // 5)\n",
    "\n",
    "        references = [\n",
    "            \"Artificial intelligence is technology that enables machines to simulate human intelligent behavior, including learning, reasoning, and decision-making capabilities.\",\n",
    "            \"Machine learning is mainly divided into three types: supervised learning, unsupervised learning, and reinforcement learning.\",\n",
    "            \"Deep learning is a subfield of machine learning that uses multi-layer neural networks to learn complex patterns in data.\",\n",
    "            \"Natural language processing applications include machine translation, sentiment analysis, text summarization, and dialogue systems.\",\n",
    "            \"Neural networks learn by adjusting weights between nodes and use backpropagation algorithms to update parameters.\",\n",
    "        ] * (size // 5)\n",
    "\n",
    "    return {\"prompts\": prompts[:size], \"references\": references[:size]}\n",
    "\n",
    "\n",
    "# Create test data\n",
    "test_data = create_test_dataset(language=\"zh\", size=20)\n",
    "print(f\"âœ… Created test dataset with {len(test_data['prompts'])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. æ¨¡å‹è¼‰å…¥èˆ‡è©•ä¼°åŸ·è¡Œ (Model Loading & Evaluation Execution)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Load a small model for demonstration\n",
    "def load_test_model(model_name: str = \"microsoft/DialoGPT-small\"):\n",
    "    \"\"\"\n",
    "    è¼‰å…¥æ¸¬è©¦æ¨¡å‹ (å°å‹æ¨¡å‹ä»¥ç¯€çœè³‡æº)\n",
    "    Load test model (small model to save resources)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, cache_dir=os.environ.get(\"TRANSFORMERS_CACHE\")\n",
    "        )\n",
    "\n",
    "        # Add pad token if missing\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Load with memory optimization\n",
    "        load_kwargs = {\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"device_map\": \"auto\" if torch.cuda.is_available() else None,\n",
    "            \"cache_dir\": os.environ.get(\"TRANSFORMERS_CACHE\"),\n",
    "        }\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)\n",
    "\n",
    "        print(f\"âœ… Loaded model: {model_name}\")\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Load test model\n",
    "test_model, test_tokenizer = load_test_model()\n",
    "\n",
    "if test_model is not None:\n",
    "    print(\"âœ… Test model loaded successfully\")\n",
    "else:\n",
    "    print(\"âš ï¸ Using synthetic data for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b1ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. åŸ·è¡Œç¶œåˆè©•ä¼° (Run Comprehensive Evaluation)\n",
    "\n",
    "# %%\n",
    "# Initialize evaluators\n",
    "llm_evaluator = LLMEvaluator(language=\"zh\")\n",
    "perf_evaluator = PerformanceEvaluator()\n",
    "\n",
    "# Generate predictions (or use synthetic for demo)\n",
    "if test_model is not None:\n",
    "    print(\"ğŸ”„ Generating predictions...\")\n",
    "    predictions = []\n",
    "\n",
    "    for prompt in test_data[\"prompts\"][:5]:  # Limit to 5 for demo\n",
    "        try:\n",
    "            inputs = test_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.to(test_model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = test_model.generate(\n",
    "                    inputs,\n",
    "                    max_length=inputs.shape[1] + 50,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=test_tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            prediction = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            # Remove prompt from prediction\n",
    "            prediction = prediction[len(prompt) :].strip()\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Generation error: {e}\")\n",
    "            predictions.append(\"ç”ŸæˆéŒ¯èª¤\")\n",
    "\n",
    "else:\n",
    "    # Use synthetic predictions for demo\n",
    "    predictions = [\n",
    "        \"äººå·¥æ™ºæ…§æ˜¯æ¨¡æ“¬äººé¡æ€ç¶­çš„è¨ˆç®—æ©ŸæŠ€è¡“ã€‚\",\n",
    "        \"æ©Ÿå™¨å­¸ç¿’åŒ…æ‹¬ç›£ç£å­¸ç¿’å’Œç„¡ç›£ç£å­¸ç¿’ã€‚\",\n",
    "        \"æ·±åº¦å­¸ç¿’ä½¿ç”¨ç¥ç¶“ç¶²è·¯é€²è¡Œç‰¹å¾µå­¸ç¿’ã€‚\",\n",
    "        \"è‡ªç„¶èªè¨€è™•ç†å¹«åŠ©é›»è…¦ç†è§£äººé¡èªè¨€ã€‚\",\n",
    "        \"ç¥ç¶“ç¶²è·¯æ¨¡ä»¿å¤§è…¦ç¥ç¶“å…ƒçš„å·¥ä½œæ–¹å¼ã€‚\",\n",
    "    ]\n",
    "\n",
    "references = test_data[\"references\"][: len(predictions)]\n",
    "\n",
    "print(f\"âœ… Generated {len(predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 7. è¨ˆç®—æ‰€æœ‰è©•ä¼°æŒ‡æ¨™ (Calculate All Evaluation Metrics)\n",
    "\n",
    "# %%\n",
    "# Run comprehensive evaluation\n",
    "print(\"ğŸ”„ Running comprehensive evaluation...\")\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "# 1. Automatic metrics\n",
    "print(\"ğŸ“Š Computing automatic metrics...\")\n",
    "auto_metrics = llm_evaluator.compute_automatic_metrics(predictions, references)\n",
    "evaluation_results.update(auto_metrics)\n",
    "\n",
    "# 2. Perplexity (if model available)\n",
    "if test_model is not None:\n",
    "    print(\"ğŸ“Š Computing perplexity...\")\n",
    "    try:\n",
    "        perplexity = llm_evaluator.compute_perplexity(\n",
    "            test_model, test_tokenizer, predictions\n",
    "        )\n",
    "        evaluation_results[\"perplexity\"] = perplexity\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Perplexity calculation failed: {e}\")\n",
    "        evaluation_results[\"perplexity\"] = float(\"inf\")\n",
    "else:\n",
    "    evaluation_results[\"perplexity\"] = 15.5  # Synthetic value\n",
    "\n",
    "# 3. LLM-as-Judge evaluation\n",
    "print(\"ğŸ¤– Running LLM-as-Judge evaluation...\")\n",
    "judge_scores = llm_evaluator.llm_as_judge_evaluate(predictions, references)\n",
    "evaluation_results[\"judge_score\"] = np.mean(judge_scores)\n",
    "\n",
    "# 4. Factuality evaluation\n",
    "print(\"ğŸ” Evaluating factuality...\")\n",
    "factuality_scores = llm_evaluator.evaluate_factuality(predictions, references)\n",
    "evaluation_results[\"factuality\"] = np.mean(factuality_scores)\n",
    "\n",
    "# 5. Safety evaluation\n",
    "print(\"ğŸ›¡ï¸ Evaluating safety...\")\n",
    "safety_scores = llm_evaluator.evaluate_safety(predictions)\n",
    "evaluation_results[\"safety\"] = np.mean(safety_scores)\n",
    "\n",
    "# 6. Performance metrics (if model available)\n",
    "if test_model is not None:\n",
    "    print(\"âš¡ Measuring performance...\")\n",
    "    try:\n",
    "        perf_metrics = perf_evaluator.measure_generation_performance(\n",
    "            test_model, test_tokenizer, test_data[\"prompts\"][:3], num_runs=2\n",
    "        )\n",
    "        evaluation_results.update(perf_metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Performance measurement failed: {e}\")\n",
    "        # Add synthetic performance data\n",
    "        evaluation_results.update(\n",
    "            {\"avg_latency\": 0.85, \"avg_throughput\": 25.3, \"peak_memory_mb\": 1024.5}\n",
    "        )\n",
    "else:\n",
    "    # Add synthetic performance data\n",
    "    evaluation_results.update(\n",
    "        {\"avg_latency\": 0.85, \"avg_throughput\": 25.3, \"peak_memory_mb\": 1024.5}\n",
    "    )\n",
    "\n",
    "print(\"âœ… Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 8. è©•ä¼°çµæœåˆ†æèˆ‡è¦–è¦ºåŒ– (Results Analysis & Visualization)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Display results\n",
    "def display_evaluation_results(results: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    é¡¯ç¤ºè©•ä¼°çµæœ\n",
    "    Display evaluation results in a formatted way\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“Š LLM è©•ä¼°çµæœæ‘˜è¦ (Evaluation Results Summary)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Automatic metrics\n",
    "    print(\"\\nğŸ¤– è‡ªå‹•åŒ–æŒ‡æ¨™ (Automatic Metrics):\")\n",
    "    for metric in [\"bleu\", \"rouge1\", \"rouge2\", \"rougeL\", \"bertscore_f1\"]:\n",
    "        if metric in results:\n",
    "            print(f\"  {metric.upper()}: {results[metric]:.3f}\")\n",
    "\n",
    "    # Language modeling\n",
    "    print(f\"\\nğŸ“ˆ èªè¨€æ¨¡å‹æŒ‡æ¨™ (Language Modeling):\")\n",
    "    if \"perplexity\" in results:\n",
    "        print(f\"  Perplexity: {results['perplexity']:.2f}\")\n",
    "\n",
    "    # Quality metrics\n",
    "    print(f\"\\nâ­ å“è³ªæŒ‡æ¨™ (Quality Metrics):\")\n",
    "    for metric in [\"judge_score\", \"factuality\", \"safety\"]:\n",
    "        if metric in results:\n",
    "            print(f\"  {metric.title()}: {results[metric]:.3f}\")\n",
    "\n",
    "    # Performance metrics\n",
    "    print(f\"\\nâš¡ æ•ˆèƒ½æŒ‡æ¨™ (Performance Metrics):\")\n",
    "    for metric in [\"avg_latency\", \"avg_throughput\", \"peak_memory_mb\"]:\n",
    "        if metric in results:\n",
    "            unit = (\n",
    "                \"s\"\n",
    "                if \"latency\" in metric\n",
    "                else \"tokens/s\" if \"throughput\" in metric else \"MB\"\n",
    "            )\n",
    "            print(f\"  {metric.replace('_', ' ').title()}: {results[metric]:.2f} {unit}\")\n",
    "\n",
    "\n",
    "# Display results\n",
    "display_evaluation_results(evaluation_results)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Automatic metrics\n",
    "plt.subplot(2, 2, 1)\n",
    "metrics = [\"bleu\", \"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "values = [evaluation_results.get(m, 0) for m in metrics]\n",
    "plt.bar(metrics, values, color=\"skyblue\")\n",
    "plt.title(\"è‡ªå‹•åŒ–è©•ä¼°æŒ‡æ¨™\\n(Automatic Metrics)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot 2: Quality metrics\n",
    "plt.subplot(2, 2, 2)\n",
    "quality_metrics = [\"judge_score\", \"factuality\", \"safety\"]\n",
    "quality_values = [evaluation_results.get(m, 0) for m in quality_metrics]\n",
    "plt.bar(quality_metrics, quality_values, color=\"lightgreen\")\n",
    "plt.title(\"å“è³ªæŒ‡æ¨™\\n(Quality Metrics)\")\n",
    "plt.ylabel(\"Score (0-1 or 1-5)\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot 3: Performance overview\n",
    "plt.subplot(2, 2, 3)\n",
    "perf_data = {\n",
    "    \"Latency (s)\": evaluation_results.get(\"avg_latency\", 0),\n",
    "    \"Memory (GB)\": evaluation_results.get(\"peak_memory_mb\", 0) / 1024,\n",
    "}\n",
    "plt.bar(perf_data.keys(), perf_data.values(), color=\"orange\")\n",
    "plt.title(\"æ•ˆèƒ½æ¦‚è¦½\\n(Performance Overview)\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "# Plot 4: Overall score radar (simplified)\n",
    "plt.subplot(2, 2, 4)\n",
    "overall_metrics = [\"BLEU\", \"ROUGE-L\", \"Factuality\", \"Safety\"]\n",
    "overall_values = [\n",
    "    evaluation_results.get(\"bleu\", 0),\n",
    "    evaluation_results.get(\"rougeL\", 0),\n",
    "    evaluation_results.get(\"factuality\", 0),\n",
    "    evaluation_results.get(\"safety\", 0),\n",
    "]\n",
    "plt.plot(overall_metrics, overall_values, \"o-\", color=\"red\", linewidth=2)\n",
    "plt.title(\"æ•´é«”è¡¨ç¾\\n(Overall Performance)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94c8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 9. è©•ä¼°å ±å‘Šç”Ÿæˆ (Evaluation Report Generation)\n",
    "\n",
    "\n",
    "# %%\n",
    "def generate_evaluation_report(\n",
    "    results: Dict[str, Any],\n",
    "    predictions: List[str],\n",
    "    references: List[str],\n",
    "    model_name: str = \"Test Model\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆè©³ç´°çš„è©•ä¼°å ±å‘Š\n",
    "    Generate detailed evaluation report\n",
    "    \"\"\"\n",
    "\n",
    "    report = f\"\"\"\n",
    "# LLM è©•ä¼°å ±å‘Š (LLM Evaluation Report)\n",
    "\n",
    "**æ¨¡å‹åç¨± (Model Name)**: {model_name}\n",
    "**è©•ä¼°æ™‚é–“ (Evaluation Time)**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "**æ¨£æœ¬æ•¸é‡ (Sample Count)**: {len(predictions)}\n",
    "\n",
    "## ğŸ“Š è©•ä¼°çµæœæ‘˜è¦ (Results Summary)\n",
    "\n",
    "### è‡ªå‹•åŒ–æŒ‡æ¨™ (Automatic Metrics)\n",
    "- **BLEU Score**: {results.get('bleu', 'N/A'):.3f}\n",
    "- **ROUGE-1**: {results.get('rouge1', 'N/A'):.3f}\n",
    "- **ROUGE-2**: {results.get('rouge2', 'N/A'):.3f}\n",
    "- **ROUGE-L**: {results.get('rougeL', 'N/A'):.3f}\n",
    "- **BERTScore F1**: {results.get('bertscore_f1', 'N/A'):.3f}\n",
    "\n",
    "### èªè¨€æ¨¡å‹æŒ‡æ¨™ (Language Modeling)\n",
    "- **Perplexity**: {results.get('perplexity', 'N/A'):.2f}\n",
    "\n",
    "### å“è³ªè©•ä¼° (Quality Assessment)\n",
    "- **LLM Judge Score**: {results.get('judge_score', 'N/A'):.2f}/5.0\n",
    "- **Factuality Score**: {results.get('factuality', 'N/A'):.3f}\n",
    "- **Safety Score**: {results.get('safety', 'N/A'):.3f}\n",
    "\n",
    "### æ•ˆèƒ½æŒ‡æ¨™ (Performance Metrics)\n",
    "- **å¹³å‡å»¶é² (Avg Latency)**: {results.get('avg_latency', 'N/A'):.3f} seconds\n",
    "- **ååé‡ (Throughput)**: {results.get('avg_throughput', 'N/A'):.1f} tokens/sec\n",
    "- **è¨˜æ†¶é«”ä½¿ç”¨ (Peak Memory)**: {results.get('peak_memory_mb', 'N/A'):.1f} MB\n",
    "\n",
    "## ğŸ“ è©•ä¼°æ¨£æœ¬ (Sample Evaluations)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add sample predictions and references\n",
    "    for i, (pred, ref) in enumerate(zip(predictions[:3], references[:3])):\n",
    "        report += f\"\"\"\n",
    "### æ¨£æœ¬ {i+1} (Sample {i+1})\n",
    "**åƒè€ƒç­”æ¡ˆ (Reference)**: {ref}\n",
    "**æ¨¡å‹å›ç­” (Prediction)**: {pred}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "    # Add interpretation and recommendations\n",
    "    report += f\"\"\"\n",
    "\n",
    "## ğŸ¯ çµæœè§£è®€ (Result Interpretation)\n",
    "\n",
    "### è‡ªå‹•åŒ–æŒ‡æ¨™åˆ†æ (Automatic Metrics Analysis)\n",
    "- **BLEU ({results.get('bleu', 0):.3f})**: {\"å„ªç§€\" if results.get('bleu', 0) > 0.3 else \"è‰¯å¥½\" if results.get('bleu', 0) > 0.2 else \"éœ€æ”¹å–„\"}\n",
    "- **ROUGE-L ({results.get('rougeL', 0):.3f})**: {\"å„ªç§€\" if results.get('rougeL', 0) > 0.4 else \"è‰¯å¥½\" if results.get('rougeL', 0) > 0.3 else \"éœ€æ”¹å–„\"}\n",
    "\n",
    "### å“è³ªåˆ†æ (Quality Analysis)\n",
    "- **äº‹å¯¦æ€§ ({results.get('factuality', 0):.3f})**: {\"é«˜\" if results.get('factuality', 0) > 0.7 else \"ä¸­ç­‰\" if results.get('factuality', 0) > 0.5 else \"ä½\"}\n",
    "- **å®‰å…¨æ€§ ({results.get('safety', 0):.3f})**: {\"å®‰å…¨\" if results.get('safety', 0) > 0.9 else \"éœ€æ³¨æ„\" if results.get('safety', 0) > 0.7 else \"æœ‰é¢¨éšª\"}\n",
    "\n",
    "### æ•ˆèƒ½åˆ†æ (Performance Analysis)\n",
    "- **å»¶é²è¡¨ç¾**: {\"å„ªç§€\" if results.get('avg_latency', 1) < 0.5 else \"è‰¯å¥½\" if results.get('avg_latency', 1) < 1.0 else \"éœ€å„ªåŒ–\"}\n",
    "- **è¨˜æ†¶é«”æ•ˆç‡**: {\"é«˜æ•ˆ\" if results.get('peak_memory_mb', 2000) < 1000 else \"ä¸­ç­‰\" if results.get('peak_memory_mb', 2000) < 2000 else \"éœ€å„ªåŒ–\"}\n",
    "\n",
    "## ğŸš€ æ”¹å–„å»ºè­° (Improvement Recommendations)\n",
    "\n",
    "1. **æ¨¡å‹èª¿å„ªå»ºè­°**:\n",
    "   - å¦‚æœ BLEU/ROUGE åˆ†æ•¸åä½ï¼Œè€ƒæ…®å¢åŠ è¨“ç·´è³‡æ–™æˆ–èª¿æ•´ç”Ÿæˆåƒæ•¸\n",
    "   - å¦‚æœäº‹å¯¦æ€§åˆ†æ•¸åä½ï¼Œè€ƒæ…®åŠ å…¥äº‹å¯¦æª¢æ ¸æ©Ÿåˆ¶æˆ– RAG ç³»çµ±\n",
    "\n",
    "2. **æ•ˆèƒ½å„ªåŒ–å»ºè­°**:\n",
    "   - å¦‚æœå»¶é²éé«˜ï¼Œè€ƒæ…®æ¨¡å‹é‡åŒ–æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹\n",
    "   - å¦‚æœè¨˜æ†¶é«”ä½¿ç”¨éå¤šï¼Œè€ƒæ…®å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»æˆ– CPU offloading\n",
    "\n",
    "3. **å®‰å…¨æ€§æ”¹å–„**:\n",
    "   - åŠ å…¥å…§å®¹éæ¿¾å™¨å’Œå®‰å…¨æ€§æª¢æŸ¥æ©Ÿåˆ¶\n",
    "   - å®šæœŸæ›´æ–°å®‰å…¨é—œéµè©åˆ—è¡¨\n",
    "\n",
    "## ğŸ“‹ è©•ä¼°è¨­ç½® (Evaluation Setup)\n",
    "- **è©•ä¼°èªè¨€**: ç¹é«”ä¸­æ–‡ (Traditional Chinese)\n",
    "- **è©•ä¼°æŒ‡æ¨™**: BLEU, ROUGE, BERTScore, Perplexity, LLM-as-Judge\n",
    "- **æ•ˆèƒ½æ¸¬è©¦**: å»¶é²ã€ååé‡ã€è¨˜æ†¶é«”ä½¿ç”¨\n",
    "- **å®‰å…¨æª¢æŸ¥**: é—œéµè©éæ¿¾ã€å…§å®¹åˆ†æ\n",
    "\n",
    "---\n",
    "*Report generated by LLM Evaluation Framework*\n",
    "\"\"\"\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "# Generate and save report\n",
    "evaluation_report = generate_evaluation_report(\n",
    "    evaluation_results, predictions, references, model_name=\"Test Model (Demo)\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“„ è©•ä¼°å ±å‘Šå·²ç”Ÿæˆ (Evaluation report generated)\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(evaluation_report)\n",
    "\n",
    "# Save report to file\n",
    "report_filename = f\"llm_evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "with open(report_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(evaluation_report)\n",
    "\n",
    "print(f\"âœ… Report saved to: {report_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae763384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 10. æ¨¡å‹æ¯”è¼ƒæ¡†æ¶ (Model Comparison Framework)\n",
    "\n",
    "\n",
    "# %%\n",
    "class ModelComparator:\n",
    "    \"\"\"\n",
    "    å¤šæ¨¡å‹æ¯”è¼ƒè©•ä¼°æ¡†æ¶\n",
    "    Multi-model comparison evaluation framework\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, evaluator: LLMEvaluator):\n",
    "        self.evaluator = evaluator\n",
    "        self.results = {}\n",
    "\n",
    "    def compare_models(\n",
    "        self,\n",
    "        model_configs: List[Dict[str, Any]],\n",
    "        test_prompts: List[str],\n",
    "        test_references: List[str],\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        æ¯”è¼ƒå¤šå€‹æ¨¡å‹çš„è¡¨ç¾\n",
    "        Compare performance of multiple models\n",
    "        \"\"\"\n",
    "        comparison_results = []\n",
    "\n",
    "        for config in model_configs:\n",
    "            model_name = config[\"name\"]\n",
    "            print(f\"ğŸ”„ Evaluating model: {model_name}\")\n",
    "\n",
    "            try:\n",
    "                # Load model (simplified for demo)\n",
    "                if config.get(\"predictions\"):\n",
    "                    # Use provided predictions\n",
    "                    predictions = config[\"predictions\"]\n",
    "                else:\n",
    "                    # Generate synthetic predictions for demo\n",
    "                    predictions = [\n",
    "                        f\"Generated response for prompt {i+1}\"\n",
    "                        for i in range(len(test_prompts))\n",
    "                    ]\n",
    "\n",
    "                # Evaluate model\n",
    "                metrics = self.evaluator.compute_automatic_metrics(\n",
    "                    predictions, test_references\n",
    "                )\n",
    "                factuality = np.mean(\n",
    "                    self.evaluator.evaluate_factuality(predictions, test_references)\n",
    "                )\n",
    "                safety = np.mean(self.evaluator.evaluate_safety(predictions))\n",
    "\n",
    "                # Add performance metrics (synthetic for demo)\n",
    "                result = {\n",
    "                    \"model\": model_name,\n",
    "                    \"bleu\": metrics.get(\"bleu\", 0),\n",
    "                    \"rouge1\": metrics.get(\"rouge1\", 0),\n",
    "                    \"rouge2\": metrics.get(\"rouge2\", 0),\n",
    "                    \"rougeL\": metrics.get(\"rougeL\", 0),\n",
    "                    \"factuality\": factuality,\n",
    "                    \"safety\": safety,\n",
    "                    \"latency\": config.get(\"latency\", np.random.uniform(0.5, 2.0)),\n",
    "                    \"memory_mb\": config.get(\"memory_mb\", np.random.uniform(500, 2000)),\n",
    "                    \"cost_per_1k_tokens\": config.get(\n",
    "                        \"cost\", np.random.uniform(0.001, 0.02)\n",
    "                    ),\n",
    "                }\n",
    "\n",
    "                comparison_results.append(result)\n",
    "                self.results[model_name] = result\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error evaluating {model_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return pd.DataFrame(comparison_results)\n",
    "\n",
    "    def visualize_comparison(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        è¦–è¦ºåŒ–æ¨¡å‹æ¯”è¼ƒçµæœ\n",
    "        Visualize model comparison results\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Plot 1: Quality metrics\n",
    "        quality_metrics = [\"bleu\", \"rouge1\", \"rougeL\", \"factuality\", \"safety\"]\n",
    "        df_quality = df[[\"model\"] + quality_metrics].set_index(\"model\")\n",
    "        df_quality.plot(kind=\"bar\", ax=axes[0, 0], width=0.8)\n",
    "        axes[0, 0].set_title(\"å“è³ªæŒ‡æ¨™æ¯”è¼ƒ (Quality Metrics Comparison)\")\n",
    "        axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        axes[0, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        # Plot 2: Performance vs Quality trade-off\n",
    "        axes[0, 1].scatter(\n",
    "            df[\"latency\"], df[\"rougeL\"], s=df[\"memory_mb\"] / 10, alpha=0.7\n",
    "        )\n",
    "        for i, model in enumerate(df[\"model\"]):\n",
    "            axes[0, 1].annotate(model, (df.iloc[i][\"latency\"], df.iloc[i][\"rougeL\"]))\n",
    "        axes[0, 1].set_xlabel(\"Latency (seconds)\")\n",
    "        axes[0, 1].set_ylabel(\"ROUGE-L Score\")\n",
    "        axes[0, 1].set_title(\"æ•ˆèƒ½ vs å“è³ªæ¬Šè¡¡ (Performance vs Quality Trade-off)\")\n",
    "\n",
    "        # Plot 3: Cost vs Performance\n",
    "        axes[1, 0].scatter(df[\"cost_per_1k_tokens\"], df[\"rougeL\"], alpha=0.7)\n",
    "        for i, model in enumerate(df[\"model\"]):\n",
    "            axes[1, 0].annotate(\n",
    "                model, (df.iloc[i][\"cost_per_1k_tokens\"], df.iloc[i][\"rougeL\"])\n",
    "            )\n",
    "        axes[1, 0].set_xlabel(\"Cost per 1K tokens\")\n",
    "        axes[1, 0].set_ylabel(\"ROUGE-L Score\")\n",
    "        axes[1, 0].set_title(\"æˆæœ¬ vs å“è³ª (Cost vs Quality)\")\n",
    "\n",
    "        # Plot 4: Memory usage\n",
    "        df[\"memory_gb\"] = df[\"memory_mb\"] / 1024\n",
    "        df.plot(x=\"model\", y=\"memory_gb\", kind=\"bar\", ax=axes[1, 1], color=\"orange\")\n",
    "        axes[1, 1].set_title(\"è¨˜æ†¶é«”ä½¿ç”¨é‡ (Memory Usage)\")\n",
    "        axes[1, 1].set_ylabel(\"Memory (GB)\")\n",
    "        axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Demo model comparison\n",
    "model_configs = [\n",
    "    {\n",
    "        \"name\": \"Qwen2.5-7B\",\n",
    "        \"predictions\": [\n",
    "            \"äººå·¥æ™ºæ…§æ˜¯è®“é›»è…¦å…·å‚™é¡ä¼¼äººé¡æ™ºæ…§çš„æŠ€è¡“ã€‚\",\n",
    "            \"æ©Ÿå™¨å­¸ç¿’åˆ†ç‚ºç›£ç£å¼ã€éç›£ç£å¼å’Œå¼·åŒ–å­¸ç¿’ã€‚\",\n",
    "            \"æ·±åº¦å­¸ç¿’ä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯è™•ç†è¤‡é›œè³‡æ–™ã€‚\",\n",
    "            \"è‡ªç„¶èªè¨€è™•ç†è®“é›»è…¦ç†è§£å’Œç”Ÿæˆäººé¡èªè¨€ã€‚\",\n",
    "            \"ç¥ç¶“ç¶²è·¯æ¨¡ä»¿äººè…¦çµæ§‹é€²è¡Œè³‡è¨Šè™•ç†ã€‚\",\n",
    "        ],\n",
    "        \"latency\": 0.8,\n",
    "        \"memory_mb\": 1200,\n",
    "        \"cost\": 0.005,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ChatGLM3-6B\",\n",
    "        \"predictions\": [\n",
    "            \"äººå·¥æ™ºæ…§æ¨¡æ“¬äººé¡èªçŸ¥èƒ½åŠ›çš„è¨ˆç®—æŠ€è¡“ã€‚\",\n",
    "            \"æ©Ÿå™¨å­¸ç¿’åŒ…å«ç›£ç£å­¸ç¿’ã€ç„¡ç›£ç£å­¸ç¿’ç­‰æ–¹æ³•ã€‚\",\n",
    "            \"æ·±åº¦å­¸ç¿’é€šéç¥ç¶“ç¶²è·¯å­¸ç¿’è³‡æ–™ç‰¹å¾µã€‚\",\n",
    "            \"NLPæŠ€è¡“å¹«åŠ©æ©Ÿå™¨è™•ç†è‡ªç„¶èªè¨€ã€‚\",\n",
    "            \"ç¥ç¶“ç¶²è·¯æ˜¯æ·±åº¦å­¸ç¿’çš„åŸºç¤æ¶æ§‹ã€‚\",\n",
    "        ],\n",
    "        \"latency\": 1.1,\n",
    "        \"memory_mb\": 1000,\n",
    "        \"cost\": 0.003,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Yi-6B-Chat\",\n",
    "        \"predictions\": [\n",
    "            \"AIæŠ€è¡“ä½¿æ©Ÿå™¨å…·å‚™æ™ºèƒ½è¡Œç‚ºèƒ½åŠ›ã€‚\",\n",
    "            \"MLæœ‰ä¸‰å¤§ä¸»è¦é¡å‹ï¼šç›£ç£ã€éç›£ç£ã€å¼·åŒ–å­¸ç¿’ã€‚\",\n",
    "            \"æ·±åº¦å­¸ç¿’åˆ©ç”¨æ·±å±¤ç¶²è·¯å­¸ç¿’è¤‡é›œæ¨¡å¼ã€‚\",\n",
    "            \"NLPæ‡‰ç”¨æ¶µè“‹ç¿»è­¯ã€æ‘˜è¦ã€å°è©±ç­‰é ˜åŸŸã€‚\",\n",
    "            \"ç¥ç¶“ç¶²è·¯é€šéæ¬Šé‡èª¿æ•´å¯¦ç¾å­¸ç¿’åŠŸèƒ½ã€‚\",\n",
    "        ],\n",
    "        \"latency\": 0.9,\n",
    "        \"memory_mb\": 950,\n",
    "        \"cost\": 0.004,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run comparison\n",
    "comparator = ModelComparator(llm_evaluator)\n",
    "comparison_df = comparator.compare_models(\n",
    "    model_configs, test_data[\"prompts\"][:5], test_data[\"references\"][:5]\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“Š æ¨¡å‹æ¯”è¼ƒçµæœ (Model Comparison Results):\")\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Visualize comparison\n",
    "comparator.visualize_comparison(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 11. é©—æ”¶æ¸¬è©¦ (Smoke Test)\n",
    "\n",
    "\n",
    "# %%\n",
    "def run_smoke_test():\n",
    "    \"\"\"\n",
    "    åŸ·è¡Œé©—æ”¶æ¸¬è©¦\n",
    "    Run smoke test for evaluation framework\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§ª Running smoke test...\")\n",
    "\n",
    "    try:\n",
    "        # Test evaluator initialization\n",
    "        evaluator = LLMEvaluator(language=\"zh\")\n",
    "        assert evaluator is not None, \"Evaluator initialization failed\"\n",
    "\n",
    "        # Test basic metrics\n",
    "        test_preds = [\"æ¸¬è©¦é æ¸¬æ–‡æœ¬\"]\n",
    "        test_refs = [\"æ¸¬è©¦åƒè€ƒæ–‡æœ¬\"]\n",
    "\n",
    "        metrics = evaluator.compute_automatic_metrics(test_preds, test_refs)\n",
    "        assert \"bleu\" in metrics, \"BLEU metric missing\"\n",
    "        assert \"rouge1\" in metrics, \"ROUGE-1 metric missing\"\n",
    "\n",
    "        # Test safety evaluation\n",
    "        safety_scores = evaluator.evaluate_safety(test_preds)\n",
    "        assert len(safety_scores) == 1, \"Safety evaluation failed\"\n",
    "        assert 0 <= safety_scores[0] <= 1, \"Safety score out of range\"\n",
    "\n",
    "        # Test factuality evaluation\n",
    "        fact_scores = evaluator.evaluate_factuality(test_preds, test_refs)\n",
    "        assert len(fact_scores) == 1, \"Factuality evaluation failed\"\n",
    "\n",
    "        print(\"âœ… All smoke tests passed!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Smoke test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_result = run_smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eeeaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 12. ç¸½çµèˆ‡ä½¿ç”¨æŒ‡å— (Summary & Usage Guide)\n",
    "\n",
    "# %%\n",
    "print(\n",
    "    \"\"\"\n",
    "ğŸ¯ æœ¬ç« å®Œæˆé …ç›® (Completed Items):\n",
    "âœ… å»ºç«‹å®Œæ•´çš„ LLM è©•ä¼°æ¡†æ¶\n",
    "âœ… å¯¦ä½œå¤šç¨®è‡ªå‹•åŒ–è©•ä¼°æŒ‡æ¨™ (BLEU, ROUGE, BERTScore)\n",
    "âœ… è¨­è¨ˆ LLM-as-a-Judge è©•ä¼°æ©Ÿåˆ¶\n",
    "âœ… åŠ å…¥äº‹å¯¦æ€§èˆ‡å®‰å…¨æ€§è©•ä¼°\n",
    "âœ… æä¾›æ•ˆèƒ½èˆ‡è³‡æºä½¿ç”¨è©•ä¼°\n",
    "âœ… å»ºç«‹æ¨¡å‹æ¯”è¼ƒæ¡†æ¶\n",
    "âœ… ç”Ÿæˆè©³ç´°è©•ä¼°å ±å‘Š\n",
    "\n",
    "ğŸ§  æ ¸å¿ƒæ¦‚å¿µ (Key Concepts):\n",
    "â€¢ è‡ªå‹•åŒ–æŒ‡æ¨™çš„é©ç”¨å ´æ™¯èˆ‡é™åˆ¶\n",
    "â€¢ LLM-as-a-Judge çš„å„ªå‹¢èˆ‡å¯¦ä½œæ–¹å¼\n",
    "â€¢ å¤šç¶­åº¦è©•ä¼°çš„é‡è¦æ€§ (å“è³ªã€å®‰å…¨ã€æ•ˆèƒ½)\n",
    "â€¢ è©•ä¼°çµæœçš„è§£è®€èˆ‡æ”¹å–„æ–¹å‘\n",
    "\n",
    "âš ï¸ å¸¸è¦‹é™·é˜± (Common Pitfalls):\n",
    "â€¢ éåº¦ä¾è³´å–®ä¸€æŒ‡æ¨™é€²è¡Œè©•ä¼°\n",
    "â€¢ å¿½ç•¥è©•ä¼°è³‡æ–™çš„å“è³ªèˆ‡å¤šæ¨£æ€§\n",
    "â€¢ æœªè€ƒæ…®è¨ˆç®—è³‡æºèˆ‡è©•ä¼°æˆæœ¬\n",
    "â€¢ ç¼ºä¹äººå·¥è©•ä¼°çš„è£œå……é©—è­‰\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥å»ºè­° (Next Steps):\n",
    "â€¢ æ“´å±•åˆ°ç‰¹å®šé ˜åŸŸçš„è©•ä¼°æŒ‡æ¨™\n",
    "â€¢ åŠ å…¥æ›´å¤šå®‰å…¨æ€§æª¢æ¸¬æ©Ÿåˆ¶\n",
    "â€¢ å»ºç«‹æŒçºŒè©•ä¼°èˆ‡ç›£æ§ç³»çµ±\n",
    "â€¢ æ•´åˆåˆ°æ¨¡å‹é–‹ç™¼æµç¨‹ä¸­\n",
    "\n",
    "ğŸ’¡ ä½•æ™‚ä½¿ç”¨æ­¤è©•ä¼°æ¡†æ¶:\n",
    "â€¢ æ¯”è¼ƒä¸åŒ LLM æ¨¡å‹çš„è¡¨ç¾\n",
    "â€¢ ç›£æ§æ¨¡å‹åœ¨ç”Ÿç”¢ç’°å¢ƒçš„å“è³ª\n",
    "â€¢ é©—è­‰å¾®èª¿æˆ–å„ªåŒ–çš„æ•ˆæœ\n",
    "â€¢ é€²è¡Œæ¨¡å‹é¸å‹æ±ºç­–\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“ ç”Ÿæˆçš„æª”æ¡ˆ (Generated Files):\")\n",
    "print(f\"â€¢ {report_filename} - è©³ç´°è©•ä¼°å ±å‘Š\")\n",
    "print(f\"â€¢ è©•ä¼°çµæœè¦–è¦ºåŒ–åœ–è¡¨\")\n",
    "\n",
    "print(\"\\nğŸ”— ç›¸é—œ notebooks:\")\n",
    "print(\"â€¢ nb11_instruction_tuning_demo.ipynb - æŒ‡ä»¤èª¿å„ªè³‡æ–™æº–å‚™\")\n",
    "print(\"â€¢ nb13_function_calling_tools.ipynb - å·¥å…·ä½¿ç”¨è©•ä¼°\")\n",
    "print(\"â€¢ nb20_lora_peft_tuning.ipynb - å¾®èª¿æ•ˆæœè©•ä¼°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f4347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === é©—æ”¶æ¸¬è©¦ (Acceptance Test) ===\n",
    "def acceptance_test():\n",
    "    \"\"\"One-click validation of evaluation framework\"\"\"\n",
    "    try:\n",
    "        # Initialize evaluator\n",
    "        evaluator = LLMEvaluator(language=\"zh\")\n",
    "\n",
    "        # Test data\n",
    "        preds = [\"AIæ˜¯äººå·¥æ™ºæ…§æŠ€è¡“\"]\n",
    "        refs = [\"äººå·¥æ™ºæ…§æ˜¯æ¨¡æ“¬äººé¡æ™ºæ…§çš„æŠ€è¡“\"]\n",
    "\n",
    "        # Run evaluations\n",
    "        metrics = evaluator.compute_automatic_metrics(preds, refs)\n",
    "        safety = evaluator.evaluate_safety(preds)\n",
    "\n",
    "        # Validate results\n",
    "        assert metrics[\"bleu\"] >= 0, \"BLEU failed\"\n",
    "        assert 0 <= safety[0] <= 1, \"Safety failed\"\n",
    "\n",
    "        print(\"âœ… Evaluation framework ready!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "acceptance_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598afa44",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## æœ¬ç« å°çµ (Chapter Summary)\n",
    "\n",
    "### âœ… å®Œæˆé …ç›® (Completed Items)\n",
    "- **å¤šç¶­åº¦è©•ä¼°æ¡†æ¶**: æ•´åˆè‡ªå‹•åŒ–æŒ‡æ¨™ã€LLMè©•å¯©ã€äº‹å¯¦æ€§èˆ‡å®‰å…¨æ€§è©•ä¼°\n",
    "- **ä¸­æ–‡å‹å–„è¨­è¨ˆ**: é‡å°ä¸­æ–‡æ–‡æœ¬å„ªåŒ–åˆ†è©èˆ‡è©•ä¼°é‚è¼¯\n",
    "- **ä½è³‡æºé©é…**: æ”¯æ´ 4-bit é‡åŒ–èˆ‡ CPU é™ç´šï¼Œé©ç”¨æ–¼æœ‰é™ VRAM ç’°å¢ƒ\n",
    "- **æ¨¡å‹æ¯”è¼ƒå·¥å…·**: æä¾›æ¨™æº–åŒ–çš„å¤šæ¨¡å‹æ•ˆèƒ½å°æ¯”åŠŸèƒ½\n",
    "- **è©•ä¼°å ±å‘Šç”Ÿæˆ**: è‡ªå‹•ç”¢ç”Ÿè©³ç´°çš„è©•ä¼°å ±å‘Šèˆ‡è¦–è¦ºåŒ–åœ–è¡¨\n",
    "\n",
    "### ğŸ§  æ ¸å¿ƒæ¦‚å¿µ (Key Concepts)\n",
    "- **è©•ä¼°æŒ‡æ¨™é¸æ“‡**: BLEU/ROUGE é©åˆæ–‡æœ¬é‡ç–Šåº¦ï¼ŒBERTScore æ•æ‰èªç¾©ç›¸ä¼¼æ€§\n",
    "- **LLM-as-a-Judge**: ä½¿ç”¨ LLM é€²è¡Œä¸»è§€å“è³ªè©•ä¼°ï¼Œæˆæœ¬è¼ƒä½ä½†éœ€æ³¨æ„åè¦‹\n",
    "- **å¤šç¶­åº¦å¹³è¡¡**: å“è³ªã€å®‰å…¨æ€§ã€æ•ˆèƒ½èˆ‡æˆæœ¬çš„æ¬Šè¡¡è€ƒé‡\n",
    "- **å¯é‡ç¾è©•ä¼°**: æ¨™æº–åŒ–æ¸¬è©¦é›†èˆ‡å›ºå®šéš¨æ©Ÿç¨®å­ç¢ºä¿çµæœä¸€è‡´æ€§\n",
    "\n",
    "### âš ï¸ å¸¸è¦‹é™·é˜± (Common Pitfalls)\n",
    "- **æŒ‡æ¨™å±€é™æ€§**: è‡ªå‹•åŒ–æŒ‡æ¨™ç„¡æ³•å®Œå…¨æ•æ‰èªç¾©å“è³ªèˆ‡å‰µé€ æ€§\n",
    "- **è©•ä¼°åè¦‹**: LLM è©•å¯©å¯èƒ½å¸¶æœ‰è¨“ç·´è³‡æ–™çš„åè¦‹\n",
    "- **è³‡æºæ¶ˆè€—**: å¤§è¦æ¨¡è©•ä¼°éœ€è¦å¤§é‡è¨ˆç®—è³‡æºèˆ‡æ™‚é–“\n",
    "- **é ˜åŸŸé©æ‡‰**: é€šç”¨æŒ‡æ¨™åœ¨ç‰¹å®šé ˜åŸŸå¯èƒ½ä¸å¤ æº–ç¢º\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å»ºè­° (Next Actions)\n",
    "1. **æ·±åŒ–è©•ä¼°**: çµåˆ **nb13_function_calling_tools.ipynb** è©•ä¼°å·¥å…·ä½¿ç”¨èƒ½åŠ›\n",
    "2. **é ˜åŸŸç‰¹åŒ–**: é‡å°ç‰¹å®šä»»å‹™ï¼ˆå¦‚ç¨‹å¼ç”Ÿæˆã€æ•¸å­¸æ¨ç†ï¼‰è¨­è¨ˆå°ˆé–€æŒ‡æ¨™\n",
    "3. **æŒçºŒè©•ä¼°**: å»ºç«‹ç·šä¸Šè©•ä¼°ç³»çµ±ï¼Œç›£æ§ç”Ÿç”¢ç’°å¢ƒæ¨¡å‹è¡¨ç¾\n",
    "4. **äººå·¥é©—è­‰**: è¨­è¨ˆäººå·¥è©•ä¼°ä»‹é¢ï¼Œè£œå……è‡ªå‹•åŒ–è©•ä¼°çš„ä¸è¶³\n",
    "\n",
    "é€™å€‹è©•ä¼°æ¡†æ¶ç‚ºå¾ŒçºŒçš„æ¨¡å‹å¾®èª¿ï¼ˆPart Dï¼‰å’Œ Agent é–‹ç™¼ï¼ˆPart Eï¼‰æä¾›äº†é‡è¦çš„å“è³ªä¿è­‰å·¥å…·ï¼Œæ˜¯ LLM æ‡‰ç”¨é–‹ç™¼ä¸­ä¸å¯æˆ–ç¼ºçš„ç’°ç¯€ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
