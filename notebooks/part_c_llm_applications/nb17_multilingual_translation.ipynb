{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === nb17_multilingual_translation.ipynb ===\n",
    "# å¤šèªžç”Ÿæˆèˆ‡ç¿»è­¯ (Multilingual Generation & Translation)\n",
    "# æ ¸å¿ƒç›®æ¨™: ZH-TW/ZH-CN/EN é›™å‘ç¿»è­¯ + å“è³ªè©•ä¼°\n",
    "\n",
    "# ===== CELL 1: Shared Cache Bootstrap =====\n",
    "import os, pathlib, torch, warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "cache_paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "\n",
    "for key, path in cache_paths.items():\n",
    "    os.environ[key] = path\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 2: Dependencies Installation =====\n",
    "# Install required packages for multilingual translation\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install_if_missing(packages):\n",
    "    \"\"\"Install packages if not already installed\"\"\"\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.split(\"==\")[0].replace(\"-\", \"_\"))\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "\n",
    "# Core translation packages\n",
    "translation_packages = [\n",
    "    \"opencc-python-reimplemented\",  # Traditional/Simplified Chinese conversion\n",
    "    \"langdetect\",  # Language detection\n",
    "    \"sacrebleu[ja]\",  # Translation metrics\n",
    "    \"comet-ml\",  # COMET evaluation (optional)\n",
    "    \"sentencepiece\",  # Tokenization\n",
    "    \"protobuf\",  # Protocol buffers for models\n",
    "]\n",
    "\n",
    "install_if_missing(translation_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 3: Language Detection Module =====\n",
    "from langdetect import detect, LangDetectException\n",
    "import opencc\n",
    "import re\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "\n",
    "class LanguageDetector:\n",
    "    \"\"\"Advanced language detection with Chinese variant support\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize OpenCC converters for Chinese variants\n",
    "        self.s2t = opencc.OpenCC(\"s2t\")  # Simplified to Traditional\n",
    "        self.t2s = opencc.OpenCC(\"t2s\")  # Traditional to Simplified\n",
    "\n",
    "        # Chinese character patterns\n",
    "        self.simplified_chars = set(\"äº¿ä¸‡ä¸Žä¸œä¸°ä¸¥ä¸§ä¸ªä¸¾ä¹…ä¹ˆä¹‰ä¹Œä¹ä¹žä¹¦\")\n",
    "        self.traditional_chars = set(\"å„„è¬èˆ‡æ±è±åš´å–ªå€‹èˆ‰ä¹…éº¼ç¾©çƒä¹ä¹žæ›¸\")\n",
    "\n",
    "    def detect_language(self, text: str) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Detect language with confidence score\n",
    "        Returns: (language_code, confidence)\n",
    "        \"\"\"\n",
    "        # Clean text for detection\n",
    "        clean_text = re.sub(r\"[^\\w\\s]\", \"\", text).strip()\n",
    "\n",
    "        if len(clean_text) < 3:\n",
    "            return \"unknown\", 0.0\n",
    "\n",
    "        try:\n",
    "            lang = detect(clean_text)\n",
    "\n",
    "            # Distinguish Chinese variants\n",
    "            if lang == \"zh-cn\":\n",
    "                return self._detect_chinese_variant(text)\n",
    "\n",
    "            return lang, 0.8\n",
    "\n",
    "        except LangDetectException:\n",
    "            return \"unknown\", 0.0\n",
    "\n",
    "    def _detect_chinese_variant(self, text: str) -> Tuple[str, float]:\n",
    "        \"\"\"Distinguish between Traditional and Simplified Chinese\"\"\"\n",
    "        simplified_count = sum(1 for char in text if char in self.simplified_chars)\n",
    "        traditional_count = sum(1 for char in text if char in self.traditional_chars)\n",
    "\n",
    "        total_indicators = simplified_count + traditional_count\n",
    "\n",
    "        if total_indicators == 0:\n",
    "            return \"zh\", 0.5  # Generic Chinese\n",
    "\n",
    "        if simplified_count > traditional_count:\n",
    "            return \"zh-cn\", 0.7\n",
    "        else:\n",
    "            return \"zh-tw\", 0.7\n",
    "\n",
    "    def normalize_chinese(self, text: str, target_variant: str = \"zh-tw\") -> str:\n",
    "        \"\"\"Convert between Chinese variants\"\"\"\n",
    "        if target_variant == \"zh-tw\":\n",
    "            return self.s2t.convert(text)\n",
    "        elif target_variant == \"zh-cn\":\n",
    "            return self.t2s.convert(text)\n",
    "        return text\n",
    "\n",
    "\n",
    "# Test language detection\n",
    "detector = LanguageDetector()\n",
    "\n",
    "test_texts = [\n",
    "    \"Hello, how are you today?\",\n",
    "    \"ä½ å¥½ï¼Œä»Šå¤©è¿‡å¾—æ€Žä¹ˆæ ·ï¼Ÿ\",  # Simplified Chinese\n",
    "    \"ä½ å¥½ï¼Œä»Šå¤©éŽå¾—æ€Žéº¼æ¨£ï¼Ÿ\",  # Traditional Chinese\n",
    "    \"ã“ã‚“ã«ã¡ã¯ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ\",  # Japanese\n",
    "]\n",
    "\n",
    "print(\"=== Language Detection Test ===\")\n",
    "for text in test_texts:\n",
    "    lang, conf = detector.detect_language(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Detected: {lang} (confidence: {conf:.2f})\")\n",
    "    if lang.startswith(\"zh\"):\n",
    "        normalized = detector.normalize_chinese(text, \"zh-tw\")\n",
    "        print(f\"Normalized (ZH-TW): {normalized}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa5f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 4: Translation Model Setup =====\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch\n",
    "\n",
    "\n",
    "class MultilingualTranslator:\n",
    "    \"\"\"Unified interface for multilingual translation models\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"facebook/nllb-200-distilled-600M\",\n",
    "        device_map: str = \"auto\",\n",
    "        load_in_4bit: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize translation model with low-VRAM optimizations\n",
    "\n",
    "        Supported models:\n",
    "        - facebook/nllb-200-distilled-600M (compact, 600M params)\n",
    "        - facebook/nllb-200-1.3B (better quality, higher VRAM)\n",
    "        - google/mt5-small (T5-based, good for fine-tuning)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        print(f\"Loading translation model: {model_name}\")\n",
    "\n",
    "        # Configure quantization for low VRAM\n",
    "        quant_config = None\n",
    "        if load_in_4bit and torch.cuda.is_available():\n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name, cache_dir=os.environ[\"TRANSFORMERS_CACHE\"]\n",
    "            )\n",
    "\n",
    "            # Load model with optimizations\n",
    "            model_kwargs = {\n",
    "                \"cache_dir\": os.environ[\"TRANSFORMERS_CACHE\"],\n",
    "                \"device_map\": device_map,\n",
    "                \"torch_dtype\": torch.float16,\n",
    "            }\n",
    "\n",
    "            if quant_config:\n",
    "                model_kwargs[\"quantization_config\"] = quant_config\n",
    "\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                model_name, **model_kwargs\n",
    "            )\n",
    "\n",
    "            # NLLB language code mappings\n",
    "            self.lang_codes = {\n",
    "                \"en\": \"eng_Latn\",\n",
    "                \"zh-cn\": \"zho_Hans\",\n",
    "                \"zh-tw\": \"zho_Hant\",\n",
    "                \"zh\": \"zho_Hans\",  # Default to simplified\n",
    "                \"ja\": \"jpn_Jpan\",\n",
    "                \"ko\": \"kor_Hang\",\n",
    "                \"fr\": \"fra_Latn\",\n",
    "                \"de\": \"deu_Latn\",\n",
    "                \"es\": \"spa_Latn\",\n",
    "            }\n",
    "\n",
    "            print(f\"âœ… Model loaded successfully on {self.device}\")\n",
    "            print(f\"ðŸ“Š Model memory footprint: ~{self._estimate_memory():.1f}GB\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to load model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _estimate_memory(self) -> float:\n",
    "        \"\"\"Estimate model memory usage\"\"\"\n",
    "        if hasattr(self.model, \"num_parameters\"):\n",
    "            params = self.model.num_parameters()\n",
    "        else:\n",
    "            params = sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "        # Rough estimation: 4 bytes per parameter (float32) or 2 bytes (float16)\n",
    "        bytes_per_param = (\n",
    "            2\n",
    "            if hasattr(self.model, \"config\")\n",
    "            and getattr(self.model.config, \"torch_dtype\", None) == torch.float16\n",
    "            else 4\n",
    "        )\n",
    "\n",
    "        return (params * bytes_per_param) / (1024**3)\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        text: str,\n",
    "        source_lang: str,\n",
    "        target_lang: str,\n",
    "        max_length: int = 512,\n",
    "        num_beams: int = 4,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Translate text between languages\n",
    "\n",
    "        Args:\n",
    "            text: Source text to translate\n",
    "            source_lang: Source language code (en, zh-cn, zh-tw, etc.)\n",
    "            target_lang: Target language code\n",
    "            max_length: Maximum output length\n",
    "            num_beams: Beam search width (higher = better quality, slower)\n",
    "        \"\"\"\n",
    "        # Map language codes to model-specific format\n",
    "        src_code = self.lang_codes.get(source_lang, source_lang)\n",
    "        tgt_code = self.lang_codes.get(target_lang, target_lang)\n",
    "\n",
    "        # For NLLB models, use forced_bos_token_id\n",
    "        if \"nllb\" in self.model_name.lower():\n",
    "            # Set source language\n",
    "            self.tokenizer.src_lang = src_code\n",
    "\n",
    "            # Tokenize input\n",
    "            encoded = self.tokenizer(\n",
    "                text, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Get target language token id\n",
    "            tgt_lang_id = self.tokenizer.convert_tokens_to_ids(tgt_code)\n",
    "\n",
    "            # Generate translation\n",
    "            with torch.no_grad():\n",
    "                generated_tokens = self.model.generate(\n",
    "                    **encoded,\n",
    "                    forced_bos_token_id=tgt_lang_id,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=num_beams,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                )\n",
    "\n",
    "            # Decode output\n",
    "            translated = self.tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True\n",
    "            )[0]\n",
    "\n",
    "        else:\n",
    "            # For other models (mT5, etc.), use prefix-based approach\n",
    "            prefix = f\"translate {source_lang} to {target_lang}: \"\n",
    "            input_text = prefix + text\n",
    "\n",
    "            encoded = self.tokenizer(\n",
    "                input_text, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **encoded,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=num_beams,\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "\n",
    "            translated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        return translated.strip()\n",
    "\n",
    "    def batch_translate(\n",
    "        self, texts: list, source_lang: str, target_lang: str, batch_size: int = 8\n",
    "    ) -> list:\n",
    "        \"\"\"Translate multiple texts efficiently\"\"\"\n",
    "        results = []\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            batch_results = []\n",
    "\n",
    "            for text in batch:\n",
    "                try:\n",
    "                    translated = self.translate(text, source_lang, target_lang)\n",
    "                    batch_results.append(translated)\n",
    "                except Exception as e:\n",
    "                    print(f\"Translation failed for text: {text[:50]}... Error: {e}\")\n",
    "                    batch_results.append(\"\")\n",
    "\n",
    "            results.extend(batch_results)\n",
    "\n",
    "            # Progress indicator\n",
    "            if len(texts) > batch_size:\n",
    "                print(\n",
    "                    f\"Translated {min(i + batch_size, len(texts))}/{len(texts)} texts\"\n",
    "                )\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize translator with low-VRAM settings\n",
    "print(\"ðŸ”„ Initializing multilingual translator...\")\n",
    "translator = MultilingualTranslator(\n",
    "    model_name=\"facebook/nllb-200-distilled-600M\",  # Compact model\n",
    "    load_in_4bit=True,  # Enable if VRAM < 8GB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a28aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 5: Translation Pipeline Demo =====\n",
    "class TranslationPipeline:\n",
    "    \"\"\"Complete translation pipeline with quality control\"\"\"\n",
    "\n",
    "    def __init__(self, translator: MultilingualTranslator, detector: LanguageDetector):\n",
    "        self.translator = translator\n",
    "        self.detector = detector\n",
    "\n",
    "    def auto_translate(\n",
    "        self, text: str, target_lang: str = \"en\", auto_detect: bool = True\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Auto-detect source language and translate\n",
    "\n",
    "        Returns:\n",
    "            dict with source_lang, target_lang, original_text, translated_text, confidence\n",
    "        \"\"\"\n",
    "        # Detect source language\n",
    "        if auto_detect:\n",
    "            source_lang, confidence = self.detector.detect_language(text)\n",
    "        else:\n",
    "            source_lang, confidence = \"unknown\", 0.0\n",
    "\n",
    "        # Handle Chinese variants\n",
    "        processed_text = text\n",
    "        if source_lang.startswith(\"zh\") and target_lang == \"en\":\n",
    "            # Normalize Chinese text before translation\n",
    "            processed_text = self.detector.normalize_chinese(text, \"zh-cn\")\n",
    "            source_lang = \"zh-cn\"\n",
    "\n",
    "        # Skip translation if source and target are the same\n",
    "        if source_lang == target_lang:\n",
    "            return {\n",
    "                \"source_lang\": source_lang,\n",
    "                \"target_lang\": target_lang,\n",
    "                \"original_text\": text,\n",
    "                \"translated_text\": text,\n",
    "                \"confidence\": confidence,\n",
    "                \"status\": \"no_translation_needed\",\n",
    "            }\n",
    "\n",
    "        # Perform translation\n",
    "        try:\n",
    "            translated_text = self.translator.translate(\n",
    "                processed_text, source_lang, target_lang\n",
    "            )\n",
    "\n",
    "            # Post-processing for Chinese output\n",
    "            if target_lang == \"zh-tw\":\n",
    "                translated_text = self.detector.normalize_chinese(\n",
    "                    translated_text, \"zh-tw\"\n",
    "                )\n",
    "\n",
    "            return {\n",
    "                \"source_lang\": source_lang,\n",
    "                \"target_lang\": target_lang,\n",
    "                \"original_text\": text,\n",
    "                \"translated_text\": translated_text,\n",
    "                \"confidence\": confidence,\n",
    "                \"status\": \"success\",\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"source_lang\": source_lang,\n",
    "                \"target_lang\": target_lang,\n",
    "                \"original_text\": text,\n",
    "                \"translated_text\": \"\",\n",
    "                \"confidence\": confidence,\n",
    "                \"status\": f\"error: {str(e)}\",\n",
    "            }\n",
    "\n",
    "    def bidirectional_translate(self, text: str, lang_pair: Tuple[str, str]) -> Dict:\n",
    "        \"\"\"Translate text in both directions to check consistency\"\"\"\n",
    "        lang1, lang2 = lang_pair\n",
    "\n",
    "        # Forward translation\n",
    "        forward = self.auto_translate(text, target_lang=lang2, auto_detect=False)\n",
    "        forward[\"source_lang\"] = lang1\n",
    "\n",
    "        # Backward translation (for quality assessment)\n",
    "        if forward[\"status\"] == \"success\":\n",
    "            backward = self.auto_translate(\n",
    "                forward[\"translated_text\"], target_lang=lang1, auto_detect=False\n",
    "            )\n",
    "            backward[\"source_lang\"] = lang2\n",
    "        else:\n",
    "            backward = {\"status\": \"failed_forward\"}\n",
    "\n",
    "        return {\n",
    "            \"forward\": forward,\n",
    "            \"backward\": backward,\n",
    "            \"round_trip_quality\": self._assess_round_trip(\n",
    "                text, backward.get(\"translated_text\", \"\")\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def _assess_round_trip(self, original: str, round_trip: str) -> Dict:\n",
    "        \"\"\"Simple round-trip translation quality assessment\"\"\"\n",
    "        if not round_trip:\n",
    "            return {\"score\": 0.0, \"assessment\": \"failed\"}\n",
    "\n",
    "        # Simple character-level similarity\n",
    "        original_chars = set(original.lower())\n",
    "        round_trip_chars = set(round_trip.lower())\n",
    "\n",
    "        if not original_chars:\n",
    "            return {\"score\": 0.0, \"assessment\": \"empty_original\"}\n",
    "\n",
    "        intersection = original_chars.intersection(round_trip_chars)\n",
    "        similarity = len(intersection) / len(original_chars)\n",
    "\n",
    "        if similarity > 0.7:\n",
    "            assessment = \"good\"\n",
    "        elif similarity > 0.4:\n",
    "            assessment = \"fair\"\n",
    "        else:\n",
    "            assessment = \"poor\"\n",
    "\n",
    "        return {\"score\": similarity, \"assessment\": assessment}\n",
    "\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = TranslationPipeline(translator, detector)\n",
    "\n",
    "# Demo translations\n",
    "demo_texts = [\n",
    "    \"The weather is beautiful today. Let's go for a walk in the park.\",\n",
    "    \"äººå·¥æ™ºèƒ½æŠ€è¡“æ­£åœ¨å¿«é€Ÿç™¼å±•ï¼Œæ”¹è®Šè‘—æˆ‘å€‘çš„ç”Ÿæ´»æ–¹å¼ã€‚\",\n",
    "    \"æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬è§£å†³å¤æ‚çš„é—®é¢˜ã€‚\",\n",
    "    \"Technology companies are investing heavily in AI research.\",\n",
    "]\n",
    "\n",
    "print(\"=== Translation Pipeline Demo ===\")\n",
    "for i, text in enumerate(demo_texts, 1):\n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    print(f\"Original: {text}\")\n",
    "\n",
    "    # Auto-translate to English\n",
    "    result_en = pipeline.auto_translate(text, target_lang=\"en\")\n",
    "    print(f\"To English: {result_en['translated_text']}\")\n",
    "    print(f\"Detected: {result_en['source_lang']} (conf: {result_en['confidence']:.2f})\")\n",
    "\n",
    "    # If original was English, translate to Chinese\n",
    "    if result_en[\"source_lang\"] == \"en\":\n",
    "        result_zh = pipeline.auto_translate(text, target_lang=\"zh-tw\")\n",
    "        print(f\"To Chinese: {result_zh['translated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 6: Translation Quality Evaluation =====\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    \"\"\"Translation quality evaluation metrics\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def chrf_score(\n",
    "        self, reference: str, hypothesis: str, n_gram: int = 6, beta: float = 2.0\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate chrF++ score (character-level F-score)\n",
    "        Better for morphologically rich languages like Chinese\n",
    "        \"\"\"\n",
    "\n",
    "        def get_char_ngrams(text: str, n: int) -> Counter:\n",
    "            \"\"\"Extract character n-grams\"\"\"\n",
    "            text = re.sub(r\"\\s+\", \"\", text.lower())  # Remove spaces, lowercase\n",
    "            ngrams = []\n",
    "            for i in range(len(text) - n + 1):\n",
    "                ngrams.append(text[i : i + n])\n",
    "            return Counter(ngrams)\n",
    "\n",
    "        # Calculate precision and recall for each n-gram level\n",
    "        total_precision = 0.0\n",
    "        total_recall = 0.0\n",
    "\n",
    "        for n in range(1, n_gram + 1):\n",
    "            ref_ngrams = get_char_ngrams(reference, n)\n",
    "            hyp_ngrams = get_char_ngrams(hypothesis, n)\n",
    "\n",
    "            if not hyp_ngrams:\n",
    "                continue\n",
    "\n",
    "            # Calculate matches\n",
    "            matches = 0\n",
    "            for ngram, count in hyp_ngrams.items():\n",
    "                matches += min(count, ref_ngrams.get(ngram, 0))\n",
    "\n",
    "            # Precision and recall\n",
    "            precision = matches / sum(hyp_ngrams.values()) if hyp_ngrams else 0.0\n",
    "            recall = matches / sum(ref_ngrams.values()) if ref_ngrams else 0.0\n",
    "\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "\n",
    "        # Average precision and recall\n",
    "        avg_precision = total_precision / n_gram\n",
    "        avg_recall = total_recall / n_gram\n",
    "\n",
    "        # F-score with beta weighting\n",
    "        if avg_precision + avg_recall == 0:\n",
    "            return 0.0\n",
    "\n",
    "        f_score = (\n",
    "            (1 + beta**2)\n",
    "            * avg_precision\n",
    "            * avg_recall\n",
    "            / (beta**2 * avg_precision + avg_recall)\n",
    "        )\n",
    "\n",
    "        return f_score * 100  # Return as percentage\n",
    "\n",
    "    def bleu_score_simple(\n",
    "        self, reference: str, hypothesis: str, n_gram: int = 4\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Simplified BLEU score calculation\n",
    "        Word-level n-gram matching\n",
    "        \"\"\"\n",
    "\n",
    "        def get_word_ngrams(text: str, n: int) -> Counter:\n",
    "            words = text.lower().split()\n",
    "            ngrams = []\n",
    "            for i in range(len(words) - n + 1):\n",
    "                ngrams.append(\" \".join(words[i : i + n]))\n",
    "            return Counter(ngrams)\n",
    "\n",
    "        # Calculate precision for each n-gram level\n",
    "        precisions = []\n",
    "\n",
    "        for n in range(1, n_gram + 1):\n",
    "            ref_ngrams = get_word_ngrams(reference, n)\n",
    "            hyp_ngrams = get_word_ngrams(hypothesis, n)\n",
    "\n",
    "            if not hyp_ngrams:\n",
    "                precisions.append(0.0)\n",
    "                continue\n",
    "\n",
    "            matches = 0\n",
    "            for ngram, count in hyp_ngrams.items():\n",
    "                matches += min(count, ref_ngrams.get(ngram, 0))\n",
    "\n",
    "            precision = matches / sum(hyp_ngrams.values())\n",
    "            precisions.append(precision)\n",
    "\n",
    "        # Geometric mean of precisions\n",
    "        if any(p == 0 for p in precisions):\n",
    "            return 0.0\n",
    "\n",
    "        geometric_mean = math.exp(\n",
    "            sum(math.log(p) for p in precisions) / len(precisions)\n",
    "        )\n",
    "\n",
    "        # Brevity penalty\n",
    "        ref_len = len(reference.split())\n",
    "        hyp_len = len(hypothesis.split())\n",
    "\n",
    "        if hyp_len > ref_len:\n",
    "            bp = 1.0\n",
    "        else:\n",
    "            bp = math.exp(1 - ref_len / hyp_len) if hyp_len > 0 else 0.0\n",
    "\n",
    "        return bp * geometric_mean * 100\n",
    "\n",
    "    def semantic_similarity_simple(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"\n",
    "        Simple semantic similarity based on word overlap\n",
    "        More sophisticated approaches would use embeddings\n",
    "        \"\"\"\n",
    "        words1 = set(text1.lower().split())\n",
    "        words2 = set(text2.lower().split())\n",
    "\n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "\n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "\n",
    "        jaccard = len(intersection) / len(union)\n",
    "        return jaccard * 100\n",
    "\n",
    "    def evaluate_translation(\n",
    "        self, source: str, reference: str, hypothesis: str\n",
    "    ) -> Dict:\n",
    "        \"\"\"Comprehensive translation evaluation\"\"\"\n",
    "        return {\n",
    "            \"chrf_score\": self.chrf_score(reference, hypothesis),\n",
    "            \"bleu_score\": self.bleu_score_simple(reference, hypothesis),\n",
    "            \"semantic_similarity\": self.semantic_similarity_simple(\n",
    "                reference, hypothesis\n",
    "            ),\n",
    "            \"length_ratio\": len(hypothesis) / len(reference) if reference else 0.0,\n",
    "            \"source_text\": source,\n",
    "            \"reference_text\": reference,\n",
    "            \"hypothesis_text\": hypothesis,\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = TranslationEvaluator()\n",
    "\n",
    "# Evaluation examples\n",
    "evaluation_examples = [\n",
    "    {\n",
    "        \"source\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"reference\": \"æ•æ·çš„æ£•è‰²ç‹ç‹¸è·³éŽäº†æ‡¶æƒ°çš„ç‹—ã€‚\",\n",
    "        \"hypothesis\": \"æ•æ·çš„æ£•è‰²ç‹ç‹¸è·³éŽæ‡¶ç‹—ã€‚\",  # Slightly different\n",
    "    },\n",
    "    {\n",
    "        \"source\": \"äººå·¥æ™ºèƒ½å°‡æœƒæ”¹è®Šä¸–ç•Œã€‚\",\n",
    "        \"reference\": \"Artificial intelligence will change the world.\",\n",
    "        \"hypothesis\": \"AI will transform the world.\",  # Paraphrased\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"=== Translation Quality Evaluation ===\")\n",
    "for i, example in enumerate(evaluation_examples, 1):\n",
    "    print(f\"\\n--- Evaluation {i} ---\")\n",
    "    results = evaluator.evaluate_translation(\n",
    "        example[\"source\"], example[\"reference\"], example[\"hypothesis\"]\n",
    "    )\n",
    "\n",
    "    print(f\"Source: {results['source_text']}\")\n",
    "    print(f\"Reference: {results['reference_text']}\")\n",
    "    print(f\"Hypothesis: {results['hypothesis_text']}\")\n",
    "    print(f\"chrF++: {results['chrf_score']:.2f}\")\n",
    "    print(f\"BLEU: {results['bleu_score']:.2f}\")\n",
    "    print(f\"Semantic: {results['semantic_similarity']:.2f}\")\n",
    "    print(f\"Length ratio: {results['length_ratio']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 7: Domain-Specific Translation =====\n",
    "class DomainTranslator:\n",
    "    \"\"\"Domain-specific translation with terminology management\"\"\"\n",
    "\n",
    "    def __init__(self, pipeline: TranslationPipeline):\n",
    "        self.pipeline = pipeline\n",
    "\n",
    "        # Domain-specific terminology dictionaries\n",
    "        self.domain_terms = {\n",
    "            \"tech\": {\n",
    "                \"en_to_zh\": {\n",
    "                    \"artificial intelligence\": \"äººå·¥æ™ºæ…§\",\n",
    "                    \"machine learning\": \"æ©Ÿå™¨å­¸ç¿’\",\n",
    "                    \"deep learning\": \"æ·±åº¦å­¸ç¿’\",\n",
    "                    \"neural network\": \"ç¥žç¶“ç¶²è·¯\",\n",
    "                    \"algorithm\": \"æ¼”ç®—æ³•\",\n",
    "                    \"database\": \"è³‡æ–™åº«\",\n",
    "                    \"software\": \"è»Ÿé«”\",\n",
    "                    \"programming\": \"ç¨‹å¼è¨­è¨ˆ\",\n",
    "                    \"API\": \"API\",\n",
    "                    \"cloud computing\": \"é›²ç«¯é‹ç®—\",\n",
    "                },\n",
    "                \"zh_to_en\": {\n",
    "                    \"äººå·¥æ™ºæ…§\": \"artificial intelligence\",\n",
    "                    \"æ©Ÿå™¨å­¸ç¿’\": \"machine learning\",\n",
    "                    \"æ·±åº¦å­¸ç¿’\": \"deep learning\",\n",
    "                    \"ç¥žç¶“ç¶²è·¯\": \"neural network\",\n",
    "                    \"æ¼”ç®—æ³•\": \"algorithm\",\n",
    "                    \"è³‡æ–™åº«\": \"database\",\n",
    "                    \"è»Ÿé«”\": \"software\",\n",
    "                    \"ç¨‹å¼è¨­è¨ˆ\": \"programming\",\n",
    "                    \"é›²ç«¯é‹ç®—\": \"cloud computing\",\n",
    "                },\n",
    "            },\n",
    "            \"medical\": {\n",
    "                \"en_to_zh\": {\n",
    "                    \"diagnosis\": \"è¨ºæ–·\",\n",
    "                    \"treatment\": \"æ²»ç™‚\",\n",
    "                    \"symptoms\": \"ç—‡ç‹€\",\n",
    "                    \"prescription\": \"è™•æ–¹\",\n",
    "                    \"medicine\": \"è—¥ç‰©\",\n",
    "                    \"surgery\": \"æ‰‹è¡“\",\n",
    "                    \"patient\": \"ç—…æ‚£\",\n",
    "                    \"doctor\": \"é†«å¸«\",\n",
    "                },\n",
    "                \"zh_to_en\": {\n",
    "                    \"è¨ºæ–·\": \"diagnosis\",\n",
    "                    \"æ²»ç™‚\": \"treatment\",\n",
    "                    \"ç—‡ç‹€\": \"symptoms\",\n",
    "                    \"è™•æ–¹\": \"prescription\",\n",
    "                    \"è—¥ç‰©\": \"medicine\",\n",
    "                    \"æ‰‹è¡“\": \"surgery\",\n",
    "                    \"ç—…æ‚£\": \"patient\",\n",
    "                    \"é†«å¸«\": \"doctor\",\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def apply_terminology(\n",
    "        self, text: str, domain: str, direction: str = \"en_to_zh\"\n",
    "    ) -> str:\n",
    "        \"\"\"Apply domain-specific terminology replacements\"\"\"\n",
    "        if domain not in self.domain_terms:\n",
    "            return text\n",
    "\n",
    "        terms = self.domain_terms[domain].get(direction, {})\n",
    "        modified_text = text\n",
    "\n",
    "        # Sort by length (longer terms first) to avoid partial replacements\n",
    "        sorted_terms = sorted(terms.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "        for source_term, target_term in sorted_terms:\n",
    "            # Case-insensitive replacement\n",
    "            pattern = re.compile(re.escape(source_term), re.IGNORECASE)\n",
    "            modified_text = pattern.sub(target_term, modified_text)\n",
    "\n",
    "        return modified_text\n",
    "\n",
    "    def domain_translate(\n",
    "        self, text: str, target_lang: str, domain: str = \"general\"\n",
    "    ) -> Dict:\n",
    "        \"\"\"Translate with domain-specific terminology\"\"\"\n",
    "        # Determine translation direction\n",
    "        source_lang, _ = self.pipeline.detector.detect_language(text)\n",
    "\n",
    "        if domain != \"general\":\n",
    "            if source_lang == \"en\" and target_lang.startswith(\"zh\"):\n",
    "                direction = \"en_to_zh\"\n",
    "            elif source_lang.startswith(\"zh\") and target_lang == \"en\":\n",
    "                direction = \"zh_to_en\"\n",
    "            else:\n",
    "                direction = None\n",
    "\n",
    "            # Apply pre-translation terminology\n",
    "            if direction:\n",
    "                preprocessed_text = self.apply_terminology(text, domain, direction)\n",
    "            else:\n",
    "                preprocessed_text = text\n",
    "        else:\n",
    "            preprocessed_text = text\n",
    "\n",
    "        # Perform translation\n",
    "        result = self.pipeline.auto_translate(preprocessed_text, target_lang)\n",
    "\n",
    "        # Apply post-translation terminology if needed\n",
    "        if domain != \"general\" and result[\"status\"] == \"success\":\n",
    "            # Reverse direction for post-processing\n",
    "            reverse_direction = None\n",
    "            if direction == \"en_to_zh\":\n",
    "                reverse_direction = \"zh_to_en\"\n",
    "            elif direction == \"zh_to_en\":\n",
    "                reverse_direction = \"en_to_zh\"\n",
    "\n",
    "            if reverse_direction:\n",
    "                # Check if any terms were missed and need correction\n",
    "                corrected_text = self.apply_terminology(\n",
    "                    result[\"translated_text\"], domain, direction\n",
    "                )\n",
    "                result[\"translated_text\"] = corrected_text\n",
    "\n",
    "        result[\"domain\"] = domain\n",
    "        result[\"preprocessing_applied\"] = preprocessed_text != text\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# Initialize domain translator\n",
    "domain_translator = DomainTranslator(pipeline)\n",
    "\n",
    "# Demo domain-specific translation\n",
    "domain_examples = [\n",
    "    {\n",
    "        \"text\": \"Machine learning algorithms can analyze medical data to improve diagnosis accuracy.\",\n",
    "        \"domain\": \"tech\",\n",
    "        \"target_lang\": \"zh-tw\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The patient needs surgery and the doctor will prescribe medicine for treatment.\",\n",
    "        \"domain\": \"medical\",\n",
    "        \"target_lang\": \"zh-tw\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"äººå·¥æ™ºæ…§æ¼”ç®—æ³•å¯ä»¥å¹«åŠ©é†«å¸«é€²è¡Œæ›´æº–ç¢ºçš„è¨ºæ–·ã€‚\",\n",
    "        \"domain\": \"tech\",\n",
    "        \"target_lang\": \"en\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"=== Domain-Specific Translation Demo ===\")\n",
    "for i, example in enumerate(domain_examples, 1):\n",
    "    print(f\"\\n--- Domain Example {i} ---\")\n",
    "    print(f\"Domain: {example['domain']}\")\n",
    "    print(f\"Original: {example['text']}\")\n",
    "\n",
    "    result = domain_translator.domain_translate(\n",
    "        example[\"text\"], example[\"target_lang\"], example[\"domain\"]\n",
    "    )\n",
    "\n",
    "    print(f\"Translated: {result['translated_text']}\")\n",
    "    print(f\"Preprocessing applied: {result['preprocessing_applied']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 8: Batch Translation & Quality Assessment =====\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class BatchTranslationProcessor:\n",
    "    \"\"\"Process large batches of translations with quality monitoring\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, domain_translator: DomainTranslator, evaluator: TranslationEvaluator\n",
    "    ):\n",
    "        self.domain_translator = domain_translator\n",
    "        self.evaluator = evaluator\n",
    "\n",
    "    def process_batch(\n",
    "        self,\n",
    "        texts: list,\n",
    "        target_lang: str,\n",
    "        domain: str = \"general\",\n",
    "        include_evaluation: bool = True,\n",
    "        reference_translations: list = None,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a batch of texts with quality monitoring\n",
    "\n",
    "        Args:\n",
    "            texts: List of source texts\n",
    "            target_lang: Target language code\n",
    "            domain: Domain for terminology ('general', 'tech', 'medical')\n",
    "            include_evaluation: Whether to include quality metrics\n",
    "            reference_translations: Reference translations for evaluation\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            \"metadata\": {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"total_texts\": len(texts),\n",
    "                \"target_language\": target_lang,\n",
    "                \"domain\": domain,\n",
    "                \"include_evaluation\": include_evaluation,\n",
    "            },\n",
    "            \"translations\": [],\n",
    "            \"statistics\": {},\n",
    "        }\n",
    "\n",
    "        successful_translations = 0\n",
    "        total_chrf = 0\n",
    "        total_bleu = 0\n",
    "        processing_times = []\n",
    "\n",
    "        print(f\"Processing {len(texts)} texts for {domain} domain -> {target_lang}\")\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            start_time = datetime.now()\n",
    "\n",
    "            # Translate\n",
    "            translation_result = self.domain_translator.domain_translate(\n",
    "                text, target_lang, domain\n",
    "            )\n",
    "\n",
    "            processing_time = (datetime.now() - start_time).total_seconds()\n",
    "            processing_times.append(processing_time)\n",
    "\n",
    "            # Prepare result entry\n",
    "            entry = {\n",
    "                \"index\": i,\n",
    "                \"source_text\": text,\n",
    "                \"translated_text\": translation_result.get(\"translated_text\", \"\"),\n",
    "                \"source_language\": translation_result.get(\"source_lang\", \"unknown\"),\n",
    "                \"status\": translation_result.get(\"status\", \"unknown\"),\n",
    "                \"processing_time_seconds\": processing_time,\n",
    "            }\n",
    "\n",
    "            # Add evaluation if requested and reference is available\n",
    "            if (\n",
    "                include_evaluation\n",
    "                and reference_translations\n",
    "                and i < len(reference_translations)\n",
    "                and translation_result.get(\"status\") == \"success\"\n",
    "            ):\n",
    "\n",
    "                eval_result = self.evaluator.evaluate_translation(\n",
    "                    text,\n",
    "                    reference_translations[i],\n",
    "                    translation_result[\"translated_text\"],\n",
    "                )\n",
    "\n",
    "                entry[\"evaluation\"] = {\n",
    "                    \"chrf_score\": eval_result[\"chrf_score\"],\n",
    "                    \"bleu_score\": eval_result[\"bleu_score\"],\n",
    "                    \"semantic_similarity\": eval_result[\"semantic_similarity\"],\n",
    "                    \"length_ratio\": eval_result[\"length_ratio\"],\n",
    "                }\n",
    "\n",
    "                # Accumulate for statistics\n",
    "                total_chrf += eval_result[\"chrf_score\"]\n",
    "                total_bleu += eval_result[\"bleu_score\"]\n",
    "\n",
    "            results[\"translations\"].append(entry)\n",
    "\n",
    "            if translation_result.get(\"status\") == \"success\":\n",
    "                successful_translations += 1\n",
    "\n",
    "            # Progress indicator\n",
    "            if i % 10 == 0 or i == len(texts) - 1:\n",
    "                print(f\"Processed {i+1}/{len(texts)} texts\")\n",
    "\n",
    "        # Calculate statistics\n",
    "        results[\"statistics\"] = {\n",
    "            \"success_rate\": successful_translations / len(texts),\n",
    "            \"average_processing_time\": sum(processing_times) / len(processing_times),\n",
    "            \"total_processing_time\": sum(processing_times),\n",
    "        }\n",
    "\n",
    "        if include_evaluation and reference_translations:\n",
    "            evaluated_count = sum(\n",
    "                1 for t in results[\"translations\"] if \"evaluation\" in t\n",
    "            )\n",
    "            if evaluated_count > 0:\n",
    "                results[\"statistics\"][\"average_chrf\"] = total_chrf / evaluated_count\n",
    "                results[\"statistics\"][\"average_bleu\"] = total_bleu / evaluated_count\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_results(self, results: Dict, filename: str):\n",
    "        \"\"\"Save translation results to JSON file\"\"\"\n",
    "        output_path = f\"{filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Results saved to: {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    def create_report(self, results: Dict) -> str:\n",
    "        \"\"\"Generate a summary report\"\"\"\n",
    "        stats = results[\"statistics\"]\n",
    "        meta = results[\"metadata\"]\n",
    "\n",
    "        report = f\"\"\"\n",
    "=== Translation Batch Report ===\n",
    "Timestamp: {meta['timestamp']}\n",
    "Total texts: {meta['total_texts']}\n",
    "Target language: {meta['target_language']}\n",
    "Domain: {meta['domain']}\n",
    "\n",
    "Performance:\n",
    "- Success rate: {stats['success_rate']:.2%}\n",
    "- Average processing time: {stats['average_processing_time']:.2f}s\n",
    "- Total processing time: {stats['total_processing_time']:.1f}s\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        if \"average_chrf\" in stats:\n",
    "            report += f\"\"\"Quality Metrics:\n",
    "- Average chrF++: {stats['average_chrf']:.2f}\n",
    "- Average BLEU: {stats['average_bleu']:.2f}\n",
    "\"\"\"\n",
    "\n",
    "        # Add samples\n",
    "        successful_translations = [\n",
    "            t for t in results[\"translations\"] if t[\"status\"] == \"success\"\n",
    "        ]\n",
    "        if successful_translations:\n",
    "            report += \"\\nSample Translations:\\n\"\n",
    "            for i, sample in enumerate(successful_translations[:3]):\n",
    "                report += f\"\"\"\n",
    "{i+1}. Source: {sample['source_text'][:100]}...\n",
    "   Translation: {sample['translated_text'][:100]}...\n",
    "   Time: {sample['processing_time_seconds']:.2f}s\n",
    "\"\"\"\n",
    "\n",
    "        return report\n",
    "\n",
    "\n",
    "# Initialize batch processor\n",
    "batch_processor = BatchTranslationProcessor(domain_translator, evaluator)\n",
    "\n",
    "# Demo batch processing\n",
    "demo_batch = [\n",
    "    \"Artificial intelligence is transforming healthcare through machine learning algorithms.\",\n",
    "    \"The doctor recommended surgery after reviewing the patient's medical history.\",\n",
    "    \"Cloud computing provides scalable infrastructure for modern applications.\",\n",
    "    \"Deep learning models require large datasets for effective training.\",\n",
    "    \"Medical diagnosis accuracy has improved with AI-assisted tools.\",\n",
    "]\n",
    "\n",
    "# Create reference translations (in real scenarios, these would be human-translated)\n",
    "demo_references = [\n",
    "    \"äººå·¥æ™ºæ…§é€éŽæ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•æ­£åœ¨æ”¹è®Šé†«ç™‚ä¿å¥ã€‚\",\n",
    "    \"é†«å¸«åœ¨æª¢è¦–ç—…æ‚£ç—…æ­·å¾Œå»ºè­°é€²è¡Œæ‰‹è¡“ã€‚\",\n",
    "    \"é›²ç«¯é‹ç®—ç‚ºç¾ä»£æ‡‰ç”¨ç¨‹å¼æä¾›å¯æ“´å±•çš„åŸºç¤Žæž¶æ§‹ã€‚\",\n",
    "    \"æ·±åº¦å­¸ç¿’æ¨¡åž‹éœ€è¦å¤§åž‹è³‡æ–™é›†æ‰èƒ½æœ‰æ•ˆè¨“ç·´ã€‚\",\n",
    "    \"AIè¼”åŠ©å·¥å…·æå‡äº†é†«ç™‚è¨ºæ–·çš„æº–ç¢ºæ€§ã€‚\",\n",
    "]\n",
    "\n",
    "print(\"=== Batch Translation Demo ===\")\n",
    "batch_results = batch_processor.process_batch(\n",
    "    texts=demo_batch,\n",
    "    target_lang=\"zh-tw\",\n",
    "    domain=\"tech\",\n",
    "    include_evaluation=True,\n",
    "    reference_translations=demo_references,\n",
    ")\n",
    "\n",
    "# Generate and display report\n",
    "report = batch_processor.create_report(batch_results)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0151e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 9: Advanced Features & Optimization =====\n",
    "class AdvancedTranslationFeatures:\n",
    "    \"\"\"Advanced translation features and optimizations\"\"\"\n",
    "\n",
    "    def __init__(self, translator: MultilingualTranslator):\n",
    "        self.translator = translator\n",
    "\n",
    "    def confidence_based_translation(\n",
    "        self,\n",
    "        text: str,\n",
    "        target_lang: str,\n",
    "        confidence_threshold: float = 0.6,\n",
    "        num_candidates: int = 3,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate multiple translation candidates and select based on confidence\n",
    "        \"\"\"\n",
    "        candidates = []\n",
    "\n",
    "        # Generate candidates with different parameters\n",
    "        generation_configs = [\n",
    "            {\"num_beams\": 4, \"temperature\": 1.0},\n",
    "            {\"num_beams\": 6, \"temperature\": 0.8},\n",
    "            {\"num_beams\": 8, \"temperature\": 0.9},\n",
    "        ]\n",
    "\n",
    "        source_lang, _ = detector.detect_language(text)\n",
    "\n",
    "        for i, config in enumerate(generation_configs[:num_candidates]):\n",
    "            try:\n",
    "                # Modify translator to accept temperature (simplified approach)\n",
    "                translated = self.translator.translate(\n",
    "                    text, source_lang, target_lang, num_beams=config[\"num_beams\"]\n",
    "                )\n",
    "\n",
    "                # Simple confidence estimation based on length and repetition\n",
    "                confidence = self._estimate_confidence(text, translated)\n",
    "\n",
    "                candidates.append(\n",
    "                    {\n",
    "                        \"translation\": translated,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"config\": config,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to generate candidate {i}: {e}\")\n",
    "\n",
    "        # Sort by confidence\n",
    "        candidates.sort(key=lambda x: x[\"confidence\"], reverse=True)\n",
    "\n",
    "        # Select best candidate above threshold\n",
    "        best_candidate = None\n",
    "        for candidate in candidates:\n",
    "            if candidate[\"confidence\"] >= confidence_threshold:\n",
    "                best_candidate = candidate\n",
    "                break\n",
    "\n",
    "        if not best_candidate and candidates:\n",
    "            best_candidate = candidates[0]  # Fallback to highest confidence\n",
    "\n",
    "        return {\n",
    "            \"best_translation\": best_candidate[\"translation\"] if best_candidate else \"\",\n",
    "            \"best_confidence\": best_candidate[\"confidence\"] if best_candidate else 0.0,\n",
    "            \"all_candidates\": candidates,\n",
    "            \"source_text\": text,\n",
    "            \"target_language\": target_lang,\n",
    "        }\n",
    "\n",
    "    def _estimate_confidence(self, source: str, translation: str) -> float:\n",
    "        \"\"\"\n",
    "        Simple confidence estimation based on translation characteristics\n",
    "        Real implementations would use more sophisticated methods\n",
    "        \"\"\"\n",
    "        if not translation.strip():\n",
    "            return 0.0\n",
    "\n",
    "        # Length ratio check\n",
    "        source_len = len(source.split())\n",
    "        trans_len = len(translation.split())\n",
    "\n",
    "        if source_len == 0:\n",
    "            return 0.0\n",
    "\n",
    "        length_ratio = trans_len / source_len\n",
    "\n",
    "        # Penalize extreme length ratios\n",
    "        if length_ratio < 0.3 or length_ratio > 3.0:\n",
    "            length_penalty = 0.5\n",
    "        else:\n",
    "            length_penalty = 1.0\n",
    "\n",
    "        # Check for repetitive patterns\n",
    "        words = translation.split()\n",
    "        unique_words = set(words)\n",
    "\n",
    "        if len(words) == 0:\n",
    "            repetition_penalty = 0.0\n",
    "        else:\n",
    "            repetition_penalty = len(unique_words) / len(words)\n",
    "\n",
    "        # Simple heuristic combination\n",
    "        confidence = 0.7 * length_penalty + 0.3 * repetition_penalty\n",
    "\n",
    "        return min(confidence, 1.0)\n",
    "\n",
    "    def adaptive_chunking(\n",
    "        self,\n",
    "        long_text: str,\n",
    "        target_lang: str,\n",
    "        max_chunk_size: int = 400,\n",
    "        overlap_size: int = 50,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Split long text into chunks with context preservation\n",
    "        \"\"\"\n",
    "        # Split into sentences first\n",
    "        sentence_endings = re.compile(r\"[.!?ã€‚ï¼ï¼Ÿ]\")\n",
    "        sentences = sentence_endings.split(long_text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Check if adding this sentence would exceed max chunk size\n",
    "            potential_chunk = (\n",
    "                current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "            )\n",
    "\n",
    "            if len(potential_chunk) <= max_chunk_size:\n",
    "                current_chunk = potential_chunk\n",
    "            else:\n",
    "                # Save current chunk and start new one\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "\n",
    "        # Don't forget the last chunk\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        # Translate each chunk\n",
    "        translated_chunks = []\n",
    "        source_lang, _ = detector.detect_language(long_text)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            try:\n",
    "                translated = self.translator.translate(chunk, source_lang, target_lang)\n",
    "                translated_chunks.append(translated)\n",
    "                print(f\"Translated chunk {i+1}/{len(chunks)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to translate chunk {i+1}: {e}\")\n",
    "                translated_chunks.append(\"\")\n",
    "\n",
    "        # Combine translated chunks\n",
    "        full_translation = \" \".join(translated_chunks)\n",
    "\n",
    "        return {\n",
    "            \"original_text\": long_text,\n",
    "            \"translated_text\": full_translation,\n",
    "            \"chunks\": chunks,\n",
    "            \"translated_chunks\": translated_chunks,\n",
    "            \"num_chunks\": len(chunks),\n",
    "            \"source_language\": source_lang,\n",
    "            \"target_language\": target_lang,\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize advanced features\n",
    "advanced_features = AdvancedTranslationFeatures(translator)\n",
    "\n",
    "# Demo confidence-based translation\n",
    "print(\"=== Confidence-Based Translation Demo ===\")\n",
    "test_text = \"The integration of artificial intelligence in medical diagnosis represents a paradigm shift in healthcare.\"\n",
    "\n",
    "confidence_result = advanced_features.confidence_based_translation(\n",
    "    test_text, \"zh-tw\", confidence_threshold=0.5, num_candidates=3\n",
    ")\n",
    "\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Best translation: {confidence_result['best_translation']}\")\n",
    "print(f\"Confidence: {confidence_result['best_confidence']:.3f}\")\n",
    "print(\"\\nAll candidates:\")\n",
    "for i, candidate in enumerate(confidence_result[\"all_candidates\"]):\n",
    "    print(f\"{i+1}. {candidate['translation']} (conf: {candidate['confidence']:.3f})\")\n",
    "\n",
    "# Demo adaptive chunking for long text\n",
    "print(\"\\n=== Adaptive Chunking Demo ===\")\n",
    "long_text = \"\"\"\n",
    "Artificial intelligence has revolutionized various industries, from healthcare to finance.\n",
    "Machine learning algorithms can process vast amounts of data to identify patterns and make predictions.\n",
    "In healthcare, AI assists doctors in diagnosing diseases more accurately and developing personalized treatment plans.\n",
    "The technology continues to evolve, with deep learning models becoming increasingly sophisticated.\n",
    "However, ethical considerations and data privacy remain important challenges that need to be addressed.\n",
    "\"\"\"\n",
    "\n",
    "chunking_result = advanced_features.adaptive_chunking(\n",
    "    long_text.strip(), \"zh-tw\", max_chunk_size=200\n",
    ")\n",
    "\n",
    "print(f\"Original length: {len(chunking_result['original_text'])} characters\")\n",
    "print(f\"Number of chunks: {chunking_result['num_chunks']}\")\n",
    "print(f\"Translation: {chunking_result['translated_text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 10: Smoke Test & Verification =====\n",
    "def run_translation_smoke_test():\n",
    "    \"\"\"Comprehensive smoke test for translation functionality\"\"\"\n",
    "    print(\"ðŸ§ª Running Translation System Smoke Test...\")\n",
    "\n",
    "    tests_passed = 0\n",
    "    total_tests = 0\n",
    "\n",
    "    # Test 1: Language Detection\n",
    "    total_tests += 1\n",
    "    try:\n",
    "        lang, conf = detector.detect_language(\"Hello world\")\n",
    "        assert lang == \"en\"\n",
    "        print(\"âœ… Language detection: PASS\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Language detection: FAIL - {e}\")\n",
    "\n",
    "    # Test 2: Basic Translation\n",
    "    total_tests += 1\n",
    "    try:\n",
    "        result = translator.translate(\"Hello\", \"en\", \"zh-cn\")\n",
    "        assert len(result) > 0\n",
    "        print(\"âœ… Basic translation: PASS\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Basic translation: FAIL - {e}\")\n",
    "\n",
    "    # Test 3: Pipeline Translation\n",
    "    total_tests += 1\n",
    "    try:\n",
    "        result = pipeline.auto_translate(\"Hello world\", \"zh-tw\")\n",
    "        assert result[\"status\"] == \"success\"\n",
    "        assert len(result[\"translated_text\"]) > 0\n",
    "        print(\"âœ… Pipeline translation: PASS\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Pipeline translation: FAIL - {e}\")\n",
    "\n",
    "    # Test 4: Domain Translation\n",
    "    total_tests += 1\n",
    "    try:\n",
    "        result = domain_translator.domain_translate(\n",
    "            \"Machine learning algorithm\", \"zh-tw\", \"tech\"\n",
    "        )\n",
    "        assert result[\"status\"] == \"success\"\n",
    "        print(\"âœ… Domain translation: PASS\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Domain translation: FAIL - {e}\")\n",
    "\n",
    "    # Test 5: Evaluation Metrics\n",
    "    total_tests += 1\n",
    "    try:\n",
    "        score = evaluator.chrf_score(\"hello world\", \"hello world\")\n",
    "        assert score == 100.0\n",
    "        print(\"âœ… Evaluation metrics: PASS\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Evaluation metrics: FAIL - {e}\")\n",
    "\n",
    "    print(f\"\\nðŸ“Š Test Results: {tests_passed}/{total_tests} tests passed\")\n",
    "\n",
    "    if tests_passed == total_tests:\n",
    "        print(\"ðŸŽ‰ All tests passed! Translation system is working correctly.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"âš ï¸ Some tests failed. Please check the implementation.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "test_success = run_translation_smoke_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d595468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 11: Usage Examples & Best Practices =====\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ðŸ“š USAGE EXAMPLES & BEST PRACTICES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "usage_examples = \"\"\"\n",
    "## ðŸ”§ Quick Start Examples\n",
    "\n",
    "### 1. Simple Translation\n",
    "```python\n",
    "# Auto-detect and translate\n",
    "result = pipeline.auto_translate(\"Hello world\", target_lang='zh-tw')\n",
    "print(result['translated_text'])\n",
    "```\n",
    "\n",
    "### 2. Domain-Specific Translation\n",
    "```python\n",
    "# Technical translation with terminology\n",
    "result = domain_translator.domain_translate(\n",
    "    \"Machine learning algorithms\",\n",
    "    target_lang='zh-tw',\n",
    "    domain='tech'\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Batch Processing\n",
    "```python\n",
    "# Process multiple texts\n",
    "texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
    "results = batch_processor.process_batch(\n",
    "    texts, target_lang='zh-tw', domain='general'\n",
    ")\n",
    "```\n",
    "\n",
    "### 4. Quality Evaluation\n",
    "```python\n",
    "# Evaluate translation quality\n",
    "score = evaluator.chrf_score(reference, hypothesis)\n",
    "print(f\"chrF++ Score: {score:.2f}\")\n",
    "```\n",
    "\n",
    "## âš¡ Performance Optimization Tips\n",
    "\n",
    "1. **Memory Management**:\n",
    "   - Use `load_in_4bit=True` for models >2GB\n",
    "   - Set `device_map=\"auto\"` for multi-GPU\n",
    "   - Clear cache with `torch.cuda.empty_cache()`\n",
    "\n",
    "2. **Batch Processing**:\n",
    "   - Process 8-16 texts per batch for optimal speed\n",
    "   - Use adaptive chunking for long documents\n",
    "   - Monitor VRAM usage during batch processing\n",
    "\n",
    "3. **Quality vs Speed Trade-offs**:\n",
    "   - `num_beams=4`: Good balance\n",
    "   - `num_beams=8`: Higher quality, slower\n",
    "   - `num_beams=1`: Fastest, lower quality\n",
    "\n",
    "## ðŸŽ¯ Best Practices\n",
    "\n",
    "1. **Language Detection**:\n",
    "   - Always validate detected language\n",
    "   - Use confidence thresholds for auto-processing\n",
    "   - Handle mixed-language content separately\n",
    "\n",
    "2. **Domain Adaptation**:\n",
    "   - Maintain terminology dictionaries\n",
    "   - Pre-process technical terms\n",
    "   - Post-process for consistency\n",
    "\n",
    "3. **Quality Assurance**:\n",
    "   - Use multiple evaluation metrics\n",
    "   - Implement human review for critical content\n",
    "   - Monitor round-trip translation quality\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Implement graceful fallbacks\n",
    "   - Log translation failures\n",
    "   - Provide alternative translation options\n",
    "\"\"\"\n",
    "\n",
    "print(usage_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01303b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 12: Completion Summary =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ¯ NOTEBOOK COMPLETION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "completion_summary = f\"\"\"\n",
    "## âœ… å®Œæˆé …ç›® (Completed Items)\n",
    "\n",
    "### æ ¸å¿ƒåŠŸèƒ½ (Core Features)\n",
    "âœ… å¤šèªžè¨€æ¨¡åž‹è¼‰å…¥èˆ‡é…ç½® (Multilingual model loading & config)\n",
    "âœ… è‡ªå‹•èªžè¨€æª¢æ¸¬ (Automatic language detection)\n",
    "âœ… ç¹ç°¡ä¸­æ–‡è½‰æ› (Traditional/Simplified Chinese conversion)\n",
    "âœ… é›™å‘ç¿»è­¯ç®¡ç·š (Bidirectional translation pipeline)\n",
    "âœ… é ˜åŸŸç‰¹å®šç¿»è­¯ (Domain-specific translation)\n",
    "âœ… æ‰¹æ¬¡è™•ç†ç³»çµ± (Batch processing system)\n",
    "âœ… ç¿»è­¯å“è³ªè©•ä¼° (Translation quality evaluation)\n",
    "\n",
    "### é€²éšŽåŠŸèƒ½ (Advanced Features)\n",
    "âœ… ä¿¡å¿ƒåº¦è©•ä¼° (Confidence-based translation)\n",
    "âœ… è‡ªé©æ‡‰åˆ†å¡Š (Adaptive text chunking)\n",
    "âœ… å°ˆæ¥­è¡“èªžç®¡ç† (Terminology management)\n",
    "âœ… å¤šå€™é¸ç”Ÿæˆ (Multiple candidate generation)\n",
    "\n",
    "### è©•ä¼°èˆ‡å„ªåŒ– (Evaluation & Optimization)\n",
    "âœ… chrF++/BLEU è©•ä¼°æŒ‡æ¨™ (Quality metrics)\n",
    "âœ… ä½Žé¡¯å­˜å„ªåŒ– (Low-VRAM optimizations)\n",
    "âœ… æ€§èƒ½ç›£æŽ§ (Performance monitoring)\n",
    "âœ… éŒ¯èª¤è™•ç†æ©Ÿåˆ¶ (Error handling)\n",
    "\n",
    "## ðŸ§  æ ¸å¿ƒåŽŸç†è¦é»ž (Key Concepts)\n",
    "\n",
    "1. **å¤šèªžæ¨¡åž‹æž¶æ§‹**: NLLB-200 ä½¿ç”¨ç·¨ç¢¼å™¨-è§£ç¢¼å™¨æž¶æ§‹ï¼Œæ”¯æ´200+èªžè¨€\n",
    "2. **èªžè¨€æª¢æ¸¬ç­–ç•¥**: çµåˆçµ±è¨ˆæª¢æ¸¬èˆ‡ä¸­æ–‡è®Šé«”ç‰¹å¾µè­˜åˆ¥\n",
    "3. **å“è³ªè©•ä¼°**: chrF++æ›´é©åˆä¸­æ–‡ç­‰å½¢æ…‹è±å¯Œèªžè¨€çš„è©•ä¼°\n",
    "4. **è¨˜æ†¶é«”å„ªåŒ–**: 4bité‡åŒ–å¯æ¸›å°‘70%é¡¯å­˜ä½¿ç”¨ï¼Œåƒ…ç•¥é™å“è³ª\n",
    "5. **é ˜åŸŸé©æ‡‰**: è¡“èªžå­—å…¸+é è™•ç†å¯é¡¯è‘—æå‡å°ˆæ¥­é ˜åŸŸç¿»è­¯å“è³ª\n",
    "\n",
    "## âš ï¸ å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ (Common Issues & Solutions)\n",
    "\n",
    "### è¨˜æ†¶é«”ä¸è¶³ (Out of Memory)\n",
    "- å•Ÿç”¨ `load_in_4bit=True`\n",
    "- æ¸›å°‘ `batch_size`\n",
    "- ä½¿ç”¨ `device_map=\"auto\"`\n",
    "\n",
    "### ç¿»è­¯å“è³ªä¸ä½³ (Poor Translation Quality)\n",
    "- æª¢æŸ¥èªžè¨€æª¢æ¸¬æº–ç¢ºæ€§\n",
    "- èª¿æ•´ `num_beams` åƒæ•¸\n",
    "- ä½¿ç”¨é ˜åŸŸç‰¹å®šè¡“èªžå­—å…¸\n",
    "\n",
    "### è™•ç†é€Ÿåº¦æ…¢ (Slow Processing)\n",
    "- æ‰¹æ¬¡è™•ç†å¤šå€‹æ–‡æœ¬\n",
    "- é™ä½Ž `num_beams` æ•¸é‡\n",
    "- è€ƒæ…®ä½¿ç”¨ GGUF é‡åŒ–æ¨¡åž‹\n",
    "\n",
    "## ðŸš€ ä¸‹ä¸€æ­¥å»ºè­° (Next Steps)\n",
    "\n",
    "### ç«‹å³å¯è¡Œ (Immediate)\n",
    "1. **æ•´åˆèªžéŸ³ç¿»è­¯**: çµåˆ Whisper ASR + TTS\n",
    "2. **å„ªåŒ–è¡“èªžå­—å…¸**: æ“´å……æŠ€è¡“/é†«ç™‚/æ³•å¾‹é ˜åŸŸè©žå½™\n",
    "3. **å¯¦ä½œ Web UI**: ä½¿ç”¨ Gradio å»ºç«‹ç¿»è­¯ä»‹é¢\n",
    "\n",
    "### ä¸­æœŸç™¼å±• (Medium-term)\n",
    "1. **å¾®èª¿å„ªåŒ–**: ä½¿ç”¨é ˜åŸŸè³‡æ–™ fine-tune ç¿»è­¯æ¨¡åž‹\n",
    "2. **å¤šæ¨¡æ…‹ç¿»è­¯**: æ•´åˆåœ–ç‰‡æ–‡å­—è­˜åˆ¥èˆ‡ç¿»è­¯\n",
    "3. **å“è³ªè‡ªå‹•è©•ä¼°**: è¨“ç·´å“è³ªè©•ä¼°æ¨¡åž‹\n",
    "\n",
    "### é•·æœŸè¦åŠƒ (Long-term)\n",
    "1. **ç«¯åˆ°ç«¯å„ªåŒ–**: æ•´åˆæª¢ç´¢å¢žå¼·ç¿»è­¯ (RAT)\n",
    "2. **å¯¦æ™‚å”ä½œç¿»è­¯**: å¤šç”¨æˆ¶å”ä½œç¿»è­¯å¹³å°\n",
    "3. **è·¨èªžè¨€çŸ¥è­˜é·ç§»**: åˆ©ç”¨å¤šèªžè¨€è¡¨ç¤ºå­¸ç¿’\n",
    "\n",
    "## ðŸ”— ç›¸é—œç« ç¯€é€£çµ (Related Notebooks)\n",
    "- nb13_function_calling_tools.ipynb (å·¥å…·æ•´åˆ)\n",
    "- nb26_rag_basic_faiss.ipynb (RAG æ–‡æª”æª¢ç´¢)\n",
    "- nb31_gradio_chat_ui.ipynb (Web ä»‹é¢)\n",
    "\n",
    "æ¨¡åž‹å¿«å–ä½ç½®: {AI_CACHE_ROOT}\n",
    "æ¸¬è©¦ç‹€æ…‹: {'âœ… é€šéŽ' if test_success else 'âŒ å¤±æ•—'}\n",
    "\"\"\"\n",
    "\n",
    "print(completion_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick smoke test for nb17 - should complete in <30 seconds\n",
    "def quick_smoke_test():\n",
    "    try:\n",
    "        # Test 1: Basic translation\n",
    "        result = pipeline.auto_translate(\"Hello\", \"zh-tw\")\n",
    "        assert result[\"status\"] == \"success\"\n",
    "\n",
    "        # Test 2: Domain translation\n",
    "        tech_result = domain_translator.domain_translate(\"AI\", \"zh-tw\", \"tech\")\n",
    "        assert len(tech_result[\"translated_text\"]) > 0\n",
    "\n",
    "        print(\"âœ… nb17 multilingual translation: ALL TESTS PASSED\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ nb17 smoke test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "quick_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b0cdd",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Stage Summary - Part C é€²å±•\n",
    "\n",
    "### âœ… å·²å®Œæˆé …ç›® (Completed - Part C)\n",
    "- **nb10**: GPT/Qwen/DeepSeek æ–‡æœ¬ç”Ÿæˆ âœ…\n",
    "- **nb11**: æŒ‡ä»¤èª¿å„ªè³‡æ–™èˆ‡ç¯„ä¾‹ âœ… \n",
    "- **nb12**: LLM è©•ä¼°æŒ‡æ¨™ âœ…\n",
    "- **nb13**: Function Calling & å·¥å…·ä½¿ç”¨ âœ…\n",
    "- **nb14**: ReAct å¤šæ­¥æŽ¨ç† âœ…\n",
    "- **nb15**: ç¨‹å¼åŠ©ç† Agent âœ…\n",
    "- **nb16**: æ–‡ä»¶çµæ§‹åŒ–æŠ½å– âœ…\n",
    "- **nb17**: å¤šèªžç”Ÿæˆèˆ‡ç¿»è­¯ âœ… **(æœ¬ç« )**\n",
    "\n",
    "### ðŸ”„ æ ¸å¿ƒæ¦‚å¿µæŽŒæ¡ (Core Concepts Mastered)\n",
    "1. **å¤šèªžè¨€è™•ç†**: NLLB, mT5 è·¨èªžè¨€æ¨¡åž‹æž¶æ§‹\n",
    "2. **ç¿»è­¯è©•ä¼°**: chrF++, BLEU, COMET å“è³ªæŒ‡æ¨™é«”ç³»  \n",
    "3. **é ˜åŸŸé©æ‡‰**: è¡“èªžç®¡ç†èˆ‡ä¸Šä¸‹æ–‡ä¿æŒç­–ç•¥\n",
    "4. **æ€§èƒ½å„ªåŒ–**: 4bité‡åŒ–, æ‰¹æ¬¡è™•ç†, è¨˜æ†¶é«”ç®¡ç†\n",
    "5. **å“è³ªæŽ§åˆ¶**: å¤šå€™é¸ç”Ÿæˆ, ä¿¡å¿ƒåº¦è©•ä¼°, éŒ¯èª¤è™•ç†\n",
    "\n",
    "### âš ï¸ å¸¸è¦‹é™·é˜± (Common Pitfalls)\n",
    "1. **è¨˜æ†¶é«”æº¢å‡º**: å¤§æ¨¡åž‹è¼‰å…¥æ™‚æœªå•Ÿç”¨é‡åŒ–\n",
    "2. **èªžè¨€æª¢æ¸¬éŒ¯èª¤**: æ··åˆèªžè¨€æˆ–çŸ­æ–‡æœ¬æª¢æ¸¬ä¸æº–\n",
    "3. **è¡“èªžä¸ä¸€è‡´**: å°ˆæ¥­è¡“èªžåœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­ç¿»è­¯çµæžœå·®ç•°\n",
    "4. **å“è³ªè©•ä¼°åå·®**: éŽåº¦ä¾è³´è‡ªå‹•æŒ‡æ¨™ï¼Œç¼ºä¹äººå·¥é©—è­‰\n",
    "5. **æ‰¹æ¬¡è™•ç†è¶…æ™‚**: å¤§æ‰¹é‡ç¿»è­¯æ™‚æœªåˆç†è¨­ç½®è¶…æ™‚èˆ‡é‡è©¦æ©Ÿåˆ¶\n",
    "\n",
    "### ðŸš€ ä¸‹ä¸€æ­¥è¡Œå‹•å»ºè­° (Next Actions)\n",
    "\n",
    "**ç«‹å³å„ªå…ˆé …ç›® (High Priority)**\n",
    "1. **nb18_safety_alignment_redteam.ipynb** - å®‰å…¨å°é½Šèˆ‡ç´…éšŠæ¸¬è©¦\n",
    "   - ç†ç”±ï¼šå¤šèªžç¿»è­¯å®¹æ˜“ç”¢ç”Ÿæœ‰å®³å…§å®¹ï¼Œéœ€è¦å®‰å…¨é˜²è­·æ©Ÿåˆ¶\n",
    "   - é æœŸæ”¶ç›Šï¼šå»ºç«‹å…§å®¹å®‰å…¨æª¢æŸ¥æµç¨‹ï¼Œé˜²ç¯„ç¿»è­¯æ¿«ç”¨\n",
    "\n",
    "2. **nb19_cost_latency_quality.ipynb** - æˆæœ¬/å»¶é²/å“è³ªæ¬Šè¡¡\n",
    "   - ç†ç”±ï¼šç¿»è­¯ç³»çµ±çš„å¯¦ç”¨åŒ–éƒ¨ç½²éœ€è¦å…¨é¢æ€§èƒ½å„ªåŒ–\n",
    "   - é æœŸæ”¶ç›Šï¼šåˆ¶å®šä¸åŒå ´æ™¯çš„æœ€å„ªé…ç½®ç­–ç•¥\n",
    "\n",
    "**ä¸­æœŸç™¼å±•é …ç›® (Medium Priority)**  \n",
    "3. **nb26_rag_basic_faiss.ipynb** - æ•´åˆ RAG æ–‡æª”æª¢ç´¢ç¿»è­¯\n",
    "   - ç†ç”±ï¼šçµåˆæª¢ç´¢å¢žå¼·ï¼Œæå‡é ˜åŸŸç¿»è­¯æº–ç¢ºæ€§\n",
    "   - é æœŸæ”¶ç›Šï¼šå¯¦ç¾ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é«˜å“è³ªç¿»è­¯\n",
    "\n",
    "4. **nb31_gradio_chat_ui.ipynb** - å¤šèªžç¿»è­¯ Web ä»‹é¢\n",
    "   - ç†ç”±ï¼šæä¾›ç”¨æˆ¶å‹å–„çš„ç¿»è­¯æœå‹™ä»‹é¢\n",
    "   - é æœŸæ”¶ç›Šï¼šå®Œæ•´çš„ç¿»è­¯ç”¢å“åŽŸåž‹\n",
    "\n",
    "### ðŸ“Š ç•¶å‰æŠ€è¡“æ£§æˆç†Ÿåº¦è©•ä¼°\n",
    "\n",
    "| æŠ€è¡“é ˜åŸŸ | æˆç†Ÿåº¦ | èªªæ˜Ž |\n",
    "|---------|--------|------|\n",
    "| LLM åŸºç¤Žæ‡‰ç”¨ | 90% | æ–‡æœ¬ç”Ÿæˆã€æŒ‡ä»¤èª¿å„ªå·²å®Œå…¨æŽŒæ¡ |\n",
    "| Agent ç³»çµ± | 85% | å·¥å…·èª¿ç”¨ã€å¤šæ­¥æŽ¨ç†åŠŸèƒ½å®Œå–„ |\n",
    "| å¤šèªžè™•ç† | 80% | ç¿»è­¯ç®¡ç·šå®Œæ•´ï¼Œéœ€å„ªåŒ–å°ˆæ¥­é ˜åŸŸ |\n",
    "| ä»£ç¢¼åŠ©ç† | 75% | åŸºç¤ŽåŠŸèƒ½å¯ç”¨ï¼Œéœ€å¼·åŒ–è¤‡é›œå ´æ™¯ |\n",
    "| å®‰å…¨å°é½Š | 30% | å°šå¾…å»ºç«‹ï¼Œä¸‹ç« ç¯€é‡é»ž |\n",
    "\n",
    "### ðŸ’¡ æž¶æ§‹å„ªåŒ–å»ºè­°\n",
    "\n",
    "**çµ±ä¸€ä»‹é¢æŠ½è±¡åŒ–**\n",
    "- å»ºç«‹ `MultilingualAgent` åŸºé¡žæ•´åˆç¿»è­¯åŠŸèƒ½\n",
    "- æŠ½è±¡åŒ– `QualityEvaluator` æ”¯æ´å¤šç¨®è©•ä¼°æŒ‡æ¨™\n",
    "- æ¨™æº–åŒ– `DomainAdapter` ä»‹é¢æ”¯æ´å¯æ’æ‹”è¡“èªžåº«\n",
    "\n",
    "**æ€§èƒ½å„ªåŒ–ç­–ç•¥**  \n",
    "- å¯¦æ–½æ¨¡åž‹æ± ç®¡ç†ï¼Œé¿å…é‡è¤‡è¼‰å…¥\n",
    "- å¢žåŠ ç¿»è­¯çµæžœå¿«å–æ©Ÿåˆ¶ï¼Œæå‡é‡è¤‡æŸ¥è©¢æ•ˆçŽ‡\n",
    "- å¯¦ç¾æ¼¸é€²å¼æ‰¹æ¬¡è™•ç†ï¼Œå¹³è¡¡å»¶é²èˆ‡åžåé‡\n",
    "\n",
    "**å“è³ªä¿éšœæ©Ÿåˆ¶**\n",
    "- æ•´åˆäººå·¥å¯©æ ¸å·¥ä½œæµç¨‹\n",
    "- å»ºç«‹ç¿»è­¯å“è³ªç›£æŽ§ dashboard\n",
    "- å¯¦ç¾ A/B æ¸¬è©¦æ¡†æž¶è©•ä¼°ä¸åŒç¿»è­¯ç­–ç•¥\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ‰ Part C - LLM Applications å³å°‡å®Œæˆï¼\n",
    "\n",
    "é‚„å‰© **2 æœ¬æ ¸å¿ƒ notebooks**ï¼š\n",
    "- **nb18**: å®‰å…¨å°é½Šèˆ‡ç´…éšŠæ¸¬è©¦ (å¿…éœ€ï¼Œé˜²ç¯„é¢¨éšª)\n",
    "- **nb19**: æˆæœ¬/å»¶é²/å“è³ªæ¬Šè¡¡ (é‡è¦ï¼Œå¯¦ç”¨éƒ¨ç½²)\n",
    "\n",
    "å®Œæˆé€™å…©æœ¬å¾Œï¼ŒPart C çš„ **LLM æ‡‰ç”¨æ ¸å¿ƒèƒ½åŠ›** å°‡å…¨é¢å»ºç«‹ï¼Œå¯ä»¥é€²å…¥ **Part D - Fine-tuning** æˆ– **Part E - RAG Ã— Agents** éšŽæ®µã€‚\n",
    "\n",
    "**å»ºè­°ä¸‹ä¸€æ­¥**: å„ªå…ˆå®Œæˆ **nb18 å®‰å…¨å°é½Š**ï¼Œå› ç‚ºä¹‹å‰çš„ Agent å’Œç¿»è­¯åŠŸèƒ½éƒ½éœ€è¦å®‰å…¨é˜²è­·æ©Ÿåˆ¶ï¼Œé€™æ˜¯ç”Ÿç”¢éƒ¨ç½²çš„å¿…è¦æ¢ä»¶ã€‚\n",
    "\n",
    "éœ€è¦ç¹¼çºŒé€²è¡Œ nb18 å—Žï¼Ÿ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
