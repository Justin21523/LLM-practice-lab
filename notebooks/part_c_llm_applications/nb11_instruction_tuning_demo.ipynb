{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb11_instruction_tuning_demo.ipynb\n",
    "# 指令調優示範 - Instruction Tuning with LoRA/QLoRA\n",
    "\n",
    "# === Cell 1: Shared Cache Bootstrap & Environment Setup ===\n",
    "import os, pathlib, torch, warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "for k, v in paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e855cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check required packages\n",
    "try:\n",
    "    import transformers, datasets, peft, bitsandbytes\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForCausalLM,\n",
    "        TrainingArguments,\n",
    "        Trainer,\n",
    "    )\n",
    "    from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "    from datasets import load_dataset\n",
    "    import json, random\n",
    "\n",
    "    print(\"✅ All required packages imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Missing package: {e}\")\n",
    "    print(\n",
    "        \"Install with: pip install transformers datasets peft bitsandbytes accelerate\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194cf4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Load and Explore Instruction Dataset ===\n",
    "print(\"🔍 Loading instruction dataset...\")\n",
    "\n",
    "# Load Alpaca-style instruction dataset (Stanford Alpaca or similar)\n",
    "try:\n",
    "    # Option 1: Stanford Alpaca (English)\n",
    "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "    print(f\"✅ Loaded Stanford Alpaca dataset: {len(dataset)} examples\")\n",
    "except:\n",
    "    try:\n",
    "        # Option 2: Databricks Dolly (alternative)\n",
    "        dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "        print(f\"✅ Loaded Databricks Dolly dataset: {len(dataset)} examples\")\n",
    "    except:\n",
    "        # Option 3: Chinese instruction dataset (fallback)\n",
    "        print(\"⚠️ Using synthetic examples (original datasets unavailable)\")\n",
    "        synthetic_data = [\n",
    "            {\n",
    "                \"instruction\": \"請解釋什麼是機器學習\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": \"機器學習是人工智慧的一個分支，它使計算機系統能夠通過經驗自動學習和改進，而無需被明確編程。\",\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"將以下句子翻譯成英文\",\n",
    "                \"input\": \"今天天氣很好\",\n",
    "                \"output\": \"The weather is very nice today.\",\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"列出三個程式設計的最佳實踐\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": \"1. 寫清晰的註解和文檔\\n2. 使用有意義的變數和函數名稱\\n3. 保持代碼簡潔和模組化\",\n",
    "            },\n",
    "        ]\n",
    "        from datasets import Dataset\n",
    "\n",
    "        dataset = Dataset.from_list(synthetic_data * 100)  # Repeat for demo\n",
    "        print(f\"✅ Created synthetic dataset: {len(dataset)} examples\")\n",
    "\n",
    "# Explore dataset structure\n",
    "print(\"\\n📊 Dataset Structure:\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "print(f\"First example:\")\n",
    "example = dataset[0]\n",
    "for key, value in example.items():\n",
    "    print(f\"  {key}: {repr(value[:100] + '...' if len(str(value)) > 100 else value)}\")\n",
    "\n",
    "# Basic statistics\n",
    "instructions_with_input = sum(1 for item in dataset if item.get(\"input\", \"\").strip())\n",
    "print(f\"\\n📈 Dataset Statistics:\")\n",
    "print(f\"Total examples: {len(dataset)}\")\n",
    "print(f\"Examples with input: {instructions_with_input}\")\n",
    "print(f\"Examples without input: {len(dataset) - instructions_with_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c25dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Data Preprocessing and Formatting ===\n",
    "print(\"🔧 Preprocessing instruction data...\")\n",
    "\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"\n",
    "    Format instruction data into a single text for training.\n",
    "    Uses Alpaca-style prompt template.\n",
    "    \"\"\"\n",
    "    instruction = example[\"instruction\"].strip()\n",
    "    input_text = example.get(\"input\", \"\").strip()\n",
    "    output = example[\"output\"].strip()\n",
    "\n",
    "    if input_text:\n",
    "        # Instruction with input\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        # Instruction only\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "\n",
    "    # Full text for training (prompt + response)\n",
    "    full_text = prompt + output\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": output,\n",
    "        \"full_text\": full_text,\n",
    "        \"length\": len(full_text),\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = dataset.map(format_instruction)\n",
    "\n",
    "# Filter by length (avoid too long sequences)\n",
    "MAX_LENGTH = 512  # Adjust based on your GPU memory\n",
    "filtered_dataset = formatted_dataset.filter(lambda x: x[\"length\"] <= MAX_LENGTH)\n",
    "\n",
    "print(\n",
    "    f\"✅ Formatted dataset: {len(formatted_dataset)} → {len(filtered_dataset)} examples (after length filtering)\"\n",
    ")\n",
    "\n",
    "# Show example\n",
    "print(\"\\n📝 Formatted Example:\")\n",
    "example = filtered_dataset[0]\n",
    "print(\"Prompt:\")\n",
    "print(example[\"prompt\"])\n",
    "print(\"Response:\")\n",
    "print(example[\"response\"])\n",
    "print(f\"Total length: {example['length']} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7827521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Load Base Model and Baseline Test ===\n",
    "print(\"🤖 Loading base model for instruction tuning...\")\n",
    "\n",
    "# Model selection based on available VRAM\n",
    "MODEL_NAME = \"microsoft/DialoGPT-small\"  # Lightweight for demo\n",
    "# Alternative: \"Qwen/Qwen2.5-0.5B-Instruct\" or \"google/flan-t5-small\"\n",
    "\n",
    "# Configure 4-bit quantization for low VRAM\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=(\n",
    "        torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config if torch.cuda.is_available() else None,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Model loaded successfully\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"Falling back to CPU-only mode...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "# Test baseline performance\n",
    "def test_instruction_following(model, tokenizer, prompt, max_length=100):\n",
    "    \"\"\"Test model's instruction following capability\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated[len(prompt) :].strip()\n",
    "\n",
    "\n",
    "# Baseline test\n",
    "test_prompt = \"### Instruction:\\nExplain what is machine learning in simple terms.\\n\\n### Response:\\n\"\n",
    "baseline_response = test_instruction_following(model, tokenizer, test_prompt)\n",
    "print(f\"\\n🧪 Baseline Test:\")\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"Response: {baseline_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: LoRA Configuration and Training Setup ===\n",
    "print(\"⚙️ Setting up LoRA for parameter-efficient fine-tuning...\")\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,  # Low rank\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Attention layers\n",
    "    # Note: target_modules may vary by model architecture\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "try:\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"✅ LoRA applied successfully\")\n",
    "    print(f\"Trainable parameters: {model.num_parameters(only_trainable=True):,}\")\n",
    "    print(f\"Total parameters: {model.num_parameters():,}\")\n",
    "    print(\n",
    "        f\"Trainable %: {100 * model.num_parameters(only_trainable=True) / model.num_parameters():.2f}%\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error applying LoRA: {e}\")\n",
    "    print(\n",
    "        \"This might be due to model architecture. Continuing with full fine-tuning...\"\n",
    "    )\n",
    "\n",
    "# Data collator for language modeling\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the full text for training\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"full_text\"], truncation=True, padding=False, max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "\n",
    "# Tokenize dataset\n",
    "train_dataset = filtered_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=filtered_dataset.column_names\n",
    ").shuffle(seed=42)\n",
    "\n",
    "# Take a subset for quick demo (full dataset for real training)\n",
    "train_dataset = train_dataset.select(range(min(100, len(train_dataset))))\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False  # Causal language modeling\n",
    ")\n",
    "\n",
    "print(f\"✅ Prepared training dataset: {len(train_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Lightweight Fine-tuning Execution ===\n",
    "print(\"🚀 Starting instruction fine-tuning...\")\n",
    "\n",
    "# Training arguments (conservative for demo)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{AI_CACHE_ROOT}/instruction_tuning_demo\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,  # Very short for demo\n",
    "    per_device_train_batch_size=1,  # Low VRAM friendly\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,  # Conservative learning rate\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",  # Don't save checkpoints for demo\n",
    "    dataloader_drop_last=False,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision if available\n",
    "    report_to=None,  # Disable wandb/tensorboard\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"📊 Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(\n",
    "    f\"  Total steps: ~{len(train_dataset) // training_args.per_device_train_batch_size}\"\n",
    ")\n",
    "\n",
    "# Start training\n",
    "try:\n",
    "    print(\"\\n🎯 Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"✅ Training completed successfully!\")\n",
    "\n",
    "    # Save LoRA adapter\n",
    "    if hasattr(model, \"save_pretrained\"):\n",
    "        adapter_path = f\"{AI_CACHE_ROOT}/instruction_lora_adapter\"\n",
    "        model.save_pretrained(adapter_path)\n",
    "        print(f\"💾 LoRA adapter saved to: {adapter_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Training error: {e}\")\n",
    "    print(\n",
    "        \"This might be due to memory constraints. Try reducing batch size or sequence length.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d05435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Post-Training Evaluation and Comparison ===\n",
    "print(\"📈 Evaluating fine-tuned model...\")\n",
    "\n",
    "# Test the same prompt as baseline\n",
    "tuned_response = test_instruction_following(model, tokenizer, test_prompt)\n",
    "\n",
    "print(\"🔄 Before vs After Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"BASELINE (pre-training):\")\n",
    "print(baseline_response)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINE-TUNED (post-training):\")\n",
    "print(tuned_response)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Additional test cases\n",
    "test_cases = [\n",
    "    \"### Instruction:\\nList three benefits of exercise.\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nWrite a haiku about technology.\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nExplain why the sky is blue.\\n\\n### Response:\\n\",\n",
    "]\n",
    "\n",
    "print(\"\\n🧪 Additional Test Cases:\")\n",
    "for i, prompt in enumerate(test_cases):\n",
    "    print(f\"\\nTest {i+1}:\")\n",
    "    print(\n",
    "        f\"Instruction: {prompt.split('Response:')[0].replace('### Instruction:', '').replace('###', '').strip()}\"\n",
    "    )\n",
    "    response = test_instruction_following(model, tokenizer, prompt, max_length=80)\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "# Qualitative assessment checklist\n",
    "print(\"\\n✅ Qualitative Assessment Checklist:\")\n",
    "print(\"1. Does the model follow instructions more consistently? [Manual check]\")\n",
    "print(\"2. Are responses more structured and coherent? [Manual check]\")\n",
    "print(\"3. Does the model use appropriate formatting? [Manual check]\")\n",
    "print(\"4. Is the model still safe and not generating harmful content? [Manual check]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Smoke Test and Usage Notes ===\n",
    "print(\"🧪 Final Smoke Test...\")\n",
    "\n",
    "\n",
    "def smoke_test():\n",
    "    \"\"\"Quick validation that everything works\"\"\"\n",
    "    try:\n",
    "        # Test tokenizer\n",
    "        test_text = \"Hello world\"\n",
    "        tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "        assert tokens[\"input_ids\"].shape[1] > 0\n",
    "\n",
    "        # Test model inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "            assert outputs.logits.shape[-1] == tokenizer.vocab_size\n",
    "\n",
    "        # Test instruction format\n",
    "        formatted = format_instruction(\n",
    "            {\"instruction\": \"Test instruction\", \"input\": \"\", \"output\": \"Test output\"}\n",
    "        )\n",
    "        assert \"### Instruction:\" in formatted[\"full_text\"]\n",
    "        assert \"### Response:\" in formatted[\"full_text\"]\n",
    "\n",
    "        print(\"✅ All smoke tests passed!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Smoke test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "smoke_test_result = smoke_test()\n",
    "\n",
    "print(\"\\n📚 Usage Notes and Next Steps:\")\n",
    "print(\n",
    "    \"\"\"\n",
    "🎯 What we accomplished:\n",
    "- Loaded and formatted instruction dataset (Alpaca-style)\n",
    "- Applied LoRA for parameter-efficient fine-tuning\n",
    "- Trained model on instruction-following tasks\n",
    "- Compared before/after performance\n",
    "\n",
    "🔧 Key Parameters:\n",
    "- LoRA rank: 16 (balance between efficiency and capacity)\n",
    "- Learning rate: 2e-4 (conservative to avoid catastrophic forgetting)\n",
    "- Max sequence length: 512 (adjust based on your needs)\n",
    "\n",
    "⚠️ Common Issues:\n",
    "- OOM errors: Reduce batch_size, use gradient_checkpointing\n",
    "- Poor convergence: Increase epochs, check learning rate\n",
    "- Quality issues: Need more diverse training data\n",
    "\n",
    "🚀 Next Steps:\n",
    "- Scale up to larger datasets (10K+ examples)\n",
    "- Try different base models (Qwen2.5, Yi, Llama)\n",
    "- Experiment with QLoRA for even lower memory usage\n",
    "- Add evaluation metrics (ROUGE, BLEU, human eval)\n",
    "- Deploy as a chat interface\n",
    "\n",
    "💡 Production Tips:\n",
    "- Use validation set to prevent overfitting\n",
    "- Monitor loss curves and stop early if needed\n",
    "- Consider multi-epoch training with lr scheduling\n",
    "- Save multiple checkpoints for comparison\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🎉 nb11 Instruction Tuning Demo completed!\")\n",
    "print(f\"💾 Model artifacts saved to: {AI_CACHE_ROOT}/\")\n",
    "print(f\"🔧 Smoke test status: {'PASSED' if smoke_test_result else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdc41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation that instruction tuning setup works correctly\n",
    "def validate_instruction_tuning():\n",
    "    checks = []\n",
    "\n",
    "    # Check 1: Dataset formatting\n",
    "    try:\n",
    "        sample = format_instruction(\n",
    "            {\"instruction\": \"Test\", \"input\": \"\", \"output\": \"Response\"}\n",
    "        )\n",
    "        assert \"### Instruction:\" in sample[\"full_text\"]\n",
    "        checks.append(\"✅ Dataset formatting works\")\n",
    "    except:\n",
    "        checks.append(\"❌ Dataset formatting failed\")\n",
    "\n",
    "    # Check 2: Model loading with quantization\n",
    "    try:\n",
    "        assert model is not None\n",
    "        assert tokenizer is not None\n",
    "        checks.append(\"✅ Model and tokenizer loaded\")\n",
    "    except:\n",
    "        checks.append(\"❌ Model loading failed\")\n",
    "\n",
    "    # Check 3: LoRA application\n",
    "    try:\n",
    "        trainable_params = model.num_parameters(only_trainable=True)\n",
    "        total_params = model.num_parameters()\n",
    "        ratio = trainable_params / total_params\n",
    "        assert ratio < 0.1  # LoRA should train <10% of parameters\n",
    "        checks.append(f\"✅ LoRA applied ({ratio:.2%} trainable)\")\n",
    "    except:\n",
    "        checks.append(\"❌ LoRA not properly applied\")\n",
    "\n",
    "    return checks\n",
    "\n",
    "\n",
    "validation_results = validate_instruction_tuning()\n",
    "for result in validation_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230338d9",
   "metadata": {},
   "source": [
    "\n",
    "## 📋 本章小結\n",
    "\n",
    "### ✅ 完成項目\n",
    "- **指令數據處理**: Alpaca 格式解析、長度過濾、模板格式化\n",
    "- **LoRA 微調流程**: 參數效率訓練、4bit 量化支援、低顯存優化\n",
    "- **評估對比**: 微調前後指令跟隨能力比較、多案例測試\n",
    "- **實用工具**: 可重用的格式化函數、煙霧測試、使用指南\n",
    "\n",
    "### 🎯 核心原理要點\n",
    "- **指令調優本質**: 教會模型遵循特定格式的指令，提升任務執行能力\n",
    "- **LoRA 效率**: 只訓練 1-10% 參數即可達到接近全量微調的效果\n",
    "- **數據質量重要性**: 高質量指令-回應對比隨機數據更重要\n",
    "- **量化權衡**: 4bit 量化降低精度但大幅節省顯存，適合資源受限環境\n",
    "\n",
    "### 🚀 下一步建議\n",
    "1. **擴展到中文**: 使用中文指令數據集（如 BELLE、Chinese-Alpaca）\n",
    "2. **QLoRA 進階**: 下一章實作 QLoRA 的 int4 量化微調技術\n",
    "3. **評估指標**: 加入 ROUGE、BLEU、人工評估等量化指標\n",
    "4. **多輪對話**: 處理對話歷史，支援上下文相關的指令跟隨\n",
    "\n",
    "**何時使用指令調優**: 當基礎模型無法很好地遵循特定格式指令，或需要針對特定領域任務優化時，指令調優是最直接有效的方法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
