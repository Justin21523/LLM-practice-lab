{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb11_instruction_tuning_demo.ipynb\n",
    "# Êåá‰ª§Ë™øÂÑ™Á§∫ÁØÑ - Instruction Tuning with LoRA/QLoRA\n",
    "\n",
    "# === Cell 1: Shared Cache Bootstrap & Environment Setup ===\n",
    "import os, pathlib, torch, warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "for k, v in paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e855cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check required packages\n",
    "try:\n",
    "    import transformers, datasets, peft, bitsandbytes\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForCausalLM,\n",
    "        TrainingArguments,\n",
    "        Trainer,\n",
    "    )\n",
    "    from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "    from datasets import load_dataset\n",
    "    import json, random\n",
    "\n",
    "    print(\"‚úÖ All required packages imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Missing package: {e}\")\n",
    "    print(\n",
    "        \"Install with: pip install transformers datasets peft bitsandbytes accelerate\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194cf4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Load and Explore Instruction Dataset ===\n",
    "print(\"üîç Loading instruction dataset...\")\n",
    "\n",
    "# Load Alpaca-style instruction dataset (Stanford Alpaca or similar)\n",
    "try:\n",
    "    # Option 1: Stanford Alpaca (English)\n",
    "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "    print(f\"‚úÖ Loaded Stanford Alpaca dataset: {len(dataset)} examples\")\n",
    "except:\n",
    "    try:\n",
    "        # Option 2: Databricks Dolly (alternative)\n",
    "        dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "        print(f\"‚úÖ Loaded Databricks Dolly dataset: {len(dataset)} examples\")\n",
    "    except:\n",
    "        # Option 3: Chinese instruction dataset (fallback)\n",
    "        print(\"‚ö†Ô∏è Using synthetic examples (original datasets unavailable)\")\n",
    "        synthetic_data = [\n",
    "            {\n",
    "                \"instruction\": \"Ë´ãËß£Èáã‰ªÄÈ∫ºÊòØÊ©üÂô®Â≠∏Áøí\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": \"Ê©üÂô®Â≠∏ÁøíÊòØ‰∫∫Â∑•Êô∫ÊÖßÁöÑ‰∏ÄÂÄãÂàÜÊîØÔºåÂÆÉ‰ΩøË®àÁÆóÊ©üÁ≥ªÁµ±ËÉΩÂ§†ÈÄöÈÅéÁ∂ìÈ©óËá™ÂãïÂ≠∏ÁøíÂíåÊîπÈÄ≤ÔºåËÄåÁÑ°ÈúÄË¢´ÊòéÁ¢∫Á∑®Á®ã„ÄÇ\",\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Â∞á‰ª•‰∏ãÂè•Â≠êÁøªË≠ØÊàêËã±Êñá\",\n",
    "                \"input\": \"‰ªäÂ§©Â§©Ê∞£ÂæàÂ•Ω\",\n",
    "                \"output\": \"The weather is very nice today.\",\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"ÂàóÂá∫‰∏âÂÄãÁ®ãÂºèË®≠Ë®àÁöÑÊúÄ‰Ω≥ÂØ¶Ë∏ê\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": \"1. ÂØ´Ê∏ÖÊô∞ÁöÑË®ªËß£ÂíåÊñáÊ™î\\n2. ‰ΩøÁî®ÊúâÊÑèÁæ©ÁöÑËÆäÊï∏ÂíåÂáΩÊï∏ÂêçÁ®±\\n3. ‰øùÊåÅ‰ª£Á¢ºÁ∞°ÊΩîÂíåÊ®°ÁµÑÂåñ\",\n",
    "            },\n",
    "        ]\n",
    "        from datasets import Dataset\n",
    "\n",
    "        dataset = Dataset.from_list(synthetic_data * 100)  # Repeat for demo\n",
    "        print(f\"‚úÖ Created synthetic dataset: {len(dataset)} examples\")\n",
    "\n",
    "# Explore dataset structure\n",
    "print(\"\\nüìä Dataset Structure:\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "print(f\"First example:\")\n",
    "example = dataset[0]\n",
    "for key, value in example.items():\n",
    "    print(f\"  {key}: {repr(value[:100] + '...' if len(str(value)) > 100 else value)}\")\n",
    "\n",
    "# Basic statistics\n",
    "instructions_with_input = sum(1 for item in dataset if item.get(\"input\", \"\").strip())\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"Total examples: {len(dataset)}\")\n",
    "print(f\"Examples with input: {instructions_with_input}\")\n",
    "print(f\"Examples without input: {len(dataset) - instructions_with_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c25dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Data Preprocessing and Formatting ===\n",
    "print(\"üîß Preprocessing instruction data...\")\n",
    "\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"\n",
    "    Format instruction data into a single text for training.\n",
    "    Uses Alpaca-style prompt template.\n",
    "    \"\"\"\n",
    "    instruction = example[\"instruction\"].strip()\n",
    "    input_text = example.get(\"input\", \"\").strip()\n",
    "    output = example[\"output\"].strip()\n",
    "\n",
    "    if input_text:\n",
    "        # Instruction with input\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        # Instruction only\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "\n",
    "    # Full text for training (prompt + response)\n",
    "    full_text = prompt + output\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": output,\n",
    "        \"full_text\": full_text,\n",
    "        \"length\": len(full_text),\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = dataset.map(format_instruction)\n",
    "\n",
    "# Filter by length (avoid too long sequences)\n",
    "MAX_LENGTH = 512  # Adjust based on your GPU memory\n",
    "filtered_dataset = formatted_dataset.filter(lambda x: x[\"length\"] <= MAX_LENGTH)\n",
    "\n",
    "print(\n",
    "    f\"‚úÖ Formatted dataset: {len(formatted_dataset)} ‚Üí {len(filtered_dataset)} examples (after length filtering)\"\n",
    ")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nüìù Formatted Example:\")\n",
    "example = filtered_dataset[0]\n",
    "print(\"Prompt:\")\n",
    "print(example[\"prompt\"])\n",
    "print(\"Response:\")\n",
    "print(example[\"response\"])\n",
    "print(f\"Total length: {example['length']} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7827521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Load Base Model and Baseline Test ===\n",
    "print(\"ü§ñ Loading base model for instruction tuning...\")\n",
    "\n",
    "# Model selection based on available VRAM\n",
    "MODEL_NAME = \"microsoft/DialoGPT-small\"  # Lightweight for demo\n",
    "# Alternative: \"Qwen/Qwen2.5-0.5B-Instruct\" or \"google/flan-t5-small\"\n",
    "\n",
    "# Configure 4-bit quantization for low VRAM\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=(\n",
    "        torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config if torch.cuda.is_available() else None,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model loaded successfully\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"Falling back to CPU-only mode...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "# Test baseline performance\n",
    "def test_instruction_following(model, tokenizer, prompt, max_length=100):\n",
    "    \"\"\"Test model's instruction following capability\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated[len(prompt) :].strip()\n",
    "\n",
    "\n",
    "# Baseline test\n",
    "test_prompt = \"### Instruction:\\nExplain what is machine learning in simple terms.\\n\\n### Response:\\n\"\n",
    "baseline_response = test_instruction_following(model, tokenizer, test_prompt)\n",
    "print(f\"\\nüß™ Baseline Test:\")\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"Response: {baseline_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: LoRA Configuration and Training Setup ===\n",
    "print(\"‚öôÔ∏è Setting up LoRA for parameter-efficient fine-tuning...\")\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,  # Low rank\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Attention layers\n",
    "    # Note: target_modules may vary by model architecture\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "try:\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"‚úÖ LoRA applied successfully\")\n",
    "    print(f\"Trainable parameters: {model.num_parameters(only_trainable=True):,}\")\n",
    "    print(f\"Total parameters: {model.num_parameters():,}\")\n",
    "    print(\n",
    "        f\"Trainable %: {100 * model.num_parameters(only_trainable=True) / model.num_parameters():.2f}%\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error applying LoRA: {e}\")\n",
    "    print(\n",
    "        \"This might be due to model architecture. Continuing with full fine-tuning...\"\n",
    "    )\n",
    "\n",
    "# Data collator for language modeling\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the full text for training\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"full_text\"], truncation=True, padding=False, max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "\n",
    "# Tokenize dataset\n",
    "train_dataset = filtered_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=filtered_dataset.column_names\n",
    ").shuffle(seed=42)\n",
    "\n",
    "# Take a subset for quick demo (full dataset for real training)\n",
    "train_dataset = train_dataset.select(range(min(100, len(train_dataset))))\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False  # Causal language modeling\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Prepared training dataset: {len(train_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Lightweight Fine-tuning Execution ===\n",
    "print(\"üöÄ Starting instruction fine-tuning...\")\n",
    "\n",
    "# Training arguments (conservative for demo)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{AI_CACHE_ROOT}/instruction_tuning_demo\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,  # Very short for demo\n",
    "    per_device_train_batch_size=1,  # Low VRAM friendly\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,  # Conservative learning rate\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",  # Don't save checkpoints for demo\n",
    "    dataloader_drop_last=False,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision if available\n",
    "    report_to=None,  # Disable wandb/tensorboard\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"üìä Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(\n",
    "    f\"  Total steps: ~{len(train_dataset) // training_args.per_device_train_batch_size}\"\n",
    ")\n",
    "\n",
    "# Start training\n",
    "try:\n",
    "    print(\"\\nüéØ Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "\n",
    "    # Save LoRA adapter\n",
    "    if hasattr(model, \"save_pretrained\"):\n",
    "        adapter_path = f\"{AI_CACHE_ROOT}/instruction_lora_adapter\"\n",
    "        model.save_pretrained(adapter_path)\n",
    "        print(f\"üíæ LoRA adapter saved to: {adapter_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Training error: {e}\")\n",
    "    print(\n",
    "        \"This might be due to memory constraints. Try reducing batch size or sequence length.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d05435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Post-Training Evaluation and Comparison ===\n",
    "print(\"üìà Evaluating fine-tuned model...\")\n",
    "\n",
    "# Test the same prompt as baseline\n",
    "tuned_response = test_instruction_following(model, tokenizer, test_prompt)\n",
    "\n",
    "print(\"üîÑ Before vs After Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"BASELINE (pre-training):\")\n",
    "print(baseline_response)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINE-TUNED (post-training):\")\n",
    "print(tuned_response)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Additional test cases\n",
    "test_cases = [\n",
    "    \"### Instruction:\\nList three benefits of exercise.\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nWrite a haiku about technology.\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nExplain why the sky is blue.\\n\\n### Response:\\n\",\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Additional Test Cases:\")\n",
    "for i, prompt in enumerate(test_cases):\n",
    "    print(f\"\\nTest {i+1}:\")\n",
    "    print(\n",
    "        f\"Instruction: {prompt.split('Response:')[0].replace('### Instruction:', '').replace('###', '').strip()}\"\n",
    "    )\n",
    "    response = test_instruction_following(model, tokenizer, prompt, max_length=80)\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "# Qualitative assessment checklist\n",
    "print(\"\\n‚úÖ Qualitative Assessment Checklist:\")\n",
    "print(\"1. Does the model follow instructions more consistently? [Manual check]\")\n",
    "print(\"2. Are responses more structured and coherent? [Manual check]\")\n",
    "print(\"3. Does the model use appropriate formatting? [Manual check]\")\n",
    "print(\"4. Is the model still safe and not generating harmful content? [Manual check]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Smoke Test and Usage Notes ===\n",
    "print(\"üß™ Final Smoke Test...\")\n",
    "\n",
    "\n",
    "def smoke_test():\n",
    "    \"\"\"Quick validation that everything works\"\"\"\n",
    "    try:\n",
    "        # Test tokenizer\n",
    "        test_text = \"Hello world\"\n",
    "        tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "        assert tokens[\"input_ids\"].shape[1] > 0\n",
    "\n",
    "        # Test model inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "            assert outputs.logits.shape[-1] == tokenizer.vocab_size\n",
    "\n",
    "        # Test instruction format\n",
    "        formatted = format_instruction(\n",
    "            {\"instruction\": \"Test instruction\", \"input\": \"\", \"output\": \"Test output\"}\n",
    "        )\n",
    "        assert \"### Instruction:\" in formatted[\"full_text\"]\n",
    "        assert \"### Response:\" in formatted[\"full_text\"]\n",
    "\n",
    "        print(\"‚úÖ All smoke tests passed!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Smoke test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "smoke_test_result = smoke_test()\n",
    "\n",
    "print(\"\\nüìö Usage Notes and Next Steps:\")\n",
    "print(\n",
    "    \"\"\"\n",
    "üéØ What we accomplished:\n",
    "- Loaded and formatted instruction dataset (Alpaca-style)\n",
    "- Applied LoRA for parameter-efficient fine-tuning\n",
    "- Trained model on instruction-following tasks\n",
    "- Compared before/after performance\n",
    "\n",
    "üîß Key Parameters:\n",
    "- LoRA rank: 16 (balance between efficiency and capacity)\n",
    "- Learning rate: 2e-4 (conservative to avoid catastrophic forgetting)\n",
    "- Max sequence length: 512 (adjust based on your needs)\n",
    "\n",
    "‚ö†Ô∏è Common Issues:\n",
    "- OOM errors: Reduce batch_size, use gradient_checkpointing\n",
    "- Poor convergence: Increase epochs, check learning rate\n",
    "- Quality issues: Need more diverse training data\n",
    "\n",
    "üöÄ Next Steps:\n",
    "- Scale up to larger datasets (10K+ examples)\n",
    "- Try different base models (Qwen2.5, Yi, Llama)\n",
    "- Experiment with QLoRA for even lower memory usage\n",
    "- Add evaluation metrics (ROUGE, BLEU, human eval)\n",
    "- Deploy as a chat interface\n",
    "\n",
    "üí° Production Tips:\n",
    "- Use validation set to prevent overfitting\n",
    "- Monitor loss curves and stop early if needed\n",
    "- Consider multi-epoch training with lr scheduling\n",
    "- Save multiple checkpoints for comparison\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ nb11 Instruction Tuning Demo completed!\")\n",
    "print(f\"üíæ Model artifacts saved to: {AI_CACHE_ROOT}/\")\n",
    "print(f\"üîß Smoke test status: {'PASSED' if smoke_test_result else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdc41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation that instruction tuning setup works correctly\n",
    "def validate_instruction_tuning():\n",
    "    checks = []\n",
    "\n",
    "    # Check 1: Dataset formatting\n",
    "    try:\n",
    "        sample = format_instruction(\n",
    "            {\"instruction\": \"Test\", \"input\": \"\", \"output\": \"Response\"}\n",
    "        )\n",
    "        assert \"### Instruction:\" in sample[\"full_text\"]\n",
    "        checks.append(\"‚úÖ Dataset formatting works\")\n",
    "    except:\n",
    "        checks.append(\"‚ùå Dataset formatting failed\")\n",
    "\n",
    "    # Check 2: Model loading with quantization\n",
    "    try:\n",
    "        assert model is not None\n",
    "        assert tokenizer is not None\n",
    "        checks.append(\"‚úÖ Model and tokenizer loaded\")\n",
    "    except:\n",
    "        checks.append(\"‚ùå Model loading failed\")\n",
    "\n",
    "    # Check 3: LoRA application\n",
    "    try:\n",
    "        trainable_params = model.num_parameters(only_trainable=True)\n",
    "        total_params = model.num_parameters()\n",
    "        ratio = trainable_params / total_params\n",
    "        assert ratio < 0.1  # LoRA should train <10% of parameters\n",
    "        checks.append(f\"‚úÖ LoRA applied ({ratio:.2%} trainable)\")\n",
    "    except:\n",
    "        checks.append(\"‚ùå LoRA not properly applied\")\n",
    "\n",
    "    return checks\n",
    "\n",
    "\n",
    "validation_results = validate_instruction_tuning()\n",
    "for result in validation_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230338d9",
   "metadata": {},
   "source": [
    "\n",
    "## üìã Êú¨Á´†Â∞èÁµê\n",
    "\n",
    "### ‚úÖ ÂÆåÊàêÈ†ÖÁõÆ\n",
    "- **Êåá‰ª§Êï∏ÊìöËôïÁêÜ**: Alpaca Ê†ºÂºèËß£Êûê„ÄÅÈï∑Â∫¶ÈÅéÊøæ„ÄÅÊ®°ÊùøÊ†ºÂºèÂåñ\n",
    "- **LoRA ÂæÆË™øÊµÅÁ®ã**: ÂèÉÊï∏ÊïàÁéáË®ìÁ∑¥„ÄÅ4bit ÈáèÂåñÊîØÊè¥„ÄÅ‰ΩéÈ°ØÂ≠òÂÑ™Âåñ\n",
    "- **Ë©ï‰º∞Â∞çÊØî**: ÂæÆË™øÂâçÂæåÊåá‰ª§Ë∑üÈö®ËÉΩÂäõÊØîËºÉ„ÄÅÂ§öÊ°à‰æãÊ∏¨Ë©¶\n",
    "- **ÂØ¶Áî®Â∑•ÂÖ∑**: ÂèØÈáçÁî®ÁöÑÊ†ºÂºèÂåñÂáΩÊï∏„ÄÅÁÖôÈúßÊ∏¨Ë©¶„ÄÅ‰ΩøÁî®ÊåáÂçó\n",
    "\n",
    "### üéØ Ê†∏ÂøÉÂéüÁêÜË¶ÅÈªû\n",
    "- **Êåá‰ª§Ë™øÂÑ™Êú¨Ë≥™**: ÊïôÊúÉÊ®°ÂûãÈÅµÂæ™ÁâπÂÆöÊ†ºÂºèÁöÑÊåá‰ª§ÔºåÊèêÂçá‰ªªÂãôÂü∑Ë°åËÉΩÂäõ\n",
    "- **LoRA ÊïàÁéá**: Âè™Ë®ìÁ∑¥ 1-10% ÂèÉÊï∏Âç≥ÂèØÈÅîÂà∞Êé•ËøëÂÖ®ÈáèÂæÆË™øÁöÑÊïàÊûú\n",
    "- **Êï∏ÊìöË≥™ÈáèÈáçË¶ÅÊÄß**: È´òË≥™ÈáèÊåá‰ª§-ÂõûÊáâÂ∞çÊØîÈö®Ê©üÊï∏ÊìöÊõ¥ÈáçË¶Å\n",
    "- **ÈáèÂåñÊ¨äË°°**: 4bit ÈáèÂåñÈôç‰ΩéÁ≤æÂ∫¶‰ΩÜÂ§ßÂπÖÁØÄÁúÅÈ°ØÂ≠òÔºåÈÅ©ÂêàË≥áÊ∫êÂèóÈôêÁí∞Â¢É\n",
    "\n",
    "### üöÄ ‰∏ã‰∏ÄÊ≠•Âª∫Ë≠∞\n",
    "1. **Êì¥Â±ïÂà∞‰∏≠Êñá**: ‰ΩøÁî®‰∏≠ÊñáÊåá‰ª§Êï∏ÊìöÈõÜÔºàÂ¶Ç BELLE„ÄÅChinese-AlpacaÔºâ\n",
    "2. **QLoRA ÈÄ≤Èöé**: ‰∏ã‰∏ÄÁ´†ÂØ¶‰Ωú QLoRA ÁöÑ int4 ÈáèÂåñÂæÆË™øÊäÄË°ì\n",
    "3. **Ë©ï‰º∞ÊåáÊ®ô**: Âä†ÂÖ• ROUGE„ÄÅBLEU„ÄÅ‰∫∫Â∑•Ë©ï‰º∞Á≠âÈáèÂåñÊåáÊ®ô\n",
    "4. **Â§öËº™Â∞çË©±**: ËôïÁêÜÂ∞çË©±Ê≠∑Âè≤ÔºåÊîØÊè¥‰∏ä‰∏ãÊñáÁõ∏ÈóúÁöÑÊåá‰ª§Ë∑üÈö®\n",
    "\n",
    "**‰ΩïÊôÇ‰ΩøÁî®Êåá‰ª§Ë™øÂÑ™**: Áï∂Âü∫Á§éÊ®°ÂûãÁÑ°Ê≥ïÂæàÂ•ΩÂú∞ÈÅµÂæ™ÁâπÂÆöÊ†ºÂºèÊåá‰ª§ÔºåÊàñÈúÄË¶ÅÈáùÂ∞çÁâπÂÆöÈ†òÂüü‰ªªÂãôÂÑ™ÂåñÊôÇÔºåÊåá‰ª§Ë™øÂÑ™ÊòØÊúÄÁõ¥Êé•ÊúâÊïàÁöÑÊñπÊ≥ï„ÄÇ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
