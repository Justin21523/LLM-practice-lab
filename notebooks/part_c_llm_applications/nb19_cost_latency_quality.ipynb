{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db71ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb19_cost_latency_quality.ipynb - Cost/Latency/Quality Trade-off Analysis\n",
    "# ÊïàËÉΩÊ¨äË°°ÂàÜÊûêÔºöÊàêÊú¨„ÄÅÂª∂ÈÅ≤ËàáÂìÅË≥™ÁöÑ‰∏âËßíÈóú‰øÇ\n",
    "\n",
    "# ================================\n",
    "# Cell 1: Environment Setup & Dependencies\n",
    "# ================================\n",
    "\n",
    "# Shared cache bootstrap\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e1b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies for performance measurement\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "\n",
    "packages = [\n",
    "    \"psutil\",\n",
    "    \"nvidia-ml-py\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"rouge-score\",\n",
    "    \"sacrebleu\",\n",
    "]\n",
    "for pkg in packages:\n",
    "    install_if_missing(pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78296761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 2: Performance Profiler Classes\n",
    "# ================================\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import pynvml\n",
    "\n",
    "    pynvml.nvmlInit()\n",
    "    NVIDIA_ML_AVAILABLE = True\n",
    "except:\n",
    "    NVIDIA_ML_AVAILABLE = False\n",
    "    print(\"[Warning] nvidia-ml-py not available, GPU metrics will be limited\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Performance measurement results\"\"\"\n",
    "\n",
    "    model_name: str\n",
    "    quantization: str\n",
    "    prompt_length: int\n",
    "    generation_length: int\n",
    "\n",
    "    # Latency metrics (Âª∂ÈÅ≤ÊåáÊ®ô)\n",
    "    time_to_first_token: float  # TTFT - È¶ñÂÄã token ÁîüÊàêÊôÇÈñì\n",
    "    tokens_per_second: float  # TPS - ÊØèÁßíÁîüÊàê token Êï∏\n",
    "    total_time: float  # Á∏ΩÁîüÊàêÊôÇÈñì\n",
    "\n",
    "    # Resource metrics (Ë≥áÊ∫êÊåáÊ®ô)\n",
    "    peak_gpu_memory_mb: float  # Â≥∞ÂÄº GPU Ë®òÊÜ∂È´î‰ΩøÁî®Èáè\n",
    "    avg_gpu_utilization: float  # Âπ≥Âùá GPU ‰ΩøÁî®Áéá\n",
    "    avg_cpu_percent: float  # Âπ≥Âùá CPU ‰ΩøÁî®Áéá\n",
    "\n",
    "    # Quality metrics (ÂìÅË≥™ÊåáÊ®ô)\n",
    "    perplexity: Optional[float] = None\n",
    "    rouge_l: Optional[float] = None\n",
    "    bleu_score: Optional[float] = None\n",
    "\n",
    "\n",
    "class PerformanceProfiler:\n",
    "    \"\"\"Comprehensive performance profiler for LLM inference\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.gpu_available = torch.cuda.is_available()\n",
    "\n",
    "        if NVIDIA_ML_AVAILABLE and self.gpu_available:\n",
    "            self.gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "    def measure_inference(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 100,\n",
    "        **generate_kwargs\n",
    "    ) -> PerformanceMetrics:\n",
    "        \"\"\"Measure inference performance for a single generation\"\"\"\n",
    "\n",
    "        # Prepare input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        prompt_length = inputs.input_ids.shape[1]\n",
    "\n",
    "        # Pre-generation memory snapshot\n",
    "        if self.gpu_available:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            initial_memory = torch.cuda.memory_allocated()\n",
    "\n",
    "        # CPU monitoring setup\n",
    "        cpu_percentages = []\n",
    "        gpu_utilizations = []\n",
    "\n",
    "        # Generation with timing\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # TTFT measurement (È¶ñÂÄã token ÊôÇÈñì)\n",
    "        with torch.inference_mode():\n",
    "            # Generate first token\n",
    "            first_token_start = time.perf_counter()\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                **generate_kwargs\n",
    "            )\n",
    "            ttft = time.perf_counter() - first_token_start\n",
    "\n",
    "            # Continue generation for remaining tokens\n",
    "            if max_new_tokens > 1:\n",
    "                remaining_outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    **generate_kwargs\n",
    "                )\n",
    "                outputs = remaining_outputs\n",
    "\n",
    "        total_time = time.perf_counter() - start_time\n",
    "\n",
    "        # Calculate generation metrics\n",
    "        generated_tokens = outputs.shape[1] - prompt_length\n",
    "        tps = generated_tokens / total_time if total_time > 0 else 0\n",
    "\n",
    "        # Memory metrics\n",
    "        if self.gpu_available:\n",
    "            peak_memory = torch.cuda.max_memory_allocated()\n",
    "            peak_memory_mb = peak_memory / 1024 / 1024\n",
    "        else:\n",
    "            peak_memory_mb = 0\n",
    "\n",
    "        # Resource utilization (Á∞°ÂåñÁâàÊú¨ÔºåÂØ¶ÈöõÊáâÁî®‰∏≠ÂèØÁî®Êõ¥Ë§áÈõúÁöÑÁõ£Êéß)\n",
    "        avg_cpu = psutil.cpu_percent(interval=None)\n",
    "\n",
    "        if NVIDIA_ML_AVAILABLE and self.gpu_available:\n",
    "            try:\n",
    "                gpu_util = pynvml.nvmlDeviceGetUtilizationRates(self.gpu_handle)\n",
    "                avg_gpu = gpu_util.gpu\n",
    "            except:\n",
    "                avg_gpu = 0\n",
    "        else:\n",
    "            avg_gpu = 0\n",
    "\n",
    "        return PerformanceMetrics(\n",
    "            model_name=getattr(model, \"name_or_path\", \"unknown\"),\n",
    "            quantization=getattr(model, \"quantization_config\", \"fp16\"),\n",
    "            prompt_length=prompt_length,\n",
    "            generation_length=generated_tokens,\n",
    "            time_to_first_token=ttft,\n",
    "            tokens_per_second=tps,\n",
    "            total_time=total_time,\n",
    "            peak_gpu_memory_mb=peak_memory_mb,\n",
    "            avg_gpu_utilization=avg_gpu,\n",
    "            avg_cpu_percent=avg_cpu,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3771487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 3: Quality Evaluator\n",
    "# ================================\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu import BLEU\n",
    "\n",
    "\n",
    "class QualityEvaluator:\n",
    "    \"\"\"Evaluate generation quality using multiple metrics\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            [\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True\n",
    "        )\n",
    "        self.bleu = BLEU()\n",
    "\n",
    "    def calculate_perplexity(self, model, tokenizer, text: str) -> float:\n",
    "        \"\"\"Calculate perplexity of generated text\"\"\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**inputs, labels=inputs.input_ids)\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss).item()\n",
    "\n",
    "        return perplexity\n",
    "\n",
    "    def calculate_rouge(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"Calculate ROUGE-L F1 score\"\"\"\n",
    "        scores = self.rouge_scorer.score(reference, generated)\n",
    "        return scores[\"rougeL\"].fmeasure\n",
    "\n",
    "    def calculate_bleu(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"Calculate BLEU score\"\"\"\n",
    "        score = self.bleu.sentence_score(generated, [reference])\n",
    "        return score.score / 100.0  # Convert to 0-1 range\n",
    "\n",
    "    def evaluate_quality(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        generated_text: str,\n",
    "        reference_text: Optional[str] = None,\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Comprehensive quality evaluation\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Perplexity (always calculated)\n",
    "        try:\n",
    "            metrics[\"perplexity\"] = self.calculate_perplexity(\n",
    "                model, tokenizer, generated_text\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Perplexity calculation failed: {e}\")\n",
    "            metrics[\"perplexity\"] = float(\"inf\")\n",
    "\n",
    "        # Reference-based metrics (if reference provided)\n",
    "        if reference_text:\n",
    "            try:\n",
    "                metrics[\"rouge_l\"] = self.calculate_rouge(\n",
    "                    generated_text, reference_text\n",
    "                )\n",
    "                metrics[\"bleu\"] = self.calculate_bleu(generated_text, reference_text)\n",
    "            except Exception as e:\n",
    "                print(f\"[Warning] Reference-based metrics failed: {e}\")\n",
    "                metrics[\"rouge_l\"] = 0.0\n",
    "                metrics[\"bleu\"] = 0.0\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 4: Test Dataset & Model Configurations\n",
    "# ================================\n",
    "\n",
    "# Standard test prompts with varying complexity\n",
    "TEST_PROMPTS = [\n",
    "    {\n",
    "        \"prompt\": \"Explain artificial intelligence in simple terms.\",\n",
    "        \"reference\": \"Artificial intelligence is technology that enables machines to perform tasks that typically require human intelligence, such as learning, reasoning, and problem-solving.\",\n",
    "        \"category\": \"simple\",\n",
    "        \"expected_length\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a detailed analysis of climate change impacts on global agriculture, including specific examples and potential solutions.\",\n",
    "        \"reference\": \"Climate change significantly affects global agriculture through altered precipitation patterns, increased temperatures, and extreme weather events. For example, drought in wheat-growing regions reduces yields, while flooding destroys crops. Solutions include drought-resistant crops, improved irrigation, and sustainable farming practices.\",\n",
    "        \"category\": \"complex\",\n",
    "        \"expected_length\": 150,\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Translate and explain: 'Machine learning algorithms can identify patterns in large datasets.'\",\n",
    "        \"reference\": \"Machine learning algorithms are computational methods that can automatically discover patterns, relationships, and trends within large amounts of data without being explicitly programmed to look for specific patterns.\",\n",
    "        \"category\": \"medium\",\n",
    "        \"expected_length\": 100,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Model configurations to test\n",
    "MODEL_CONFIGS = [\n",
    "    {\n",
    "        \"model_id\": \"microsoft/DialoGPT-small\",  # ~117M parameters\n",
    "        \"name\": \"DialoGPT-small\",\n",
    "        \"size_category\": \"small\",\n",
    "        \"quantization_options\": [\"fp16\", \"int8\"],\n",
    "    },\n",
    "    {\n",
    "        \"model_id\": \"microsoft/DialoGPT-medium\",  # ~345M parameters\n",
    "        \"name\": \"DialoGPT-medium\",\n",
    "        \"size_category\": \"medium\",\n",
    "        \"quantization_options\": [\"fp16\", \"int8\", \"int4\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Cost calculation parameters (ÊàêÊú¨Ë®àÁÆóÂèÉÊï∏)\n",
    "COST_PARAMS = {\n",
    "    \"gpu_hourly_cost\": {  # USD per hour\n",
    "        \"RTX_3060\": 0.15,\n",
    "        \"RTX_4070\": 0.25,\n",
    "        \"RTX_4090\": 0.50,\n",
    "        \"A100\": 2.00,\n",
    "    },\n",
    "    \"electricity_kwh\": 0.12,  # USD per kWh\n",
    "    \"gpu_power_watts\": {\"RTX_3060\": 170, \"RTX_4070\": 200, \"RTX_4090\": 450, \"A100\": 400},\n",
    "}\n",
    "\n",
    "\n",
    "def detect_gpu_type() -> str:\n",
    "    \"\"\"Detect current GPU type for cost calculation\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return \"CPU\"\n",
    "\n",
    "    gpu_name = torch.cuda.get_device_name(0).lower()\n",
    "    if \"3060\" in gpu_name:\n",
    "        return \"RTX_3060\"\n",
    "    elif \"4070\" in gpu_name:\n",
    "        return \"RTX_4070\"\n",
    "    elif \"4090\" in gpu_name:\n",
    "        return \"RTX_4090\"\n",
    "    elif \"a100\" in gpu_name:\n",
    "        return \"A100\"\n",
    "    else:\n",
    "        return \"RTX_4070\"  # Default assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 5: Automated Performance Testing\n",
    "# ================================\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"Automated benchmark suite for cost/latency/quality analysis\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.profiler = PerformanceProfiler()\n",
    "        self.evaluator = QualityEvaluator()\n",
    "        self.results = []\n",
    "        self.gpu_type = detect_gpu_type()\n",
    "\n",
    "    def load_model_with_quantization(self, model_id: str, quantization: str):\n",
    "        \"\"\"Load model with specified quantization\"\"\"\n",
    "        print(f\"Loading {model_id} with {quantization}...\")\n",
    "\n",
    "        if quantization == \"int4\":\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "            )\n",
    "        elif quantization == \"int8\":\n",
    "            bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id, quantization_config=bnb_config, device_map=\"auto\"\n",
    "            )\n",
    "        else:  # fp16\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "            )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    def calculate_cost_per_1k_tokens(self, metrics: PerformanceMetrics) -> float:\n",
    "        \"\"\"Calculate cost per 1000 tokens generated\"\"\"\n",
    "        if metrics.tokens_per_second <= 0:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        # Time to generate 1000 tokens\n",
    "        time_for_1k = 1000 / metrics.tokens_per_second\n",
    "\n",
    "        # GPU cost\n",
    "        gpu_cost_per_hour = COST_PARAMS[\"gpu_hourly_cost\"].get(self.gpu_type, 0.25)\n",
    "        gpu_cost_1k = gpu_cost_per_hour * (time_for_1k / 3600)\n",
    "\n",
    "        # Electricity cost\n",
    "        power_watts = COST_PARAMS[\"gpu_power_watts\"].get(self.gpu_type, 200)\n",
    "        electricity_cost_1k = (\n",
    "            (power_watts / 1000) * (time_for_1k / 3600) * COST_PARAMS[\"electricity_kwh\"]\n",
    "        )\n",
    "\n",
    "        return gpu_cost_1k + electricity_cost_1k\n",
    "\n",
    "    def run_single_benchmark(\n",
    "        self, model_config: Dict, quantization: str, test_prompt: Dict\n",
    "    ) -> Dict:\n",
    "        \"\"\"Run benchmark for single configuration\"\"\"\n",
    "        try:\n",
    "            # Load model\n",
    "            model, tokenizer = self.load_model_with_quantization(\n",
    "                model_config[\"model_id\"], quantization\n",
    "            )\n",
    "\n",
    "            # Performance measurement\n",
    "            perf_metrics = self.profiler.measure_inference(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                test_prompt[\"prompt\"],\n",
    "                max_new_tokens=test_prompt[\"expected_length\"],\n",
    "            )\n",
    "\n",
    "            # Generate text for quality evaluation\n",
    "            inputs = tokenizer(test_prompt[\"prompt\"], return_tensors=\"pt\")\n",
    "            with torch.inference_mode():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=test_prompt[\"expected_length\"],\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            generated_only = generated_text[len(test_prompt[\"prompt\"]) :].strip()\n",
    "\n",
    "            # Quality evaluation\n",
    "            quality_metrics = self.evaluator.evaluate_quality(\n",
    "                model, tokenizer, generated_only, test_prompt.get(\"reference\")\n",
    "            )\n",
    "\n",
    "            # Cost calculation\n",
    "            cost_per_1k = self.calculate_cost_per_1k_tokens(perf_metrics)\n",
    "\n",
    "            # Combine results\n",
    "            result = {\n",
    "                \"model_name\": model_config[\"name\"],\n",
    "                \"quantization\": quantization,\n",
    "                \"prompt_category\": test_prompt[\"category\"],\n",
    "                \"perf_metrics\": perf_metrics,\n",
    "                \"quality_metrics\": quality_metrics,\n",
    "                \"cost_per_1k_tokens\": cost_per_1k,\n",
    "                \"generated_text\": generated_only,\n",
    "            }\n",
    "\n",
    "            # Cleanup\n",
    "            del model, tokenizer\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Benchmark failed for {model_config['name']} + {quantization}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def run_full_benchmark(self) -> List[Dict]:\n",
    "        \"\"\"Run comprehensive benchmark across all configurations\"\"\"\n",
    "        print(\"üöÄ Starting comprehensive performance benchmark...\")\n",
    "        print(f\"GPU Type: {self.gpu_type}\")\n",
    "\n",
    "        for model_config in MODEL_CONFIGS:\n",
    "            for quantization in model_config[\"quantization_options\"]:\n",
    "                for test_prompt in TEST_PROMPTS:\n",
    "                    print(\n",
    "                        f\"\\nüìä Testing: {model_config['name']} | {quantization} | {test_prompt['category']}\"\n",
    "                    )\n",
    "\n",
    "                    result = self.run_single_benchmark(\n",
    "                        model_config, quantization, test_prompt\n",
    "                    )\n",
    "                    if result:\n",
    "                        self.results.append(result)\n",
    "                        print(\n",
    "                            f\"‚úÖ TPS: {result['perf_metrics'].tokens_per_second:.2f} | \"\n",
    "                            f\"Cost: ${result['cost_per_1k_tokens']:.6f}/1K | \"\n",
    "                            f\"VRAM: {result['perf_metrics'].peak_gpu_memory_mb:.0f}MB\"\n",
    "                        )\n",
    "\n",
    "        print(\n",
    "            f\"\\nüéØ Benchmark completed! Total configurations tested: {len(self.results)}\"\n",
    "        )\n",
    "        return self.results\n",
    "\n",
    "\n",
    "# Run the benchmark\n",
    "benchmark = PerformanceBenchmark()\n",
    "results = benchmark.run_full_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 6: Results Analysis & Visualization\n",
    "# ================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "class ResultsAnalyzer:\n",
    "    \"\"\"Analyze and visualize benchmark results\"\"\"\n",
    "\n",
    "    def __init__(self, results: List[Dict]):\n",
    "        self.results = results\n",
    "        self.df = self._create_dataframe()\n",
    "\n",
    "    def _create_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Convert results to pandas DataFrame for analysis\"\"\"\n",
    "        rows = []\n",
    "        for result in self.results:\n",
    "            perf = result[\"perf_metrics\"]\n",
    "            quality = result[\"quality_metrics\"]\n",
    "\n",
    "            row = {\n",
    "                \"model_name\": result[\"model_name\"],\n",
    "                \"quantization\": result[\"quantization\"],\n",
    "                \"prompt_category\": result[\"prompt_category\"],\n",
    "                \"tokens_per_second\": perf.tokens_per_second,\n",
    "                \"ttft\": perf.time_to_first_token,\n",
    "                \"peak_memory_mb\": perf.peak_gpu_memory_mb,\n",
    "                \"cost_per_1k\": result[\"cost_per_1k_tokens\"],\n",
    "                \"perplexity\": quality.get(\"perplexity\", float(\"inf\")),\n",
    "                \"rouge_l\": quality.get(\"rouge_l\", 0.0),\n",
    "                \"bleu\": quality.get(\"bleu\", 0.0),\n",
    "                \"quality_score\": self._calculate_composite_quality(quality),\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def _calculate_composite_quality(self, quality_metrics: Dict) -> float:\n",
    "        \"\"\"Calculate composite quality score (0-1, higher is better)\"\"\"\n",
    "        # Inverse of perplexity (lower perplexity = higher quality)\n",
    "        perp = quality_metrics.get(\"perplexity\", float(\"inf\"))\n",
    "        perp_score = 1 / (1 + perp) if perp != float(\"inf\") else 0\n",
    "\n",
    "        rouge = quality_metrics.get(\"rouge_l\", 0.0)\n",
    "        bleu = quality_metrics.get(\"bleu\", 0.0)\n",
    "\n",
    "        # Weighted average (adjust weights as needed)\n",
    "        composite = 0.4 * perp_score + 0.3 * rouge + 0.3 * bleu\n",
    "        return composite\n",
    "\n",
    "    def plot_cost_latency_quality_3d(self):\n",
    "        \"\"\"Create 3D scatter plot of cost vs latency vs quality\"\"\"\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "        # Aggregate by model + quantization\n",
    "        agg_df = (\n",
    "            self.df.groupby([\"model_name\", \"quantization\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"cost_per_1k\": \"mean\",\n",
    "                    \"tokens_per_second\": \"mean\",\n",
    "                    \"quality_score\": \"mean\",\n",
    "                    \"peak_memory_mb\": \"mean\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Color by quantization\n",
    "        colors = {\"fp16\": \"red\", \"int8\": \"green\", \"int4\": \"blue\"}\n",
    "\n",
    "        for quant in agg_df[\"quantization\"].unique():\n",
    "            data = agg_df[agg_df[\"quantization\"] == quant]\n",
    "            ax.scatter(\n",
    "                data[\"cost_per_1k\"],\n",
    "                data[\"tokens_per_second\"],\n",
    "                data[\"quality_score\"],\n",
    "                c=colors.get(quant, \"gray\"),\n",
    "                label=f\"{quant}\",\n",
    "                s=data[\"peak_memory_mb\"] / 10,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "\n",
    "        ax.set_xlabel(\"Cost per 1K tokens (USD)\")\n",
    "        ax.set_ylabel(\"Tokens per Second\")\n",
    "        ax.set_zlabel(\"Quality Score\")\n",
    "        ax.set_title(\n",
    "            \"Cost vs Latency vs Quality Trade-off\\n(Bubble size = Memory usage)\"\n",
    "        )\n",
    "        ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_pareto_frontier(self):\n",
    "        \"\"\"Plot Pareto frontier for cost vs quality\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "        # Cost vs Quality\n",
    "        agg_df = (\n",
    "            self.df.groupby([\"model_name\", \"quantization\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"cost_per_1k\": \"mean\",\n",
    "                    \"quality_score\": \"mean\",\n",
    "                    \"tokens_per_second\": \"mean\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        sns.scatterplot(\n",
    "            data=agg_df,\n",
    "            x=\"cost_per_1k\",\n",
    "            y=\"quality_score\",\n",
    "            hue=\"quantization\",\n",
    "            style=\"model_name\",\n",
    "            s=100,\n",
    "            ax=axes[0],\n",
    "        )\n",
    "        axes[0].set_title(\"Cost vs Quality Pareto Frontier\")\n",
    "        axes[0].set_xlabel(\"Cost per 1K tokens (USD)\")\n",
    "        axes[0].set_ylabel(\"Quality Score\")\n",
    "\n",
    "        # Speed vs Quality\n",
    "        sns.scatterplot(\n",
    "            data=agg_df,\n",
    "            x=\"tokens_per_second\",\n",
    "            y=\"quality_score\",\n",
    "            hue=\"quantization\",\n",
    "            style=\"model_name\",\n",
    "            s=100,\n",
    "            ax=axes[1],\n",
    "        )\n",
    "        axes[1].set_title(\"Speed vs Quality Trade-off\")\n",
    "        axes[1].set_xlabel(\"Tokens per Second\")\n",
    "        axes[1].set_ylabel(\"Quality Score\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def generate_summary_table(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate summary table with key metrics\"\"\"\n",
    "        summary = (\n",
    "            self.df.groupby([\"model_name\", \"quantization\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"tokens_per_second\": [\"mean\", \"std\"],\n",
    "                    \"cost_per_1k\": [\"mean\", \"std\"],\n",
    "                    \"quality_score\": [\"mean\", \"std\"],\n",
    "                    \"peak_memory_mb\": [\"mean\", \"max\"],\n",
    "                    \"ttft\": [\"mean\", \"std\"],\n",
    "                }\n",
    "            )\n",
    "            .round(4)\n",
    "        )\n",
    "\n",
    "        # Flatten column names\n",
    "        summary.columns = [\"_\".join(col).strip() for col in summary.columns]\n",
    "        summary = summary.reset_index()\n",
    "\n",
    "        # Add efficiency score (quality per cost)\n",
    "        summary[\"efficiency_score\"] = (\n",
    "            summary[\"quality_score_mean\"] / summary[\"cost_per_1k_mean\"]\n",
    "        )\n",
    "\n",
    "        return summary.sort_values(\"efficiency_score\", ascending=False)\n",
    "\n",
    "    def find_optimal_configs(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Find optimal configurations for different use cases\"\"\"\n",
    "        agg_df = (\n",
    "            self.df.groupby([\"model_name\", \"quantization\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"cost_per_1k\": \"mean\",\n",
    "                    \"quality_score\": \"mean\",\n",
    "                    \"tokens_per_second\": \"mean\",\n",
    "                    \"peak_memory_mb\": \"mean\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        recommendations = {}\n",
    "\n",
    "        # Best for low cost\n",
    "        lowest_cost = agg_df.loc[agg_df[\"cost_per_1k\"].idxmin()]\n",
    "        recommendations[\"lowest_cost\"] = {\n",
    "            \"config\": f\"{lowest_cost['model_name']} + {lowest_cost['quantization']}\",\n",
    "            \"cost_per_1k\": lowest_cost[\"cost_per_1k\"],\n",
    "            \"quality\": lowest_cost[\"quality_score\"],\n",
    "            \"tps\": lowest_cost[\"tokens_per_second\"],\n",
    "            \"memory_mb\": lowest_cost[\"peak_memory_mb\"],\n",
    "        }\n",
    "\n",
    "        # Best for highest quality\n",
    "        highest_quality = agg_df.loc[agg_df[\"quality_score\"].idxmax()]\n",
    "        recommendations[\"highest_quality\"] = {\n",
    "            \"config\": f\"{highest_quality['model_name']} + {highest_quality['quantization']}\",\n",
    "            \"cost_per_1k\": highest_quality[\"cost_per_1k\"],\n",
    "            \"quality\": highest_quality[\"quality_score\"],\n",
    "            \"tps\": highest_quality[\"tokens_per_second\"],\n",
    "            \"memory_mb\": highest_quality[\"peak_memory_mb\"],\n",
    "        }\n",
    "\n",
    "        # Best for speed\n",
    "        fastest = agg_df.loc[agg_df[\"tokens_per_second\"].idxmax()]\n",
    "        recommendations[\"fastest\"] = {\n",
    "            \"config\": f\"{fastest['model_name']} + {fastest['quantization']}\",\n",
    "            \"cost_per_1k\": fastest[\"cost_per_1k\"],\n",
    "            \"quality\": fastest[\"quality_score\"],\n",
    "            \"tps\": fastest[\"tokens_per_second\"],\n",
    "            \"memory_mb\": fastest[\"peak_memory_mb\"],\n",
    "        }\n",
    "\n",
    "        # Best efficiency (quality per cost)\n",
    "        agg_df[\"efficiency\"] = agg_df[\"quality_score\"] / agg_df[\"cost_per_1k\"]\n",
    "        most_efficient = agg_df.loc[agg_df[\"efficiency\"].idxmax()]\n",
    "        recommendations[\"most_efficient\"] = {\n",
    "            \"config\": f\"{most_efficient['model_name']} + {most_efficient['quantization']}\",\n",
    "            \"cost_per_1k\": most_efficient[\"cost_per_1k\"],\n",
    "            \"quality\": most_efficient[\"quality_score\"],\n",
    "            \"tps\": most_efficient[\"tokens_per_second\"],\n",
    "            \"memory_mb\": most_efficient[\"peak_memory_mb\"],\n",
    "            \"efficiency\": most_efficient[\"efficiency\"],\n",
    "        }\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "print(\"üìà Analyzing benchmark results...\")\n",
    "analyzer = ResultsAnalyzer(results)\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\nüìä Performance Summary Table:\")\n",
    "summary_table = analyzer.generate_summary_table()\n",
    "print(summary_table.to_string(index=False))\n",
    "\n",
    "# Generate visualizations\n",
    "print(\"\\nüé® Generating visualizations...\")\n",
    "analyzer.plot_cost_latency_quality_3d()\n",
    "analyzer.plot_pareto_frontier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 7: Recommendation Engine\n",
    "# ================================\n",
    "\n",
    "\n",
    "class RecommendationEngine:\n",
    "    \"\"\"Generate recommendations based on use case requirements\"\"\"\n",
    "\n",
    "    def __init__(self, analyzer: ResultsAnalyzer):\n",
    "        self.analyzer = analyzer\n",
    "        self.df = analyzer.df\n",
    "\n",
    "    def recommend_for_use_case(\n",
    "        self,\n",
    "        use_case: str,\n",
    "        max_cost: float = None,\n",
    "        min_quality: float = None,\n",
    "        min_speed: float = None,\n",
    "        max_memory: float = None,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Recommend optimal configuration for specific use case\"\"\"\n",
    "\n",
    "        # Define use case profiles\n",
    "        use_case_profiles = {\n",
    "            \"research\": {\n",
    "                \"priority\": \"quality\",\n",
    "                \"description\": \"Research & experimentation - prioritize quality over cost\",\n",
    "                \"constraints\": {\"min_quality\": 0.3},\n",
    "            },\n",
    "            \"production\": {\n",
    "                \"priority\": \"balanced\",\n",
    "                \"description\": \"Production deployment - balance quality, cost, and speed\",\n",
    "                \"constraints\": {\"min_speed\": 5.0, \"max_cost\": 0.01},\n",
    "            },\n",
    "            \"demo\": {\n",
    "                \"priority\": \"speed\",\n",
    "                \"description\": \"Interactive demos - prioritize speed and low latency\",\n",
    "                \"constraints\": {\"min_speed\": 10.0},\n",
    "            },\n",
    "            \"batch\": {\n",
    "                \"priority\": \"cost\",\n",
    "                \"description\": \"Batch processing - minimize cost per token\",\n",
    "                \"constraints\": {\"max_cost\": 0.005},\n",
    "            },\n",
    "            \"edge\": {\n",
    "                \"priority\": \"memory\",\n",
    "                \"description\": \"Edge deployment - minimize memory usage\",\n",
    "                \"constraints\": {\"max_memory\": 4000},\n",
    "            },\n",
    "        }\n",
    "\n",
    "        profile = use_case_profiles.get(use_case.lower(), {})\n",
    "        constraints = profile.get(\"constraints\", {})\n",
    "\n",
    "        # Apply user constraints\n",
    "        if max_cost:\n",
    "            constraints[\"max_cost\"] = max_cost\n",
    "        if min_quality:\n",
    "            constraints[\"min_quality\"] = min_quality\n",
    "        if min_speed:\n",
    "            constraints[\"min_speed\"] = min_speed\n",
    "        if max_memory:\n",
    "            constraints[\"max_memory\"] = max_memory\n",
    "\n",
    "        # Filter candidates based on constraints\n",
    "        candidates = (\n",
    "            self.df.groupby([\"model_name\", \"quantization\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"cost_per_1k\": \"mean\",\n",
    "                    \"quality_score\": \"mean\",\n",
    "                    \"tokens_per_second\": \"mean\",\n",
    "                    \"peak_memory_mb\": \"mean\",\n",
    "                    \"ttft\": \"mean\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Apply filters\n",
    "        for constraint, value in constraints.items():\n",
    "            if constraint == \"max_cost\":\n",
    "                candidates = candidates[candidates[\"cost_per_1k\"] <= value]\n",
    "            elif constraint == \"min_quality\":\n",
    "                candidates = candidates[candidates[\"quality_score\"] >= value]\n",
    "            elif constraint == \"min_speed\":\n",
    "                candidates = candidates[candidates[\"tokens_per_second\"] >= value]\n",
    "            elif constraint == \"max_memory\":\n",
    "                candidates = candidates[candidates[\"peak_memory_mb\"] <= value]\n",
    "\n",
    "        if len(candidates) == 0:\n",
    "            return {\"error\": \"No configurations meet the specified constraints\"}\n",
    "\n",
    "        # Select best candidate based on priority\n",
    "        priority = profile.get(\"priority\", \"balanced\")\n",
    "\n",
    "        if priority == \"quality\":\n",
    "            best = candidates.loc[candidates[\"quality_score\"].idxmax()]\n",
    "        elif priority == \"speed\":\n",
    "            best = candidates.loc[candidates[\"tokens_per_second\"].idxmax()]\n",
    "        elif priority == \"cost\":\n",
    "            best = candidates.loc[candidates[\"cost_per_1k\"].idxmin()]\n",
    "        elif priority == \"memory\":\n",
    "            best = candidates.loc[candidates[\"peak_memory_mb\"].idxmin()]\n",
    "        else:  # balanced\n",
    "            # Calculate composite score\n",
    "            candidates[\"composite_score\"] = (\n",
    "                candidates[\"quality_score\"] * 0.4\n",
    "                + (1 / candidates[\"cost_per_1k\"]) * 0.3  # Lower cost is better\n",
    "                + (\n",
    "                    candidates[\"tokens_per_second\"]\n",
    "                    / candidates[\"tokens_per_second\"].max()\n",
    "                )\n",
    "                * 0.3\n",
    "            )\n",
    "            best = candidates.loc[candidates[\"composite_score\"].idxmax()]\n",
    "\n",
    "        recommendation = {\n",
    "            \"use_case\": use_case,\n",
    "            \"description\": profile.get(\"description\", \"Custom use case\"),\n",
    "            \"recommended_config\": f\"{best['model_name']} + {best['quantization']}\",\n",
    "            \"metrics\": {\n",
    "                \"cost_per_1k_tokens\": f\"${best['cost_per_1k']:.6f}\",\n",
    "                \"quality_score\": f\"{best['quality_score']:.3f}\",\n",
    "                \"tokens_per_second\": f\"{best['tokens_per_second']:.1f}\",\n",
    "                \"memory_usage_mb\": f\"{best['peak_memory_mb']:.0f}\",\n",
    "                \"time_to_first_token\": f\"{best['ttft']:.3f}s\",\n",
    "            },\n",
    "            \"trade_offs\": self._analyze_trade_offs(best, candidates),\n",
    "        }\n",
    "\n",
    "        return recommendation\n",
    "\n",
    "    def _analyze_trade_offs(\n",
    "        self, selected: pd.Series, all_candidates: pd.DataFrame\n",
    "    ) -> Dict:\n",
    "        \"\"\"Analyze trade-offs of selected configuration\"\"\"\n",
    "        trade_offs = {}\n",
    "\n",
    "        # Compare to best in each dimension\n",
    "        best_cost = all_candidates.loc[all_candidates[\"cost_per_1k\"].idxmin()]\n",
    "        best_quality = all_candidates.loc[all_candidates[\"quality_score\"].idxmax()]\n",
    "        best_speed = all_candidates.loc[all_candidates[\"tokens_per_second\"].idxmax()]\n",
    "\n",
    "        cost_penalty = (selected[\"cost_per_1k\"] / best_cost[\"cost_per_1k\"] - 1) * 100\n",
    "        quality_penalty = (\n",
    "            1 - selected[\"quality_score\"] / best_quality[\"quality_score\"]\n",
    "        ) * 100\n",
    "        speed_penalty = (\n",
    "            1 - selected[\"tokens_per_second\"] / best_speed[\"tokens_per_second\"]\n",
    "        ) * 100\n",
    "\n",
    "        trade_offs = {\n",
    "            \"cost_vs_cheapest\": f\"{cost_penalty:+.1f}%\",\n",
    "            \"quality_vs_best\": f\"{quality_penalty:+.1f}%\",\n",
    "            \"speed_vs_fastest\": f\"{speed_penalty:+.1f}%\",\n",
    "        }\n",
    "\n",
    "        return trade_offs\n",
    "\n",
    "    def generate_all_recommendations(self) -> Dict:\n",
    "        \"\"\"Generate recommendations for all standard use cases\"\"\"\n",
    "        use_cases = [\"research\", \"production\", \"demo\", \"batch\", \"edge\"]\n",
    "        recommendations = {}\n",
    "\n",
    "        for use_case in use_cases:\n",
    "            recommendations[use_case] = self.recommend_for_use_case(use_case)\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "# Generate recommendations\n",
    "print(\"\\nüéØ Generating use case recommendations...\")\n",
    "recommender = RecommendationEngine(analyzer)\n",
    "all_recommendations = recommender.generate_all_recommendations()\n",
    "\n",
    "for use_case, rec in all_recommendations.items():\n",
    "    if \"error\" not in rec:\n",
    "        print(f\"\\nüìã {use_case.upper()} USE CASE:\")\n",
    "        print(f\"   Description: {rec['description']}\")\n",
    "        print(f\"   Recommended: {rec['recommended_config']}\")\n",
    "        print(f\"   Cost: {rec['metrics']['cost_per_1k_tokens']}/1K tokens\")\n",
    "        print(f\"   Quality: {rec['metrics']['quality_score']}\")\n",
    "        print(f\"   Speed: {rec['metrics']['tokens_per_second']} TPS\")\n",
    "        print(f\"   Memory: {rec['metrics']['memory_usage_mb']} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0164ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 8: Smoke Test & Validation\n",
    "# ================================\n",
    "\n",
    "\n",
    "def smoke_test_performance_analysis():\n",
    "    \"\"\"Smoke test for performance analysis functionality\"\"\"\n",
    "    print(\"üß™ Running smoke test for performance analysis...\")\n",
    "\n",
    "    # Test 1: Profiler initialization\n",
    "    profiler = PerformanceProfiler()\n",
    "    assert profiler.device.type in [\"cuda\", \"cpu\"], \"Invalid device type\"\n",
    "    print(\"‚úÖ Profiler initialization OK\")\n",
    "\n",
    "    # Test 2: Quality evaluator\n",
    "    evaluator = QualityEvaluator()\n",
    "    test_metrics = evaluator.evaluate_quality(\n",
    "        None, None, \"This is a test.\", \"This is a reference.\"\n",
    "    )\n",
    "    assert \"rouge_l\" in test_metrics, \"Rouge metric missing\"\n",
    "    print(\"‚úÖ Quality evaluator OK\")\n",
    "\n",
    "    # Test 3: Cost calculation\n",
    "    if len(results) > 0:\n",
    "        sample_result = results[0]\n",
    "        cost = benchmark.calculate_cost_per_1k_tokens(sample_result[\"perf_metrics\"])\n",
    "        assert cost >= 0, \"Invalid cost calculation\"\n",
    "        print(\"‚úÖ Cost calculation OK\")\n",
    "\n",
    "    # Test 4: Analysis functions\n",
    "    if len(results) > 0:\n",
    "        analyzer = ResultsAnalyzer(results)\n",
    "        summary = analyzer.generate_summary_table()\n",
    "        assert len(summary) > 0, \"Summary table empty\"\n",
    "        print(\"‚úÖ Results analysis OK\")\n",
    "\n",
    "    # Test 5: Recommendation engine\n",
    "    if len(results) > 0:\n",
    "        recommender = RecommendationEngine(analyzer)\n",
    "        rec = recommender.recommend_for_use_case(\"production\")\n",
    "        assert \"recommended_config\" in rec, \"Recommendation failed\"\n",
    "        print(\"‚úÖ Recommendation engine OK\")\n",
    "\n",
    "    print(\"üéâ All smoke tests passed!\")\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_performance_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92729590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 9: Export Results & Summary\n",
    "# ================================\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def export_benchmark_results(results: List[Dict], filename: str = None):\n",
    "    \"\"\"Export benchmark results to JSON file\"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"performance_benchmark_{timestamp}.json\"\n",
    "\n",
    "    # Prepare exportable data\n",
    "    export_data = {\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"gpu_type\": benchmark.gpu_type,\n",
    "            \"total_configs_tested\": len(results),\n",
    "            \"test_prompts\": len(TEST_PROMPTS),\n",
    "            \"models_tested\": list(set(r[\"model_name\"] for r in results)),\n",
    "        },\n",
    "        \"results\": [],\n",
    "    }\n",
    "\n",
    "    for result in results:\n",
    "        # Convert PerformanceMetrics to dict\n",
    "        perf_dict = {\n",
    "            \"model_name\": result[\"perf_metrics\"].model_name,\n",
    "            \"quantization\": result[\"perf_metrics\"].quantization,\n",
    "            \"tokens_per_second\": result[\"perf_metrics\"].tokens_per_second,\n",
    "            \"time_to_first_token\": result[\"perf_metrics\"].time_to_first_token,\n",
    "            \"total_time\": result[\"perf_metrics\"].total_time,\n",
    "            \"peak_gpu_memory_mb\": result[\"perf_metrics\"].peak_gpu_memory_mb,\n",
    "            \"avg_gpu_utilization\": result[\"perf_metrics\"].avg_gpu_utilization,\n",
    "            \"avg_cpu_percent\": result[\"perf_metrics\"].avg_cpu_percent,\n",
    "        }\n",
    "\n",
    "        export_result = {\n",
    "            \"model_name\": result[\"model_name\"],\n",
    "            \"quantization\": result[\"quantization\"],\n",
    "            \"prompt_category\": result[\"prompt_category\"],\n",
    "            \"performance_metrics\": perf_dict,\n",
    "            \"quality_metrics\": result[\"quality_metrics\"],\n",
    "            \"cost_per_1k_tokens\": result[\"cost_per_1k_tokens\"],\n",
    "        }\n",
    "        export_data[\"results\"].append(export_result)\n",
    "\n",
    "    # Save to file\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"üìÅ Results exported to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "\n",
    "# Export results\n",
    "if len(results) > 0:\n",
    "    export_file = export_benchmark_results(results)\n",
    "\n",
    "    # Generate final summary report\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä PERFORMANCE ANALYSIS SUMMARY REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"üñ•Ô∏è  GPU Type: {benchmark.gpu_type}\")\n",
    "    print(f\"üìà Total Configurations Tested: {len(results)}\")\n",
    "    print(f\"üíæ Results exported to: {export_file}\")\n",
    "\n",
    "    # Key findings\n",
    "    optimal_configs = analyzer.find_optimal_configs()\n",
    "    print(f\"\\nüèÜ KEY FINDINGS:\")\n",
    "    print(f\"   üí∞ Most Cost-Effective: {optimal_configs['lowest_cost']['config']}\")\n",
    "    print(f\"      ‚îî‚îÄ‚îÄ ${optimal_configs['lowest_cost']['cost_per_1k']:.6f}/1K tokens\")\n",
    "    print(f\"   üéØ Highest Quality: {optimal_configs['highest_quality']['config']}\")\n",
    "    print(\n",
    "        f\"      ‚îî‚îÄ‚îÄ Quality Score: {optimal_configs['highest_quality']['quality']:.3f}\"\n",
    "    )\n",
    "    print(f\"   ‚ö° Fastest: {optimal_configs['fastest']['config']}\")\n",
    "    print(f\"      ‚îî‚îÄ‚îÄ {optimal_configs['fastest']['tps']:.1f} tokens/second\")\n",
    "    print(f\"   üéñÔ∏è  Most Efficient: {optimal_configs['most_efficient']['config']}\")\n",
    "    print(\n",
    "        f\"      ‚îî‚îÄ‚îÄ Efficiency: {optimal_configs['most_efficient']['efficiency']:.2f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚ú® Performance analysis completed successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd70b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Smoke Test - Performance Analysis Validation\n",
    "# ================================\n",
    "\n",
    "\n",
    "def comprehensive_smoke_test():\n",
    "    \"\"\"Comprehensive validation of performance analysis components\"\"\"\n",
    "    print(\"üî¨ Running comprehensive smoke test...\")\n",
    "\n",
    "    # Test basic functionality without model loading\n",
    "    test_results = []\n",
    "\n",
    "    # 1. Environment Check\n",
    "    assert torch.cuda.is_available() or True, \"GPU check\"\n",
    "    print(\"‚úÖ Environment check passed\")\n",
    "\n",
    "    # 2. Utility Classes\n",
    "    profiler = PerformanceProfiler()\n",
    "    evaluator = QualityEvaluator()\n",
    "    print(\"‚úÖ Utility classes initialized\")\n",
    "\n",
    "    # 3. Mock performance metrics\n",
    "    mock_metrics = PerformanceMetrics(\n",
    "        model_name=\"test-model\",\n",
    "        quantization=\"fp16\",\n",
    "        prompt_length=50,\n",
    "        generation_length=100,\n",
    "        time_to_first_token=0.1,\n",
    "        tokens_per_second=25.0,\n",
    "        total_time=4.0,\n",
    "        peak_gpu_memory_mb=2048.0,\n",
    "        avg_gpu_utilization=75.0,\n",
    "        avg_cpu_percent=15.0,\n",
    "    )\n",
    "    print(\"‚úÖ Mock metrics created\")\n",
    "\n",
    "    # 4. Cost calculation test\n",
    "    benchmark_instance = PerformanceBenchmark()\n",
    "    cost = benchmark_instance.calculate_cost_per_1k_tokens(mock_metrics)\n",
    "    assert cost > 0, \"Cost should be positive\"\n",
    "    print(f\"‚úÖ Cost calculation: ${cost:.6f}/1K tokens\")\n",
    "\n",
    "    # 5. Analysis pipeline (with mock data)\n",
    "    mock_results = [\n",
    "        {\n",
    "            \"model_name\": \"test-small\",\n",
    "            \"quantization\": \"fp16\",\n",
    "            \"prompt_category\": \"simple\",\n",
    "            \"perf_metrics\": mock_metrics,\n",
    "            \"quality_metrics\": {\"perplexity\": 15.0, \"rouge_l\": 0.65, \"bleu\": 0.45},\n",
    "            \"cost_per_1k_tokens\": cost,\n",
    "            \"generated_text\": \"This is a test generation.\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    analyzer = ResultsAnalyzer(mock_results)\n",
    "    summary = analyzer.generate_summary_table()\n",
    "    assert len(summary) > 0, \"Summary table should not be empty\"\n",
    "    print(\"‚úÖ Results analysis pipeline\")\n",
    "\n",
    "    # 6. Recommendation engine\n",
    "    recommender = RecommendationEngine(analyzer)\n",
    "    rec = recommender.recommend_for_use_case(\"production\", max_cost=0.01)\n",
    "    print(\"‚úÖ Recommendation engine\")\n",
    "\n",
    "    print(\"üéâ All smoke tests completed successfully!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "comprehensive_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7eb548",
   "metadata": {},
   "source": [
    "\n",
    "## üìã 6. Êú¨Á´†Â∞èÁµê\n",
    "\n",
    "### ‚úÖ ÂÆåÊàêÈ†ÖÁõÆ\n",
    "- **ÊïàËÉΩÂàÜÊûêÊ°ÜÊû∂**: Âª∫Á´ã‰∫ÜÂÖ®Èù¢ÁöÑ LLM ÊïàËÉΩÊ∏¨Ë©¶Á≥ªÁµ±ÔºåÊîØÊè¥Â§öÁ∂≠Â∫¶Ë©ï‰º∞\n",
    "- **ÊàêÊú¨Ë®àÁÆóÊ®°ÁµÑ**: ÂØ¶‰Ωú‰∫ÜÂü∫ÊñºÁ°¨È´îÈ°ûÂûãÁöÑÁ≤æÁ¢∫ÊàêÊú¨Ë®àÁÆóÔºåÂåÖÂê´ GPU ËàáÈõªÂäõÊàêÊú¨\n",
    "- **ÂìÅË≥™Ë©ï‰º∞Á≥ªÁµ±**: Êï¥Âêà‰∫ÜÂ§öÁ®ÆË©ï‰º∞ÊåáÊ®ô (Perplexity, ROUGE, BLEU)ÔºåÊèê‰æõÁ∂úÂêàÂìÅË≥™ÂàÜÊï∏\n",
    "- **Ë¶ñË¶∫ÂåñÂàÜÊûê**: Âª∫Á´ã‰∫Ü 3D Ê¨äË°°ÂúñË°®ËàáÂ∏ïÁ¥ØÊâòÂâçÊ≤øÂàÜÊûêÔºåÁõ¥ËßÄÂ±ïÁ§∫ÊúÄ‰Ω≥ÈÖçÁΩÆ\n",
    "- **Êô∫ÊÖßÊé®Ëñ¶ÂºïÊìé**: ÈáùÂ∞ç‰∏çÂêå‰ΩøÁî®Â†¥ÊôØ (Á†îÁ©∂/ÁîüÁî¢/Â±ïÁ§∫/ÊâπÊ¨°/ÈÇäÁ∑£) Êèê‰æõÊúÄ‰Ω≥ÈÖçÁΩÆÂª∫Ë≠∞\n",
    "\n",
    "### üß† Ê†∏ÂøÉÂéüÁêÜË¶ÅÈªû\n",
    "- **‰∏âËßíÊ¨äË°°Èóú‰øÇ**: ÊàêÊú¨„ÄÅÂª∂ÈÅ≤„ÄÅÂìÅË≥™‰πãÈñìÂ≠òÂú®Âõ∫ÊúâÁöÑÊ¨äË°°Èóú‰øÇÔºåÈúÄË¶ÅÊ†πÊìö‰ΩøÁî®Â†¥ÊôØÈÅ∏ÊìáÂπ≥Ë°°Èªû\n",
    "- **ÈáèÂåñÁ≠ñÁï•ÂΩ±Èüø**: INT4/INT8 ÈáèÂåñËÉΩÈ°ØËëóÈôç‰ΩéË®òÊÜ∂È´î‰ΩøÁî®ËàáÊàêÊú¨Ôºå‰ΩÜÊúÉÂ∏∂‰æÜÂìÅË≥™ÊêçÂ§±\n",
    "- **TTFT vs TPS**: È¶ñÂÄã Token ÊôÇÈñì (TTFT) ÂΩ±Èüø‰ΩøÁî®ËÄÖÈ´îÈ©óÔºåÊØèÁßí Token Êï∏ (TPS) ÂΩ±ÈüøÂêûÂêêÈáè\n",
    "- **Â∏ïÁ¥ØÊâòÊúÄ‰Ω≥Âåñ**: Âú®Â§öÁõÆÊ®ôÊúÄ‰Ω≥Âåñ‰∏≠ÔºåÂ∏ïÁ¥ØÊâòÂâçÊ≤ø‰∏äÁöÑÈÖçÁΩÆ‰ª£Ë°®‰∏çÂèØÊîπÈÄ≤ÁöÑÊúÄ‰Ω≥Ëß£\n",
    "- **ÊàêÊú¨Ê®°Âûã**: Á∏ΩÊàêÊú¨ÂåÖÂê´Á°¨È´îÊî§ÊèêÊàêÊú¨ËàáÈõªÂäõÊ∂àËÄóÔºåÈúÄË¶ÅËÄÉÊÖÆÂØ¶Èöõ‰ΩøÁî®Ê®°Âºè\n",
    "\n",
    "### ‚ö†Ô∏è Â∏∏Ë¶ãÈô∑Èò±ËàáÊ≥®ÊÑè‰∫ãÈ†Ö\n",
    "- **Ê∏¨Ë©¶Áí∞Â¢É‰∏ÄËá¥ÊÄß**: Á¢∫‰øùÊâÄÊúâÈÖçÁΩÆÂú®Áõ∏ÂêåÁí∞Â¢É‰∏ãÊ∏¨Ë©¶ÔºåÈÅøÂÖçÂ§ñÈÉ®Âõ†Á¥†ÂΩ±ÈüøÁµêÊûú\n",
    "- **Ë®òÊÜ∂È´îÊ¥©Êºè**: Â§ßÈáèÊ®°ÂûãËºâÂÖ•Ê∏¨Ë©¶ÊôÇÈúÄË¶ÅÈÅ©Áï∂Ê∏ÖÁêÜÔºåÈÅøÂÖç CUDA OOM ÈåØË™§\n",
    "- **Áµ±Ë®àÈ°ØËëóÊÄß**: ÂñÆÊ¨°Ê∏¨Ë©¶ÁµêÊûúÂèØËÉΩÊúâËÆäÁï∞ÔºåÂª∫Ë≠∞Â§öÊ¨°Ê∏¨Ë©¶ÂèñÂπ≥ÂùáÂÄº\n",
    "- **ÊàêÊú¨Ë®àÁÆóÂÅáË®≠**: ÊàêÊú¨Ê®°ÂûãÂü∫ÊñºÁâπÂÆöÂÅáË®≠ (ÈõªÂÉπ„ÄÅÁ°¨È´îÂÉπÊ†º)ÔºåÂØ¶ÈöõÈÉ®ÁΩ≤ÊôÇÈúÄË¶ÅË™øÊï¥\n",
    "- **ÂìÅË≥™ÊåáÊ®ôÈôêÂà∂**: Ëá™ÂãïÂåñÊåáÊ®ôÁÑ°Ê≥ïÂÆåÂÖ®ÂèçÊò†‰∫∫È°ûÊÑüÁü•ÂìÅË≥™ÔºåÂª∫Ë≠∞Ëºî‰ª•‰∫∫Â∑•Ë©ï‰º∞\n",
    "\n",
    "### üéØ ‰∏ã‰∏ÄÊ≠•Âª∫Ë≠∞\n",
    "1. **Part D ÂæÆË™øÊäÄË°ì**: Êé¢Á¥¢ LoRA/QLoRA ÂæÆË™øÂ¶Ç‰ΩïÂΩ±ÈüøÊïàËÉΩÊ¨äË°°Èóú‰øÇ\n",
    "2. **ÈÉ®ÁΩ≤ÊúÄ‰Ω≥Âåñ**: Á†îÁ©∂ÁîüÁî¢Áí∞Â¢ÉÁöÑÊé®ÁêÜÊúÄ‰Ω≥ÂåñÊäÄË°ì (ÂãïÊÖãÊâπÊ¨°„ÄÅKV Âø´ÂèñÁ≠â)\n",
    "3. **Â§öÊ®°ÊÖãÊì¥Â±ï**: Â∞áÊïàËÉΩÂàÜÊûêÊì¥Â±ïÂà∞Ë¶ñË¶∫-Ë™ûË®ÄÊ®°Âûã (VLM)\n",
    "4. **Èï∑ÊñáÊú¨ËôïÁêÜ**: ÂàÜÊûê‰∏çÂêå context Èï∑Â∫¶Â∞çÊïàËÉΩÁöÑÂΩ±Èüø\n",
    "5. **ÂØ¶ÊôÇÁõ£Êéß**: Âª∫Á´ãÁîüÁî¢Áí∞Â¢ÉÁöÑÊïàËÉΩÁõ£ÊéßËàáËá™ÂãïË™øÂÑ™Á≥ªÁµ±\n",
    "\n",
    "---\n",
    "\n",
    "**ÂÆåÊàê Part C - LLM Applications ÈöéÊÆµÔºÅ** üéâ\n",
    "\n",
    "Â∑≤ÂÆåÊàê LLM ÊáâÁî®Ê†∏ÂøÉÁöÑ 10 Êú¨ notebooksÔºåÊ∂µËìã‰∫ÜÂæûÂü∫Á§éÊñáÊú¨ÁîüÊàêÂà∞ÈÄ≤ÈöéÊïàËÉΩÂàÜÊûêÁöÑÂÆåÊï¥ÊäÄË°ìÊ£ß„ÄÇÊé•‰∏ã‰æÜÂª∫Ë≠∞ÈÄ≤ÂÖ• **Part D (ÂæÆË™øÊäÄË°ì)** Êàñ **Part E (RAG + Agents)**ÔºåËÆìÊàëÁü•ÈÅìÊÇ®Â∏åÊúõÂÑ™ÂÖàÂ≠∏ÁøíÂì™ÂÄãÊñπÂêëÔºÅ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
