{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db71ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb19_cost_latency_quality.ipynb - Cost/Latency/Quality Trade-off Analysis\n",
    "# 效能權衡分析：成本、延遲與品質的三角關係\n",
    "\n",
    "# ================================\n",
    "# Cell 1: Environment Setup & Dependencies\n",
    "# ================================\n",
    "\n",
    "# Shared cache bootstrap\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e1b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies for performance measurement\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "\n",
    "packages = [\n",
    "    \"psutil\",\n",
    "    \"nvidia-ml-py\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"rouge-score\",\n",
    "    \"sacrebleu\",\n",
    "]\n",
    "for pkg in packages:\n",
    "    install_if_missing(pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78296761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 2: Performance Profiler Classes\n",
    "# ================================\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import pynvml\n",
    "\n",
    "    pynvml.nvmlInit()\n",
    "    NVIDIA_ML_AVAILABLE = True\n",
    "except:\n",
    "    NVIDIA_ML_AVAILABLE = False\n",
    "    print(\"[Warning] nvidia-ml-py not available, GPU metrics will be limited\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Performance measurement results\"\"\"\n",
    "\n",
    "    model_name: str\n",
    "    quantization: str\n",
    "    prompt_length: int\n",
    "    generation_length: int\n",
    "\n",
    "    # Latency metrics (延遲指標)\n",
    "    time_to_first_token: float  # TTFT - 首個 token 生成時間\n",
    "    tokens_per_second: float  # TPS - 每秒生成 token 數\n",
    "    total_time: float  # 總生成時間\n",
    "\n",
    "    # Resource metrics (資源指標)\n",
    "    peak_gpu_memory_mb: float  # 峰值 GPU 記憶體使用量\n",
    "    avg_gpu_utilization: float  # 平均 GPU 使用率\n",
    "    avg_cpu_percent: float  # 平均 CPU 使用率\n",
    "\n",
    "    # Quality metrics (品質指標)\n",
    "    perplexity: Optional[float] = None\n",
    "    rouge_l: Optional[float] = None\n",
    "    bleu_score: Optional[float] = None\n",
    "\n",
    "\n",
    "class PerformanceProfiler:\n",
    "    \"\"\"Comprehensive performance profiler for LLM inference\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.gpu_available = torch.cuda.is_available()\n",
    "\n",
    "        if NVIDIA_ML_AVAILABLE and self.gpu_available:\n",
    "            self.gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "    def measure_inference(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 100,\n",
    "        **generate_kwargs\n",
    "    ) -> PerformanceMetrics:\n",
    "        \"\"\"Measure inference performance for a single generation\"\"\"\n",
    "\n",
    "        # Prepare input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        prompt_length = inputs.input_ids.shape[1]\n",
    "\n",
    "        # Pre-generation memory snapshot\n",
    "        if self.gpu_available:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            initial_memory = torch.cuda.memory_allocated()\n",
    "\n",
    "        # CPU monitoring setup\n",
    "        cpu_percentages = []\n",
    "        gpu_utilizations = []\n",
    "\n",
    "        # Generation with timing\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # TTFT measurement (首個 token 時間)\n",
    "        with torch.inference_mode():\n",
    "            # Generate first token\n",
    "            first_token_start = time.perf_counter()\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                **generate_kwargs\n",
    "            )\n",
    "            ttft = time.perf_counter() - first_token_start\n",
    "\n",
    "            # Continue generation for remaining tokens\n",
    "            if max_new_tokens > 1:\n",
    "                remaining_outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    **generate_kwargs\n",
    "                )\n",
    "                outputs = remaining_outputs\n",
    "\n",
    "        total_time = time.perf_counter() - start_time\n",
    "\n",
    "        # Calculate generation metrics\n",
    "        generated_tokens = outputs.shape[1] - prompt_length\n",
    "        tps = generated_tokens / total_time if total_time > 0 else 0\n",
    "\n",
    "        # Memory metrics\n",
    "        if self.gpu_available:\n",
    "            peak_memory = torch.cuda.max_memory_allocated()\n",
    "            peak_memory_mb = peak_memory / 1024 / 1024\n",
    "        else:\n",
    "            peak_memory_mb = 0\n",
    "\n",
    "        # Resource utilization (簡化版本，實際應用中可用更複雜的監控)\n",
    "        avg_cpu = psutil.cpu_percent(interval=None)\n",
    "\n",
    "        if NVIDIA_ML_AVAILABLE and self.gpu_available:\n",
    "            try:\n",
    "                gpu_util = pynvml.nvmlDeviceGetUtilizationRates(self.gpu_handle)\n",
    "                avg_gpu = gpu_util.gpu\n",
    "            except:\n",
    "                avg_gpu = 0\n",
    "        else:\n",
    "            avg_gpu = 0\n",
    "\n",
    "        return PerformanceMetrics(\n",
    "            model_name=getattr(model, \"name_or_path\", \"unknown\"),\n",
    "            quantization=getattr(model, \"quantization_config\", \"fp16\"),\n",
    "            prompt_length=prompt_length,\n",
    "            generation_length=generated_tokens,\n",
    "            time_to_first_token=ttft,\n",
    "            tokens_per_second=tps,\n",
    "            total_time=total_time,\n",
    "            peak_gpu_memory_mb=peak_memory_mb,\n",
    "            avg_gpu_utilization=avg_gpu,\n",
    "            avg_cpu_percent=avg_cpu,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3771487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 3: Quality Evaluator\n",
    "# ================================\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu import BLEU\n",
    "\n",
    "\n",
    "class QualityEvaluator:\n",
    "    \"\"\"Evaluate generation quality using multiple metrics\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            [\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True\n",
    "        )\n",
    "        self.bleu = BLEU()\n",
    "\n",
    "    def calculate_perplexity(self, model, tokenizer, text: str) -> float:\n",
    "        \"\"\"Calculate perplexity of generated text\"\"\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**inputs, labels=inputs.input_ids)\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss).item()\n",
    "\n",
    "        return perplexity\n",
    "\n",
    "    def calculate_rouge(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"Calculate ROUGE-L F1 score\"\"\"\n",
    "        scores = self.rouge_scorer.score(reference, generated)\n",
    "        return scores[\"rougeL\"].fmeasure\n",
    "\n",
    "    def calculate_bleu(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"Calculate BLEU score\"\"\"\n",
    "        score = self.bleu.sentence_score(generated, [reference])\n",
    "        return score.score / 100.0  # Convert to 0-1 range\n",
    "\n",
    "    def evaluate_quality(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        generated_text: str,\n",
    "        reference_text: Optional[str] = None,\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Comprehensive quality evaluation\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Perplexity (always calculated)\n",
    "        try:\n",
    "            metrics[\"perplexity\"] = self.calculate_perplexity(\n",
    "                model, tokenizer, generated_text\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Perplexity calculation failed: {e}\")\n",
    "            metrics[\"perplexity\"] = float(\"inf\")\n",
    "\n",
    "        # Reference-based metrics (if reference provided)\n",
    "        if reference_text:\n",
    "            try:\n",
    "                metrics[\"rouge_l\"] = self.calculate_rouge(\n",
    "                    generated_text, reference_text\n",
    "                )\n",
    "                metrics[\"bleu\"] = self.calculate_bleu(generated_text, reference_text)\n",
    "            except Exception as e:\n",
    "                print(f\"[Warning] Reference-based metrics failed: {e}\")\n",
    "                metrics[\"rouge_l\"] = 0.0\n",
    "                metrics[\"bleu\"] = 0.0\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 4: Test Dataset & Model Configurations\n",
    "# ================================\n",
    "\n",
    "# Standard test prompts with varying complexity\n",
    "TEST_PROMPTS = [\n",
    "    {\n",
    "        \"prompt\": \"Explain artificial intelligence in simple terms.\",\n",
    "        \"reference\": \"Artificial intelligence is technology that enables machines to perform tasks that typically require human intelligence, such as learning, reasoning, and problem-solving.\",\n",
    "        \"category\": \"simple\",\n",
    "        \"expected_length\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a detailed analysis of climate change impacts on global agriculture, including specific examples and potential solutions.\",\n",
    "        \"reference\": \"Climate change significantly affects global agriculture through altered precipitation patterns, increased temperatures, and extreme weather events. For example, drought in wheat-growing regions reduces yields, while flooding destroys crops. Solutions include drought-resistant crops, improved irrigation, and sustainable farming practices.\",\n",
    "        \"category\": \"complex\",\n",
    "        \"expected_length\": 150,\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Translate and explain: 'Machine learning algorithms can identify patterns in large datasets.'\",\n",
    "        \"reference\": \"Machine learning algorithms are computational methods that can automatically discover patterns, relationships, and trends within large amounts of data without being explicitly programmed to look for specific patterns.\",\n",
    "        \"category\": \"medium\",\n",
    "        \"expected_length\": 100,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Model configurations to test\n",
    "MODEL_CONFIGS = [\n",
    "    {\n",
    "        \"model_id\": \"microsoft/DialoGPT-small\",  # ~117M parameters\n",
    "        \"name\": \"DialoGPT-small\",\n",
    "        \"size_category\": \"small\",\n",
    "        \"quantization_options\": [\"fp16\", \"int8\"],\n",
    "    },\n",
    "    {\n",
    "        \"model_id\": \"microsoft/DialoGPT-medium\",  # ~345M parameters\n",
    "        \"name\": \"DialoGPT-medium\",\n",
    "        \"size_category\": \"medium\",\n",
    "        \"quantization_options\": [\"fp16\", \"int8\", \"int4\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Cost calculation parameters (成本計算參數)\n",
    "COST_PARAMS = {\n",
    "    \"gpu_hourly_cost\": {  # USD per hour\n",
    "        \"RTX_3060\": 0.15,\n",
    "        \"RTX_4070\": 0.25,\n",
    "        \"RTX_4090\": 0.50,\n",
    "        \"A100\": 2.00,\n",
    "    },\n",
    "    \"electricity_kwh\": 0.12,  # USD per kWh\n",
    "    \"gpu_power_watts\": {\"RTX_3060\": 170, \"RTX_4070\": 200, \"RTX_4090\": 450, \"A100\": 400},\n",
    "}\n",
    "\n",
    "\n",
    "def detect_gpu_type() -> str:\n",
    "    \"\"\"Detect current GPU type for cost calculation\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return \"CPU\"\n",
    "\n",
    "    gpu_name = torch.cuda.get_device_name(0).lower()\n",
    "    if \"3060\" in gpu_name:\n",
    "        return \"RTX_3060\"\n",
    "    elif \"4070\" in gpu_name:\n",
    "        return \"RTX_4070\"\n",
    "    elif \"4090\" in gpu_name:\n",
    "        return \"RTX_4090\"\n",
    "    elif \"a100\" in gpu_name:\n",
    "        return \"A100\"\n",
    "    else:\n",
    "        return \"RTX_4070\"  # Default assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 5: Automated Performance Testing\n",
    "# ================================\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"Automated benchmark suite for cost/latency/quality analysis\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.profiler = PerformanceProfiler()\n",
    "        self.evaluator = QualityEvaluator()\n",
    "        self.results = []\n",
    "        self.gpu_type = detect_gpu_type()\n",
    "\n",
    "    def load_model_with_quantization(self, model_id: str, quantization: str):\n",
    "        \"\"\"Load model with specified quantization\"\"\"\n",
    "        print(f\"Loading {model_id} with {quantization}...\")\n",
    "\n",
    "        if quantization == \"int4\":\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "            )\n",
    "        elif quantization == \"int8\":\n",
    "            bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id, quantization_config=bnb_config, device_map=\"auto\"\n",
    "            )\n",
    "        else:  # fp16\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "            )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    def calculate_cost_per_1k_tokens(self, metrics: PerformanceMetrics) -> float:\n",
    "        \"\"\"Calculate cost per 1000 tokens generated\"\"\"\n",
    "        if metrics.tokens_per_second <= 0:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        # Time to generate 1000 tokens\n",
    "        time_for_1k = 1000 / metrics.tokens_per_second\n",
    "\n",
    "        # GPU cost\n",
    "        gpu_cost_per_hour = COST_PARAMS[\"gpu_hourly_cost\"].get(self.gpu_type, 0.25)\n",
    "        gpu_cost_1k = gpu_cost_per_hour * (time_for_1k / 3600)\n",
    "\n",
    "        # Electricity cost\n",
    "        power_watts = COST_PARAMS[\"gpu_power_watts\"].get(self.gpu_type, 200)\n",
    "        electricity_cost_1k = (\n",
    "            (power_watts / 1000) * (time_for_1k / 3600) * COST_PARAMS[\"electricity_kwh\"]\n",
    "        )\n",
    "\n",
    "        return gpu_cost_1k + electricity_cost_1k\n",
    "\n",
    "    def run_single_benchmark(\n",
    "        self, model_config: Dict, quantization: str, test_prompt: Dict\n",
    "    ) -> Dict:\n",
    "        \"\"\"Run benchmark for single configuration\"\"\"\n",
    "        try:\n",
    "            # Load model\n",
    "            model, tokenizer = self.load_model_with_quantization(\n",
    "                model_config[\"model_id\"], quantization\n",
    "            )\n",
    "\n",
    "            # Performance measurement\n",
    "            perf_metrics = self.profiler.measure_inference(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                test_prompt[\"prompt\"],\n",
    "                max_new_tokens=test_prompt[\"expected_length\"],\n",
    "            )\n",
    "\n",
    "            # Generate text for quality evaluation\n",
    "            inputs = tokenizer(test_prompt[\"prompt\"], return_tensors=\"pt\")\n",
    "            with torch.inference_mode():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=test_prompt[\"expected_length\"],\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            generated_only = generated_text[len(test_prompt[\"prompt\"]) :].strip()\n",
    "\n",
    "            # Quality evaluation\n",
    "            quality_metrics = self.evaluator.evaluate_quality(\n",
    "                model, tokenizer, generated_only, test_prompt.get(\"reference\")\n",
    "            )\n",
    "\n",
    "            # Cost calculation\n",
    "            cost_per_1k = self.calculate_cost_per_1k_tokens(perf_metrics)\n",
    "\n",
    "            # Combine results\n",
    "            result = {\n",
    "                \"model_name\": model_config[\"name\"],\n",
    "                \"quantization\": quantization,\n",
    "                \"prompt_category\": test_prompt[\"category\"],\n",
    "                \"perf_metrics\": perf_metrics,\n",
    "                \"quality_metrics\": quality_metrics,\n",
    "                \"cost_per_1k_tokens\": cost_per_1k,\n",
    "                \"generated_text\": generated_only,\n",
    "            }\n",
    "\n",
    "            # Cleanup\n",
    "            del model, tokenizer\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Benchmark failed for {model_config['name']} + {quantization}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def run_full_benchmark(self) -> List[Dict]:\n",
    "        \"\"\"Run comprehensive benchmark across all configurations\"\"\"\n",
    "        print(\"🚀 Starting comprehensive performance benchmark...\")\n",
    "        print(f\"GPU Type: {self.gpu_type}\")\n",
    "\n",
    "        for model_config in MODEL_CONFIGS:\n",
    "            for quantization in model_config[\"quantization_options\"]:\n",
    "                for test_prompt in TEST_PROMPTS:\n",
    "                    print(\n",
    "                        f\"\\n📊 Testing: {model_config['name']} | {quantization} | {test_prompt['category']}\"\n",
    "                    )\n",
    "\n",
    "                    result = self.run_single_benchmark(\n",
    "                        model_config, quantization, test_prompt\n",
    "                    )\n",
    "                    if result:\n",
    "                        self.results.append(result)\n",
    "                        print(\n",
    "                            f\"✅ TPS: {result['perf_metrics'].tokens_per_second:.2f} | \"\n",
    "                            f\"Cost: ${result['cost_per_1k_tokens']:.6f}/1K | \"\n",
    "                            f\"VRAM: {result['perf_metrics'].peak_gpu_memory_mb:.0f}MB\"\n",
    "                        )\n",
    "\n",
    "        print(\n",
    "            f\"\\n🎯 Benchmark completed! Total configurations tested: {len(self.results)}\"\n",
    "        )\n",
    "        return self.results\n",
    "\n",
    "\n",
    "# Run the benchmark\n",
    "benchmark = PerformanceBenchmark()\n",
    "results = benchmark.run_full_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 6: Results Analysis & Visualization\n",
    "# ================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "class ResultsAnalyzer:\n",
    "    \"\"\"Analyze and visualize benchmark results\"\"\"\n",
    "\n",
    "    def __init__(self, results: List[Dict]):\n",
    "        self.results = results\n",
    "        self.df = self._create_dataframe()\n",
    "\n",
    "    def _create_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Convert results to pandas DataFrame for analysis\"\"\"\n",
    "        rows = []\n",
    "        for result in self.results:\n",
    "            perf = result[\"perf_metrics\"]\n",
    "            quality = result[\"quality_metrics\"]\n",
    "\n",
    "            row = {\n",
    "                \"model_name\": result[\"model_name\"],\n",
    "                \"quantization\": result[\"quantization\"],\n",
    "                \"prompt_category\": result[\"prompt_category\"],\n",
    "                \"tokens_per_second\": perf.tokens_per_second,\n",
    "                \"ttft\": perf.time_to_first_token,\n",
    "                \"peak_memory_mb\": perf.peak_gpu_memory_mb,\n",
    "                \"cost_per_1k\": result[\"cost_per_1k_tokens\"],\n",
    "                \"perplexity\": quality.get(\"perplexity\", float(\"inf\")),\n",
    "                \"rouge_l\": quality.get(\"rouge_l\", 0.0),\n",
    "                \"bleu\": quality.get(\"bleu\", 0.0),\n",
    "                \"quality_score\": self._calculate_composite_quality(quality),\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def _calculate_composite_quality(self, quality_metrics: Dict) -> float:\n",
    "        \"\"\"Calculate composite quality score (0-1, higher is better)\"\"\"\n",
    "        # Inverse of perplexity (lower perplexity = higher quality)\n",
    "        perp = quality_metrics.get(\"perplexity\", float(\"inf\"))\n",
    "        perp_score = 1 / (1 + perp) if perp != float(\"inf\") else 0\n",
    "\n",
    "        rouge = quality_metrics.get(\"rouge_l\", 0.0)\n",
    "        bleu = quality_metrics.get(\"bleu\", 0.0)\n",
    "\n",
    "        # Weighted average (adjust weights as needed)\n",
    "        composite = 0.4 * perp_score + 0.3 * rouge + 0.3 * bleu\n",
    "        return composite\n",
    "\n",
    "    def plot_cost_latency_quality_3d(self):\n",
    "        \"\"\"Create 3D scatter plot of cost vs latency vs quality\"\"\"\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "        # Aggregate by model + quantization\n",
    "        agg_df = (\n",
    "            self.df.groupby([\"model_name\", \"quantization\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"cost_per_1k\": \"mean\",\n",
    "                    \"tokens_per_second\": \"mean\",\n",
    "                    \"quality_score\": \"mean\",\n",
    "                    \"peak_memory_mb\": \"mean\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Color by quantization\n",
    "        colors = {\"fp16\": \"red\", \"int8\": \"green\", \"int4\": \"blue\"}\n",
    "\n",
    "        for quant in agg_df[\"quantization\"].unique():\n",
    "            data = agg_df[agg_df[\"quantization\"] == quant]\n",
    "            ax.scatter(\n",
    "                data[\"cost_per_1k\"],\n",
    "                data[\"tokens_per_second\"],\n",
    "                data[\"quality_score\"],\n",
    "                c=colors.get(quant, \"gray\"),\n",
    "                label=f\"{quant}\",\n",
    "                s=data[\"peak_memory_mb\"] / 10,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "\n",
    "        ax.set_xlabel(\"Cost per 1K tokens (USD)\")\n",
    "        ax.set_ylabel(\"Tokens per Second\")\n",
    "        ax.set_zlabel(\"Quality Score\")\n",
    "        ax.set_title(\n",
    "            \"Cost vs Latency vs Quality Trade-off\\n(Bubble size = Memory usage)\"\n",
    "        )\n",
    "        ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_pareto_frontier(self):\n",
    "        \"\"\"Plot Pareto frontier for cost vs quality\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "        # Cost vs Quality\n",
    "        agg_df = (\n",
    "            self.df.groupby([\"model_name\", \"quantization\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"cost_per_1k\": \"mean\",\n",
    "                    \"quality_score\": \"mean\",\n",
    "                    \"tokens_per_second\": \"mean\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        sns.scatterplot(\n",
    "            data=agg_df,\n",
    "            x=\"cost_per_1k\",\n",
    "            y=\"quality_score\",\n",
    "            hue=\"quantization\",\n",
    "            style=\"model_name\",\n",
    "            s=100,\n",
    "            ax=axes[0],\n",
    "        )\n",
    "        axes[0].set_title(\"Cost vs Quality Pareto Frontier\")\n",
    "        axes[0].set_xlabel(\"Cost per 1K tokens (USD)\")\n",
    "        axes[0].set_ylabel(\"Quality Score\")\n",
    "\n",
    "        # Speed vs Quality\n",
    "        sns.scatterplot(\n",
    "            data=agg_df,\n",
    "            x=\"tokens_per_second\",\n",
    "            y=\"quality_score\",\n",
    "            hue=\"quantization\",\n",
    "            style=\"model_name\",\n",
    "            s=100,\n",
    "            ax=axes[1],\n",
    "        )\n",
    "        axes[1].set_title(\"Speed vs Quality Trade-off\")\n",
    "        axes[1].set_xlabel(\"Tokens per Second\")\n",
    "        axes[1].set_ylabel(\"Quality Score\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def generate_summary_table(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate summary table with key metrics\"\"\"\n",
    "        summary = (\n",
    "            self.df.groupby([\"model_name\", \"quantization\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"tokens_per_second\": [\"mean\", \"std\"],\n",
    "                    \"cost_per_1k\": [\"mean\", \"std\"],\n",
    "                    \"quality_score\": [\"mean\", \"std\"],\n",
    "                    \"peak_memory_mb\": [\"mean\", \"max\"],\n",
    "                    \"ttft\": [\"mean\", \"std\"],\n",
    "                }\n",
    "            )\n",
    "            .round(4)\n",
    "        )\n",
    "\n",
    "        # Flatten column names\n",
    "        summary.columns = [\"_\".join(col).strip() for col in summary.columns]\n",
    "        summary = summary.reset_index()\n",
    "\n",
    "        # Add efficiency score (quality per cost)\n",
    "        summary[\"efficiency_score\"] = (\n",
    "            summary[\"quality_score_mean\"] / summary[\"cost_per_1k_mean\"]\n",
    "        )\n",
    "\n",
    "        return summary.sort_values(\"efficiency_score\", ascending=False)\n",
    "\n",
    "    def find_optimal_configs(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Find optimal configurations for different use cases\"\"\"\n",
    "        agg_df = (\n",
    "            self.df.groupby([\"model_name\", \"quantization\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"cost_per_1k\": \"mean\",\n",
    "                    \"quality_score\": \"mean\",\n",
    "                    \"tokens_per_second\": \"mean\",\n",
    "                    \"peak_memory_mb\": \"mean\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        recommendations = {}\n",
    "\n",
    "        # Best for low cost\n",
    "        lowest_cost = agg_df.loc[agg_df[\"cost_per_1k\"].idxmin()]\n",
    "        recommendations[\"lowest_cost\"] = {\n",
    "            \"config\": f\"{lowest_cost['model_name']} + {lowest_cost['quantization']}\",\n",
    "            \"cost_per_1k\": lowest_cost[\"cost_per_1k\"],\n",
    "            \"quality\": lowest_cost[\"quality_score\"],\n",
    "            \"tps\": lowest_cost[\"tokens_per_second\"],\n",
    "            \"memory_mb\": lowest_cost[\"peak_memory_mb\"],\n",
    "        }\n",
    "\n",
    "        # Best for highest quality\n",
    "        highest_quality = agg_df.loc[agg_df[\"quality_score\"].idxmax()]\n",
    "        recommendations[\"highest_quality\"] = {\n",
    "            \"config\": f\"{highest_quality['model_name']} + {highest_quality['quantization']}\",\n",
    "            \"cost_per_1k\": highest_quality[\"cost_per_1k\"],\n",
    "            \"quality\": highest_quality[\"quality_score\"],\n",
    "            \"tps\": highest_quality[\"tokens_per_second\"],\n",
    "            \"memory_mb\": highest_quality[\"peak_memory_mb\"],\n",
    "        }\n",
    "\n",
    "        # Best for speed\n",
    "        fastest = agg_df.loc[agg_df[\"tokens_per_second\"].idxmax()]\n",
    "        recommendations[\"fastest\"] = {\n",
    "            \"config\": f\"{fastest['model_name']} + {fastest['quantization']}\",\n",
    "            \"cost_per_1k\": fastest[\"cost_per_1k\"],\n",
    "            \"quality\": fastest[\"quality_score\"],\n",
    "            \"tps\": fastest[\"tokens_per_second\"],\n",
    "            \"memory_mb\": fastest[\"peak_memory_mb\"],\n",
    "        }\n",
    "\n",
    "        # Best efficiency (quality per cost)\n",
    "        agg_df[\"efficiency\"] = agg_df[\"quality_score\"] / agg_df[\"cost_per_1k\"]\n",
    "        most_efficient = agg_df.loc[agg_df[\"efficiency\"].idxmax()]\n",
    "        recommendations[\"most_efficient\"] = {\n",
    "            \"config\": f\"{most_efficient['model_name']} + {most_efficient['quantization']}\",\n",
    "            \"cost_per_1k\": most_efficient[\"cost_per_1k\"],\n",
    "            \"quality\": most_efficient[\"quality_score\"],\n",
    "            \"tps\": most_efficient[\"tokens_per_second\"],\n",
    "            \"memory_mb\": most_efficient[\"peak_memory_mb\"],\n",
    "            \"efficiency\": most_efficient[\"efficiency\"],\n",
    "        }\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "print(\"📈 Analyzing benchmark results...\")\n",
    "analyzer = ResultsAnalyzer(results)\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\n📊 Performance Summary Table:\")\n",
    "summary_table = analyzer.generate_summary_table()\n",
    "print(summary_table.to_string(index=False))\n",
    "\n",
    "# Generate visualizations\n",
    "print(\"\\n🎨 Generating visualizations...\")\n",
    "analyzer.plot_cost_latency_quality_3d()\n",
    "analyzer.plot_pareto_frontier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 7: Recommendation Engine\n",
    "# ================================\n",
    "\n",
    "\n",
    "class RecommendationEngine:\n",
    "    \"\"\"Generate recommendations based on use case requirements\"\"\"\n",
    "\n",
    "    def __init__(self, analyzer: ResultsAnalyzer):\n",
    "        self.analyzer = analyzer\n",
    "        self.df = analyzer.df\n",
    "\n",
    "    def recommend_for_use_case(\n",
    "        self,\n",
    "        use_case: str,\n",
    "        max_cost: float = None,\n",
    "        min_quality: float = None,\n",
    "        min_speed: float = None,\n",
    "        max_memory: float = None,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Recommend optimal configuration for specific use case\"\"\"\n",
    "\n",
    "        # Define use case profiles\n",
    "        use_case_profiles = {\n",
    "            \"research\": {\n",
    "                \"priority\": \"quality\",\n",
    "                \"description\": \"Research & experimentation - prioritize quality over cost\",\n",
    "                \"constraints\": {\"min_quality\": 0.3},\n",
    "            },\n",
    "            \"production\": {\n",
    "                \"priority\": \"balanced\",\n",
    "                \"description\": \"Production deployment - balance quality, cost, and speed\",\n",
    "                \"constraints\": {\"min_speed\": 5.0, \"max_cost\": 0.01},\n",
    "            },\n",
    "            \"demo\": {\n",
    "                \"priority\": \"speed\",\n",
    "                \"description\": \"Interactive demos - prioritize speed and low latency\",\n",
    "                \"constraints\": {\"min_speed\": 10.0},\n",
    "            },\n",
    "            \"batch\": {\n",
    "                \"priority\": \"cost\",\n",
    "                \"description\": \"Batch processing - minimize cost per token\",\n",
    "                \"constraints\": {\"max_cost\": 0.005},\n",
    "            },\n",
    "            \"edge\": {\n",
    "                \"priority\": \"memory\",\n",
    "                \"description\": \"Edge deployment - minimize memory usage\",\n",
    "                \"constraints\": {\"max_memory\": 4000},\n",
    "            },\n",
    "        }\n",
    "\n",
    "        profile = use_case_profiles.get(use_case.lower(), {})\n",
    "        constraints = profile.get(\"constraints\", {})\n",
    "\n",
    "        # Apply user constraints\n",
    "        if max_cost:\n",
    "            constraints[\"max_cost\"] = max_cost\n",
    "        if min_quality:\n",
    "            constraints[\"min_quality\"] = min_quality\n",
    "        if min_speed:\n",
    "            constraints[\"min_speed\"] = min_speed\n",
    "        if max_memory:\n",
    "            constraints[\"max_memory\"] = max_memory\n",
    "\n",
    "        # Filter candidates based on constraints\n",
    "        candidates = (\n",
    "            self.df.groupby([\"model_name\", \"quantization\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"cost_per_1k\": \"mean\",\n",
    "                    \"quality_score\": \"mean\",\n",
    "                    \"tokens_per_second\": \"mean\",\n",
    "                    \"peak_memory_mb\": \"mean\",\n",
    "                    \"ttft\": \"mean\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Apply filters\n",
    "        for constraint, value in constraints.items():\n",
    "            if constraint == \"max_cost\":\n",
    "                candidates = candidates[candidates[\"cost_per_1k\"] <= value]\n",
    "            elif constraint == \"min_quality\":\n",
    "                candidates = candidates[candidates[\"quality_score\"] >= value]\n",
    "            elif constraint == \"min_speed\":\n",
    "                candidates = candidates[candidates[\"tokens_per_second\"] >= value]\n",
    "            elif constraint == \"max_memory\":\n",
    "                candidates = candidates[candidates[\"peak_memory_mb\"] <= value]\n",
    "\n",
    "        if len(candidates) == 0:\n",
    "            return {\"error\": \"No configurations meet the specified constraints\"}\n",
    "\n",
    "        # Select best candidate based on priority\n",
    "        priority = profile.get(\"priority\", \"balanced\")\n",
    "\n",
    "        if priority == \"quality\":\n",
    "            best = candidates.loc[candidates[\"quality_score\"].idxmax()]\n",
    "        elif priority == \"speed\":\n",
    "            best = candidates.loc[candidates[\"tokens_per_second\"].idxmax()]\n",
    "        elif priority == \"cost\":\n",
    "            best = candidates.loc[candidates[\"cost_per_1k\"].idxmin()]\n",
    "        elif priority == \"memory\":\n",
    "            best = candidates.loc[candidates[\"peak_memory_mb\"].idxmin()]\n",
    "        else:  # balanced\n",
    "            # Calculate composite score\n",
    "            candidates[\"composite_score\"] = (\n",
    "                candidates[\"quality_score\"] * 0.4\n",
    "                + (1 / candidates[\"cost_per_1k\"]) * 0.3  # Lower cost is better\n",
    "                + (\n",
    "                    candidates[\"tokens_per_second\"]\n",
    "                    / candidates[\"tokens_per_second\"].max()\n",
    "                )\n",
    "                * 0.3\n",
    "            )\n",
    "            best = candidates.loc[candidates[\"composite_score\"].idxmax()]\n",
    "\n",
    "        recommendation = {\n",
    "            \"use_case\": use_case,\n",
    "            \"description\": profile.get(\"description\", \"Custom use case\"),\n",
    "            \"recommended_config\": f\"{best['model_name']} + {best['quantization']}\",\n",
    "            \"metrics\": {\n",
    "                \"cost_per_1k_tokens\": f\"${best['cost_per_1k']:.6f}\",\n",
    "                \"quality_score\": f\"{best['quality_score']:.3f}\",\n",
    "                \"tokens_per_second\": f\"{best['tokens_per_second']:.1f}\",\n",
    "                \"memory_usage_mb\": f\"{best['peak_memory_mb']:.0f}\",\n",
    "                \"time_to_first_token\": f\"{best['ttft']:.3f}s\",\n",
    "            },\n",
    "            \"trade_offs\": self._analyze_trade_offs(best, candidates),\n",
    "        }\n",
    "\n",
    "        return recommendation\n",
    "\n",
    "    def _analyze_trade_offs(\n",
    "        self, selected: pd.Series, all_candidates: pd.DataFrame\n",
    "    ) -> Dict:\n",
    "        \"\"\"Analyze trade-offs of selected configuration\"\"\"\n",
    "        trade_offs = {}\n",
    "\n",
    "        # Compare to best in each dimension\n",
    "        best_cost = all_candidates.loc[all_candidates[\"cost_per_1k\"].idxmin()]\n",
    "        best_quality = all_candidates.loc[all_candidates[\"quality_score\"].idxmax()]\n",
    "        best_speed = all_candidates.loc[all_candidates[\"tokens_per_second\"].idxmax()]\n",
    "\n",
    "        cost_penalty = (selected[\"cost_per_1k\"] / best_cost[\"cost_per_1k\"] - 1) * 100\n",
    "        quality_penalty = (\n",
    "            1 - selected[\"quality_score\"] / best_quality[\"quality_score\"]\n",
    "        ) * 100\n",
    "        speed_penalty = (\n",
    "            1 - selected[\"tokens_per_second\"] / best_speed[\"tokens_per_second\"]\n",
    "        ) * 100\n",
    "\n",
    "        trade_offs = {\n",
    "            \"cost_vs_cheapest\": f\"{cost_penalty:+.1f}%\",\n",
    "            \"quality_vs_best\": f\"{quality_penalty:+.1f}%\",\n",
    "            \"speed_vs_fastest\": f\"{speed_penalty:+.1f}%\",\n",
    "        }\n",
    "\n",
    "        return trade_offs\n",
    "\n",
    "    def generate_all_recommendations(self) -> Dict:\n",
    "        \"\"\"Generate recommendations for all standard use cases\"\"\"\n",
    "        use_cases = [\"research\", \"production\", \"demo\", \"batch\", \"edge\"]\n",
    "        recommendations = {}\n",
    "\n",
    "        for use_case in use_cases:\n",
    "            recommendations[use_case] = self.recommend_for_use_case(use_case)\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "# Generate recommendations\n",
    "print(\"\\n🎯 Generating use case recommendations...\")\n",
    "recommender = RecommendationEngine(analyzer)\n",
    "all_recommendations = recommender.generate_all_recommendations()\n",
    "\n",
    "for use_case, rec in all_recommendations.items():\n",
    "    if \"error\" not in rec:\n",
    "        print(f\"\\n📋 {use_case.upper()} USE CASE:\")\n",
    "        print(f\"   Description: {rec['description']}\")\n",
    "        print(f\"   Recommended: {rec['recommended_config']}\")\n",
    "        print(f\"   Cost: {rec['metrics']['cost_per_1k_tokens']}/1K tokens\")\n",
    "        print(f\"   Quality: {rec['metrics']['quality_score']}\")\n",
    "        print(f\"   Speed: {rec['metrics']['tokens_per_second']} TPS\")\n",
    "        print(f\"   Memory: {rec['metrics']['memory_usage_mb']} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0164ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 8: Smoke Test & Validation\n",
    "# ================================\n",
    "\n",
    "\n",
    "def smoke_test_performance_analysis():\n",
    "    \"\"\"Smoke test for performance analysis functionality\"\"\"\n",
    "    print(\"🧪 Running smoke test for performance analysis...\")\n",
    "\n",
    "    # Test 1: Profiler initialization\n",
    "    profiler = PerformanceProfiler()\n",
    "    assert profiler.device.type in [\"cuda\", \"cpu\"], \"Invalid device type\"\n",
    "    print(\"✅ Profiler initialization OK\")\n",
    "\n",
    "    # Test 2: Quality evaluator\n",
    "    evaluator = QualityEvaluator()\n",
    "    test_metrics = evaluator.evaluate_quality(\n",
    "        None, None, \"This is a test.\", \"This is a reference.\"\n",
    "    )\n",
    "    assert \"rouge_l\" in test_metrics, \"Rouge metric missing\"\n",
    "    print(\"✅ Quality evaluator OK\")\n",
    "\n",
    "    # Test 3: Cost calculation\n",
    "    if len(results) > 0:\n",
    "        sample_result = results[0]\n",
    "        cost = benchmark.calculate_cost_per_1k_tokens(sample_result[\"perf_metrics\"])\n",
    "        assert cost >= 0, \"Invalid cost calculation\"\n",
    "        print(\"✅ Cost calculation OK\")\n",
    "\n",
    "    # Test 4: Analysis functions\n",
    "    if len(results) > 0:\n",
    "        analyzer = ResultsAnalyzer(results)\n",
    "        summary = analyzer.generate_summary_table()\n",
    "        assert len(summary) > 0, \"Summary table empty\"\n",
    "        print(\"✅ Results analysis OK\")\n",
    "\n",
    "    # Test 5: Recommendation engine\n",
    "    if len(results) > 0:\n",
    "        recommender = RecommendationEngine(analyzer)\n",
    "        rec = recommender.recommend_for_use_case(\"production\")\n",
    "        assert \"recommended_config\" in rec, \"Recommendation failed\"\n",
    "        print(\"✅ Recommendation engine OK\")\n",
    "\n",
    "    print(\"🎉 All smoke tests passed!\")\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_performance_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92729590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 9: Export Results & Summary\n",
    "# ================================\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def export_benchmark_results(results: List[Dict], filename: str = None):\n",
    "    \"\"\"Export benchmark results to JSON file\"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"performance_benchmark_{timestamp}.json\"\n",
    "\n",
    "    # Prepare exportable data\n",
    "    export_data = {\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"gpu_type\": benchmark.gpu_type,\n",
    "            \"total_configs_tested\": len(results),\n",
    "            \"test_prompts\": len(TEST_PROMPTS),\n",
    "            \"models_tested\": list(set(r[\"model_name\"] for r in results)),\n",
    "        },\n",
    "        \"results\": [],\n",
    "    }\n",
    "\n",
    "    for result in results:\n",
    "        # Convert PerformanceMetrics to dict\n",
    "        perf_dict = {\n",
    "            \"model_name\": result[\"perf_metrics\"].model_name,\n",
    "            \"quantization\": result[\"perf_metrics\"].quantization,\n",
    "            \"tokens_per_second\": result[\"perf_metrics\"].tokens_per_second,\n",
    "            \"time_to_first_token\": result[\"perf_metrics\"].time_to_first_token,\n",
    "            \"total_time\": result[\"perf_metrics\"].total_time,\n",
    "            \"peak_gpu_memory_mb\": result[\"perf_metrics\"].peak_gpu_memory_mb,\n",
    "            \"avg_gpu_utilization\": result[\"perf_metrics\"].avg_gpu_utilization,\n",
    "            \"avg_cpu_percent\": result[\"perf_metrics\"].avg_cpu_percent,\n",
    "        }\n",
    "\n",
    "        export_result = {\n",
    "            \"model_name\": result[\"model_name\"],\n",
    "            \"quantization\": result[\"quantization\"],\n",
    "            \"prompt_category\": result[\"prompt_category\"],\n",
    "            \"performance_metrics\": perf_dict,\n",
    "            \"quality_metrics\": result[\"quality_metrics\"],\n",
    "            \"cost_per_1k_tokens\": result[\"cost_per_1k_tokens\"],\n",
    "        }\n",
    "        export_data[\"results\"].append(export_result)\n",
    "\n",
    "    # Save to file\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"📁 Results exported to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "\n",
    "# Export results\n",
    "if len(results) > 0:\n",
    "    export_file = export_benchmark_results(results)\n",
    "\n",
    "    # Generate final summary report\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"📊 PERFORMANCE ANALYSIS SUMMARY REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"🖥️  GPU Type: {benchmark.gpu_type}\")\n",
    "    print(f\"📈 Total Configurations Tested: {len(results)}\")\n",
    "    print(f\"💾 Results exported to: {export_file}\")\n",
    "\n",
    "    # Key findings\n",
    "    optimal_configs = analyzer.find_optimal_configs()\n",
    "    print(f\"\\n🏆 KEY FINDINGS:\")\n",
    "    print(f\"   💰 Most Cost-Effective: {optimal_configs['lowest_cost']['config']}\")\n",
    "    print(f\"      └── ${optimal_configs['lowest_cost']['cost_per_1k']:.6f}/1K tokens\")\n",
    "    print(f\"   🎯 Highest Quality: {optimal_configs['highest_quality']['config']}\")\n",
    "    print(\n",
    "        f\"      └── Quality Score: {optimal_configs['highest_quality']['quality']:.3f}\"\n",
    "    )\n",
    "    print(f\"   ⚡ Fastest: {optimal_configs['fastest']['config']}\")\n",
    "    print(f\"      └── {optimal_configs['fastest']['tps']:.1f} tokens/second\")\n",
    "    print(f\"   🎖️  Most Efficient: {optimal_configs['most_efficient']['config']}\")\n",
    "    print(\n",
    "        f\"      └── Efficiency: {optimal_configs['most_efficient']['efficiency']:.2f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✨ Performance analysis completed successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd70b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Smoke Test - Performance Analysis Validation\n",
    "# ================================\n",
    "\n",
    "\n",
    "def comprehensive_smoke_test():\n",
    "    \"\"\"Comprehensive validation of performance analysis components\"\"\"\n",
    "    print(\"🔬 Running comprehensive smoke test...\")\n",
    "\n",
    "    # Test basic functionality without model loading\n",
    "    test_results = []\n",
    "\n",
    "    # 1. Environment Check\n",
    "    assert torch.cuda.is_available() or True, \"GPU check\"\n",
    "    print(\"✅ Environment check passed\")\n",
    "\n",
    "    # 2. Utility Classes\n",
    "    profiler = PerformanceProfiler()\n",
    "    evaluator = QualityEvaluator()\n",
    "    print(\"✅ Utility classes initialized\")\n",
    "\n",
    "    # 3. Mock performance metrics\n",
    "    mock_metrics = PerformanceMetrics(\n",
    "        model_name=\"test-model\",\n",
    "        quantization=\"fp16\",\n",
    "        prompt_length=50,\n",
    "        generation_length=100,\n",
    "        time_to_first_token=0.1,\n",
    "        tokens_per_second=25.0,\n",
    "        total_time=4.0,\n",
    "        peak_gpu_memory_mb=2048.0,\n",
    "        avg_gpu_utilization=75.0,\n",
    "        avg_cpu_percent=15.0,\n",
    "    )\n",
    "    print(\"✅ Mock metrics created\")\n",
    "\n",
    "    # 4. Cost calculation test\n",
    "    benchmark_instance = PerformanceBenchmark()\n",
    "    cost = benchmark_instance.calculate_cost_per_1k_tokens(mock_metrics)\n",
    "    assert cost > 0, \"Cost should be positive\"\n",
    "    print(f\"✅ Cost calculation: ${cost:.6f}/1K tokens\")\n",
    "\n",
    "    # 5. Analysis pipeline (with mock data)\n",
    "    mock_results = [\n",
    "        {\n",
    "            \"model_name\": \"test-small\",\n",
    "            \"quantization\": \"fp16\",\n",
    "            \"prompt_category\": \"simple\",\n",
    "            \"perf_metrics\": mock_metrics,\n",
    "            \"quality_metrics\": {\"perplexity\": 15.0, \"rouge_l\": 0.65, \"bleu\": 0.45},\n",
    "            \"cost_per_1k_tokens\": cost,\n",
    "            \"generated_text\": \"This is a test generation.\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    analyzer = ResultsAnalyzer(mock_results)\n",
    "    summary = analyzer.generate_summary_table()\n",
    "    assert len(summary) > 0, \"Summary table should not be empty\"\n",
    "    print(\"✅ Results analysis pipeline\")\n",
    "\n",
    "    # 6. Recommendation engine\n",
    "    recommender = RecommendationEngine(analyzer)\n",
    "    rec = recommender.recommend_for_use_case(\"production\", max_cost=0.01)\n",
    "    print(\"✅ Recommendation engine\")\n",
    "\n",
    "    print(\"🎉 All smoke tests completed successfully!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "comprehensive_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7eb548",
   "metadata": {},
   "source": [
    "\n",
    "## 📋 6. 本章小結\n",
    "\n",
    "### ✅ 完成項目\n",
    "- **效能分析框架**: 建立了全面的 LLM 效能測試系統，支援多維度評估\n",
    "- **成本計算模組**: 實作了基於硬體類型的精確成本計算，包含 GPU 與電力成本\n",
    "- **品質評估系統**: 整合了多種評估指標 (Perplexity, ROUGE, BLEU)，提供綜合品質分數\n",
    "- **視覺化分析**: 建立了 3D 權衡圖表與帕累托前沿分析，直觀展示最佳配置\n",
    "- **智慧推薦引擎**: 針對不同使用場景 (研究/生產/展示/批次/邊緣) 提供最佳配置建議\n",
    "\n",
    "### 🧠 核心原理要點\n",
    "- **三角權衡關係**: 成本、延遲、品質之間存在固有的權衡關係，需要根據使用場景選擇平衡點\n",
    "- **量化策略影響**: INT4/INT8 量化能顯著降低記憶體使用與成本，但會帶來品質損失\n",
    "- **TTFT vs TPS**: 首個 Token 時間 (TTFT) 影響使用者體驗，每秒 Token 數 (TPS) 影響吞吐量\n",
    "- **帕累托最佳化**: 在多目標最佳化中，帕累托前沿上的配置代表不可改進的最佳解\n",
    "- **成本模型**: 總成本包含硬體攤提成本與電力消耗，需要考慮實際使用模式\n",
    "\n",
    "### ⚠️ 常見陷阱與注意事項\n",
    "- **測試環境一致性**: 確保所有配置在相同環境下測試，避免外部因素影響結果\n",
    "- **記憶體洩漏**: 大量模型載入測試時需要適當清理，避免 CUDA OOM 錯誤\n",
    "- **統計顯著性**: 單次測試結果可能有變異，建議多次測試取平均值\n",
    "- **成本計算假設**: 成本模型基於特定假設 (電價、硬體價格)，實際部署時需要調整\n",
    "- **品質指標限制**: 自動化指標無法完全反映人類感知品質，建議輔以人工評估\n",
    "\n",
    "### 🎯 下一步建議\n",
    "1. **Part D 微調技術**: 探索 LoRA/QLoRA 微調如何影響效能權衡關係\n",
    "2. **部署最佳化**: 研究生產環境的推理最佳化技術 (動態批次、KV 快取等)\n",
    "3. **多模態擴展**: 將效能分析擴展到視覺-語言模型 (VLM)\n",
    "4. **長文本處理**: 分析不同 context 長度對效能的影響\n",
    "5. **實時監控**: 建立生產環境的效能監控與自動調優系統\n",
    "\n",
    "---\n",
    "\n",
    "**完成 Part C - LLM Applications 階段！** 🎉\n",
    "\n",
    "已完成 LLM 應用核心的 10 本 notebooks，涵蓋了從基礎文本生成到進階效能分析的完整技術棧。接下來建議進入 **Part D (微調技術)** 或 **Part E (RAG + Agents)**，讓我知道您希望優先學習哪個方向！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
