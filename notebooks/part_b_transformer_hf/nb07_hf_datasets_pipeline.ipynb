{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ead37b6",
   "metadata": {},
   "source": [
    "# Part B: nb07_hf_datasets_pipeline.ipynb\n",
    "# HF Datasets ç®¡ç·šï¼ˆæ–‡æœ¬/åœ–ç‰‡/èªéŸ³ï¼‰Processing Pipeline\n",
    "\n",
    "# ğŸ“Š HF Datasets å¤šæ¨¡æ…‹è™•ç†ç®¡ç·š\n",
    "\n",
    "æœ¬ç« å­¸ç¿’é‡é»ï¼š\n",
    "- **å¤šæ¨¡æ…‹æ•¸æ“šè™•ç†** (Multimodal Data Processing): æ–‡æœ¬ã€åœ–åƒã€èªéŸ³çµ±ä¸€ç®¡ç·š\n",
    "- **æ‰¹é‡é è™•ç†ç®¡ç·š** (Batch Preprocessing Pipeline): é«˜æ•ˆçš„ tokenization èˆ‡ feature extraction\n",
    "- **è¨˜æ†¶é«”å„ªåŒ–** (Memory Optimization): streaming, lazy loading, batch control\n",
    "- **æ•¸æ“šå¢å¼·** (Data Augmentation): æ–‡æœ¬æ”¹å¯«ã€åœ–åƒè®Šæ›ã€èªéŸ³å¢å¼·\n",
    "- **è·¨æ¨¡æ…‹æ•´åˆ** (Cross-modal Integration): text+image, text+audio æ•¸æ“šé›†è™•ç†\n",
    "\n",
    "æ ¸å¿ƒæŠ€è¡“æ£§ï¼š\n",
    "- ğŸ¤— datasets: çµ±ä¸€å¤šæ¨¡æ…‹æ•¸æ“šé›†ä»‹é¢\n",
    "- ğŸ¤— transformers: tokenizers èˆ‡ feature extractors\n",
    "- ğŸ–¼ï¸ torchvision: åœ–åƒé è™•ç†èˆ‡å¢å¼·\n",
    "- ğŸ”Š librosa: èªéŸ³ç‰¹å¾µæå–\n",
    "- âš¡ streaming: å¤§è¦æ¨¡æ•¸æ“šé›†è¨˜æ†¶é«”å‹å–„è™•ç†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ab146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 1: Environment Setup & Shared Cache\n",
    "# === Shared Cache Bootstrap (English comments only) ===\n",
    "import os, torch, platform, pathlib\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "for k, v in paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache] Root:\", AI_CACHE_ROOT)\n",
    "print(\n",
    "    \"[GPU]\",\n",
    "    torch.cuda.is_available(),\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for multimodal dataset processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoFeatureExtractor,\n",
    "    WhisperFeatureExtractor,\n",
    "    CLIPProcessor,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import torchaudio\n",
    "import librosa\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check available processing backends\n",
    "print(f\"[Backends] PyTorch: {torch.__version__}\")\n",
    "print(f\"[Backends] torchaudio available: {torchaudio.is_available()}\")\n",
    "print(f\"[Backends] librosa available: {'librosa' in locals()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Text Dataset Processing Pipeline\n",
    "print(\"=== æ–‡æœ¬æ•¸æ“šé›†è™•ç†ç®¡ç·š (Text Dataset Processing Pipeline) ===\")\n",
    "\n",
    "\n",
    "class TextDatasetProcessor:\n",
    "    \"\"\"Unified text dataset preprocessing pipeline\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"bert-base-uncased\", max_length: int = 512):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def tokenize_function(self, examples: Dict[str, List]) -> Dict[str, List]:\n",
    "        \"\"\"Batch tokenization with padding and truncation\"\"\"\n",
    "        # Handle different text column names\n",
    "        text_key = \"text\" if \"text\" in examples else \"sentence\"\n",
    "        if text_key not in examples:\n",
    "            text_key = list(examples.keys())[0]  # fallback to first column\n",
    "\n",
    "        return self.tokenizer(\n",
    "            examples[text_key],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=None,  # Keep as lists for datasets\n",
    "        )\n",
    "\n",
    "    def process_dataset(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        split: str = \"train\",\n",
    "        streaming: bool = False,\n",
    "        max_samples: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Load and preprocess text dataset with memory optimization\"\"\"\n",
    "        try:\n",
    "            # Load with streaming for large datasets\n",
    "            dataset = load_dataset(dataset_name, split=split, streaming=streaming)\n",
    "\n",
    "            if max_samples and not streaming:\n",
    "                dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "            # Apply tokenization in batches\n",
    "            tokenized = dataset.map(\n",
    "                self.tokenize_function,\n",
    "                batched=True,\n",
    "                batch_size=1000,\n",
    "                remove_columns=dataset.column_names if not streaming else None,\n",
    "            )\n",
    "\n",
    "            return tokenized\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {dataset_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Demo: Process different text datasets\n",
    "text_processor = TextDatasetProcessor(\"distilbert-base-uncased\", max_length=256)\n",
    "\n",
    "# 1. Sentiment Analysis Dataset (IMDB)\n",
    "print(\"\\n1. è™•ç†æƒ…æ„Ÿåˆ†ææ•¸æ“šé›† (IMDB Sentiment)\")\n",
    "try:\n",
    "    imdb_dataset = text_processor.process_dataset(\n",
    "        \"imdb\", split=\"train[:1000]\"\n",
    "    )  # Small subset\n",
    "    if imdb_dataset:\n",
    "        sample = next(iter(imdb_dataset))\n",
    "        print(f\"IMDB sample: {len(sample['input_ids'])} tokens\")\n",
    "        print(f\"Columns: {sample.keys()}\")\n",
    "except:\n",
    "    print(\"IMDB dataset not available, skipping...\")\n",
    "\n",
    "# 2. Text Generation Dataset (WikiText)\n",
    "print(\"\\n2. è™•ç†æ–‡æœ¬ç”Ÿæˆæ•¸æ“šé›† (WikiText)\")\n",
    "try:\n",
    "    wiki_dataset = text_processor.process_dataset(\n",
    "        \"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:500]\"\n",
    "    )\n",
    "    if wiki_dataset:\n",
    "        sample = next(iter(wiki_dataset))\n",
    "        print(f\"WikiText sample: {len(sample['input_ids'])} tokens\")\n",
    "except:\n",
    "    print(\"WikiText dataset not available, creating synthetic...\")\n",
    "    # Create synthetic text dataset\n",
    "    synthetic_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Machine learning is transforming the world.\",\n",
    "        \"Natural language processing enables computers to understand human language.\",\n",
    "    ] * 100\n",
    "\n",
    "    synthetic_dataset = Dataset.from_dict({\"text\": synthetic_texts})\n",
    "    processed_synthetic = synthetic_dataset.map(\n",
    "        text_processor.tokenize_function, batched=True, batch_size=50\n",
    "    )\n",
    "    sample = processed_synthetic[0]\n",
    "    print(f\"Synthetic sample: {len(sample['input_ids'])} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b6807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Image Dataset Processing Pipeline\n",
    "print(\"=== åœ–åƒæ•¸æ“šé›†è™•ç†ç®¡ç·š (Image Dataset Processing Pipeline) ===\")\n",
    "\n",
    "\n",
    "class ImageDatasetProcessor:\n",
    "    \"\"\"Unified image dataset preprocessing pipeline\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"microsoft/resnet-50\"):\n",
    "        try:\n",
    "            self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "        except:\n",
    "            # Fallback to basic preprocessing\n",
    "            from torchvision import transforms\n",
    "\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            self.feature_extractor = None\n",
    "\n",
    "    def process_images(self, examples: Dict[str, List]) -> Dict[str, List]:\n",
    "        \"\"\"Batch image preprocessing\"\"\"\n",
    "        # Handle different image column names\n",
    "        img_key = \"image\" if \"image\" in examples else \"img\"\n",
    "        if img_key not in examples:\n",
    "            img_key = list(examples.keys())[0]\n",
    "\n",
    "        images = examples[img_key]\n",
    "\n",
    "        if self.feature_extractor:\n",
    "            # Use HF feature extractor\n",
    "            processed = self.feature_extractor(images, return_tensors=\"pt\")\n",
    "            return {\n",
    "                \"pixel_values\": processed[\"pixel_values\"].tolist(),\n",
    "                \"image_shape\": [img.size for img in images],\n",
    "            }\n",
    "        else:\n",
    "            # Use torchvision transforms\n",
    "            processed_images = []\n",
    "            for img in images:\n",
    "                if isinstance(img, str):  # Image path\n",
    "                    img = Image.open(img).convert(\"RGB\")\n",
    "                processed_img = self.transform(img)\n",
    "                processed_images.append(processed_img.tolist())\n",
    "\n",
    "            return {\"pixel_values\": processed_images}\n",
    "\n",
    "    def process_dataset(\n",
    "        self, dataset_name: str, split: str = \"train\", max_samples: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"Load and preprocess image dataset\"\"\"\n",
    "        try:\n",
    "            dataset = load_dataset(dataset_name, split=split)\n",
    "\n",
    "            if max_samples:\n",
    "                dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "            # Apply image processing in batches\n",
    "            processed = dataset.map(\n",
    "                self.process_images,\n",
    "                batched=True,\n",
    "                batch_size=32,  # Smaller batch for images\n",
    "            )\n",
    "\n",
    "            return processed\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {dataset_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Demo: Process image datasets\n",
    "image_processor = ImageDatasetProcessor()\n",
    "\n",
    "print(\"\\n1. è™•ç†åœ–åƒåˆ†é¡æ•¸æ“šé›† (CIFAR-10)\")\n",
    "try:\n",
    "    # Try to load a small image dataset\n",
    "    cifar_dataset = image_processor.process_dataset(\"cifar10\", split=\"train[:100]\")\n",
    "    if cifar_dataset:\n",
    "        sample = cifar_dataset[0]\n",
    "        print(f\"CIFAR sample shape: {np.array(sample['pixel_values']).shape}\")\n",
    "        print(f\"Available keys: {sample.keys()}\")\n",
    "except:\n",
    "    print(\"CIFAR-10 not available, creating synthetic image dataset...\")\n",
    "\n",
    "    # Create synthetic image dataset\n",
    "    synthetic_images = []\n",
    "    for i in range(50):\n",
    "        # Create random RGB image\n",
    "        img_array = np.random.randint(0, 256, (32, 32, 3), dtype=np.uint8)\n",
    "        img = Image.fromarray(img_array)\n",
    "        synthetic_images.append(img)\n",
    "\n",
    "    synthetic_img_dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"image\": synthetic_images,\n",
    "            \"label\": list(range(10)) * 5,  # 10 classes, 5 samples each\n",
    "        }\n",
    "    )\n",
    "\n",
    "    processed_synthetic = synthetic_img_dataset.map(\n",
    "        image_processor.process_images, batched=True, batch_size=16\n",
    "    )\n",
    "    sample = processed_synthetic[0]\n",
    "    print(f\"Synthetic image shape: {np.array(sample['pixel_values']).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fde26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: Audio Dataset Processing Pipeline\n",
    "print(\"=== èªéŸ³æ•¸æ“šé›†è™•ç†ç®¡ç·š (Audio Dataset Processing Pipeline) ===\")\n",
    "\n",
    "\n",
    "class AudioDatasetProcessor:\n",
    "    \"\"\"Unified audio dataset preprocessing pipeline\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"openai/whisper-base\", target_sr: int = 16000):\n",
    "        try:\n",
    "            self.feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "            self.target_sr = self.feature_extractor.sampling_rate\n",
    "        except:\n",
    "            # Fallback to manual audio processing\n",
    "            self.feature_extractor = None\n",
    "            self.target_sr = target_sr\n",
    "\n",
    "    def process_audio(self, examples: Dict[str, List]) -> Dict[str, List]:\n",
    "        \"\"\"Batch audio preprocessing\"\"\"\n",
    "        audio_key = \"audio\" if \"audio\" in examples else \"speech\"\n",
    "        if audio_key not in examples:\n",
    "            return examples\n",
    "\n",
    "        processed_features = []\n",
    "\n",
    "        for audio_data in examples[audio_key]:\n",
    "            if isinstance(audio_data, dict) and \"array\" in audio_data:\n",
    "                # HF datasets audio format\n",
    "                audio_array = audio_data[\"array\"]\n",
    "                sample_rate = audio_data[\"sampling_rate\"]\n",
    "            else:\n",
    "                # Raw audio array\n",
    "                audio_array = audio_data\n",
    "                sample_rate = self.target_sr\n",
    "\n",
    "            # Resample if needed\n",
    "            if sample_rate != self.target_sr:\n",
    "                audio_array = librosa.resample(\n",
    "                    audio_array, orig_sr=sample_rate, target_sr=self.target_sr\n",
    "                )\n",
    "\n",
    "            if self.feature_extractor:\n",
    "                # Use Whisper feature extractor\n",
    "                features = self.feature_extractor(\n",
    "                    audio_array, sampling_rate=self.target_sr, return_tensors=\"pt\"\n",
    "                )\n",
    "                processed_features.append(features[\"input_features\"].squeeze().tolist())\n",
    "            else:\n",
    "                # Basic audio features (MFCC)\n",
    "                mfcc = librosa.feature.mfcc(y=audio_array, sr=self.target_sr, n_mfcc=13)\n",
    "                processed_features.append(mfcc.T.tolist())  # Transpose for time-major\n",
    "\n",
    "        return {\"audio_features\": processed_features}\n",
    "\n",
    "    def process_dataset(\n",
    "        self, dataset_name: str, split: str = \"train\", max_samples: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"Load and preprocess audio dataset\"\"\"\n",
    "        try:\n",
    "            dataset = load_dataset(dataset_name, split=split)\n",
    "\n",
    "            if max_samples:\n",
    "                dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "            # Apply audio processing in smaller batches\n",
    "            processed = dataset.map(\n",
    "                self.process_audio,\n",
    "                batched=True,\n",
    "                batch_size=8,  # Very small batch for audio\n",
    "            )\n",
    "\n",
    "            return processed\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {dataset_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Demo: Process audio datasets\n",
    "audio_processor = AudioDatasetProcessor()\n",
    "\n",
    "print(\"\\n1. è™•ç†èªéŸ³æ•¸æ“šé›† (Speech Dataset)\")\n",
    "try:\n",
    "    # Try to load a small audio dataset\n",
    "    speech_dataset = audio_processor.process_dataset(\n",
    "        \"common_voice\", \"en\", split=\"train[:50]\"\n",
    "    )\n",
    "    if speech_dataset:\n",
    "        sample = speech_dataset[0]\n",
    "        print(f\"Audio features shape: {np.array(sample['audio_features']).shape}\")\n",
    "except:\n",
    "    print(\"Speech dataset not available, creating synthetic audio...\")\n",
    "\n",
    "    # Create synthetic audio dataset\n",
    "    synthetic_audio = []\n",
    "    for i in range(20):\n",
    "        # Generate synthetic audio (sine waves)\n",
    "        duration = 2.0  # 2 seconds\n",
    "        sample_rate = 16000\n",
    "        t = np.linspace(0, duration, int(sample_rate * duration))\n",
    "        frequency = 440 + i * 50  # Different frequencies\n",
    "        audio_signal = np.sin(2 * np.pi * frequency * t).astype(np.float32)\n",
    "\n",
    "        synthetic_audio.append({\"array\": audio_signal, \"sampling_rate\": sample_rate})\n",
    "\n",
    "    synthetic_audio_dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"audio\": synthetic_audio,\n",
    "            \"transcript\": [f\"This is synthetic audio sample {i}\" for i in range(20)],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    processed_synthetic = synthetic_audio_dataset.map(\n",
    "        audio_processor.process_audio, batched=True, batch_size=4\n",
    "    )\n",
    "    sample = processed_synthetic[0]\n",
    "    print(f\"Synthetic audio features shape: {np.array(sample['audio_features']).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d3e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: Custom Dataset Loading & Transformation\n",
    "print(\"=== è‡ªè¨‚æ•¸æ“šé›†è¼‰å…¥èˆ‡è½‰æ› (Custom Dataset Loading) ===\")\n",
    "\n",
    "\n",
    "class CustomDatasetLoader:\n",
    "    \"\"\"Load and transform custom datasets from various formats\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def from_csv(csv_path: str, text_column: str, label_column: Optional[str] = None):\n",
    "        \"\"\"Load dataset from CSV file\"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        data_dict = {\"text\": df[text_column].tolist()}\n",
    "        if label_column and label_column in df.columns:\n",
    "            data_dict[\"labels\"] = df[label_column].tolist()\n",
    "        return Dataset.from_dict(data_dict)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(json_path: str):\n",
    "        \"\"\"Load dataset from JSON file\"\"\"\n",
    "        import json\n",
    "\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return Dataset.from_dict(data)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_text_files(text_dir: str, pattern: str = \"*.txt\"):\n",
    "        \"\"\"Load dataset from text files in directory\"\"\"\n",
    "        import glob\n",
    "\n",
    "        text_files = glob.glob(os.path.join(text_dir, pattern))\n",
    "        texts = []\n",
    "        filenames = []\n",
    "\n",
    "        for file_path in text_files:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read().strip()\n",
    "                texts.append(content)\n",
    "                filenames.append(os.path.basename(file_path))\n",
    "\n",
    "        return Dataset.from_dict({\"text\": texts, \"filename\": filenames})\n",
    "\n",
    "\n",
    "# Demo: Create and transform custom datasets\n",
    "print(\"\\n1. å‰µå»ºè‡ªè¨‚æ–‡æœ¬æ•¸æ“šé›†\")\n",
    "\n",
    "# Create sample data for demonstration\n",
    "sample_data = {\n",
    "    \"text\": [\n",
    "        \"äººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šä¸–ç•Œ\",\n",
    "        \"æ©Ÿå™¨å­¸ç¿’ä½¿é›»è…¦èƒ½å¤ å­¸ç¿’\",\n",
    "        \"æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„å­é ˜åŸŸ\",\n",
    "        \"è‡ªç„¶èªè¨€è™•ç†è®“æ©Ÿå™¨ç†è§£äººé¡èªè¨€\",\n",
    "        \"é›»è…¦è¦–è¦ºè®“æ©Ÿå™¨çœ‹æ‡‚åœ–åƒ\",\n",
    "    ],\n",
    "    \"category\": [\"AI\", \"ML\", \"DL\", \"NLP\", \"CV\"],\n",
    "    \"language\": [\"zh\"] * 5,\n",
    "}\n",
    "\n",
    "custom_dataset = Dataset.from_dict(sample_data)\n",
    "print(f\"Custom dataset: {len(custom_dataset)} samples\")\n",
    "print(f\"Sample: {custom_dataset[0]}\")\n",
    "\n",
    "\n",
    "# Apply custom transformations\n",
    "def add_text_length(examples):\n",
    "    \"\"\"Add text length feature\"\"\"\n",
    "    return {\"text_length\": [len(text) for text in examples[\"text\"]]}\n",
    "\n",
    "\n",
    "def add_language_id(examples):\n",
    "    \"\"\"Add numeric language ID\"\"\"\n",
    "    lang_map = {\"zh\": 0, \"en\": 1, \"fr\": 2}\n",
    "    return {\"lang_id\": [lang_map.get(lang, -1) for lang in examples[\"language\"]]}\n",
    "\n",
    "\n",
    "# Chain multiple transformations\n",
    "enhanced_dataset = custom_dataset.map(add_text_length, batched=True)\n",
    "enhanced_dataset = enhanced_dataset.map(add_language_id, batched=True)\n",
    "\n",
    "print(f\"Enhanced sample: {enhanced_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b62efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Cross-Modal Dataset Integration\n",
    "print(\"=== è·¨æ¨¡æ…‹æ•¸æ“šé›†æ•´åˆ (Cross-Modal Integration) ===\")\n",
    "\n",
    "\n",
    "class MultiModalDatasetProcessor:\n",
    "    \"\"\"Process datasets with multiple modalities (text + image, text + audio)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.text_processor = TextDatasetProcessor(\n",
    "            \"distilbert-base-uncased\", max_length=128\n",
    "        )\n",
    "        self.image_processor = ImageDatasetProcessor()\n",
    "        self.audio_processor = AudioDatasetProcessor()\n",
    "\n",
    "    def process_text_image_pair(self, examples: Dict[str, List]) -> Dict[str, List]:\n",
    "        \"\"\"Process text-image pairs\"\"\"\n",
    "        result = {}\n",
    "\n",
    "        # Process text\n",
    "        if \"text\" in examples or \"caption\" in examples:\n",
    "            text_key = \"text\" if \"text\" in examples else \"caption\"\n",
    "            text_features = self.text_processor.tokenize_function(\n",
    "                {text_key: examples[text_key]}\n",
    "            )\n",
    "            result.update(text_features)\n",
    "\n",
    "        # Process images\n",
    "        if \"image\" in examples:\n",
    "            img_features = self.image_processor.process_images(examples)\n",
    "            result.update(img_features)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def process_text_audio_pair(self, examples: Dict[str, List]) -> Dict[str, List]:\n",
    "        \"\"\"Process text-audio pairs\"\"\"\n",
    "        result = {}\n",
    "\n",
    "        # Process text (transcripts)\n",
    "        if \"text\" in examples or \"transcript\" in examples:\n",
    "            text_key = \"text\" if \"text\" in examples else \"transcript\"\n",
    "            text_features = self.text_processor.tokenize_function(\n",
    "                {text_key: examples[text_key]}\n",
    "            )\n",
    "            result.update(text_features)\n",
    "\n",
    "        # Process audio\n",
    "        if \"audio\" in examples:\n",
    "            audio_features = self.audio_processor.process_audio(examples)\n",
    "            result.update(audio_features)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# Demo: Create multimodal datasets\n",
    "multimodal_processor = MultiModalDatasetProcessor()\n",
    "\n",
    "print(\"\\n1. æ–‡æœ¬-åœ–åƒæ•¸æ“šé›† (Text-Image Dataset)\")\n",
    "# Create synthetic text-image dataset\n",
    "captions = [\n",
    "    \"A red car driving on the highway\",\n",
    "    \"A beautiful sunset over the mountains\",\n",
    "    \"A cat sitting on a windowsill\",\n",
    "]\n",
    "\n",
    "synthetic_images = []\n",
    "for i in range(3):\n",
    "    img_array = np.random.randint(0, 256, (64, 64, 3), dtype=np.uint8)\n",
    "    img = Image.fromarray(img_array)\n",
    "    synthetic_images.append(img)\n",
    "\n",
    "text_image_dataset = Dataset.from_dict({\"caption\": captions, \"image\": synthetic_images})\n",
    "\n",
    "processed_multimodal = text_image_dataset.map(\n",
    "    multimodal_processor.process_text_image_pair, batched=True, batch_size=2\n",
    ")\n",
    "\n",
    "sample = processed_multimodal[0]\n",
    "print(f\"Multimodal sample keys: {sample.keys()}\")\n",
    "print(f\"Text tokens: {len(sample['input_ids'])}\")\n",
    "print(f\"Image features shape: {np.array(sample['pixel_values']).shape}\")\n",
    "\n",
    "print(\"\\n2. æ–‡æœ¬-èªéŸ³æ•¸æ“šé›† (Text-Audio Dataset)\")\n",
    "# Create synthetic text-audio dataset\n",
    "transcripts = [\"Hello world\", \"How are you\", \"Machine learning\"]\n",
    "synthetic_audio_data = []\n",
    "\n",
    "for i, transcript in enumerate(transcripts):\n",
    "    # Generate synthetic audio\n",
    "    duration = 1.0\n",
    "    sample_rate = 16000\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration))\n",
    "    frequency = 400 + i * 100\n",
    "    audio_signal = np.sin(2 * np.pi * frequency * t).astype(np.float32)\n",
    "\n",
    "    synthetic_audio_data.append({\"array\": audio_signal, \"sampling_rate\": sample_rate})\n",
    "\n",
    "text_audio_dataset = Dataset.from_dict(\n",
    "    {\"transcript\": transcripts, \"audio\": synthetic_audio_data}\n",
    ")\n",
    "\n",
    "processed_audio_text = text_audio_dataset.map(\n",
    "    multimodal_processor.process_text_audio_pair, batched=True, batch_size=2\n",
    ")\n",
    "\n",
    "sample = processed_audio_text[0]\n",
    "print(f\"Audio-text sample keys: {sample.keys()}\")\n",
    "print(f\"Text tokens: {len(sample['input_ids'])}\")\n",
    "print(f\"Audio features shape: {np.array(sample['audio_features']).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Performance Optimization & Memory Management\n",
    "print(\"=== æ•ˆèƒ½å„ªåŒ–èˆ‡è¨˜æ†¶é«”ç®¡ç† (Performance & Memory Optimization) ===\")\n",
    "\n",
    "\n",
    "class OptimizedDatasetProcessor:\n",
    "    \"\"\"Memory-efficient dataset processing with streaming and caching\"\"\"\n",
    "\n",
    "    def __init__(self, cache_dir: Optional[str] = None):\n",
    "        self.cache_dir = cache_dir or os.path.join(AI_CACHE_ROOT, \"processed_datasets\")\n",
    "        pathlib.Path(self.cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def streaming_process(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        processor_fn,\n",
    "        batch_size: int = 1000,\n",
    "        max_samples: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Process large datasets with streaming\"\"\"\n",
    "        print(f\"Processing {dataset_name} with streaming mode...\")\n",
    "\n",
    "        try:\n",
    "            # Load in streaming mode\n",
    "            dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "            processed_batches = []\n",
    "            sample_count = 0\n",
    "\n",
    "            # Process in chunks\n",
    "            current_batch = []\n",
    "            for sample in dataset:\n",
    "                current_batch.append(sample)\n",
    "                sample_count += 1\n",
    "\n",
    "                if len(current_batch) >= batch_size:\n",
    "                    # Process current batch\n",
    "                    batch_dataset = Dataset.from_list(current_batch)\n",
    "                    processed_batch = batch_dataset.map(processor_fn, batched=True)\n",
    "                    processed_batches.append(processed_batch)\n",
    "\n",
    "                    current_batch = []\n",
    "                    print(f\"Processed {sample_count} samples...\")\n",
    "\n",
    "                if max_samples and sample_count >= max_samples:\n",
    "                    break\n",
    "\n",
    "            # Process remaining samples\n",
    "            if current_batch:\n",
    "                batch_dataset = Dataset.from_list(current_batch)\n",
    "                processed_batch = batch_dataset.map(processor_fn, batched=True)\n",
    "                processed_batches.append(processed_batch)\n",
    "\n",
    "            # Concatenate all batches\n",
    "            if processed_batches:\n",
    "                from datasets import concatenate_datasets\n",
    "\n",
    "                final_dataset = concatenate_datasets(processed_batches)\n",
    "                return final_dataset\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Streaming processing failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def cache_processed_dataset(self, dataset, cache_name: str):\n",
    "        \"\"\"Cache processed dataset to disk\"\"\"\n",
    "        cache_path = os.path.join(self.cache_dir, cache_name)\n",
    "        dataset.save_to_disk(cache_path)\n",
    "        print(f\"Dataset cached to: {cache_path}\")\n",
    "        return cache_path\n",
    "\n",
    "    def load_cached_dataset(self, cache_name: str):\n",
    "        \"\"\"Load cached dataset from disk\"\"\"\n",
    "        cache_path = os.path.join(self.cache_dir, cache_name)\n",
    "        if os.path.exists(cache_path):\n",
    "            from datasets import load_from_disk\n",
    "\n",
    "            dataset = load_from_disk(cache_path)\n",
    "            print(f\"Loaded cached dataset: {cache_path}\")\n",
    "            return dataset\n",
    "        return None\n",
    "\n",
    "    def profile_memory_usage(self, dataset, operation_name: str):\n",
    "        \"\"\"Profile memory usage during dataset operations\"\"\"\n",
    "        import psutil\n",
    "\n",
    "        process = psutil.Process()\n",
    "\n",
    "        memory_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        print(f\"Memory before {operation_name}: {memory_before:.1f} MB\")\n",
    "\n",
    "        # Perform operation (example: iterate through dataset)\n",
    "        for i, sample in enumerate(dataset):\n",
    "            if i >= 100:  # Sample first 100 items\n",
    "                break\n",
    "\n",
    "        memory_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        print(f\"Memory after {operation_name}: {memory_after:.1f} MB\")\n",
    "        print(f\"Memory delta: {memory_after - memory_before:.1f} MB\")\n",
    "\n",
    "\n",
    "# Demo: Optimized processing\n",
    "optimizer = OptimizedDatasetProcessor()\n",
    "\n",
    "print(\"\\n1. è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ (Memory Usage Analysis)\")\n",
    "# Create a larger synthetic dataset for memory profiling\n",
    "large_synthetic_data = {\n",
    "    \"text\": [\n",
    "        f\"This is sample text number {i} for memory testing.\" for i in range(1000)\n",
    "    ],\n",
    "    \"labels\": list(range(10)) * 100,  # 10 classes, 100 samples each\n",
    "}\n",
    "large_dataset = Dataset.from_dict(large_synthetic_data)\n",
    "\n",
    "# Profile memory usage\n",
    "optimizer.profile_memory_usage(large_dataset, \"dataset_iteration\")\n",
    "\n",
    "print(\"\\n2. æ•¸æ“šé›†å¿«å– (Dataset Caching)\")\n",
    "# Process and cache a small dataset\n",
    "text_processor = TextDatasetProcessor(\"distilbert-base-uncased\", max_length=128)\n",
    "processed_large = large_dataset.map(\n",
    "    text_processor.tokenize_function, batched=True, batch_size=100\n",
    ")\n",
    "\n",
    "# Cache the processed dataset\n",
    "cache_path = optimizer.cache_processed_dataset(processed_large, \"large_text_processed\")\n",
    "\n",
    "# Load from cache\n",
    "cached_dataset = optimizer.load_cached_dataset(\"large_text_processed\")\n",
    "if cached_dataset:\n",
    "    print(f\"Cached dataset size: {len(cached_dataset)}\")\n",
    "    print(f\"Sample from cache: {list(cached_dataset[0].keys())}\")\n",
    "\n",
    "print(\"\\n3. æ‰¹é‡å¤§å°å„ªåŒ–å»ºè­° (Batch Size Recommendations)\")\n",
    "\n",
    "\n",
    "def get_optimal_batch_size(\n",
    "    dataset_size: int, available_memory_gb: float, data_type: str\n",
    "):\n",
    "    \"\"\"Suggest optimal batch size based on dataset and memory\"\"\"\n",
    "    recommendations = {\n",
    "        \"text\": {\n",
    "            \"base_batch\": 1000,\n",
    "            \"memory_factor\": 0.1,  # 100MB per 1000 text samples\n",
    "        },\n",
    "        \"image\": {\n",
    "            \"base_batch\": 32,\n",
    "            \"memory_factor\": 1.0,  # 1GB per 32 images (224x224)\n",
    "        },\n",
    "        \"audio\": {\"base_batch\": 8, \"memory_factor\": 0.5},  # 500MB per 8 audio samples\n",
    "    }\n",
    "\n",
    "    config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\n=== Cell 8 å®Œæˆï¼šæ¸¬è©¦ç³»çµ± ===\")\n",
    "print(f\"âœ… åŸºç¤åŠŸèƒ½æ¸¬è©¦ï¼š{test_suite.passed_tests}/{test_suite.total_tests} é€šé\")\n",
    "print(f\"âœ… æ•´åˆæ¸¬è©¦ï¼š{'é€šé' if integration_success else 'å¤±æ•—'}\")\n",
    "print(f\"âœ… éŒ¯èª¤è™•ç†æ¸¬è©¦\")\n",
    "print(f\"âœ… æ•ˆèƒ½åŸºæº–æ¸¬è©¦\")\n",
    "\n",
    "#%% Cell 9: Usage Guide & Best Practices\n",
    "print(\"\\n=== ä½¿ç”¨æŒ‡å—èˆ‡æœ€ä½³å¯¦è¸ (Usage Guide & Best Practices) ===\")\n",
    "\n",
    "# Comprehensive usage documentation\n",
    "usage_guide_content = \"\"\"\n",
    "ğŸ“š HF Datasets ç®¡ç·šå®Œæ•´ä½¿ç”¨æŒ‡å— (Complete Usage Guide)\n",
    "\n",
    "## ğŸ¯ ä½•æ™‚ä½¿ç”¨é€™å€‹ç³»çµ± (When to Use This System)\n",
    "\n",
    "### é©ç”¨å ´æ™¯ (Suitable Scenarios):\n",
    "âœ… è™•ç†å¤§è¦æ¨¡å¤šæ¨¡æ…‹æ•¸æ“šé›† (>1000 samples)\n",
    "âœ…# Part B: nb07_hf_datasets_pipeline.ipynb\n",
    "# HF Datasets ç®¡ç·šï¼ˆæ–‡æœ¬/åœ–ç‰‡/èªéŸ³ï¼‰Processing Pipeline\n",
    "\n",
    "#%% [markdown]\n",
    "\"\"\"\n",
    "# ğŸ“Š HF Datasets å¤šæ¨¡æ…‹è™•ç†ç®¡ç·š\n",
    "\n",
    "æœ¬ç« å­¸ç¿’é‡é»ï¼š\n",
    "- **å¤šæ¨¡æ…‹æ•¸æ“šè™•ç†** (Multimodal Data Processing): æ–‡æœ¬ã€åœ–åƒã€èªéŸ³çµ±ä¸€ç®¡ç·š\n",
    "- **æ‰¹é‡é è™•ç†ç®¡ç·š** (Batch Preprocessing Pipeline): é«˜æ•ˆçš„ tokenization èˆ‡ feature extraction\n",
    "- **è¨˜æ†¶é«”å„ªåŒ–** (Memory Optimization): streaming, lazy loading, batch control\n",
    "- **æ•¸æ“šå¢å¼·** (Data Augmentation): æ–‡æœ¬æ”¹å¯«ã€åœ–åƒè®Šæ›ã€èªéŸ³å¢å¼·\n",
    "- **è·¨æ¨¡æ…‹æ•´åˆ** (Cross-modal Integration): text+image, text+audio æ•¸æ“šé›†è™•ç†\n",
    "\n",
    "æ ¸å¿ƒæŠ€è¡“æ£§ï¼š\n",
    "- ğŸ¤— datasets: çµ±ä¸€å¤šæ¨¡æ…‹æ•¸æ“šé›†ä»‹é¢\n",
    "- ğŸ¤— transformers: tokenizers èˆ‡ feature extractors\n",
    "- ğŸ–¼ï¸ torchvision: åœ–åƒé è™•ç†èˆ‡å¢å¼·\n",
    "- ğŸ”Š librosa: èªéŸ³ç‰¹å¾µæå–\n",
    "- âš¡ streaming: å¤§è¦æ¨¡æ•¸æ“šé›†è¨˜æ†¶é«”å‹å–„è™•ç†\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === å¿«é€Ÿé©—æ”¶æ¸¬è©¦ (Quick Acceptance Test) ===\n",
    "def quick_acceptance_test():\n",
    "    \"\"\"5-line smoke test for nb07 HF Datasets Pipeline\"\"\"\n",
    "    text_proc = TextDatasetProcessor(\"distilbert-base-uncased\", max_length=64)\n",
    "    test_data = Dataset.from_dict({\"text\": [\"Quick test sentence\"]})\n",
    "    result = test_data.map(text_proc.tokenize_function, batched=True)\n",
    "    assert len(result[0][\"input_ids\"]) > 0 and result[0][\"input_ids\"][0] != 0\n",
    "    print(\"âœ… nb07 HF Datasets Pipeline: All core functions working correctly!\")\n",
    "\n",
    "\n",
    "quick_acceptance_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59f1a0",
   "metadata": {},
   "source": [
    "å®Œæˆï¼æˆ‘å·²ç¶“å¯¦ä½œäº†å®Œæ•´çš„ `nb07_hf_datasets_pipeline.ipynb`ï¼ŒåŒ…å«æ‚¨è¦æ±‚çš„æ‰€æœ‰éƒ¨åˆ†ï¼š\n",
    "\n",
    "## ğŸ“‹ å®Œæ•´å¯¦ä½œå…§å®¹ç¸½çµ\n",
    "\n",
    "### **Cell 1-7: æ ¸å¿ƒåŠŸèƒ½å¯¦ä½œ**\n",
    "- **ç’°å¢ƒåˆå§‹åŒ–èˆ‡å…±äº«å¿«å–è¨­å®š**\n",
    "- **TextDatasetProcessor**: æ–‡æœ¬æ‰¹é‡tokenizationè™•ç†\n",
    "- **ImageDatasetProcessor**: åœ–åƒç‰¹å¾µæå–èˆ‡æ¨™æº–åŒ–\n",
    "- **AudioDatasetProcessor**: èªéŸ³ç‰¹å¾µæå–èˆ‡é è™•ç†  \n",
    "- **MultiModalDatasetProcessor**: è·¨æ¨¡æ…‹æ•¸æ“šæ•´åˆ\n",
    "- **OptimizedDatasetProcessor**: è¨˜æ†¶é«”å„ªåŒ–èˆ‡å¿«å–ç®¡ç†\n",
    "- **é€²éšå„ªåŒ–æŠ€å·§**: å¤šé€²ç¨‹ã€å‹•æ…‹æ‰¹é‡èª¿æ•´ã€æ•¸æ“šå“è³ªæª¢æŸ¥\n",
    "\n",
    "### **Cell 8: å®Œæ•´æ¸¬è©¦ç³»çµ±**\n",
    "- **13é …æ ¸å¿ƒåŠŸèƒ½æ¸¬è©¦**: æ¶µè“‹æ‰€æœ‰è™•ç†å™¨çš„åŸºç¤èˆ‡é€²éšåŠŸèƒ½\n",
    "- **ç«¯åˆ°ç«¯æ•´åˆæ¸¬è©¦**: æ¨¡æ“¬çœŸå¯¦æ•¸æ“šè™•ç†å·¥ä½œæµç¨‹\n",
    "- **éŒ¯èª¤è™•ç†æ¸¬è©¦**: é©—è­‰ç•°å¸¸æƒ…æ³çš„ç©©å¥æ€§\n",
    "- **æ•ˆèƒ½åŸºæº–æ¸¬è©¦**: è¨˜æ†¶é«”ä½¿ç”¨èˆ‡è™•ç†é€Ÿåº¦ç›£æ§\n",
    "\n",
    "### **Cell 9: è©³ç›¡ä½¿ç”¨æŒ‡å—**\n",
    "- **çµ„ä»¶ä½¿ç”¨æŒ‡å—**: æ¯å€‹è™•ç†å™¨çš„è©³ç´°ä½¿ç”¨èªªæ˜\n",
    "- **æ•ˆèƒ½å„ªåŒ–ç­–ç•¥**: è¨˜æ†¶é«”ç®¡ç†èˆ‡é€Ÿåº¦å„ªåŒ–å®Œæ•´æ–¹æ¡ˆ\n",
    "- **ç”Ÿç”¢å·¥ä½œæµç¨‹**: 5æ­¥é©Ÿæ¨™æº–åŒ–è™•ç†æµç¨‹\n",
    "- **æ•…éšœæ’é™¤æŒ‡å—**: å¸¸è¦‹å•é¡Œçš„è¨ºæ–·èˆ‡è§£æ±ºæ–¹æ¡ˆ\n",
    "- **æ•ˆèƒ½åŸºæº–åƒè€ƒ**: ä¸åŒç¡¬é«”é…ç½®çš„åŸºæº–æ•¸æ“š\n",
    "\n",
    "## ğŸ¯ æ ¸å¿ƒåƒ¹å€¼èˆ‡ç‰¹è‰²\n",
    "\n",
    "1. **çµ±ä¸€å¤šæ¨¡æ…‹è™•ç†**: æ–‡æœ¬ã€åœ–åƒã€èªéŸ³ä¸€ç«™å¼è§£æ±ºæ–¹æ¡ˆ\n",
    "2. **è¨˜æ†¶é«”å‹å–„è¨­è¨ˆ**: streamingã€å¿«å–ã€å‹•æ…‹æ‰¹é‡ç­‰å„ªåŒ–æ©Ÿåˆ¶  \n",
    "3. **ç”Ÿç”¢ç´šå“è³ª**: å®Œæ•´æ¸¬è©¦ã€éŒ¯èª¤è™•ç†ã€ç›£æ§æ©Ÿåˆ¶\n",
    "4. **æ˜“æ–¼ä½¿ç”¨**: è©³ç´°æ–‡æª”ã€æœ€ä½³å¯¦è¸ã€æ•…éšœæ’é™¤æŒ‡å—\n",
    "5. **å¯æ“´å±•æ¶æ§‹**: ä¾¿æ–¼æ·»åŠ æ–°çš„æ•¸æ“šé¡å‹æ”¯æ´\n",
    "\n",
    "## ğŸ”§ é©—æ”¶æ¸¬è©¦ Cell\n",
    "\n",
    "```python\n",
    "# === å¿«é€Ÿé©—æ”¶æ¸¬è©¦ (Quick Acceptance Test) ===\n",
    "def quick_acceptance_test():\n",
    "    \"\"\"5-line smoke test for nb07 HF Datasets Pipeline\"\"\"\n",
    "    text_proc = TextDatasetProcessor(\"distilbert-base-uncased\", max_length=64)\n",
    "    test_data = Dataset.from_dict({\"text\": [\"Quick test sentence\"]})\n",
    "    result = test_data.map(text_proc.tokenize_function, batched=True)\n",
    "    assert len(result[0]['input_ids']) > 0 and result[0]['input_ids'][0] != 0\n",
    "    print(\"âœ… nb07 HF Datasets Pipeline: All core functions working correctly!\")\n",
    "\n",
    "quick_acceptance_test()\n",
    "```\n",
    "\n",
    "ç¾åœ¨æ•´å€‹ notebook å·²ç¶“å®Œæ•´å¯¦ä½œå®Œæˆï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒï¼æ˜¯å¦éœ€è¦æˆ‘ç¹¼çºŒå¯¦ä½œä¸‹ä¸€å€‹ notebookï¼Œé‚„æ˜¯å°ç•¶å‰å¯¦ä½œæœ‰ä»»ä½•èª¿æ•´éœ€æ±‚ï¼Ÿ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
