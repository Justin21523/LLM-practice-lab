{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b148826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb06_attention_transformer.ipynb\n",
    "# Attention Mechanism & Transformer Block Implementation\n",
    "# æ³¨æ„åŠ›æ©Ÿåˆ¶èˆ‡ Transformer Block å¯¦ä½œ\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 1: Environment & Cache Setup\n",
    "# =============================================================================\n",
    "\n",
    "# === Shared Cache Bootstrap (English comments only) ===\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "for k, v in paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache] Root:\", AI_CACHE_ROOT)\n",
    "print(\n",
    "    \"[GPU]\",\n",
    "    torch.cuda.is_available(),\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ffa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Dependencies & Imports\n",
    "# =============================================================================\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Scaled Dot-Product Attention Implementation\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention ç¸®æ”¾é»ç©æ³¨æ„åŠ›\n",
    "\n",
    "    Args:\n",
    "        query: [batch_size, seq_len, d_k] Query tensor\n",
    "        key: [batch_size, seq_len, d_k] Key tensor\n",
    "        value: [batch_size, seq_len, d_v] Value tensor\n",
    "        mask: Optional attention mask\n",
    "        dropout: Optional dropout layer\n",
    "\n",
    "    Returns:\n",
    "        output: [batch_size, seq_len, d_v] Attention output\n",
    "        attention_weights: [batch_size, seq_len, seq_len] Attention weights\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "\n",
    "    # Compute attention scores: Q * K^T / sqrt(d_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # Apply mask if provided (for padding tokens or causal attention)\n",
    "    if mask is not None:\n",
    "        scores.masked_fill_(mask == 0, -1e9)\n",
    "\n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Apply dropout if provided\n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "\n",
    "    # Apply attention weights to values\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# Test scaled dot-product attention\n",
    "batch_size, seq_len, d_model = 2, 8, 64\n",
    "query = torch.randn(batch_size, seq_len, d_model)\n",
    "key = torch.randn(batch_size, seq_len, d_model)\n",
    "value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(query, key, value)\n",
    "print(f\"Input shape: {query.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78bab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 4: Multi-Head Attention Module\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention å¤šé ­æ³¨æ„åŠ›æ©Ÿåˆ¶\n",
    "\n",
    "    Args:\n",
    "        d_model: Model dimension (embedding size)\n",
    "        num_heads: Number of attention heads\n",
    "        dropout_rate: Dropout rate for attention weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout for attention weights\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "\n",
    "        # Linear projections and reshape for multi-head\n",
    "        # [batch_size, seq_len, d_model] -> [batch_size, num_heads, seq_len, d_k]\n",
    "        Q = (\n",
    "            self.w_q(query)\n",
    "            .view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        K = (\n",
    "            self.w_k(key)\n",
    "            .view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        V = (\n",
    "            self.w_v(value)\n",
    "            .view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        # Adjust mask for multi-head if provided\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
    "\n",
    "        # Apply scaled dot-product attention for each head\n",
    "        attention_output, attention_weights = scaled_dot_product_attention(\n",
    "            Q, K, V, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # Concatenate heads and put through final linear layer\n",
    "        # [batch_size, num_heads, seq_len, d_k] -> [batch_size, seq_len, d_model]\n",
    "        attention_output = (\n",
    "            attention_output.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, seq_len, self.d_model)\n",
    "        )\n",
    "\n",
    "        output = self.w_o(attention_output)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "# Test Multi-Head Attention\n",
    "d_model, num_heads = 512, 8\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Create sample input\n",
    "batch_size, seq_len = 2, 10\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = mha(x, x, x)  # Self-attention\n",
    "print(f\"MHA Input shape: {x.shape}\")\n",
    "print(f\"MHA Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 5: Position Encoding (Sinusoidal)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding æ­£å¼¦ä½ç½®ç·¨ç¢¼\n",
    "\n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        max_seq_length: Maximum sequence length\n",
    "        dropout_rate: Dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_seq_length=5000, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Compute div_term for sinusoidal pattern\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Apply sin to even indices and cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add batch dimension and register as buffer\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [seq_len, batch_size, d_model] or [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        if x.dim() == 3 and x.size(0) != x.size(1):  # Assume [batch, seq, dim]\n",
    "            x = x + self.pe[: x.size(1), :].transpose(0, 1)\n",
    "        else:  # Assume [seq, batch, dim]\n",
    "            x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Test Positional Encoding\n",
    "pos_enc = PositionalEncoding(d_model=512)\n",
    "sample_input = torch.randn(2, 10, 512)  # [batch, seq, dim]\n",
    "encoded = pos_enc(sample_input)\n",
    "print(f\"Positional encoding input: {sample_input.shape}\")\n",
    "print(f\"Positional encoding output: {encoded.shape}\")\n",
    "\n",
    "# Visualize positional encoding pattern\n",
    "plt.figure(figsize=(12, 8))\n",
    "pe_vis = pos_enc.pe[:50, 0, :64].numpy()  # First 50 positions, first 64 dimensions\n",
    "sns.heatmap(pe_vis.T, cmap=\"RdYlBu\", center=0)\n",
    "plt.title(\"Positional Encoding Pattern (ä½ç½®ç·¨ç¢¼æ¨¡å¼)\")\n",
    "plt.xlabel(\"Position (ä½ç½®)\")\n",
    "plt.ylabel(\"Dimension (ç¶­åº¦)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a43b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 6: Transformer Block (Self-Attention + FFN)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer Block å®Œæ•´çš„ Transformer Block\n",
    "\n",
    "    Includes:\n",
    "    - Multi-Head Self-Attention\n",
    "    - Position-wise Feed-Forward Network\n",
    "    - Residual connections and Layer Normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
    "\n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask=None, use_checkpoint=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model] Input tensor\n",
    "            mask: Optional attention mask\n",
    "            use_checkpoint: Use gradient checkpointing for memory efficiency\n",
    "        \"\"\"\n",
    "        if use_checkpoint and self.training:\n",
    "            return checkpoint(self._forward_impl, x, mask)\n",
    "        else:\n",
    "            return self._forward_impl(x, mask)\n",
    "\n",
    "    def _forward_impl(self, x, mask=None):\n",
    "        # Multi-Head Self-Attention with residual connection and layer norm\n",
    "        attn_output, attn_weights = self.multi_head_attention(x, x, x, mask)\n",
    "        x1 = self.norm1(x + self.dropout1(attn_output))\n",
    "\n",
    "        # Feed-Forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x1)\n",
    "        x2 = self.norm2(x1 + self.dropout2(ff_output))\n",
    "\n",
    "        return x2, attn_weights\n",
    "\n",
    "\n",
    "# Test Transformer Block\n",
    "transformer_block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)\n",
    "\n",
    "# Create sample input\n",
    "batch_size, seq_len = 2, 10\n",
    "x = torch.randn(batch_size, seq_len, 512)\n",
    "\n",
    "output, attn_weights = transformer_block(x)\n",
    "print(f\"Transformer Block Input: {x.shape}\")\n",
    "print(f\"Transformer Block Output: {output.shape}\")\n",
    "print(f\"Attention Weights: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 7: Mini Transformer Model\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class MiniTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Mini Transformer Model è¿·ä½  Transformer æ¨¡å‹\n",
    "\n",
    "    A simplified transformer for demonstration purposes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        d_ff=2048,\n",
    "        max_seq_length=1000,\n",
    "        dropout_rate=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            d_model, max_seq_length, dropout_rate\n",
    "        )\n",
    "\n",
    "        # Stack of transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(d_model, num_heads, d_ff, dropout_rate)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights following standard practices\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, use_checkpoint=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len] Token indices\n",
    "            attention_mask: [batch_size, seq_len] Attention mask\n",
    "            use_checkpoint: Use gradient checkpointing for memory efficiency\n",
    "        \"\"\"\n",
    "        # Token embeddings\n",
    "        embeddings = self.token_embedding(input_ids) * math.sqrt(self.d_model)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(embeddings)\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        attention_weights_list = []\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x, attn_weights = transformer_block(x, attention_mask, use_checkpoint)\n",
    "            attention_weights_list.append(attn_weights)\n",
    "\n",
    "        # Final layer norm and output projection\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.output_projection(x)\n",
    "\n",
    "        return logits, attention_weights_list\n",
    "\n",
    "\n",
    "# Create and test mini transformer\n",
    "vocab_size = 1000\n",
    "model = MiniTransformer(vocab_size, d_model=256, num_heads=4, num_layers=2)\n",
    "\n",
    "# Sample input\n",
    "batch_size, seq_len = 2, 8\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "logits, all_attn_weights = model(input_ids)\n",
    "print(f\"Mini Transformer Input: {input_ids.shape}\")\n",
    "print(f\"Mini Transformer Output: {logits.shape}\")\n",
    "print(f\"Number of attention weight matrices: {len(all_attn_weights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135fad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 8: Attention Visualization\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def visualize_attention(attention_weights, tokens=None, layer_idx=0, head_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize attention weights è¦–è¦ºåŒ–æ³¨æ„åŠ›æ¬Šé‡\n",
    "\n",
    "    Args:\n",
    "        attention_weights: Attention weights from model\n",
    "        tokens: Optional list of tokens for labeling\n",
    "        layer_idx: Which layer to visualize\n",
    "        head_idx: Which attention head to visualize\n",
    "    \"\"\"\n",
    "    # Extract attention weights for specific layer and head\n",
    "    # Shape: [batch_size, num_heads, seq_len, seq_len]\n",
    "    attn = attention_weights[layer_idx][0, head_idx].detach().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        attn,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "    )\n",
    "    plt.title(f\"Attention Weights - Layer {layer_idx}, Head {head_idx}\")\n",
    "    plt.xlabel(\"Keys (è¢«é—œæ³¨çš„ä½ç½®)\")\n",
    "    plt.ylabel(\"Queries (æŸ¥è©¢ä½ç½®)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate sample tokens for visualization\n",
    "sample_tokens = [f\"token_{i}\" for i in range(seq_len)]\n",
    "\n",
    "# Visualize attention from the first layer, first head\n",
    "visualize_attention(all_attn_weights, sample_tokens, layer_idx=0, head_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52b76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 9: Memory Optimization & Low-VRAM Tips\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def get_model_memory_usage(model, input_shape):\n",
    "    \"\"\"Calculate approximate model memory usage\"\"\"\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    param_memory = total_params * 4 / (1024**2)  # Assuming float32, in MB\n",
    "\n",
    "    # Estimate activation memory (rough approximation)\n",
    "    batch_size, seq_len = input_shape[:2]\n",
    "    activation_memory = (\n",
    "        batch_size * seq_len * model.d_model * 4 / (1024**2) * 10\n",
    "    )  # Rough estimate\n",
    "\n",
    "    return {\n",
    "        \"parameters\": total_params,\n",
    "        \"param_memory_mb\": param_memory,\n",
    "        \"estimated_activation_mb\": activation_memory,\n",
    "        \"total_estimated_mb\": param_memory + activation_memory,\n",
    "    }\n",
    "\n",
    "\n",
    "# Check memory usage\n",
    "memory_info = get_model_memory_usage(model, (2, 8))\n",
    "print(\"Memory Usage Analysis:\")\n",
    "for key, value in memory_info.items():\n",
    "    print(f\"  {key}: {value:,.0f}\")\n",
    "\n",
    "\n",
    "# Low-VRAM configuration example\n",
    "def create_low_vram_transformer(vocab_size):\n",
    "    \"\"\"Create a memory-efficient transformer configuration\"\"\"\n",
    "    return MiniTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=128,  # Smaller model dimension\n",
    "        num_heads=4,  # Fewer attention heads\n",
    "        num_layers=2,  # Fewer layers\n",
    "        d_ff=512,  # Smaller feed-forward dimension\n",
    "        dropout_rate=0.1,\n",
    "    )\n",
    "\n",
    "\n",
    "low_vram_model = create_low_vram_transformer(vocab_size)\n",
    "low_vram_memory = get_model_memory_usage(low_vram_model, (2, 8))\n",
    "print(\"\\nLow-VRAM Model Memory Usage:\")\n",
    "for key, value in low_vram_memory.items():\n",
    "    print(f\"  {key}: {value:,.0f}\")\n",
    "\n",
    "\n",
    "# Gradient checkpointing example\n",
    "def forward_with_checkpointing(model, input_ids):\n",
    "    \"\"\"Example of using gradient checkpointing to save memory\"\"\"\n",
    "    return model(input_ids, use_checkpoint=True)\n",
    "\n",
    "\n",
    "# Memory-saving tips for training\n",
    "print(\"\\n=== Memory Optimization Tips ===\")\n",
    "print(\"1. Use gradient checkpointing: set use_checkpoint=True\")\n",
    "print(\"2. Reduce batch size: smaller batches = less memory\")\n",
    "print(\"3. Use mixed precision: torch.cuda.amp.autocast()\")\n",
    "print(\"4. Accumulate gradients: effective larger batch without memory cost\")\n",
    "print(\"5. Use smaller model dimensions: reduce d_model, num_heads, num_layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b185cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 10: Smoke Test & Validation\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def run_transformer_smoke_test():\n",
    "    \"\"\"Basic smoke test to ensure everything works\"\"\"\n",
    "    print(\"Running Transformer Smoke Test...\")\n",
    "\n",
    "    # Test 1: Basic attention mechanism\n",
    "    try:\n",
    "        q = k = v = torch.randn(1, 4, 8)\n",
    "        out, weights = scaled_dot_product_attention(q, k, v)\n",
    "        assert out.shape == (1, 4, 8), f\"Expected (1,4,8), got {out.shape}\"\n",
    "        assert weights.shape == (1, 4, 4), f\"Expected (1,4,4), got {weights.shape}\"\n",
    "        print(\"âœ“ Scaled dot-product attention works\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Attention test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Test 2: Multi-head attention\n",
    "    try:\n",
    "        mha = MultiHeadAttention(d_model=64, num_heads=4)\n",
    "        x = torch.randn(1, 8, 64)\n",
    "        out, weights = mha(x, x, x)\n",
    "        assert out.shape == (1, 8, 64), f\"Expected (1,8,64), got {out.shape}\"\n",
    "        print(\"âœ“ Multi-head attention works\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Multi-head attention test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Test 3: Transformer block\n",
    "    try:\n",
    "        block = TransformerBlock(d_model=64, num_heads=4, d_ff=128)\n",
    "        x = torch.randn(1, 8, 64)\n",
    "        out, weights = block(x)\n",
    "        assert out.shape == (1, 8, 64), f\"Expected (1,8,64), got {out.shape}\"\n",
    "        print(\"âœ“ Transformer block works\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Transformer block test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Test 4: Full model\n",
    "    try:\n",
    "        model = MiniTransformer(vocab_size=100, d_model=64, num_heads=4, num_layers=2)\n",
    "        input_ids = torch.randint(0, 100, (1, 8))\n",
    "        logits, all_weights = model(input_ids)\n",
    "        assert logits.shape == (1, 8, 100), f\"Expected (1,8,100), got {logits.shape}\"\n",
    "        assert (\n",
    "            len(all_weights) == 2\n",
    "        ), f\"Expected 2 attention weight matrices, got {len(all_weights)}\"\n",
    "        print(\"âœ“ Full transformer model works\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Full model test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    print(\"ğŸ‰ All tests passed! Transformer implementation is working correctly.\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run the smoke test\n",
    "success = run_transformer_smoke_test()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n=== Next Steps ===\")\n",
    "    print(\"1. Try modifying attention patterns with custom masks\")\n",
    "    print(\"2. Experiment with different positional encoding schemes\")\n",
    "    print(\"3. Test on real text data with tokenization\")\n",
    "    print(\"4. Move to next notebook: HF Datasets Pipeline (nb07)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd5039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation test\n",
    "def quick_test():\n",
    "    \"\"\"5-line smoke test for nb06\"\"\"\n",
    "    model = MiniTransformer(vocab_size=50, d_model=32, num_heads=2, num_layers=1)\n",
    "    input_ids = torch.randint(0, 50, (1, 4))\n",
    "    logits, weights = model(input_ids)\n",
    "    assert logits.shape == (1, 4, 50) and len(weights) == 1\n",
    "    print(\"âœ… nb06 attention & transformer working!\")\n",
    "\n",
    "\n",
    "quick_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf30158",
   "metadata": {},
   "source": [
    "### **æœ¬ç« å°çµ**\n",
    "\n",
    "**âœ… å®Œæˆé …ç›®**\n",
    "- å¾é›¶å¯¦ä½œ Scaled Dot-Product Attention ç¸®æ”¾é»ç©æ³¨æ„åŠ›æ©Ÿåˆ¶\n",
    "- æ§‹å»º Multi-Head Attention å¤šé ­æ³¨æ„åŠ›ï¼Œæ”¯æ´ä¸¦è¡Œè¨ˆç®—å¤šå€‹è¡¨ç¤ºå­ç©ºé–“  \n",
    "- å¯¦ç¾ Sinusoidal Positional Encoding æ­£å¼¦ä½ç½®ç·¨ç¢¼èˆ‡è¦–è¦ºåŒ–\n",
    "- çµ„è£å®Œæ•´ Transformer Blockï¼ŒåŒ…å«æ®˜å·®é€£æ¥èˆ‡å±¤æ­£è¦åŒ–\n",
    "- æä¾›è¨˜æ†¶é«”å„ªåŒ–æ–¹æ¡ˆï¼Œæ”¯æ´æ¢¯åº¦æª¢æŸ¥é»èˆ‡ä½é¡¯å­˜é…ç½®\n",
    "\n",
    "**ğŸ§  æ ¸å¿ƒåŸç†è¦é»**\n",
    "- **æ³¨æ„åŠ›æ©Ÿåˆ¶æœ¬è³ª**ï¼šQuery-Key-Value ä¸‰å…ƒçµ„è®“æ¨¡å‹å‹•æ…‹é—œæ³¨ç›¸é—œè³‡è¨Š\n",
    "- **å¤šé ­æ³¨æ„åŠ›å„ªå‹¢**ï¼šä¸¦è¡Œè™•ç†å¤šå€‹è¡¨ç¤ºå­ç©ºé–“ï¼Œæ•ç²ä¸åŒé¡å‹çš„é—œä¿‚æ¨¡å¼\n",
    "- **ä½ç½®ç·¨ç¢¼å¿…è¦æ€§**ï¼šTransformer ç¼ºä¹é †åºæ­¸ç´åç½®ï¼Œéœ€è¦é¡¯å¼ä½ç½®è³‡è¨Š\n",
    "- **æ®˜å·®é€£æ¥é‡è¦æ€§**ï¼šè§£æ±ºæ·±å±¤ç¶²è·¯æ¢¯åº¦æ¶ˆå¤±å•é¡Œï¼Œç©©å®šè¨“ç·´éç¨‹\n",
    "\n",
    "**âš ï¸ å¸¸è¦‹é™·é˜±**\n",
    "- æ³¨æ„åŠ›æ¬Šé‡è¨˜æ†¶é«”æ¶ˆè€—ï¼šO(nÂ²) è¤‡é›œåº¦ï¼Œé•·åºåˆ—æ™‚éœ€è¦æª¢æŸ¥è¨˜æ†¶é«”\n",
    "- ç¶­åº¦å¿…é ˆæ•´é™¤ï¼š`d_model` å¿…é ˆè¢« `num_heads` æ•´é™¤\n",
    "- é®ç½©ç¶­åº¦å°é½Šï¼šå¤šé ­æ³¨æ„åŠ›æ™‚é®ç½©éœ€è¦æ­£ç¢ºå»£æ’­\n",
    "- æ¢¯åº¦çˆ†ç‚¸ï¼šæ·±å±¤æ¨¡å‹éœ€è¦é©ç•¶çš„å­¸ç¿’ç‡èˆ‡åˆå§‹åŒ–\n",
    "\n",
    "**ğŸš€ ä¸‹ä¸€æ­¥å»ºè­°**\n",
    "1. **ç«‹å³è¡Œå‹•**ï¼šé€²å…¥ nb07 å­¸ç¿’ HF Datasets å¤šæ¨¡æ…‹è³‡æ–™è™•ç†ç®¡ç·š\n",
    "2. **å»¶ä¼¸å¯¦é©—**ï¼šå˜—è©¦ Rotary Position Embedding (RoPE) æ›¿ä»£æ–¹æ¡ˆ\n",
    "3. **æ•ˆèƒ½å„ªåŒ–**ï¼šæ¸¬è©¦ Flash Attention ç­‰é«˜æ•ˆå¯¦ä½œ\n",
    "4. **æ‡‰ç”¨å ´æ™¯**ï¼šæº–å‚™å°‡æ­¤åŸºç¤æ‡‰ç”¨åˆ°å¯¦éš› LLM æ¨¡å‹è¼‰å…¥èˆ‡æ¨ç†\n",
    "\n",
    "é€™å€‹ notebook ç‚ºå¾ŒçºŒçš„ HF æ¨¡å‹æ“ä½œèˆ‡ LLM æ‡‰ç”¨æ‰“ä¸‹äº†å …å¯¦çš„ç†è«–èˆ‡å¯¦ä½œåŸºç¤ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
