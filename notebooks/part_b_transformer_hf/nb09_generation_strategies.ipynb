{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation Strategies & Parameter Tuning\n",
    "# ÁîüÊàêÁ≠ñÁï•ËàáÂèÉÊï∏Ë™øÂÑ™\n",
    "# ================================================================\n",
    "\n",
    "\"\"\"\n",
    "Learning Goals (Â≠∏ÁøíÁõÆÊ®ô):\n",
    "1. Master text generation strategies: Temperature, Top-k, Top-p, repetition penalty\n",
    "2. Implement interactive parameter tuning with real-time feedback\n",
    "3. Build evaluation framework for generation quality assessment\n",
    "4. Support low-VRAM environments with 4-bit quantization\n",
    "5. Compare optimal parameters for Chinese vs English generation\n",
    "\"\"\"\n",
    "\n",
    "# === Shared Cache Bootstrap (English comments only) ===\n",
    "import os, torch, platform, pathlib\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "for k, v in paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache] Root:\", AI_CACHE_ROOT)\n",
    "print(\n",
    "    \"[GPU]\",\n",
    "    torch.cuda.is_available(),\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d477f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: Environment Setup ===\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üîß Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Model & Tokenizer Loading (Low-VRAM Friendly) ===\n",
    "\n",
    "\n",
    "class GenerationTester:\n",
    "    \"\"\"Unified model loader with low-VRAM support\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        use_4bit: bool = None,\n",
    "        device_map: str = \"auto\",\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Auto-detect 4bit need based on VRAM\n",
    "        if use_4bit is None:\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "                use_4bit = gpu_memory < 12  # Use 4bit if < 12GB VRAM\n",
    "            else:\n",
    "                use_4bit = False\n",
    "\n",
    "        self.use_4bit = use_4bit\n",
    "        self._load_model_and_tokenizer(device_map)\n",
    "\n",
    "    def _load_model_and_tokenizer(self, device_map: str):\n",
    "        \"\"\"Load model with optimal configuration\"\"\"\n",
    "        print(f\"üì• Loading {self.model_name}...\")\n",
    "        print(f\"üîß 4-bit quantization: {self.use_4bit}\")\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name, trust_remote_code=True, padding_side=\"left\"\n",
    "        )\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Configure model loading\n",
    "        model_kwargs = {\n",
    "            \"trust_remote_code\": True,\n",
    "            \"torch_dtype\": (\n",
    "                torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "            ),\n",
    "            \"device_map\": device_map if torch.cuda.is_available() else None,\n",
    "        }\n",
    "\n",
    "        if self.use_4bit and torch.cuda.is_available():\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            model_kwargs[\"quantization_config\"] = quantization_config\n",
    "\n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name, **model_kwargs\n",
    "        )\n",
    "\n",
    "        if not torch.cuda.is_available():\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "        print(f\"‚úÖ Model loaded successfully\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"üíæ VRAM usage: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "\n",
    "\n",
    "# Initialize model (fallback chain for compatibility)\n",
    "model_candidates = [\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",  # Lightweight, good for demos\n",
    "    \"microsoft/DialoGPT-medium\",  # Fallback option\n",
    "    \"gpt2\",  # Ultimate fallback\n",
    "]\n",
    "\n",
    "generator = None\n",
    "for model_name in model_candidates:\n",
    "    try:\n",
    "        generator = GenerationTester(model_name)\n",
    "        print(f\"üéØ Using model: {model_name}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "if generator is None:\n",
    "    raise RuntimeError(\"‚ùå No compatible model found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1407627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Generation Strategy Functions ===\n",
    "\n",
    "\n",
    "class GenerationStrategies:\n",
    "    \"\"\"Core generation strategies with parameter explanations\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def temperature_sampling(\n",
    "        logits: torch.Tensor, temperature: float = 1.0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Temperature scaling (Ê∫´Â∫¶Á∏ÆÊîæ):\n",
    "        - temperature < 1.0: More conservative, focused output\n",
    "        - temperature = 1.0: Original distribution\n",
    "        - temperature > 1.0: More random, creative output\n",
    "        \"\"\"\n",
    "        if temperature == 0:\n",
    "            return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        return torch.multinomial(torch.softmax(logits / temperature, dim=-1), 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def top_k_filtering(logits: torch.Tensor, top_k: int = 50) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Top-k filtering (ÂâçkÈÅ∏Êìá):\n",
    "        Keep only top-k highest probability tokens, set others to -inf\n",
    "        \"\"\"\n",
    "        if top_k <= 0:\n",
    "            return logits\n",
    "\n",
    "        # Get top-k values and indices\n",
    "        top_k = min(top_k, logits.size(-1))\n",
    "        values, indices = torch.topk(logits, top_k, dim=-1)\n",
    "\n",
    "        # Create mask for non-top-k tokens\n",
    "        mask = torch.full_like(logits, float(\"-inf\"))\n",
    "        mask.scatter_(-1, indices, values)\n",
    "        return mask\n",
    "\n",
    "    @staticmethod\n",
    "    def nucleus_sampling(logits: torch.Tensor, top_p: float = 0.9) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Nucleus (top-p) sampling (Ê†∏ÂøÉÊé°Ê®£):\n",
    "        Keep tokens until cumulative probability reaches top_p\n",
    "        \"\"\"\n",
    "        if top_p >= 1.0:\n",
    "            return logits\n",
    "\n",
    "        # Sort logits in descending order\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "        sorted_probs = torch.softmax(sorted_logits, dim=-1)\n",
    "\n",
    "        # Calculate cumulative probabilities\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        # Find cutoff point\n",
    "        cutoff_mask = cumulative_probs > top_p\n",
    "        cutoff_mask[..., 1:] = cutoff_mask[..., :-1].clone()\n",
    "        cutoff_mask[..., 0] = False\n",
    "\n",
    "        # Set tokens beyond cutoff to -inf\n",
    "        sorted_logits[cutoff_mask] = float(\"-inf\")\n",
    "\n",
    "        # Unsort to original order\n",
    "        original_logits = torch.full_like(logits, float(\"-inf\"))\n",
    "        original_logits.scatter_(-1, sorted_indices, sorted_logits)\n",
    "        return original_logits\n",
    "\n",
    "\n",
    "print(\"üß© Generation strategies defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec20fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Interactive Generation Tester ===\n",
    "\n",
    "\n",
    "def generate_with_strategies(\n",
    "    prompt: str,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 0.9,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    max_length: int = 100,\n",
    "    num_return_sequences: int = 3,\n",
    "    do_sample: bool = True,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate text with specified strategies\n",
    "    ÁîüÊàêÁ≠ñÁï•ÂèÉÊï∏Ë™™Êòé:\n",
    "    - temperature: ÊéßÂà∂Èö®Ê©üÊÄß (0.1=‰øùÂÆà, 1.0=Âπ≥Ë°°, 2.0=ÂâµÊÑè)\n",
    "    - top_k: ÂÄôÈÅ∏Ë©ûÂΩôÊï∏ÈáèÈôêÂà∂ (0=ÁÑ°ÈôêÂà∂, 50=‰∏≠Á≠â, 10=Âö¥Ê†º)\n",
    "    - top_p: Á¥ØÁ©çÊ©üÁéáÈñæÂÄº (0.9=Âπ≥Ë°°, 0.5=‰øùÂÆà, 0.95=ÂØ¨È¨Ü)\n",
    "    - repetition_penalty: ÈáçË§áÊá≤ÁΩ∞ (1.0=ÁÑ°Êá≤ÁΩ∞, 1.1=ËºïÂæÆ, 1.5=Âö¥Ê†º)\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = generator.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to(generator.device)\n",
    "\n",
    "    # Generation parameters\n",
    "    generation_kwargs = {\n",
    "        \"max_length\": len(inputs[0]) + max_length,\n",
    "        \"num_return_sequences\": num_return_sequences,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"temperature\": temperature if do_sample else 1.0,\n",
    "        \"top_k\": top_k if do_sample else 0,\n",
    "        \"top_p\": top_p if do_sample else 1.0,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "        \"pad_token_id\": generator.tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": generator.tokenizer.eos_token_id,\n",
    "        \"no_repeat_ngram_size\": 2,  # Prevent 2-gram repetition\n",
    "    }\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        outputs = generator.model.generate(inputs, **generation_kwargs)\n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "    # Decode outputs\n",
    "    generated_texts = []\n",
    "    input_length = len(inputs[0])\n",
    "\n",
    "    for output in outputs:\n",
    "        text = generator.tokenizer.decode(\n",
    "            output[input_length:], skip_special_tokens=True\n",
    "        )\n",
    "        generated_texts.append(text.strip())\n",
    "\n",
    "    print(f\"‚è±Ô∏è Generation time: {generation_time:.2f}s\")\n",
    "    return generated_texts\n",
    "\n",
    "\n",
    "# Test with different prompts\n",
    "test_prompts = {\n",
    "    \"en_creative\": \"Once upon a time in a magical forest,\",\n",
    "    \"en_factual\": \"The process of photosynthesis involves\",\n",
    "    \"zh_creative\": \"ÂæûÂâçÊúâ‰∏ÄÂÄãÁ•ûÂ•áÁöÑÊ£ÆÊûóÔºå\",\n",
    "    \"zh_factual\": \"ÂÖâÂêà‰ΩúÁî®ÁöÑÈÅéÁ®ãÂåÖÊã¨\",\n",
    "}\n",
    "\n",
    "print(\"üéÆ Interactive generation tester ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9917e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Parameter Comparison Demo ===\n",
    "\n",
    "\n",
    "def compare_generation_strategies():\n",
    "    \"\"\"Compare different generation strategies side by side\"\"\"\n",
    "\n",
    "    prompt = \"The future of artificial intelligence will\"\n",
    "\n",
    "    # Different parameter configurations\n",
    "    configs = {\n",
    "        \"Conservative (‰øùÂÆà)\": {\n",
    "            \"temperature\": 0.3,\n",
    "            \"top_k\": 10,\n",
    "            \"top_p\": 0.8,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "        },\n",
    "        \"Balanced (Âπ≥Ë°°)\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.9,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "        },\n",
    "        \"Creative (ÂâµÊÑè)\": {\n",
    "            \"temperature\": 1.2,\n",
    "            \"top_k\": 100,\n",
    "            \"top_p\": 0.95,\n",
    "            \"repetition_penalty\": 1.05,\n",
    "        },\n",
    "        \"Greedy (Ë≤™Â©™)\": {\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_k\": 1,\n",
    "            \"top_p\": 1.0,\n",
    "            \"repetition_penalty\": 1.0,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(f\"üéØ Prompt: '{prompt}'\\n\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    results = {}\n",
    "    for config_name, params in configs.items():\n",
    "        print(f\"\\nüìä {config_name} Configuration:\")\n",
    "        print(\n",
    "            f\"   Temperature: {params['temperature']}, Top-k: {params['top_k']}, \"\n",
    "            f\"Top-p: {params['top_p']}, Rep. Penalty: {params['repetition_penalty']}\"\n",
    "        )\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        generated = generate_with_strategies(\n",
    "            prompt=prompt, num_return_sequences=2, max_length=50, **params\n",
    "        )\n",
    "\n",
    "        results[config_name] = generated\n",
    "        for i, text in enumerate(generated, 1):\n",
    "            print(f\"   {i}. {text}\")\n",
    "        print()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run comparison\n",
    "comparison_results = compare_generation_strategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea854a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Generation Quality Metrics ===\n",
    "\n",
    "\n",
    "class GenerationMetrics:\n",
    "    \"\"\"Evaluate generation quality with multiple metrics\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_diversity(texts: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate lexical diversity metrics\"\"\"\n",
    "        if not texts:\n",
    "            return {\"distinct_1\": 0, \"distinct_2\": 0, \"distinct_3\": 0}\n",
    "\n",
    "        # Combine all texts\n",
    "        all_tokens = []\n",
    "        for text in texts:\n",
    "            tokens = text.lower().split()\n",
    "            all_tokens.extend(tokens)\n",
    "\n",
    "        if not all_tokens:\n",
    "            return {\"distinct_1\": 0, \"distinct_2\": 0, \"distinct_3\": 0}\n",
    "\n",
    "        # Calculate distinct n-grams\n",
    "        def get_ngrams(tokens: List[str], n: int) -> List[str]:\n",
    "            return [\" \".join(tokens[i : i + n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "        distinct_1 = len(set(all_tokens)) / len(all_tokens) if all_tokens else 0\n",
    "\n",
    "        bigrams = get_ngrams(all_tokens, 2)\n",
    "        distinct_2 = len(set(bigrams)) / len(bigrams) if bigrams else 0\n",
    "\n",
    "        trigrams = get_ngrams(all_tokens, 3)\n",
    "        distinct_3 = len(set(trigrams)) / len(trigrams) if trigrams else 0\n",
    "\n",
    "        return {\n",
    "            \"distinct_1\": distinct_1,\n",
    "            \"distinct_2\": distinct_2,\n",
    "            \"distinct_3\": distinct_3,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_repetition_score(text: str) -> float:\n",
    "        \"\"\"Calculate repetition penalty score (lower is better)\"\"\"\n",
    "        tokens = text.lower().split()\n",
    "        if len(tokens) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        # Count repeated adjacent tokens\n",
    "        repetitions = sum(\n",
    "            1 for i in range(len(tokens) - 1) if tokens[i] == tokens[i + 1]\n",
    "        )\n",
    "        return repetitions / (len(tokens) - 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_coherence_score(text: str) -> float:\n",
    "        \"\"\"Simple coherence score based on sentence structure\"\"\"\n",
    "        # Count complete sentences (basic heuristic)\n",
    "        sentences = re.split(r\"[.!?]+\", text.strip())\n",
    "        complete_sentences = [s.strip() for s in sentences if len(s.strip()) > 5]\n",
    "\n",
    "        if not complete_sentences:\n",
    "            return 0.0\n",
    "\n",
    "        # Simple coherence: average sentence length and punctuation usage\n",
    "        avg_length = np.mean([len(s.split()) for s in complete_sentences])\n",
    "        punctuation_ratio = (\n",
    "            len(re.findall(r\"[.!?,:;]\", text)) / len(text) if text else 0\n",
    "        )\n",
    "\n",
    "        # Normalize score (this is a simplified metric)\n",
    "        coherence = min(1.0, (avg_length / 20) * 0.7 + punctuation_ratio * 10 * 0.3)\n",
    "        return coherence\n",
    "\n",
    "\n",
    "def evaluate_generation_quality(texts: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Comprehensive quality evaluation\"\"\"\n",
    "    metrics = GenerationMetrics()\n",
    "\n",
    "    # Calculate diversity\n",
    "    diversity = metrics.calculate_diversity(texts)\n",
    "\n",
    "    # Calculate average repetition and coherence\n",
    "    repetition_scores = [metrics.calculate_repetition_score(text) for text in texts]\n",
    "    coherence_scores = [metrics.calculate_coherence_score(text) for text in texts]\n",
    "\n",
    "    results = {\n",
    "        **diversity,\n",
    "        \"avg_repetition\": np.mean(repetition_scores),\n",
    "        \"avg_coherence\": np.mean(coherence_scores),\n",
    "        \"text_length_avg\": np.mean([len(text.split()) for text in texts]),\n",
    "        \"text_length_std\": np.std([len(text.split()) for text in texts]),\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Evaluate our comparison results\n",
    "print(\"üìä Quality Evaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for config_name, generated_texts in comparison_results.items():\n",
    "    metrics = evaluate_generation_quality(generated_texts)\n",
    "    print(f\"\\n{config_name}:\")\n",
    "    print(f\"  Distinct-1: {metrics['distinct_1']:.3f}\")\n",
    "    print(f\"  Distinct-2: {metrics['distinct_2']:.3f}\")\n",
    "    print(f\"  Repetition: {metrics['avg_repetition']:.3f}\")\n",
    "    print(f\"  Coherence:  {metrics['avg_coherence']:.3f}\")\n",
    "    print(f\"  Avg Length: {metrics['text_length_avg']:.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Parameter Sensitivity Analysis ===\n",
    "\n",
    "\n",
    "def parameter_sensitivity_analysis():\n",
    "    \"\"\"Analyze how parameters affect generation quality\"\"\"\n",
    "\n",
    "    base_prompt = \"The benefits of renewable energy include\"\n",
    "    base_params = {\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"max_length\": 40,\n",
    "    }\n",
    "\n",
    "    # Temperature sensitivity\n",
    "    temperatures = [0.1, 0.3, 0.5, 0.7, 1.0, 1.3, 1.6, 2.0]\n",
    "    temp_results = []\n",
    "\n",
    "    print(\"üå°Ô∏è Temperature Sensitivity Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for temp in temperatures:\n",
    "        params = base_params.copy()\n",
    "        params[\"temperature\"] = temp\n",
    "\n",
    "        generated = generate_with_strategies(\n",
    "            prompt=base_prompt, num_return_sequences=3, **params\n",
    "        )\n",
    "\n",
    "        metrics = evaluate_generation_quality(generated)\n",
    "        temp_results.append(\n",
    "            {\n",
    "                \"temperature\": temp,\n",
    "                \"distinct_1\": metrics[\"distinct_1\"],\n",
    "                \"repetition\": metrics[\"avg_repetition\"],\n",
    "                \"coherence\": metrics[\"avg_coherence\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"T={temp:.1f}: Distinct={metrics['distinct_1']:.3f}, \"\n",
    "            f\"Rep={metrics['avg_repetition']:.3f}, Coh={metrics['avg_coherence']:.3f}\"\n",
    "        )\n",
    "\n",
    "    return temp_results\n",
    "\n",
    "\n",
    "# Run sensitivity analysis\n",
    "sensitivity_results = parameter_sensitivity_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f68c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Visualization of Results ===\n",
    "\n",
    "\n",
    "def plot_parameter_effects(results: List[Dict]):\n",
    "    \"\"\"Visualize parameter effects on generation quality\"\"\"\n",
    "\n",
    "    # Extract data\n",
    "    temperatures = [r[\"temperature\"] for r in results]\n",
    "    distinct_scores = [r[\"distinct_1\"] for r in results]\n",
    "    repetition_scores = [r[\"repetition\"] for r in results]\n",
    "    coherence_scores = [r[\"coherence\"] for r in results]\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # Plot 1: Diversity vs Temperature\n",
    "    axes[0].plot(temperatures, distinct_scores, \"b-o\", linewidth=2, markersize=6)\n",
    "    axes[0].set_xlabel(\"Temperature (Ê∫´Â∫¶)\")\n",
    "    axes[0].set_ylabel(\"Distinct-1 Score (Â§öÊ®£ÊÄß)\")\n",
    "    axes[0].set_title(\"Diversity vs Temperature\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Repetition vs Temperature\n",
    "    axes[1].plot(temperatures, repetition_scores, \"r-s\", linewidth=2, markersize=6)\n",
    "    axes[1].set_xlabel(\"Temperature (Ê∫´Â∫¶)\")\n",
    "    axes[1].set_ylabel(\"Repetition Score (ÈáçË§áÂ∫¶)\")\n",
    "    axes[1].set_title(\"Repetition vs Temperature\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Coherence vs Temperature\n",
    "    axes[2].plot(temperatures, coherence_scores, \"g-^\", linewidth=2, markersize=6)\n",
    "    axes[2].set_xlabel(\"Temperature (Ê∫´Â∫¶)\")\n",
    "    axes[2].set_ylabel(\"Coherence Score (ÈÄ£Ë≤´ÊÄß)\")\n",
    "    axes[2].set_title(\"Coherence vs Temperature\")\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print optimal temperature suggestion\n",
    "    optimal_temp = temperatures[np.argmax(distinct_scores)]\n",
    "    print(f\"\\nüéØ Suggested optimal temperature: {optimal_temp}\")\n",
    "    print(f\"   (Based on highest diversity score)\")\n",
    "\n",
    "\n",
    "# Create visualization\n",
    "plot_parameter_effects(sensitivity_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9: Chinese vs English Parameter Optimization ===\n",
    "\n",
    "\n",
    "def compare_language_parameters():\n",
    "    \"\"\"Compare optimal parameters for Chinese vs English generation\"\"\"\n",
    "\n",
    "    prompts = {\n",
    "        \"english\": \"The key advantages of machine learning are\",\n",
    "        \"chinese\": \"Ê©üÂô®Â≠∏ÁøíÁöÑ‰∏ªË¶ÅÂÑ™Âã¢ÂåÖÊã¨\",\n",
    "    }\n",
    "\n",
    "    # Test different configurations\n",
    "    configs = [\n",
    "        {\"name\": \"Low Temp\", \"temperature\": 0.3, \"top_k\": 20, \"top_p\": 0.8},\n",
    "        {\"name\": \"Med Temp\", \"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.9},\n",
    "        {\"name\": \"High Temp\", \"temperature\": 1.2, \"top_k\": 100, \"top_p\": 0.95},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for lang, prompt in prompts.items():\n",
    "        print(f\"\\nüåç Testing {lang.upper()} generation:\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        lang_results = []\n",
    "        for config in configs:\n",
    "            generated = generate_with_strategies(\n",
    "                prompt=prompt,\n",
    "                num_return_sequences=3,\n",
    "                max_length=30,\n",
    "                repetition_penalty=1.1,\n",
    "                **{k: v for k, v in config.items() if k != \"name\"},\n",
    "            )\n",
    "\n",
    "            metrics = evaluate_generation_quality(generated)\n",
    "            lang_results.append(\n",
    "                {\n",
    "                    \"config\": config[\"name\"],\n",
    "                    \"metrics\": metrics,\n",
    "                    \"sample\": (\n",
    "                        generated[0][:100] + \"...\"\n",
    "                        if len(generated[0]) > 100\n",
    "                        else generated[0]\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"{config['name']}: Distinct={metrics['distinct_1']:.3f}, \"\n",
    "                f\"Coherence={metrics['avg_coherence']:.3f}\"\n",
    "            )\n",
    "            print(f\"  Sample: {generated[0][:80]}...\")\n",
    "            print()\n",
    "\n",
    "        results[lang] = lang_results\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run language comparison\n",
    "language_results = compare_language_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5feb32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 10: Best Practices & Recommendations ===\n",
    "\n",
    "\n",
    "def print_generation_best_practices():\n",
    "    \"\"\"Print generation strategy best practices and recommendations\"\"\"\n",
    "\n",
    "    practices = {\n",
    "        \"üéØ Task-Specific Recommendations\": [\n",
    "            \"Creative Writing: temperature=1.0-1.5, top_p=0.95, top_k=100+\",\n",
    "            \"Factual Q&A: temperature=0.3-0.7, top_p=0.8-0.9, top_k=20-50\",\n",
    "            \"Code Generation: temperature=0.1-0.5, top_p=0.8, repetition_penalty=1.2\",\n",
    "            \"Dialogue: temperature=0.7-1.0, top_p=0.9, top_k=50, repetition_penalty=1.1\",\n",
    "        ],\n",
    "        \"üå°Ô∏è Temperature Guidelines\": [\n",
    "            \"0.1-0.3: Deterministic, focused (good for factual tasks)\",\n",
    "            \"0.5-0.8: Balanced creativity and consistency\",\n",
    "            \"1.0-1.5: Creative, diverse (good for brainstorming)\",\n",
    "            \"1.5+: Very random (experimental, may lack coherence)\",\n",
    "        ],\n",
    "        \"üî¢ Top-k/Top-p Balance\": [\n",
    "            \"High diversity: top_k=100+, top_p=0.95+\",\n",
    "            \"Moderate diversity: top_k=50, top_p=0.9\",\n",
    "            \"Conservative: top_k=10-20, top_p=0.7-0.8\",\n",
    "            \"Use both together: top_k filters noise, top_p ensures quality\",\n",
    "        ],\n",
    "        \"üö´ Repetition Control\": [\n",
    "            \"repetition_penalty=1.0: No penalty (natural repetition allowed)\",\n",
    "            \"repetition_penalty=1.1: Light penalty (recommended default)\",\n",
    "            \"repetition_penalty=1.2-1.5: Strong penalty (for repetitive models)\",\n",
    "            \"no_repeat_ngram_size=2-3: Prevent exact phrase repetition\",\n",
    "        ],\n",
    "        \"üåç Language-Specific Tips\": [\n",
    "            \"Chinese: Slightly higher temperature (0.8-1.2) for natural flow\",\n",
    "            \"English: Standard parameters work well across tasks\",\n",
    "            \"Code: Lower temperature (0.1-0.5) for syntax correctness\",\n",
    "            \"Multilingual: Test parameters per language separately\",\n",
    "        ],\n",
    "        \"‚ö° Performance Optimization\": [\n",
    "            \"Batch multiple sequences: num_return_sequences=3-5\",\n",
    "            \"Use caching: past_key_values for multi-turn conversations\",\n",
    "            \"Memory management: Clear cache between long generations\",\n",
    "            \"4-bit quantization: Minimal quality loss, 4x memory savings\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(\"üìö GENERATION STRATEGIES BEST PRACTICES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for category, tips in practices.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        print(\"-\" * 40)\n",
    "        for tip in tips:\n",
    "            print(f\"  ‚Ä¢ {tip}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "print_generation_best_practices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 11: Smoke Test & Validation ===\n",
    "\n",
    "\n",
    "def run_smoke_test():\n",
    "    \"\"\"Quick validation that all generation strategies work correctly\"\"\"\n",
    "    print(\"üß™ Running Generation Strategies Smoke Test...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    test_prompt = \"AI technology will\"\n",
    "\n",
    "    # Test 1: Basic generation\n",
    "    try:\n",
    "        basic_output = generate_with_strategies(\n",
    "            prompt=test_prompt, temperature=0.7, max_length=20, num_return_sequences=1\n",
    "        )\n",
    "        assert len(basic_output) == 1\n",
    "        assert len(basic_output[0]) > 0\n",
    "        print(\"‚úÖ Basic generation: PASS\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Basic generation: FAIL - {e}\")\n",
    "        return False\n",
    "\n",
    "    # Test 2: Parameter variations\n",
    "    try:\n",
    "        for temp in [0.1, 1.0, 1.5]:\n",
    "            output = generate_with_strategies(\n",
    "                prompt=test_prompt,\n",
    "                temperature=temp,\n",
    "                max_length=15,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "            assert len(output[0]) > 0\n",
    "        print(\"‚úÖ Temperature variations: PASS\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Temperature variations: FAIL - {e}\")\n",
    "        return False\n",
    "\n",
    "    # Test 3: Quality metrics\n",
    "    try:\n",
    "        test_texts = [\"Hello world test\", \"Another test sentence here\"]\n",
    "        metrics = evaluate_generation_quality(test_texts)\n",
    "        required_keys = [\"distinct_1\", \"avg_repetition\", \"avg_coherence\"]\n",
    "        assert all(key in metrics for key in required_keys)\n",
    "        print(\"‚úÖ Quality metrics: PASS\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Quality metrics: FAIL - {e}\")\n",
    "        return False\n",
    "\n",
    "    # Test 4: Memory efficiency\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            initial_memory = torch.cuda.memory_allocated()\n",
    "            # Generate longer sequence\n",
    "            output = generate_with_strategies(\n",
    "                prompt=test_prompt, max_length=50, num_return_sequences=2\n",
    "            )\n",
    "            final_memory = torch.cuda.memory_allocated()\n",
    "            memory_increase = (final_memory - initial_memory) / 1e6  # MB\n",
    "            print(f\"‚úÖ Memory efficiency: PASS (increase: {memory_increase:.1f}MB)\")\n",
    "        else:\n",
    "            print(\"‚úÖ Memory efficiency: PASS (CPU mode)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Memory efficiency: FAIL - {e}\")\n",
    "        return False\n",
    "\n",
    "    print(\"\\nüéâ All smoke tests passed!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_result = run_smoke_test()\n",
    "\n",
    "# === Summary Cell ===\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã NOTEBOOK COMPLETION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "completed_items = [\n",
    "    \"‚úÖ Loaded generation-capable model with low-VRAM support\",\n",
    "    \"‚úÖ Implemented core generation strategies (temperature, top-k, top-p)\",\n",
    "    \"‚úÖ Built interactive parameter comparison system\",\n",
    "    \"‚úÖ Created quality evaluation metrics (diversity, repetition, coherence)\",\n",
    "    \"‚úÖ Performed parameter sensitivity analysis\",\n",
    "    \"‚úÖ Compared Chinese vs English generation parameters\",\n",
    "    \"‚úÖ Established best practices for different generation tasks\",\n",
    "    \"‚úÖ Validated implementation with comprehensive smoke tests\",\n",
    "]\n",
    "\n",
    "print(\"\\nüìä Key Learnings & Concepts:\")\n",
    "key_concepts = [\n",
    "    \"üå°Ô∏è Temperature controls randomness: lower=focused, higher=creative\",\n",
    "    \"üî¢ Top-k limits vocabulary size, top-p uses probability mass\",\n",
    "    \"üö´ Repetition penalty prevents monotonous outputs\",\n",
    "    \"üìà Quality metrics: diversity, coherence, repetition scores\",\n",
    "    \"üåç Language-specific parameter optimization needed\",\n",
    "    \"‚ö° 4-bit quantization enables low-VRAM generation\",\n",
    "]\n",
    "\n",
    "for concept in key_concepts:\n",
    "    print(f\"  {concept}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Common Pitfalls & Solutions:\")\n",
    "pitfalls = [\n",
    "    \"High temperature (>1.5) ‚Üí incoherent text ‚Üí use 0.7-1.2 range\",\n",
    "    \"Low top-k (<10) ‚Üí repetitive output ‚Üí combine with top-p\",\n",
    "    \"Excessive repetition penalty ‚Üí unnatural flow ‚Üí keep ‚â§1.3\",\n",
    "    \"Ignoring language differences ‚Üí poor quality ‚Üí test per language\",\n",
    "    \"VRAM overflow ‚Üí crashes ‚Üí use 4-bit quantization\",\n",
    "    \"No evaluation metrics ‚Üí can't optimize ‚Üí implement quality scoring\",\n",
    "]\n",
    "\n",
    "for pitfall in pitfalls:\n",
    "    print(f\"  ‚ö†Ô∏è {pitfall}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps & Recommendations:\")\n",
    "next_steps = [\n",
    "    \"1. Implement custom sampling strategies (contrastive search, typical sampling)\",\n",
    "    \"2. Add beam search for deterministic high-quality generation\",\n",
    "    \"3. Create parameter auto-tuning based on task classification\",\n",
    "    \"4. Integrate with instruction-tuned models for better control\",\n",
    "    \"5. Build real-time generation monitoring dashboard\",\n",
    "    \"6. Experiment with mixture of experts for dynamic parameter selection\",\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"  {step}\")\n",
    "\n",
    "print(\n",
    "    f\"\\nüíæ Final VRAM usage: {torch.cuda.memory_allocated()/1e9:.2f}GB\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"\\nüíæ Running on CPU mode\"\n",
    ")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FINAL SMOKE TEST ===\n",
    "def final_acceptance_test():\n",
    "    \"\"\"Comprehensive acceptance test for nb09_generation_strategies\"\"\"\n",
    "    print(\"üèÅ Final Acceptance Test - Generation Strategies\")\n",
    "    print(\"=\" * 55)\n",
    "\n",
    "    # Test core functionality\n",
    "    test_prompt = \"The future of technology\"\n",
    "\n",
    "    # 1. Multi-strategy generation\n",
    "    strategies = [\n",
    "        {\"name\": \"Conservative\", \"temperature\": 0.3, \"top_k\": 20, \"top_p\": 0.8},\n",
    "        {\"name\": \"Balanced\", \"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.9},\n",
    "        {\"name\": \"Creative\", \"temperature\": 1.2, \"top_k\": 100, \"top_p\": 0.95},\n",
    "    ]\n",
    "\n",
    "    all_passed = True\n",
    "    for strategy in strategies:\n",
    "        try:\n",
    "            output = generate_with_strategies(\n",
    "                prompt=test_prompt,\n",
    "                max_length=25,\n",
    "                num_return_sequences=1,\n",
    "                **{k: v for k, v in strategy.items() if k != \"name\"},\n",
    "            )\n",
    "            assert len(output[0]) > 5, f\"Output too short for {strategy['name']}\"\n",
    "            print(f\"‚úÖ {strategy['name']} strategy: PASS\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {strategy['name']} strategy: FAIL - {e}\")\n",
    "            all_passed = False\n",
    "\n",
    "    # 2. Quality metrics validation\n",
    "    try:\n",
    "        sample_texts = [\"AI will transform society\", \"Technology advances rapidly\"]\n",
    "        metrics = evaluate_generation_quality(sample_texts)\n",
    "        assert 0 <= metrics[\"distinct_1\"] <= 1, \"Invalid distinct-1 score\"\n",
    "        assert metrics[\"avg_repetition\"] >= 0, \"Invalid repetition score\"\n",
    "        print(\"‚úÖ Quality metrics: PASS\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Quality metrics: FAIL - {e}\")\n",
    "        all_passed = False\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "\n",
    "# Run final test\n",
    "if final_acceptance_test():\n",
    "    print(\"\\nüéâ ALL ACCEPTANCE TESTS PASSED!\")\n",
    "    print(\"üéì Ready to proceed to next notebook\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Some tests failed - please review implementation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
