{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Shared Cache Bootstrap (English comments only) ===\n",
    "import os, torch, platform, pathlib\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "for k, v in paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache] Root:\", AI_CACHE_ROOT)\n",
    "print(\n",
    "    \"[GPU]\",\n",
    "    torch.cuda.is_available(),\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb20a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 2: Dependencies Installation & Configuration Check\n",
    "# ===================================================================\n",
    "\n",
    "# Install required packages (run once)\n",
    "\"\"\"\n",
    "pip install transformers accelerate bitsandbytes datasets torch torchvision torchaudio\n",
    "pip install openai-whisper pillow librosa soundfile\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import time\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "import json\n",
    "\n",
    "# Check key libraries\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModel,\n",
    "        AutoModelForCausalLM,\n",
    "        AutoModelForSequenceClassification,\n",
    "        AutoProcessor,\n",
    "        AutoModelForSpeechSeq2Seq,\n",
    "        pipeline,\n",
    "        BitsAndBytesConfig,\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Transformers imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Transformers import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "\n",
    "    print(\"‚úÖ BitsAndBytes available for quantization\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è BitsAndBytes not available - quantization disabled\")\n",
    "\n",
    "# Memory check\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"[GPU Memory] Total: {gpu_memory:.1f} GB\")\n",
    "\n",
    "    # Suggest models based on VRAM\n",
    "    if gpu_memory >= 16:\n",
    "        print(\"üí° Recommended: GPT-2-large, BERT-large, CLIP-large models\")\n",
    "    elif gpu_memory >= 8:\n",
    "        print(\"üí° Recommended: GPT-2-medium, BERT-base, CLIP-base with 8bit\")\n",
    "    else:\n",
    "        print(\"üí° Recommended: GPT-2-small, distil-bert with 4bit quantization\")\n",
    "else:\n",
    "    print(\"üí° CPU-only mode: Use smaller models or quantized versions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892dbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 3: Unified Model Loader Class Design\n",
    "# ===================================================================\n",
    "\n",
    "\n",
    "class HFModelLoader:\n",
    "    \"\"\"\n",
    "    Unified Hugging Face model loader with low-VRAM optimizations\n",
    "    Supports: text generation, classification, multimodal, speech models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cache_dir: str = None, device: str = \"auto\"):\n",
    "        self.cache_dir = cache_dir or os.environ.get(\"HF_HOME\")\n",
    "        self.device = device\n",
    "        self.loaded_models = {}\n",
    "\n",
    "        # Default quantization config for low VRAM\n",
    "        self.bnb_config = (\n",
    "            BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            )\n",
    "            if \"bitsandbytes\" in globals()\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def load_text_generation_model(\n",
    "        self,\n",
    "        model_name: str = \"gpt2\",\n",
    "        use_quantization: bool = True,\n",
    "        trust_remote_code: bool = False,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Load text generation model (GPT-2, GPT-J, etc.)\"\"\"\n",
    "\n",
    "        print(f\"üîÑ Loading text generation model: {model_name}\")\n",
    "\n",
    "        # Configure loading parameters\n",
    "        load_kwargs = {\n",
    "            \"cache_dir\": self.cache_dir,\n",
    "            \"trust_remote_code\": trust_remote_code,\n",
    "        }\n",
    "\n",
    "        # Add quantization if available and requested\n",
    "        if use_quantization and self.bnb_config and torch.cuda.is_available():\n",
    "            load_kwargs[\"quantization_config\"] = self.bnb_config\n",
    "            load_kwargs[\"device_map\"] = \"auto\"\n",
    "            print(\"  üîß Using 4-bit quantization\")\n",
    "        elif torch.cuda.is_available():\n",
    "            load_kwargs[\"device_map\"] = \"auto\"\n",
    "\n",
    "        try:\n",
    "            # Load tokenizer and model\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name, cache_dir=self.cache_dir\n",
    "            )\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)\n",
    "\n",
    "            # Set pad token if missing\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            model_info = {\n",
    "                \"model\": model,\n",
    "                \"tokenizer\": tokenizer,\n",
    "                \"type\": \"text_generation\",\n",
    "                \"model_name\": model_name,\n",
    "                \"memory_usage\": (\n",
    "                    self._get_model_memory(model)\n",
    "                    if torch.cuda.is_available()\n",
    "                    else \"N/A\"\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            self.loaded_models[f\"textgen_{model_name}\"] = model_info\n",
    "            print(f\"  ‚úÖ Loaded successfully | Memory: {model_info['memory_usage']}\")\n",
    "            return model_info\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed to load {model_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_classification_model(\n",
    "        self, model_name: str = \"bert-base-uncased\", use_quantization: bool = False\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Load classification/encoding model (BERT, RoBERTa, etc.)\"\"\"\n",
    "\n",
    "        print(f\"üîÑ Loading classification model: {model_name}\")\n",
    "\n",
    "        load_kwargs = {\"cache_dir\": self.cache_dir}\n",
    "\n",
    "        if use_quantization and self.bnb_config and torch.cuda.is_available():\n",
    "            load_kwargs[\"quantization_config\"] = self.bnb_config\n",
    "            load_kwargs[\"device_map\"] = \"auto\"\n",
    "        elif torch.cuda.is_available():\n",
    "            load_kwargs[\"device_map\"] = \"auto\"\n",
    "\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name, cache_dir=self.cache_dir\n",
    "            )\n",
    "            model = AutoModel.from_pretrained(model_name, **load_kwargs)\n",
    "\n",
    "            model_info = {\n",
    "                \"model\": model,\n",
    "                \"tokenizer\": tokenizer,\n",
    "                \"type\": \"classification\",\n",
    "                \"model_name\": model_name,\n",
    "                \"memory_usage\": (\n",
    "                    self._get_model_memory(model)\n",
    "                    if torch.cuda.is_available()\n",
    "                    else \"N/A\"\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            self.loaded_models[f\"cls_{model_name}\"] = model_info\n",
    "            print(f\"  ‚úÖ Loaded successfully | Memory: {model_info['memory_usage']}\")\n",
    "            return model_info\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed to load {model_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_multimodal_model(\n",
    "        self,\n",
    "        model_name: str = \"openai/clip-vit-base-patch32\",\n",
    "        use_quantization: bool = False,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Load multimodal model (CLIP, BLIP, etc.)\"\"\"\n",
    "\n",
    "        print(f\"üîÑ Loading multimodal model: {model_name}\")\n",
    "\n",
    "        load_kwargs = {\"cache_dir\": self.cache_dir}\n",
    "\n",
    "        if use_quantization and self.bnb_config and torch.cuda.is_available():\n",
    "            load_kwargs[\"quantization_config\"] = self.bnb_config\n",
    "            load_kwargs[\"device_map\"] = \"auto\"\n",
    "        elif torch.cuda.is_available():\n",
    "            load_kwargs[\"device_map\"] = \"auto\"\n",
    "\n",
    "        try:\n",
    "            processor = AutoProcessor.from_pretrained(\n",
    "                model_name, cache_dir=self.cache_dir\n",
    "            )\n",
    "            model = AutoModel.from_pretrained(model_name, **load_kwargs)\n",
    "\n",
    "            model_info = {\n",
    "                \"model\": model,\n",
    "                \"processor\": processor,\n",
    "                \"type\": \"multimodal\",\n",
    "                \"model_name\": model_name,\n",
    "                \"memory_usage\": (\n",
    "                    self._get_model_memory(model)\n",
    "                    if torch.cuda.is_available()\n",
    "                    else \"N/A\"\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            self.loaded_models[f\"mm_{model_name}\"] = model_info\n",
    "            print(f\"  ‚úÖ Loaded successfully | Memory: {model_info['memory_usage']}\")\n",
    "            return model_info\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed to load {model_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_speech_model(\n",
    "        self, model_name: str = \"openai/whisper-base\", use_quantization: bool = False\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Load speech recognition model (Whisper, Wav2Vec2, etc.)\"\"\"\n",
    "\n",
    "        print(f\"üîÑ Loading speech model: {model_name}\")\n",
    "\n",
    "        load_kwargs = {\"cache_dir\": self.cache_dir}\n",
    "\n",
    "        if use_quantization and self.bnb_config and torch.cuda.is_available():\n",
    "            load_kwargs[\"quantization_config\"] = self.bnb_config\n",
    "            load_kwargs[\"device_map\"] = \"auto\"\n",
    "        elif torch.cuda.is_available():\n",
    "            load_kwargs[\"device_map\"] = \"auto\"\n",
    "\n",
    "        try:\n",
    "            processor = AutoProcessor.from_pretrained(\n",
    "                model_name, cache_dir=self.cache_dir\n",
    "            )\n",
    "            model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name, **load_kwargs)\n",
    "\n",
    "            model_info = {\n",
    "                \"model\": model,\n",
    "                \"processor\": processor,\n",
    "                \"type\": \"speech\",\n",
    "                \"model_name\": model_name,\n",
    "                \"memory_usage\": (\n",
    "                    self._get_model_memory(model)\n",
    "                    if torch.cuda.is_available()\n",
    "                    else \"N/A\"\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            self.loaded_models[f\"speech_{model_name}\"] = model_info\n",
    "            print(f\"  ‚úÖ Loaded successfully | Memory: {model_info['memory_usage']}\")\n",
    "            return model_info\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed to load {model_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _get_model_memory(self, model) -> str:\n",
    "        \"\"\"Get approximate model memory usage\"\"\"\n",
    "        if hasattr(model, \"get_memory_footprint\"):\n",
    "            memory_mb = model.get_memory_footprint() / 1024**2\n",
    "            return f\"{memory_mb:.1f} MB\"\n",
    "        else:\n",
    "            param_count = sum(p.numel() for p in model.parameters())\n",
    "            estimated_mb = param_count * 4 / 1024**2  # Assume fp32\n",
    "            return f\"~{estimated_mb:.1f} MB\"\n",
    "\n",
    "    def list_loaded_models(self):\n",
    "        \"\"\"Display all loaded models\"\"\"\n",
    "        if not self.loaded_models:\n",
    "            print(\"üìù No models loaded yet\")\n",
    "            return\n",
    "\n",
    "        print(\"üìã Loaded Models:\")\n",
    "        for key, info in self.loaded_models.items():\n",
    "            print(\n",
    "                f\"  {key}: {info['model_name']} | {info['type']} | {info['memory_usage']}\"\n",
    "            )\n",
    "\n",
    "\n",
    "# Initialize loader\n",
    "loader = HFModelLoader()\n",
    "print(\"üöÄ HFModelLoader initialized with shared cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3188414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 4: GPT-2 Text Generation Model Loading & Inference\n",
    "# ===================================================================\n",
    "\n",
    "\n",
    "def demo_text_generation():\n",
    "    \"\"\"Demonstrate GPT-2 text generation with various decoding strategies\"\"\"\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ü§ñ GPT-2 Text Generation Demo\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Load GPT-2 model (start with smaller version for compatibility)\n",
    "    model_info = loader.load_text_generation_model(\n",
    "        model_name=\"gpt2\",  # or \"gpt2-medium\" if you have enough VRAM\n",
    "        use_quantization=True,\n",
    "    )\n",
    "\n",
    "    if not model_info:\n",
    "        print(\"‚ùå Failed to load GPT-2 model\")\n",
    "        return\n",
    "\n",
    "    model = model_info[\"model\"]\n",
    "    tokenizer = model_info[\"tokenizer\"]\n",
    "\n",
    "    # Sample prompts\n",
    "    prompts = [\n",
    "        \"The future of artificial intelligence is\",\n",
    "        \"In a world where robots and humans coexist,\",\n",
    "        \"The most important breakthrough in science was\",\n",
    "    ]\n",
    "\n",
    "    # Generation parameters to test\n",
    "    gen_configs = [\n",
    "        {\n",
    "            \"do_sample\": True,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.95,\n",
    "            \"temperature\": 0.7,\n",
    "            \"name\": \"Creative\",\n",
    "        },\n",
    "        {\n",
    "            \"do_sample\": True,\n",
    "            \"top_k\": 10,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0.3,\n",
    "            \"name\": \"Focused\",\n",
    "        },\n",
    "        {\"do_sample\": False, \"num_beams\": 3, \"name\": \"Beam Search\"},\n",
    "    ]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nüí≠ Prompt: '{prompt}'\")\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "        if torch.cuda.is_available() and not hasattr(\n",
    "            model.config, \"quantization_config\"\n",
    "        ):\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "        for config in gen_configs:\n",
    "            print(f\"\\n  üéØ Strategy: {config['name']}\")\n",
    "\n",
    "            # Prepare generation config\n",
    "            gen_kwargs = {k: v for k, v in config.items() if k != \"name\"}\n",
    "            gen_kwargs.update(\n",
    "                {\n",
    "                    \"max_new_tokens\": 50,\n",
    "                    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "                    \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(inputs[\"input_ids\"], **gen_kwargs)\n",
    "\n",
    "                # Decode and display\n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                continuation = generated_text[len(prompt) :].strip()\n",
    "\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"    üìù Output: {continuation}\")\n",
    "                print(f\"    ‚è±Ô∏è Time: {elapsed:.2f}s\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Generation failed: {e}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n‚úÖ GPT-2 demo completed\")\n",
    "\n",
    "\n",
    "# Run the demo\n",
    "demo_text_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 5: BERT Classification Model Loading & Feature Extraction\n",
    "# ===================================================================\n",
    "\n",
    "\n",
    "def demo_bert_classification():\n",
    "    \"\"\"Demonstrate BERT for text classification and feature extraction\"\"\"\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"üß† BERT Classification & Feature Extraction Demo\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Load BERT model\n",
    "    model_info = loader.load_classification_model(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        use_quantization=False,  # BERT typically doesn't need quantization\n",
    "    )\n",
    "\n",
    "    if not model_info:\n",
    "        print(\"‚ùå Failed to load BERT model\")\n",
    "        return\n",
    "\n",
    "    model = model_info[\"model\"]\n",
    "    tokenizer = model_info[\"tokenizer\"]\n",
    "\n",
    "    # Sample texts for classification/embedding\n",
    "    texts = [\n",
    "        \"I love this movie! It's absolutely fantastic and entertaining.\",\n",
    "        \"This product is terrible. I want my money back.\",\n",
    "        \"The weather today is quite nice and sunny.\",\n",
    "        \"Machine learning is revolutionizing many industries.\",\n",
    "    ]\n",
    "\n",
    "    print(\"üìù Input Texts:\")\n",
    "    for i, text in enumerate(texts, 1):\n",
    "        print(f\"  {i}. {text}\")\n",
    "\n",
    "    # Extract features/embeddings\n",
    "    print(\"\\nüîç Extracting BERT embeddings...\")\n",
    "\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Use [CLS] token embedding (first token)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(cls_embedding.flatten())\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    print(f\"  üìä Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "    # Compute similarity matrix\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    print(\"\\nüìà Cosine Similarity Matrix:\")\n",
    "    print(\"     Text1  Text2  Text3  Text4\")\n",
    "    for i, row in enumerate(similarity_matrix):\n",
    "        similarities = \" \".join([f\"{sim:.3f}\" for sim in row])\n",
    "        print(f\"Text{i+1}: {similarities}\")\n",
    "\n",
    "    # Find most similar pairs\n",
    "    print(\"\\nüîó Most Similar Text Pairs:\")\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(i + 1, len(texts)):\n",
    "            sim = similarity_matrix[i, j]\n",
    "            print(f\"  Text{i+1} ‚Üî Text{j+1}: {sim:.3f}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n‚úÖ BERT demo completed\")\n",
    "\n",
    "\n",
    "# Run the demo\n",
    "demo_bert_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 6: CLIP Multimodal Model Loading & Image-Text Matching\n",
    "# ===================================================================\n",
    "\n",
    "\n",
    "def demo_clip_multimodal():\n",
    "    \"\"\"Demonstrate CLIP for image-text matching\"\"\"\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"üñºÔ∏è CLIP Multimodal Image-Text Matching Demo\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Load CLIP model\n",
    "    model_info = loader.load_multimodal_model(\n",
    "        model_name=\"openai/clip-vit-base-patch32\", use_quantization=False\n",
    "    )\n",
    "\n",
    "    if not model_info:\n",
    "        print(\"‚ùå Failed to load CLIP model\")\n",
    "        return\n",
    "\n",
    "    model = model_info[\"model\"]\n",
    "    processor = model_info[\"processor\"]\n",
    "\n",
    "    # Download sample images\n",
    "    print(\"üîÑ Downloading sample images...\")\n",
    "\n",
    "    image_urls = [\n",
    "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/280px-PNG_transparency_demonstration_1.png\",  # Cat\n",
    "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Cat_August_2010-4.jpg/272px-Cat_August_2010-4.jpg\",  # Another cat\n",
    "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/481px-Cat03.jpg\",  # Cat\n",
    "    ]\n",
    "\n",
    "    # For demo purposes, create simple colored images if download fails\n",
    "    try:\n",
    "        images = []\n",
    "        for url in image_urls[:2]:  # Use first 2 URLs\n",
    "            response = requests.get(url, timeout=10)\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            images.append(image)\n",
    "        print(f\"  ‚úÖ Downloaded {len(images)} images\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Download failed: {e}\")\n",
    "        print(\"  üé® Creating synthetic images instead...\")\n",
    "        # Create simple colored images\n",
    "        images = [\n",
    "            Image.new(\"RGB\", (224, 224), color=\"red\"),\n",
    "            Image.new(\"RGB\", (224, 224), color=\"blue\"),\n",
    "        ]\n",
    "\n",
    "    # Text descriptions to match\n",
    "    text_descriptions = [\n",
    "        \"a red image\",\n",
    "        \"a blue image\",\n",
    "        \"a cat\",\n",
    "        \"a dog\",\n",
    "        \"a beautiful landscape\",\n",
    "        \"a car on the road\",\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüìù Text descriptions ({len(text_descriptions)}):\")\n",
    "    for i, desc in enumerate(text_descriptions):\n",
    "        print(f\"  {i+1}. {desc}\")\n",
    "\n",
    "    # Process images and texts\n",
    "    print(f\"\\nüîÑ Processing {len(images)} images and {len(text_descriptions)} texts...\")\n",
    "\n",
    "    try:\n",
    "        # Prepare inputs\n",
    "        inputs = processor(\n",
    "            text=text_descriptions, images=images, return_tensors=\"pt\", padding=True\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            image_embeds = outputs.image_embeds\n",
    "            text_embeds = outputs.text_embeds\n",
    "\n",
    "            # Normalize embeddings\n",
    "            image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "            text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # Compute similarity scores\n",
    "            similarity_scores = torch.matmul(text_embeds, image_embeds.T)\n",
    "\n",
    "        print(\"\\nüìä Image-Text Similarity Scores:\")\n",
    "        print(\"Text \\\\ Image        Image1    Image2\")\n",
    "        print(\"-\" * 35)\n",
    "\n",
    "        for i, desc in enumerate(text_descriptions):\n",
    "            scores = similarity_scores[i].cpu().numpy()\n",
    "            score_str = \"  \".join([f\"{score:.3f}\" for score in scores])\n",
    "            print(f\"{desc:20s} {score_str}\")\n",
    "\n",
    "        # Find best matches\n",
    "        print(\"\\nüéØ Best Matches:\")\n",
    "        for i in range(len(images)):\n",
    "            best_text_idx = similarity_scores[:, i].argmax().item()\n",
    "            best_score = similarity_scores[best_text_idx, i].item()\n",
    "            print(\n",
    "                f\"  Image{i+1} ‚Üî '{text_descriptions[best_text_idx]}' (score: {best_score:.3f})\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CLIP processing failed: {e}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n‚úÖ CLIP demo completed\")\n",
    "\n",
    "\n",
    "# Run the demo\n",
    "demo_clip_multimodal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 7: Whisper Speech Recognition Model Loading & Transcription\n",
    "# ===================================================================\n",
    "\n",
    "\n",
    "def demo_whisper_speech():\n",
    "    \"\"\"Demonstrate Whisper for speech recognition (using synthetic audio)\"\"\"\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"üé§ Whisper Speech Recognition Demo\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Check if we can create synthetic audio\n",
    "    try:\n",
    "        import numpy as np\n",
    "        import torch\n",
    "\n",
    "        print(\"‚úÖ Audio processing libraries available\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå Audio libraries not available - skipping Whisper demo\")\n",
    "        return\n",
    "\n",
    "    # Load Whisper model\n",
    "    model_info = loader.load_speech_model(\n",
    "        model_name=\"openai/whisper-base\", use_quantization=False\n",
    "    )\n",
    "\n",
    "    if not model_info:\n",
    "        print(\"‚ùå Failed to load Whisper model\")\n",
    "        return\n",
    "\n",
    "    model = model_info[\"model\"]\n",
    "    processor = model_info[\"processor\"]\n",
    "\n",
    "    print(\"üéµ Creating synthetic speech audio...\")\n",
    "\n",
    "    # Create synthetic audio that resembles speech patterns\n",
    "    # This is for demo purposes - in real use, you'd load actual audio files\n",
    "    sample_rate = 16000\n",
    "    duration = 3  # 3 seconds\n",
    "    time = np.linspace(0, duration, int(sample_rate * duration))\n",
    "\n",
    "    # Create multiple frequency components to simulate speech\n",
    "    frequencies = [200, 400, 800, 1600]  # Typical speech frequency range\n",
    "    synthetic_audio = np.zeros_like(time)\n",
    "\n",
    "    for freq in frequencies:\n",
    "        # Add some frequency modulation and amplitude variation\n",
    "        modulated_freq = freq * (1 + 0.1 * np.sin(2 * np.pi * 5 * time))\n",
    "        amplitude = 0.1 * (1 + np.sin(2 * np.pi * 2 * time)) / len(frequencies)\n",
    "        synthetic_audio += amplitude * np.sin(2 * np.pi * modulated_freq * time)\n",
    "\n",
    "    # Add some noise to make it more realistic\n",
    "    noise = 0.01 * np.random.randn(len(time))\n",
    "    synthetic_audio += noise\n",
    "\n",
    "    # Normalize\n",
    "    synthetic_audio = synthetic_audio.astype(np.float32)\n",
    "    synthetic_audio = synthetic_audio / np.max(np.abs(synthetic_audio))\n",
    "\n",
    "    print(f\"  üìä Audio shape: {synthetic_audio.shape}\")\n",
    "    print(f\"  üéöÔ∏è Sample rate: {sample_rate} Hz\")\n",
    "    print(f\"  ‚è±Ô∏è Duration: {duration} seconds\")\n",
    "\n",
    "    try:\n",
    "        # Process audio\n",
    "        print(\"\\nüîÑ Processing audio with Whisper...\")\n",
    "\n",
    "        inputs = processor(\n",
    "            synthetic_audio, sampling_rate=sample_rate, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "        # Generate transcription\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(inputs[\"input_features\"])\n",
    "            transcription = processor.batch_decode(\n",
    "                predicted_ids, skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "        print(f\"\\nüìù Transcription Result:\")\n",
    "        print(f\"  '{transcription[0]}'\")\n",
    "\n",
    "        # Note about synthetic audio\n",
    "        print(\"\\nüí° Note: This is synthetic audio, so the transcription\")\n",
    "        print(\"    may not be meaningful. In practice, use real speech audio.\")\n",
    "\n",
    "        # Demonstrate with pipeline for easier usage\n",
    "        print(\"\\nüîÑ Alternative: Using Whisper pipeline...\")\n",
    "        try:\n",
    "            # Create pipeline (more user-friendly)\n",
    "            pipe = pipeline(\n",
    "                \"automatic-speech-recognition\",\n",
    "                model=\"openai/whisper-base\",\n",
    "                cache_dir=loader.cache_dir,\n",
    "            )\n",
    "\n",
    "            # Process the same audio\n",
    "            result = pipe(synthetic_audio, sampling_rate=sample_rate)\n",
    "            print(f\"  Pipeline result: '{result['text']}'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Pipeline method failed: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Whisper processing failed: {e}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n‚úÖ Whisper demo completed\")\n",
    "\n",
    "\n",
    "# Run the demo\n",
    "demo_whisper_speech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a6c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 8: Low-VRAM Optimization Strategies\n",
    "# ===================================================================\n",
    "\n",
    "\n",
    "def demo_low_vram_strategies():\n",
    "    \"\"\"Demonstrate various low-VRAM optimization techniques\"\"\"\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"‚ö° Low-VRAM Optimization Strategies Demo\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"üí° Running on CPU - VRAM optimizations not applicable\")\n",
    "        return\n",
    "\n",
    "    # Check initial VRAM usage\n",
    "    torch.cuda.empty_cache()\n",
    "    initial_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "    print(f\"üîç Initial VRAM usage: {initial_memory:.1f} MB\")\n",
    "\n",
    "    strategies = []\n",
    "\n",
    "    # Strategy 1: Standard loading\n",
    "    print(\"\\n1Ô∏è‚É£ Strategy: Standard Loading\")\n",
    "    try:\n",
    "        model_standard = AutoModelForCausalLM.from_pretrained(\n",
    "            \"gpt2\", cache_dir=loader.cache_dir\n",
    "        ).cuda()\n",
    "\n",
    "        memory_standard = torch.cuda.memory_allocated() / 1024**2\n",
    "        memory_used = memory_standard - initial_memory\n",
    "        strategies.append((\"Standard Loading\", memory_used))\n",
    "        print(f\"   üìä VRAM used: {memory_used:.1f} MB\")\n",
    "\n",
    "        # Clean up\n",
    "        del model_standard\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Standard loading failed: {e}\")\n",
    "\n",
    "    # Strategy 2: 8-bit quantization\n",
    "    print(\"\\n2Ô∏è‚É£ Strategy: 8-bit Quantization\")\n",
    "    try:\n",
    "        if loader.bnb_config:\n",
    "            config_8bit = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "                \"gpt2\",\n",
    "                cache_dir=loader.cache_dir,\n",
    "                quantization_config=config_8bit,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "\n",
    "            memory_8bit = torch.cuda.memory_allocated() / 1024**2\n",
    "            memory_used = memory_8bit - initial_memory\n",
    "            strategies.append((\"8-bit Quantization\", memory_used))\n",
    "            print(f\"   üìä VRAM used: {memory_used:.1f} MB\")\n",
    "\n",
    "            del model_8bit\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è BitsAndBytes not available\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå 8-bit loading failed: {e}\")\n",
    "\n",
    "    # Strategy 3: 4-bit quantization\n",
    "    print(\"\\n3Ô∏è‚É£ Strategy: 4-bit Quantization\")\n",
    "    try:\n",
    "        if loader.bnb_config:\n",
    "            model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "                \"gpt2\",\n",
    "                cache_dir=loader.cache_dir,\n",
    "                quantization_config=loader.bnb_config,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "\n",
    "            memory_4bit = torch.cuda.memory_allocated() / 1024**2\n",
    "            memory_used = memory_4bit - initial_memory\n",
    "            strategies.append((\"4-bit Quantization\", memory_used))\n",
    "            print(f\"   üìä VRAM used: {memory_used:.1f} MB\")\n",
    "\n",
    "            del model_4bit\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è BitsAndBytes not available\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå 4-bit loading failed: {e}\")\n",
    "\n",
    "    # Strategy 4: CPU offloading\n",
    "    print(\"\\n4Ô∏è‚É£ Strategy: CPU Offloading\")\n",
    "    try:\n",
    "        model_offload = AutoModelForCausalLM.from_pretrained(\n",
    "            \"gpt2\",\n",
    "            cache_dir=loader.cache_dir,\n",
    "            device_map=\"auto\",\n",
    "            offload_folder=\"./offload_temp\",\n",
    "            offload_state_dict=True,\n",
    "        )\n",
    "\n",
    "        memory_offload = torch.cuda.memory_allocated() / 1024**2\n",
    "        memory_used = memory_offload - initial_memory\n",
    "        strategies.append((\"CPU Offloading\", memory_used))\n",
    "        print(f\"   üìä VRAM used: {memory_used:.1f} MB\")\n",
    "\n",
    "        del model_offload\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Clean up temporary offload folder\n",
    "        import shutil\n",
    "\n",
    "        if os.path.exists(\"./offload_temp\"):\n",
    "            shutil.rmtree(\"./offload_temp\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå CPU offloading failed: {e}\")\n",
    "\n",
    "    # Strategy 5: Gradient checkpointing (for training)\n",
    "    print(\"\\n5Ô∏è‚É£ Strategy: Gradient Checkpointing (Training Mode)\")\n",
    "    try:\n",
    "        model_checkpoint = AutoModelForCausalLM.from_pretrained(\n",
    "            \"gpt2\", cache_dir=loader.cache_dir\n",
    "        ).cuda()\n",
    "\n",
    "        # Enable gradient checkpointing\n",
    "        model_checkpoint.gradient_checkpointing_enable()\n",
    "\n",
    "        memory_checkpoint = torch.cuda.memory_allocated() / 1024**2\n",
    "        memory_used = memory_checkpoint - initial_memory\n",
    "        strategies.append((\"Gradient Checkpointing\", memory_used))\n",
    "        print(f\"   üìä VRAM used: {memory_used:.1f} MB (training optimized)\")\n",
    "\n",
    "        del model_checkpoint\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Gradient checkpointing failed: {e}\")\n",
    "\n",
    "    # Summary comparison\n",
    "    print(\"\\nüìã VRAM Usage Comparison:\")\n",
    "    print(\"-\" * 40)\n",
    "    strategies.sort(key=lambda x: x[1])\n",
    "\n",
    "    for i, (strategy, memory) in enumerate(strategies, 1):\n",
    "        efficiency = \"üü¢\" if memory < 200 else \"üü°\" if memory < 500 else \"üî¥\"\n",
    "        print(f\"{i}. {strategy:20s}: {memory:6.1f} MB {efficiency}\")\n",
    "\n",
    "    if strategies:\n",
    "        best_strategy = strategies[0]\n",
    "        print(f\"\\nüèÜ Most efficient: {best_strategy[0]} ({best_strategy[1]:.1f} MB)\")\n",
    "\n",
    "    # Recommendations based on VRAM\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"\\nüí° Recommendations for {gpu_memory:.1f} GB VRAM:\")\n",
    "\n",
    "    if gpu_memory >= 16:\n",
    "        print(\"   ‚Ä¢ Use standard loading for most models\")\n",
    "        print(\"   ‚Ä¢ Consider larger models (GPT-2-large, BERT-large)\")\n",
    "    elif gpu_memory >= 8:\n",
    "        print(\"   ‚Ä¢ Use 8-bit quantization for larger models\")\n",
    "        print(\"   ‚Ä¢ Standard loading for base models\")\n",
    "    elif gpu_memory >= 4:\n",
    "        print(\"   ‚Ä¢ Use 4-bit quantization for all models\")\n",
    "        print(\"   ‚Ä¢ CPU offloading for very large models\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ Use 4-bit quantization + CPU offloading\")\n",
    "        print(\"   ‚Ä¢ Consider smaller model variants\")\n",
    "\n",
    "    print(\"\\n‚úÖ Low-VRAM optimization demo completed\")\n",
    "\n",
    "\n",
    "# Run the demo\n",
    "demo_low_vram_strategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Smoke Test for nb08_hf_models_loading ===\n",
    "def smoke_test():\n",
    "    \"\"\"5-line smoke test to verify notebook functionality\"\"\"\n",
    "    loader = HFModelLoader()\n",
    "    model_info = loader.load_text_generation_model(\"gpt2\", use_quantization=True)\n",
    "    assert model_info is not None, \"Model loading failed\"\n",
    "    inputs = model_info[\"tokenizer\"](\"Test\", return_tensors=\"pt\")\n",
    "    outputs = model_info[\"model\"].generate(inputs[\"input_ids\"], max_new_tokens=3)\n",
    "    print(\"üéâ Smoke test PASSED - Model loading system works!\")\n",
    "\n",
    "\n",
    "smoke_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
