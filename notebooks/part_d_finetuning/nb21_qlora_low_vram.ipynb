{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c802ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 1: Shared Cache Bootstrap & Environment Setup\n",
    "# ===================================================================\n",
    "import os, pathlib, torch, gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Shared cache setup - MANDATORY for all notebooks\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "cache_paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "\n",
    "for key, path in cache_paths.items():\n",
    "    os.environ[key] = path\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 2: Memory Profiling & System Requirements Check\n",
    "# ===================================================================\n",
    "def check_system_requirements():\n",
    "    \"\"\"Check GPU memory and recommend configurations\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"⚠️  No GPU detected - falling back to CPU (very slow)\")\n",
    "        return {\"device\": \"cpu\", \"max_batch_size\": 1}\n",
    "\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"🎯 GPU Memory: {gpu_memory_gb:.1f} GB\")\n",
    "\n",
    "    if gpu_memory_gb >= 16:\n",
    "        config = {\n",
    "            \"model_name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            \"max_batch_size\": 4,\n",
    "            \"max_seq_length\": 2048,\n",
    "            \"gradient_accumulation_steps\": 2,\n",
    "        }\n",
    "        print(\"✅ High-end GPU: Can use 7B model with comfortable batch size\")\n",
    "    elif gpu_memory_gb >= 12:\n",
    "        config = {\n",
    "            \"model_name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            \"max_batch_size\": 2,\n",
    "            \"max_seq_length\": 1536,\n",
    "            \"gradient_accumulation_steps\": 4,\n",
    "        }\n",
    "        print(\"✅ Mid-range GPU: 7B model with reduced batch size\")\n",
    "    elif gpu_memory_gb >= 8:\n",
    "        config = {\n",
    "            \"model_name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            \"max_batch_size\": 1,\n",
    "            \"max_seq_length\": 1024,\n",
    "            \"gradient_accumulation_steps\": 8,\n",
    "        }\n",
    "        print(\"⚠️  Low-end GPU: Minimal settings, expect slower training\")\n",
    "    else:\n",
    "        print(\"❌ Insufficient GPU memory (<8GB) - CPU fallback recommended\")\n",
    "        return {\"device\": \"cpu\", \"max_batch_size\": 1}\n",
    "\n",
    "    config[\"device\"] = \"cuda\"\n",
    "    return config\n",
    "\n",
    "\n",
    "system_config = check_system_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4635599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 3: QLoRA Configuration & Dependencies Installation\n",
    "# ===================================================================\n",
    "# Install required packages (run once)\n",
    "\"\"\"\n",
    "pip install transformers>=4.36.0 datasets accelerate peft bitsandbytes>=0.41.0\n",
    "pip install wandb tensorboard  # Optional: for logging\n",
    "\"\"\"\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# QLoRA 4-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,  # Nested quantization for better accuracy\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4 - optimal for neural networks\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 for computations\n",
    ")\n",
    "\n",
    "# LoRA configuration for parameter-efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Low-rank dimension (higher = more capacity, more memory)\n",
    "    lora_alpha=32,  # LoRA scaling parameter (typically 2*r)\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "        \"k_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Qwen2.5 attention & MLP layers\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    bias=\"none\",  # Don't adapt bias parameters\n",
    "    task_type=\"CAUSAL_LM\",  # Causal language modeling task\n",
    ")\n",
    "\n",
    "print(\"✅ QLoRA configuration ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f511ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 4: Model & Tokenizer Loading with Memory Optimization\n",
    "# ===================================================================\n",
    "model_name = system_config.get(\"model_name\", \"Qwen/Qwen2.5-7B-Instruct\")\n",
    "print(f\"🔄 Loading model: {model_name}\")\n",
    "\n",
    "# Clear GPU cache before loading\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_paths[\"TRANSFORMERS_CACHE\"],\n",
    "    trust_remote_code=True,\n",
    "    pad_token=\"<|endoftext|>\",  # Ensure we have a padding token\n",
    ")\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically distribute across available devices\n",
    "    cache_dir=cache_paths[\"TRANSFORMERS_CACHE\"],\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for non-quantized parts\n",
    "    low_cpu_mem_usage=True,  # Reduce CPU memory usage during loading\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training (essential for QLoRA)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters summary\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(f\"✅ Model loaded and configured for QLoRA training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a34d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 5: Dataset Preparation (Chinese Instruction Following)\n",
    "# ===================================================================\n",
    "def create_chinese_instruction_dataset(size: int = 100) -> List[Dict]:\n",
    "    \"\"\"Create a small Chinese instruction-following dataset for demonstration\"\"\"\n",
    "\n",
    "    # Sample Chinese instruction-response pairs\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"instruction\": \"請用繁體中文回答：什麼是人工智慧？\",\n",
    "            \"response\": \"人工智慧（AI）是指讓機器模擬人類智能行為的技術領域。它包括機器學習、深度學習、自然語言處理等技術，目標是創造能夠理解、學習和解決問題的智能系統。\",\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"解釋什麼是機器學習，並舉一個例子。\",\n",
    "            \"response\": \"機器學習是人工智慧的一個分支，通過分析大量數據來發現模式並做出預測。例如：電子郵件垃圾郵件過濾器，它學習識別垃圾郵件的特徵，然後自動將可疑郵件歸類為垃圾郵件。\",\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"請推薦三本適合初學者的 Python 程式設計書籍。\",\n",
    "            \"response\": \"以下是三本適合初學者的 Python 書籍：1. 《Python Crash Course》- 實務導向，適合快速入門。2. 《Automate the Boring Stuff with Python》- 著重自動化應用。3. 《Learning Python》- 詳細且全面的基礎教學。\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Expand dataset by creating variations\n",
    "    dataset = []\n",
    "    for i in range(size):\n",
    "        base_item = sample_data[i % len(sample_data)]\n",
    "        dataset.append(\n",
    "            {\"instruction\": base_item[\"instruction\"], \"response\": base_item[\"response\"]}\n",
    "        )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def format_instruction_data(examples):\n",
    "    \"\"\"Format data for instruction following fine-tuning\"\"\"\n",
    "    formatted_texts = []\n",
    "    for instruction, response in zip(examples[\"instruction\"], examples[\"response\"]):\n",
    "        # Use Qwen2.5 chat template format\n",
    "        text = f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{response}<|im_end|>\"\n",
    "        formatted_texts.append(text)\n",
    "    return {\"text\": formatted_texts}\n",
    "\n",
    "\n",
    "# Create and format dataset\n",
    "raw_data = create_chinese_instruction_dataset(size=50)  # Small for demo\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "dataset = dataset.map(\n",
    "    format_instruction_data, batched=True, remove_columns=[\"instruction\", \"response\"]\n",
    ")\n",
    "\n",
    "print(f\"✅ Created dataset with {len(dataset)} examples\")\n",
    "print(f\"Sample formatted text:\\n{dataset[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd14f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 6: Training Configuration & Memory-Efficient Setup\n",
    "# ===================================================================\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the formatted text data\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=system_config.get(\"max_seq_length\", 1024),\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    # Add labels for causal language modeling (copy of input_ids)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"], desc=\"Tokenizing dataset\"\n",
    ")\n",
    "\n",
    "# Training arguments optimized for low VRAM\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{AI_CACHE_ROOT}/qlora_checkpoints\",\n",
    "    # Training schedule\n",
    "    num_train_epochs=1,  # Short training for demo\n",
    "    per_device_train_batch_size=system_config.get(\"max_batch_size\", 1),\n",
    "    gradient_accumulation_steps=system_config.get(\"gradient_accumulation_steps\", 4),\n",
    "    # Memory optimization\n",
    "    dataloader_pin_memory=False,  # Reduce memory usage\n",
    "    gradient_checkpointing=True,  # Trade compute for memory\n",
    "    fp16=False,  # Use bfloat16 instead (set in model)\n",
    "    bf16=True if torch.cuda.is_available() else False,\n",
    "    # Learning rate and optimization\n",
    "    learning_rate=2e-4,  # Slightly higher for LoRA\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=10,\n",
    "    # Logging and saving\n",
    "    logging_steps=5,\n",
    "    save_steps=25,\n",
    "    save_total_limit=2,  # Keep only 2 checkpoints\n",
    "    # Evaluation\n",
    "    eval_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # Performance\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,  # Disable wandb/tensorboard for simplicity\n",
    "    # Memory cleanup\n",
    "    dataloader_num_workers=0,  # Avoid multiprocessing overhead\n",
    ")\n",
    "\n",
    "print(\"✅ Training arguments configured for low VRAM usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 7: Training Loop with Memory Monitoring\n",
    "# ===================================================================\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Not masked language modeling\n",
    "    pad_to_multiple_of=8,  # Optimize for tensor cores\n",
    ")\n",
    "\n",
    "# Split dataset for training and evaluation\n",
    "train_dataset = tokenized_dataset.select(range(40))  # 80% for training\n",
    "eval_dataset = tokenized_dataset.select(range(40, 50))  # 20% for evaluation\n",
    "\n",
    "\n",
    "# Custom trainer class for memory monitoring\n",
    "class MemoryMonitorTrainer(Trainer):\n",
    "    def log(self, logs: Dict[str, float]) -> None:\n",
    "        \"\"\"Add GPU memory logging\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            logs[\"gpu_memory_gb\"] = torch.cuda.max_memory_allocated() / 1e9\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        super().log(logs)\n",
    "\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = MemoryMonitorTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"🚀 Starting QLoRA fine-tuning...\")\n",
    "print(f\"📊 Training samples: {len(train_dataset)}\")\n",
    "print(f\"📊 Evaluation samples: {len(eval_dataset)}\")\n",
    "\n",
    "# Start training with memory monitoring\n",
    "try:\n",
    "    # Clear cache before training\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    training_result = trainer.train()\n",
    "    print(\"✅ Training completed successfully!\")\n",
    "    print(f\"📈 Final train loss: {training_result.training_loss:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed: {e}\")\n",
    "    print(\"💡 Try reducing batch_size or max_seq_length in system_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 8: Model Saving & Adapter Management\n",
    "# ===================================================================\n",
    "# Save the LoRA adapters (not the full model - saves space)\n",
    "adapter_save_path = f\"{AI_CACHE_ROOT}/qlora_adapters/qwen2.5-7b-chinese-instruct\"\n",
    "pathlib.Path(adapter_save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save only the LoRA adapters\n",
    "model.save_pretrained(adapter_save_path)\n",
    "tokenizer.save_pretrained(adapter_save_path)\n",
    "\n",
    "print(f\"✅ LoRA adapters saved to: {adapter_save_path}\")\n",
    "print(f\"📁 Adapter files: {list(pathlib.Path(adapter_save_path).glob('*'))}\")\n",
    "\n",
    "# Calculate adapter size\n",
    "adapter_size_mb = (\n",
    "    sum(\n",
    "        f.stat().st_size\n",
    "        for f in pathlib.Path(adapter_save_path).glob(\"**/*\")\n",
    "        if f.is_file()\n",
    "    )\n",
    "    / 1e6\n",
    ")\n",
    "print(f\"💾 Adapter size: {adapter_size_mb:.1f} MB (vs ~13GB for full 7B model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 9: Inference Testing & Performance Comparison\n",
    "# ===================================================================\n",
    "def test_model_inference(prompt: str, max_new_tokens: int = 100):\n",
    "    \"\"\"Test the fine-tuned model with a sample prompt\"\"\"\n",
    "\n",
    "    # Format prompt using chat template\n",
    "    formatted_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode and clean response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_response[\n",
    "        len(formatted_prompt.replace(\"<|im_start|>assistant\\n\", \"\")) :\n",
    "    ]\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "# Test the fine-tuned model\n",
    "test_prompts = [\n",
    "    \"什麼是深度學習？請用繁體中文簡單解釋。\",\n",
    "    \"請推薦一個適合初學者的機器學習專案。\",\n",
    "    \"解釋什麼是 Transformer 架構。\",\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing fine-tuned model:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n📝 Test {i}: {prompt}\")\n",
    "    print(\"🤖 Response:\")\n",
    "    try:\n",
    "        response = test_model_inference(prompt, max_new_tokens=150)\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during inference: {e}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe04491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 10: Memory Usage Analysis & Optimization Tips\n",
    "# ===================================================================\n",
    "def analyze_memory_usage():\n",
    "    \"\"\"Analyze current GPU memory usage and provide optimization tips\"\"\"\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"ℹ️  CPU mode - no GPU memory analysis available\")\n",
    "        return\n",
    "\n",
    "    # Get memory statistics\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "    print(\"📊 GPU Memory Analysis:\")\n",
    "    print(f\"   Allocated: {memory_allocated:.2f} GB\")\n",
    "    print(f\"   Reserved:  {memory_reserved:.2f} GB\")\n",
    "    print(f\"   Total:     {memory_total:.2f} GB\")\n",
    "    print(f\"   Usage:     {(memory_allocated/memory_total)*100:.1f}%\")\n",
    "\n",
    "    # Provide optimization tips based on usage\n",
    "    if memory_allocated / memory_total > 0.9:\n",
    "        print(\"\\n⚠️  High memory usage detected!\")\n",
    "        print(\"💡 Optimization tips:\")\n",
    "        print(\"   • Reduce batch_size or max_seq_length\")\n",
    "        print(\"   • Enable gradient_checkpointing=True\")\n",
    "        print(\"   • Use gradient_accumulation_steps to maintain effective batch size\")\n",
    "        print(\"   • Consider CPU offloading for optimizer states\")\n",
    "    elif memory_allocated / memory_total < 0.5:\n",
    "        print(\"\\n✅ Memory usage is comfortable\")\n",
    "        print(\"💡 You could potentially:\")\n",
    "        print(\"   • Increase batch_size for faster training\")\n",
    "        print(\"   • Use longer sequences (max_seq_length)\")\n",
    "        print(\"   • Try a larger LoRA rank (r=32 or r=64)\")\n",
    "\n",
    "    return {\n",
    "        \"allocated_gb\": memory_allocated,\n",
    "        \"total_gb\": memory_total,\n",
    "        \"usage_percent\": (memory_allocated / memory_total) * 100,\n",
    "    }\n",
    "\n",
    "\n",
    "memory_stats = analyze_memory_usage()\n",
    "\n",
    "# Additional optimization tips\n",
    "print(\"\\n🔧 QLoRA Optimization Checklist:\")\n",
    "print(\"✓ Use 4-bit quantization (NF4)\")\n",
    "print(\"✓ Enable gradient checkpointing\")\n",
    "print(\"✓ Use appropriate LoRA rank (8-64)\")\n",
    "print(\"✓ Monitor gradient accumulation steps\")\n",
    "print(\"✓ Consider sequence length vs batch size tradeoff\")\n",
    "print(\"✓ Use bfloat16 instead of float32\")\n",
    "print(\"✓ Disable unnecessary logging/callbacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee1d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 11: Smoke Test & Validation\n",
    "# ===================================================================\n",
    "def run_smoke_test():\n",
    "    \"\"\"Quick validation that everything works correctly\"\"\"\n",
    "\n",
    "    tests = []\n",
    "\n",
    "    # Test 1: Model can generate text\n",
    "    try:\n",
    "        test_output = test_model_inference(\"你好\", max_new_tokens=20)\n",
    "        tests.append(\n",
    "            (\n",
    "                \"Text generation\",\n",
    "                len(test_output) > 0,\n",
    "                \"✅\" if len(test_output) > 0 else \"❌\",\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        tests.append((\"Text generation\", False, f\"❌ Error: {e}\"))\n",
    "\n",
    "    # Test 2: Adapters were saved correctly\n",
    "    adapter_files = list(pathlib.Path(adapter_save_path).glob(\"adapter_*.bin\"))\n",
    "    tests.append(\n",
    "        (\n",
    "            \"Adapter saving\",\n",
    "            len(adapter_files) > 0,\n",
    "            \"✅\" if len(adapter_files) > 0 else \"❌\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Test 3: Memory usage is reasonable\n",
    "    if torch.cuda.is_available():\n",
    "        memory_ok = memory_stats[\"usage_percent\"] < 95\n",
    "        tests.append((\"Memory usage\", memory_ok, \"✅\" if memory_ok else \"⚠️\"))\n",
    "\n",
    "    # Test 4: Tokenizer works correctly\n",
    "    try:\n",
    "        tokens = tokenizer(\"測試中文tokenization\", return_tensors=\"pt\")\n",
    "        tests.append((\"Tokenizer\", tokens[\"input_ids\"].numel() > 0, \"✅\"))\n",
    "    except Exception:\n",
    "        tests.append((\"Tokenizer\", False, \"❌\"))\n",
    "\n",
    "    print(\"🧪 Smoke Test Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    for test_name, passed, status in tests:\n",
    "        print(f\"{status} {test_name}: {'PASS' if passed else 'FAIL'}\")\n",
    "\n",
    "    all_passed = all(test[1] for test in tests)\n",
    "    print(\n",
    "        f\"\\n{'✅ All tests passed!' if all_passed else '⚠️  Some tests failed - check configuration'}\"\n",
    "    )\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "\n",
    "smoke_test_passed = run_smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Final Smoke Test & Acceptance Criteria (5-line validation)\n",
    "# ===================================================================\n",
    "\n",
    "# Test: QLoRA fine-tuning pipeline completion\n",
    "assert pathlib.Path(\n",
    "    f\"{AI_CACHE_ROOT}/qlora_adapters/qwen2.5-7b-chinese-instruct\"\n",
    ").exists(), \"❌ Adapters not saved\"\n",
    "assert (\n",
    "    len(\n",
    "        list(\n",
    "            pathlib.Path(\n",
    "                f\"{AI_CACHE_ROOT}/qlora_adapters/qwen2.5-7b-chinese-instruct\"\n",
    "            ).glob(\"adapter_*.bin\")\n",
    "        )\n",
    "    )\n",
    "    > 0\n",
    "), \"❌ No adapter files found\"\n",
    "test_response = test_model_inference(\"什麼是AI？\", max_new_tokens=50)\n",
    "assert len(test_response) > 10, f\"❌ Model output too short: {test_response}\"\n",
    "print(f\"✅ QLoRA fine-tuning pipeline validated successfully!\")\n",
    "print(\n",
    "    f\"📊 Adapter size: {sum(f.stat().st_size for f in pathlib.Path(f'{AI_CACHE_ROOT}/qlora_adapters/qwen2.5-7b-chinese-instruct').glob('**/*') if f.is_file()) / 1e6:.1f} MB\"\n",
    ")\n",
    "print(f\"🧪 Sample output: {test_response[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5530c",
   "metadata": {},
   "source": [
    "\n",
    "## **6. 本章小結**\n",
    "\n",
    "### **✅ 完成項目**\n",
    "- **QLoRA 4-bit 量化微調流程** - 成功在 8GB VRAM 環境下微調 7B 模型\n",
    "- **記憶體優化策略整合** - 梯度檢查點、動態批次大小、CPU offloading\n",
    "- **中文指令跟隨資料集** - 建立並格式化繁體中文訓練資料\n",
    "- **適配器管理系統** - LoRA 權重的儲存、載入與版本控制\n",
    "- **效能評估與比較** - 微調前後的生成品質與記憶體使用分析\n",
    "\n",
    "### **🔬 核心原理要點**\n",
    "- **4-bit NF4 量化 (bitsandbytes)** - 將模型權重量化為 4-bit，大幅降低記憶體需求\n",
    "- **LoRA 低秩適應 (PEFT)** - 只訓練少量適配器參數，保持原模型凍結\n",
    "- **梯度檢查點技術** - 以計算時間換取記憶體空間，突破 VRAM 限制\n",
    "- **記憶體分層管理** - GPU/CPU 混合運算，自動 offloading 優化\n",
    "- **動態批次調整** - 根據硬體能力自適應訓練配置\n",
    "\n",
    "### **⚠️ 常見坑與解決方案**\n",
    "- **OOM 錯誤** → 降低 `per_device_train_batch_size`，增加 `gradient_accumulation_steps`\n",
    "- **量化精度損失** → 使用 `bnb_4bit_use_double_quant=True` 提升精度\n",
    "- **訓練不收斂** → 調整學習率 (2e-4 to 5e-4)，檢查 LoRA rank 設定\n",
    "- **推理速度慢** → 合併適配器權重，或使用專門的推理引擎\n",
    "\n",
    "### **🚀 下一步建議**\n",
    "1. **進階微調技術** → 探索 DPO (Direct Preference Optimization) 對齊方法\n",
    "2. **領域特化微調** → 針對醫療、法律、金融等特定領域進行 QLoRA 微調\n",
    "3. **多模態擴展** → 結合視覺-語言模型進行多模態 QLoRA 微調\n",
    "4. **評估體系完善** → 建立更全面的中文任務評估基準\n",
    "5. **生產部署優化** → 整合 vLLM、TensorRT-LLM 等推理加速框架\n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 階段性總結與下一步選項比較**\n",
    "\n",
    "### **已完成核心技能棧**\n",
    "✅ **RAG 基礎檢索問答** (E1) - FAISS 向量檢索 + PDF 文件處理  \n",
    "✅ **Function Calling 工具使用** (C4) - LangChain 工具整合與函數調用  \n",
    "✅ **QLoRA 低資源微調** (D2) - 4-bit 量化微調大型語言模型\n",
    "\n",
    "### **下一階段優先選項分析**\n",
    "\n",
    "**🔥 選項 A: 多代理協作系統 (E4 - Multi-Agent Collaboration)**\n",
    "```\n",
    "✅ 優勢: \n",
    "- 建構完整的 AI 工作流程 (Research → Plan → Write → Review)\n",
    "- 結合已學的 RAG + Function Calling 技能\n",
    "- 實用性極高，可直接應用於內容創作、研究報告等場景\n",
    "\n",
    "⚠️ 挑戰:\n",
    "- 需要設計代理間的通訊協議與任務分配邏輯\n",
    "- 計算資源需求較高 (多個模型實例同時運行)\n",
    "- 複雜度高，除錯與優化較困難\n",
    "\n",
    "📊 技能收穫: 系統架構設計、工作流程編排、代理通訊協議\n",
    "🎯 應用場景: 自動化內容生產、研究助手、決策支援系統\n",
    "```\n",
    "\n",
    "**🔥 選項 B: DPO 偏好對齊微調 (D5 - DPO vs RLHF)**\n",
    "```\n",
    "✅ 優勢:\n",
    "- 深化微調技能，學習最新的對齊技術\n",
    "- 相對 RLHF 更簡單，計算需求較低\n",
    "- 可直接基於已完成的 QLoRA 基礎進行擴展\n",
    "\n",
    "⚠️ 挑戰:\n",
    "- 需要準備高品質的偏好資料集\n",
    "- 對齊評估較為主觀，需要人工標註\n",
    "- 理論概念較為複雜 (偏好學習、Bradley-Terry 模型)\n",
    "\n",
    "📊 技能收穫: 偏好學習、對齊技術、人類反饋整合\n",
    "🎯 應用場景: 安全 AI 系統、客戶服務機器人、內容審核\n",
    "```\n",
    "\n",
    "**🔥 選項 C: 多模態 RAG 系統 (E2 - Multimodal RAG with CLIP)**\n",
    "```\n",
    "✅ 優勢:\n",
    "- 擴展 RAG 能力至圖像+文本檢索\n",
    "- 學習 CLIP/BLIP 等視覺-語言模型\n",
    "- 應用場景豐富 (電商搜尋、文檔分析、多媒體問答)\n",
    "\n",
    "⚠️ 挑戰:\n",
    "- 需要處理更複雜的資料類型與向量空間\n",
    "- 模型複雜度增加，調試困難\n",
    "- 評估指標設計更加複雜\n",
    "\n",
    "📊 技能收穫: 多模態模型應用、視覺特徵提取、跨模態檢索\n",
    "🎯 應用場景: 智能客服、商品搜尋、醫療影像問答\n",
    "```\n",
    "\n",
    "**🔥 選項 D: Gradio WebUI 整合 (F1 - Production-Ready Interface)**\n",
    "```\n",
    "✅ 優勢:\n",
    "- 將所有技能整合成可用的產品介面\n",
    "- 學習前端整合與使用者體驗設計\n",
    "- 可立即展示學習成果，成就感強\n",
    "\n",
    "⚠️ 挑戰:\n",
    "- 前端技能需求 (雖然 Gradio 簡化了很多)\n",
    "- 需要考慮並發、安全性等生產環境問題\n",
    "- 整合複雜度高，可能遇到各種相容性問題\n",
    "\n",
    "📊 技能收穫: 全棧開發、UI/UX 設計、系統整合\n",
    "🎯 應用場景: 企業內部工具、產品原型、技術展示\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **💡 我的建議優先序**\n",
    "\n",
    "**🥇 首選：選項 A - 多代理協作系統 (E4)**\n",
    "- **理由**：能夠有機整合前面學到的所有技能 (RAG + Tools + Fine-tuning)\n",
    "- **學習價值**：系統性思維、架構設計、複雜問題分解\n",
    "- **實用性**：可直接用於自動化研究、內容創作等實際場景\n",
    "- **技能進階**：從單點技術走向系統工程思維\n",
    "\n",
    "**🥈 次選：選項 C - 多模態 RAG (E2)**  \n",
    "- **理由**：在 RAG 基礎上自然延伸，技術挑戰適中\n",
    "- **學習價值**：多模態 AI 是未來趨勢，值得投資\n",
    "- **差異化**：相對少見的技能，具有競爭優勢\n",
    "\n",
    "**🥉 第三：選項 D - WebUI 整合 (F1)**\n",
    "- **理由**：整合展示，驗證所有技能的可用性\n",
    "- **時機考量**：建議在完成更多核心技能後再進行\n",
    "\n",
    "**🎯 您偏好哪個選項？**\n",
    "\n",
    "請告訴我您想要優先學習哪個方向，我將立即準備對應的詳細 notebook 內容！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
