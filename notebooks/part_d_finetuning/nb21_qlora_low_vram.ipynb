{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c802ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 1: Shared Cache Bootstrap & Environment Setup\n",
    "# ===================================================================\n",
    "import os, pathlib, torch, gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Shared cache setup - MANDATORY for all notebooks\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "cache_paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "\n",
    "for key, path in cache_paths.items():\n",
    "    os.environ[key] = path\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 2: Memory Profiling & System Requirements Check\n",
    "# ===================================================================\n",
    "def check_system_requirements():\n",
    "    \"\"\"Check GPU memory and recommend configurations\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"âš ï¸  No GPU detected - falling back to CPU (very slow)\")\n",
    "        return {\"device\": \"cpu\", \"max_batch_size\": 1}\n",
    "\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"ğŸ¯ GPU Memory: {gpu_memory_gb:.1f} GB\")\n",
    "\n",
    "    if gpu_memory_gb >= 16:\n",
    "        config = {\n",
    "            \"model_name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            \"max_batch_size\": 4,\n",
    "            \"max_seq_length\": 2048,\n",
    "            \"gradient_accumulation_steps\": 2,\n",
    "        }\n",
    "        print(\"âœ… High-end GPU: Can use 7B model with comfortable batch size\")\n",
    "    elif gpu_memory_gb >= 12:\n",
    "        config = {\n",
    "            \"model_name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            \"max_batch_size\": 2,\n",
    "            \"max_seq_length\": 1536,\n",
    "            \"gradient_accumulation_steps\": 4,\n",
    "        }\n",
    "        print(\"âœ… Mid-range GPU: 7B model with reduced batch size\")\n",
    "    elif gpu_memory_gb >= 8:\n",
    "        config = {\n",
    "            \"model_name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            \"max_batch_size\": 1,\n",
    "            \"max_seq_length\": 1024,\n",
    "            \"gradient_accumulation_steps\": 8,\n",
    "        }\n",
    "        print(\"âš ï¸  Low-end GPU: Minimal settings, expect slower training\")\n",
    "    else:\n",
    "        print(\"âŒ Insufficient GPU memory (<8GB) - CPU fallback recommended\")\n",
    "        return {\"device\": \"cpu\", \"max_batch_size\": 1}\n",
    "\n",
    "    config[\"device\"] = \"cuda\"\n",
    "    return config\n",
    "\n",
    "\n",
    "system_config = check_system_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4635599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 3: QLoRA Configuration & Dependencies Installation\n",
    "# ===================================================================\n",
    "# Install required packages (run once)\n",
    "\"\"\"\n",
    "pip install transformers>=4.36.0 datasets accelerate peft bitsandbytes>=0.41.0\n",
    "pip install wandb tensorboard  # Optional: for logging\n",
    "\"\"\"\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# QLoRA 4-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,  # Nested quantization for better accuracy\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4 - optimal for neural networks\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 for computations\n",
    ")\n",
    "\n",
    "# LoRA configuration for parameter-efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Low-rank dimension (higher = more capacity, more memory)\n",
    "    lora_alpha=32,  # LoRA scaling parameter (typically 2*r)\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "        \"k_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Qwen2.5 attention & MLP layers\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    bias=\"none\",  # Don't adapt bias parameters\n",
    "    task_type=\"CAUSAL_LM\",  # Causal language modeling task\n",
    ")\n",
    "\n",
    "print(\"âœ… QLoRA configuration ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f511ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 4: Model & Tokenizer Loading with Memory Optimization\n",
    "# ===================================================================\n",
    "model_name = system_config.get(\"model_name\", \"Qwen/Qwen2.5-7B-Instruct\")\n",
    "print(f\"ğŸ”„ Loading model: {model_name}\")\n",
    "\n",
    "# Clear GPU cache before loading\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_paths[\"TRANSFORMERS_CACHE\"],\n",
    "    trust_remote_code=True,\n",
    "    pad_token=\"<|endoftext|>\",  # Ensure we have a padding token\n",
    ")\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically distribute across available devices\n",
    "    cache_dir=cache_paths[\"TRANSFORMERS_CACHE\"],\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for non-quantized parts\n",
    "    low_cpu_mem_usage=True,  # Reduce CPU memory usage during loading\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training (essential for QLoRA)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters summary\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(f\"âœ… Model loaded and configured for QLoRA training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a34d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 5: Dataset Preparation (Chinese Instruction Following)\n",
    "# ===================================================================\n",
    "def create_chinese_instruction_dataset(size: int = 100) -> List[Dict]:\n",
    "    \"\"\"Create a small Chinese instruction-following dataset for demonstration\"\"\"\n",
    "\n",
    "    # Sample Chinese instruction-response pairs\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"instruction\": \"è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼šä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ\",\n",
    "            \"response\": \"äººå·¥æ™ºæ…§ï¼ˆAIï¼‰æ˜¯æŒ‡è®“æ©Ÿå™¨æ¨¡æ“¬äººé¡æ™ºèƒ½è¡Œç‚ºçš„æŠ€è¡“é ˜åŸŸã€‚å®ƒåŒ…æ‹¬æ©Ÿå™¨å­¸ç¿’ã€æ·±åº¦å­¸ç¿’ã€è‡ªç„¶èªè¨€è™•ç†ç­‰æŠ€è¡“ï¼Œç›®æ¨™æ˜¯å‰µé€ èƒ½å¤ ç†è§£ã€å­¸ç¿’å’Œè§£æ±ºå•é¡Œçš„æ™ºèƒ½ç³»çµ±ã€‚\",\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Œä¸¦èˆ‰ä¸€å€‹ä¾‹å­ã€‚\",\n",
    "            \"response\": \"æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯ï¼Œé€šéåˆ†æå¤§é‡æ•¸æ“šä¾†ç™¼ç¾æ¨¡å¼ä¸¦åšå‡ºé æ¸¬ã€‚ä¾‹å¦‚ï¼šé›»å­éƒµä»¶åƒåœ¾éƒµä»¶éæ¿¾å™¨ï¼Œå®ƒå­¸ç¿’è­˜åˆ¥åƒåœ¾éƒµä»¶çš„ç‰¹å¾µï¼Œç„¶å¾Œè‡ªå‹•å°‡å¯ç–‘éƒµä»¶æ­¸é¡ç‚ºåƒåœ¾éƒµä»¶ã€‚\",\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"è«‹æ¨è–¦ä¸‰æœ¬é©åˆåˆå­¸è€…çš„ Python ç¨‹å¼è¨­è¨ˆæ›¸ç±ã€‚\",\n",
    "            \"response\": \"ä»¥ä¸‹æ˜¯ä¸‰æœ¬é©åˆåˆå­¸è€…çš„ Python æ›¸ç±ï¼š1. ã€ŠPython Crash Courseã€‹- å¯¦å‹™å°å‘ï¼Œé©åˆå¿«é€Ÿå…¥é–€ã€‚2. ã€ŠAutomate the Boring Stuff with Pythonã€‹- è‘—é‡è‡ªå‹•åŒ–æ‡‰ç”¨ã€‚3. ã€ŠLearning Pythonã€‹- è©³ç´°ä¸”å…¨é¢çš„åŸºç¤æ•™å­¸ã€‚\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Expand dataset by creating variations\n",
    "    dataset = []\n",
    "    for i in range(size):\n",
    "        base_item = sample_data[i % len(sample_data)]\n",
    "        dataset.append(\n",
    "            {\"instruction\": base_item[\"instruction\"], \"response\": base_item[\"response\"]}\n",
    "        )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def format_instruction_data(examples):\n",
    "    \"\"\"Format data for instruction following fine-tuning\"\"\"\n",
    "    formatted_texts = []\n",
    "    for instruction, response in zip(examples[\"instruction\"], examples[\"response\"]):\n",
    "        # Use Qwen2.5 chat template format\n",
    "        text = f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{response}<|im_end|>\"\n",
    "        formatted_texts.append(text)\n",
    "    return {\"text\": formatted_texts}\n",
    "\n",
    "\n",
    "# Create and format dataset\n",
    "raw_data = create_chinese_instruction_dataset(size=50)  # Small for demo\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "dataset = dataset.map(\n",
    "    format_instruction_data, batched=True, remove_columns=[\"instruction\", \"response\"]\n",
    ")\n",
    "\n",
    "print(f\"âœ… Created dataset with {len(dataset)} examples\")\n",
    "print(f\"Sample formatted text:\\n{dataset[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd14f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 6: Training Configuration & Memory-Efficient Setup\n",
    "# ===================================================================\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the formatted text data\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=system_config.get(\"max_seq_length\", 1024),\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    # Add labels for causal language modeling (copy of input_ids)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"], desc=\"Tokenizing dataset\"\n",
    ")\n",
    "\n",
    "# Training arguments optimized for low VRAM\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{AI_CACHE_ROOT}/qlora_checkpoints\",\n",
    "    # Training schedule\n",
    "    num_train_epochs=1,  # Short training for demo\n",
    "    per_device_train_batch_size=system_config.get(\"max_batch_size\", 1),\n",
    "    gradient_accumulation_steps=system_config.get(\"gradient_accumulation_steps\", 4),\n",
    "    # Memory optimization\n",
    "    dataloader_pin_memory=False,  # Reduce memory usage\n",
    "    gradient_checkpointing=True,  # Trade compute for memory\n",
    "    fp16=False,  # Use bfloat16 instead (set in model)\n",
    "    bf16=True if torch.cuda.is_available() else False,\n",
    "    # Learning rate and optimization\n",
    "    learning_rate=2e-4,  # Slightly higher for LoRA\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=10,\n",
    "    # Logging and saving\n",
    "    logging_steps=5,\n",
    "    save_steps=25,\n",
    "    save_total_limit=2,  # Keep only 2 checkpoints\n",
    "    # Evaluation\n",
    "    eval_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # Performance\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,  # Disable wandb/tensorboard for simplicity\n",
    "    # Memory cleanup\n",
    "    dataloader_num_workers=0,  # Avoid multiprocessing overhead\n",
    ")\n",
    "\n",
    "print(\"âœ… Training arguments configured for low VRAM usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 7: Training Loop with Memory Monitoring\n",
    "# ===================================================================\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Not masked language modeling\n",
    "    pad_to_multiple_of=8,  # Optimize for tensor cores\n",
    ")\n",
    "\n",
    "# Split dataset for training and evaluation\n",
    "train_dataset = tokenized_dataset.select(range(40))  # 80% for training\n",
    "eval_dataset = tokenized_dataset.select(range(40, 50))  # 20% for evaluation\n",
    "\n",
    "\n",
    "# Custom trainer class for memory monitoring\n",
    "class MemoryMonitorTrainer(Trainer):\n",
    "    def log(self, logs: Dict[str, float]) -> None:\n",
    "        \"\"\"Add GPU memory logging\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            logs[\"gpu_memory_gb\"] = torch.cuda.max_memory_allocated() / 1e9\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        super().log(logs)\n",
    "\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = MemoryMonitorTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ Starting QLoRA fine-tuning...\")\n",
    "print(f\"ğŸ“Š Training samples: {len(train_dataset)}\")\n",
    "print(f\"ğŸ“Š Evaluation samples: {len(eval_dataset)}\")\n",
    "\n",
    "# Start training with memory monitoring\n",
    "try:\n",
    "    # Clear cache before training\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    training_result = trainer.train()\n",
    "    print(\"âœ… Training completed successfully!\")\n",
    "    print(f\"ğŸ“ˆ Final train loss: {training_result.training_loss:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training failed: {e}\")\n",
    "    print(\"ğŸ’¡ Try reducing batch_size or max_seq_length in system_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 8: Model Saving & Adapter Management\n",
    "# ===================================================================\n",
    "# Save the LoRA adapters (not the full model - saves space)\n",
    "adapter_save_path = f\"{AI_CACHE_ROOT}/qlora_adapters/qwen2.5-7b-chinese-instruct\"\n",
    "pathlib.Path(adapter_save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save only the LoRA adapters\n",
    "model.save_pretrained(adapter_save_path)\n",
    "tokenizer.save_pretrained(adapter_save_path)\n",
    "\n",
    "print(f\"âœ… LoRA adapters saved to: {adapter_save_path}\")\n",
    "print(f\"ğŸ“ Adapter files: {list(pathlib.Path(adapter_save_path).glob('*'))}\")\n",
    "\n",
    "# Calculate adapter size\n",
    "adapter_size_mb = (\n",
    "    sum(\n",
    "        f.stat().st_size\n",
    "        for f in pathlib.Path(adapter_save_path).glob(\"**/*\")\n",
    "        if f.is_file()\n",
    "    )\n",
    "    / 1e6\n",
    ")\n",
    "print(f\"ğŸ’¾ Adapter size: {adapter_size_mb:.1f} MB (vs ~13GB for full 7B model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 9: Inference Testing & Performance Comparison\n",
    "# ===================================================================\n",
    "def test_model_inference(prompt: str, max_new_tokens: int = 100):\n",
    "    \"\"\"Test the fine-tuned model with a sample prompt\"\"\"\n",
    "\n",
    "    # Format prompt using chat template\n",
    "    formatted_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode and clean response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_response[\n",
    "        len(formatted_prompt.replace(\"<|im_start|>assistant\\n\", \"\")) :\n",
    "    ]\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "# Test the fine-tuned model\n",
    "test_prompts = [\n",
    "    \"ä»€éº¼æ˜¯æ·±åº¦å­¸ç¿’ï¼Ÿè«‹ç”¨ç¹é«”ä¸­æ–‡ç°¡å–®è§£é‡‹ã€‚\",\n",
    "    \"è«‹æ¨è–¦ä¸€å€‹é©åˆåˆå­¸è€…çš„æ©Ÿå™¨å­¸ç¿’å°ˆæ¡ˆã€‚\",\n",
    "    \"è§£é‡‹ä»€éº¼æ˜¯ Transformer æ¶æ§‹ã€‚\",\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing fine-tuned model:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nğŸ“ Test {i}: {prompt}\")\n",
    "    print(\"ğŸ¤– Response:\")\n",
    "    try:\n",
    "        response = test_model_inference(prompt, max_new_tokens=150)\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during inference: {e}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe04491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 10: Memory Usage Analysis & Optimization Tips\n",
    "# ===================================================================\n",
    "def analyze_memory_usage():\n",
    "    \"\"\"Analyze current GPU memory usage and provide optimization tips\"\"\"\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"â„¹ï¸  CPU mode - no GPU memory analysis available\")\n",
    "        return\n",
    "\n",
    "    # Get memory statistics\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "    print(\"ğŸ“Š GPU Memory Analysis:\")\n",
    "    print(f\"   Allocated: {memory_allocated:.2f} GB\")\n",
    "    print(f\"   Reserved:  {memory_reserved:.2f} GB\")\n",
    "    print(f\"   Total:     {memory_total:.2f} GB\")\n",
    "    print(f\"   Usage:     {(memory_allocated/memory_total)*100:.1f}%\")\n",
    "\n",
    "    # Provide optimization tips based on usage\n",
    "    if memory_allocated / memory_total > 0.9:\n",
    "        print(\"\\nâš ï¸  High memory usage detected!\")\n",
    "        print(\"ğŸ’¡ Optimization tips:\")\n",
    "        print(\"   â€¢ Reduce batch_size or max_seq_length\")\n",
    "        print(\"   â€¢ Enable gradient_checkpointing=True\")\n",
    "        print(\"   â€¢ Use gradient_accumulation_steps to maintain effective batch size\")\n",
    "        print(\"   â€¢ Consider CPU offloading for optimizer states\")\n",
    "    elif memory_allocated / memory_total < 0.5:\n",
    "        print(\"\\nâœ… Memory usage is comfortable\")\n",
    "        print(\"ğŸ’¡ You could potentially:\")\n",
    "        print(\"   â€¢ Increase batch_size for faster training\")\n",
    "        print(\"   â€¢ Use longer sequences (max_seq_length)\")\n",
    "        print(\"   â€¢ Try a larger LoRA rank (r=32 or r=64)\")\n",
    "\n",
    "    return {\n",
    "        \"allocated_gb\": memory_allocated,\n",
    "        \"total_gb\": memory_total,\n",
    "        \"usage_percent\": (memory_allocated / memory_total) * 100,\n",
    "    }\n",
    "\n",
    "\n",
    "memory_stats = analyze_memory_usage()\n",
    "\n",
    "# Additional optimization tips\n",
    "print(\"\\nğŸ”§ QLoRA Optimization Checklist:\")\n",
    "print(\"âœ“ Use 4-bit quantization (NF4)\")\n",
    "print(\"âœ“ Enable gradient checkpointing\")\n",
    "print(\"âœ“ Use appropriate LoRA rank (8-64)\")\n",
    "print(\"âœ“ Monitor gradient accumulation steps\")\n",
    "print(\"âœ“ Consider sequence length vs batch size tradeoff\")\n",
    "print(\"âœ“ Use bfloat16 instead of float32\")\n",
    "print(\"âœ“ Disable unnecessary logging/callbacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee1d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 11: Smoke Test & Validation\n",
    "# ===================================================================\n",
    "def run_smoke_test():\n",
    "    \"\"\"Quick validation that everything works correctly\"\"\"\n",
    "\n",
    "    tests = []\n",
    "\n",
    "    # Test 1: Model can generate text\n",
    "    try:\n",
    "        test_output = test_model_inference(\"ä½ å¥½\", max_new_tokens=20)\n",
    "        tests.append(\n",
    "            (\n",
    "                \"Text generation\",\n",
    "                len(test_output) > 0,\n",
    "                \"âœ…\" if len(test_output) > 0 else \"âŒ\",\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        tests.append((\"Text generation\", False, f\"âŒ Error: {e}\"))\n",
    "\n",
    "    # Test 2: Adapters were saved correctly\n",
    "    adapter_files = list(pathlib.Path(adapter_save_path).glob(\"adapter_*.bin\"))\n",
    "    tests.append(\n",
    "        (\n",
    "            \"Adapter saving\",\n",
    "            len(adapter_files) > 0,\n",
    "            \"âœ…\" if len(adapter_files) > 0 else \"âŒ\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Test 3: Memory usage is reasonable\n",
    "    if torch.cuda.is_available():\n",
    "        memory_ok = memory_stats[\"usage_percent\"] < 95\n",
    "        tests.append((\"Memory usage\", memory_ok, \"âœ…\" if memory_ok else \"âš ï¸\"))\n",
    "\n",
    "    # Test 4: Tokenizer works correctly\n",
    "    try:\n",
    "        tokens = tokenizer(\"æ¸¬è©¦ä¸­æ–‡tokenization\", return_tensors=\"pt\")\n",
    "        tests.append((\"Tokenizer\", tokens[\"input_ids\"].numel() > 0, \"âœ…\"))\n",
    "    except Exception:\n",
    "        tests.append((\"Tokenizer\", False, \"âŒ\"))\n",
    "\n",
    "    print(\"ğŸ§ª Smoke Test Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    for test_name, passed, status in tests:\n",
    "        print(f\"{status} {test_name}: {'PASS' if passed else 'FAIL'}\")\n",
    "\n",
    "    all_passed = all(test[1] for test in tests)\n",
    "    print(\n",
    "        f\"\\n{'âœ… All tests passed!' if all_passed else 'âš ï¸  Some tests failed - check configuration'}\"\n",
    "    )\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "\n",
    "smoke_test_passed = run_smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Final Smoke Test & Acceptance Criteria (5-line validation)\n",
    "# ===================================================================\n",
    "\n",
    "# Test: QLoRA fine-tuning pipeline completion\n",
    "assert pathlib.Path(\n",
    "    f\"{AI_CACHE_ROOT}/qlora_adapters/qwen2.5-7b-chinese-instruct\"\n",
    ").exists(), \"âŒ Adapters not saved\"\n",
    "assert (\n",
    "    len(\n",
    "        list(\n",
    "            pathlib.Path(\n",
    "                f\"{AI_CACHE_ROOT}/qlora_adapters/qwen2.5-7b-chinese-instruct\"\n",
    "            ).glob(\"adapter_*.bin\")\n",
    "        )\n",
    "    )\n",
    "    > 0\n",
    "), \"âŒ No adapter files found\"\n",
    "test_response = test_model_inference(\"ä»€éº¼æ˜¯AIï¼Ÿ\", max_new_tokens=50)\n",
    "assert len(test_response) > 10, f\"âŒ Model output too short: {test_response}\"\n",
    "print(f\"âœ… QLoRA fine-tuning pipeline validated successfully!\")\n",
    "print(\n",
    "    f\"ğŸ“Š Adapter size: {sum(f.stat().st_size for f in pathlib.Path(f'{AI_CACHE_ROOT}/qlora_adapters/qwen2.5-7b-chinese-instruct').glob('**/*') if f.is_file()) / 1e6:.1f} MB\"\n",
    ")\n",
    "print(f\"ğŸ§ª Sample output: {test_response[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5530c",
   "metadata": {},
   "source": [
    "\n",
    "## **6. æœ¬ç« å°çµ**\n",
    "\n",
    "### **âœ… å®Œæˆé …ç›®**\n",
    "- **QLoRA 4-bit é‡åŒ–å¾®èª¿æµç¨‹** - æˆåŠŸåœ¨ 8GB VRAM ç’°å¢ƒä¸‹å¾®èª¿ 7B æ¨¡å‹\n",
    "- **è¨˜æ†¶é«”å„ªåŒ–ç­–ç•¥æ•´åˆ** - æ¢¯åº¦æª¢æŸ¥é»ã€å‹•æ…‹æ‰¹æ¬¡å¤§å°ã€CPU offloading\n",
    "- **ä¸­æ–‡æŒ‡ä»¤è·Ÿéš¨è³‡æ–™é›†** - å»ºç«‹ä¸¦æ ¼å¼åŒ–ç¹é«”ä¸­æ–‡è¨“ç·´è³‡æ–™\n",
    "- **é©é…å™¨ç®¡ç†ç³»çµ±** - LoRA æ¬Šé‡çš„å„²å­˜ã€è¼‰å…¥èˆ‡ç‰ˆæœ¬æ§åˆ¶\n",
    "- **æ•ˆèƒ½è©•ä¼°èˆ‡æ¯”è¼ƒ** - å¾®èª¿å‰å¾Œçš„ç”Ÿæˆå“è³ªèˆ‡è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ\n",
    "\n",
    "### **ğŸ”¬ æ ¸å¿ƒåŸç†è¦é»**\n",
    "- **4-bit NF4 é‡åŒ– (bitsandbytes)** - å°‡æ¨¡å‹æ¬Šé‡é‡åŒ–ç‚º 4-bitï¼Œå¤§å¹…é™ä½è¨˜æ†¶é«”éœ€æ±‚\n",
    "- **LoRA ä½ç§©é©æ‡‰ (PEFT)** - åªè¨“ç·´å°‘é‡é©é…å™¨åƒæ•¸ï¼Œä¿æŒåŸæ¨¡å‹å‡çµ\n",
    "- **æ¢¯åº¦æª¢æŸ¥é»æŠ€è¡“** - ä»¥è¨ˆç®—æ™‚é–“æ›å–è¨˜æ†¶é«”ç©ºé–“ï¼Œçªç ´ VRAM é™åˆ¶\n",
    "- **è¨˜æ†¶é«”åˆ†å±¤ç®¡ç†** - GPU/CPU æ··åˆé‹ç®—ï¼Œè‡ªå‹• offloading å„ªåŒ–\n",
    "- **å‹•æ…‹æ‰¹æ¬¡èª¿æ•´** - æ ¹æ“šç¡¬é«”èƒ½åŠ›è‡ªé©æ‡‰è¨“ç·´é…ç½®\n",
    "\n",
    "### **âš ï¸ å¸¸è¦‹å‘èˆ‡è§£æ±ºæ–¹æ¡ˆ**\n",
    "- **OOM éŒ¯èª¤** â†’ é™ä½ `per_device_train_batch_size`ï¼Œå¢åŠ  `gradient_accumulation_steps`\n",
    "- **é‡åŒ–ç²¾åº¦æå¤±** â†’ ä½¿ç”¨ `bnb_4bit_use_double_quant=True` æå‡ç²¾åº¦\n",
    "- **è¨“ç·´ä¸æ”¶æ–‚** â†’ èª¿æ•´å­¸ç¿’ç‡ (2e-4 to 5e-4)ï¼Œæª¢æŸ¥ LoRA rank è¨­å®š\n",
    "- **æ¨ç†é€Ÿåº¦æ…¢** â†’ åˆä½µé©é…å™¨æ¬Šé‡ï¼Œæˆ–ä½¿ç”¨å°ˆé–€çš„æ¨ç†å¼•æ“\n",
    "\n",
    "### **ğŸš€ ä¸‹ä¸€æ­¥å»ºè­°**\n",
    "1. **é€²éšå¾®èª¿æŠ€è¡“** â†’ æ¢ç´¢ DPO (Direct Preference Optimization) å°é½Šæ–¹æ³•\n",
    "2. **é ˜åŸŸç‰¹åŒ–å¾®èª¿** â†’ é‡å°é†«ç™‚ã€æ³•å¾‹ã€é‡‘èç­‰ç‰¹å®šé ˜åŸŸé€²è¡Œ QLoRA å¾®èª¿\n",
    "3. **å¤šæ¨¡æ…‹æ“´å±•** â†’ çµåˆè¦–è¦º-èªè¨€æ¨¡å‹é€²è¡Œå¤šæ¨¡æ…‹ QLoRA å¾®èª¿\n",
    "4. **è©•ä¼°é«”ç³»å®Œå–„** â†’ å»ºç«‹æ›´å…¨é¢çš„ä¸­æ–‡ä»»å‹™è©•ä¼°åŸºæº–\n",
    "5. **ç”Ÿç”¢éƒ¨ç½²å„ªåŒ–** â†’ æ•´åˆ vLLMã€TensorRT-LLM ç­‰æ¨ç†åŠ é€Ÿæ¡†æ¶\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¯ éšæ®µæ€§ç¸½çµèˆ‡ä¸‹ä¸€æ­¥é¸é …æ¯”è¼ƒ**\n",
    "\n",
    "### **å·²å®Œæˆæ ¸å¿ƒæŠ€èƒ½æ£§**\n",
    "âœ… **RAG åŸºç¤æª¢ç´¢å•ç­”** (E1) - FAISS å‘é‡æª¢ç´¢ + PDF æ–‡ä»¶è™•ç†  \n",
    "âœ… **Function Calling å·¥å…·ä½¿ç”¨** (C4) - LangChain å·¥å…·æ•´åˆèˆ‡å‡½æ•¸èª¿ç”¨  \n",
    "âœ… **QLoRA ä½è³‡æºå¾®èª¿** (D2) - 4-bit é‡åŒ–å¾®èª¿å¤§å‹èªè¨€æ¨¡å‹\n",
    "\n",
    "### **ä¸‹ä¸€éšæ®µå„ªå…ˆé¸é …åˆ†æ**\n",
    "\n",
    "**ğŸ”¥ é¸é … A: å¤šä»£ç†å”ä½œç³»çµ± (E4 - Multi-Agent Collaboration)**\n",
    "```\n",
    "âœ… å„ªå‹¢: \n",
    "- å»ºæ§‹å®Œæ•´çš„ AI å·¥ä½œæµç¨‹ (Research â†’ Plan â†’ Write â†’ Review)\n",
    "- çµåˆå·²å­¸çš„ RAG + Function Calling æŠ€èƒ½\n",
    "- å¯¦ç”¨æ€§æ¥µé«˜ï¼Œå¯ç›´æ¥æ‡‰ç”¨æ–¼å…§å®¹å‰µä½œã€ç ”ç©¶å ±å‘Šç­‰å ´æ™¯\n",
    "\n",
    "âš ï¸ æŒ‘æˆ°:\n",
    "- éœ€è¦è¨­è¨ˆä»£ç†é–“çš„é€šè¨Šå”è­°èˆ‡ä»»å‹™åˆ†é…é‚è¼¯\n",
    "- è¨ˆç®—è³‡æºéœ€æ±‚è¼ƒé«˜ (å¤šå€‹æ¨¡å‹å¯¦ä¾‹åŒæ™‚é‹è¡Œ)\n",
    "- è¤‡é›œåº¦é«˜ï¼Œé™¤éŒ¯èˆ‡å„ªåŒ–è¼ƒå›°é›£\n",
    "\n",
    "ğŸ“Š æŠ€èƒ½æ”¶ç©«: ç³»çµ±æ¶æ§‹è¨­è¨ˆã€å·¥ä½œæµç¨‹ç·¨æ’ã€ä»£ç†é€šè¨Šå”è­°\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯: è‡ªå‹•åŒ–å…§å®¹ç”Ÿç”¢ã€ç ”ç©¶åŠ©æ‰‹ã€æ±ºç­–æ”¯æ´ç³»çµ±\n",
    "```\n",
    "\n",
    "**ğŸ”¥ é¸é … B: DPO åå¥½å°é½Šå¾®èª¿ (D5 - DPO vs RLHF)**\n",
    "```\n",
    "âœ… å„ªå‹¢:\n",
    "- æ·±åŒ–å¾®èª¿æŠ€èƒ½ï¼Œå­¸ç¿’æœ€æ–°çš„å°é½ŠæŠ€è¡“\n",
    "- ç›¸å° RLHF æ›´ç°¡å–®ï¼Œè¨ˆç®—éœ€æ±‚è¼ƒä½\n",
    "- å¯ç›´æ¥åŸºæ–¼å·²å®Œæˆçš„ QLoRA åŸºç¤é€²è¡Œæ“´å±•\n",
    "\n",
    "âš ï¸ æŒ‘æˆ°:\n",
    "- éœ€è¦æº–å‚™é«˜å“è³ªçš„åå¥½è³‡æ–™é›†\n",
    "- å°é½Šè©•ä¼°è¼ƒç‚ºä¸»è§€ï¼Œéœ€è¦äººå·¥æ¨™è¨»\n",
    "- ç†è«–æ¦‚å¿µè¼ƒç‚ºè¤‡é›œ (åå¥½å­¸ç¿’ã€Bradley-Terry æ¨¡å‹)\n",
    "\n",
    "ğŸ“Š æŠ€èƒ½æ”¶ç©«: åå¥½å­¸ç¿’ã€å°é½ŠæŠ€è¡“ã€äººé¡åé¥‹æ•´åˆ\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯: å®‰å…¨ AI ç³»çµ±ã€å®¢æˆ¶æœå‹™æ©Ÿå™¨äººã€å…§å®¹å¯©æ ¸\n",
    "```\n",
    "\n",
    "**ğŸ”¥ é¸é … C: å¤šæ¨¡æ…‹ RAG ç³»çµ± (E2 - Multimodal RAG with CLIP)**\n",
    "```\n",
    "âœ… å„ªå‹¢:\n",
    "- æ“´å±• RAG èƒ½åŠ›è‡³åœ–åƒ+æ–‡æœ¬æª¢ç´¢\n",
    "- å­¸ç¿’ CLIP/BLIP ç­‰è¦–è¦º-èªè¨€æ¨¡å‹\n",
    "- æ‡‰ç”¨å ´æ™¯è±å¯Œ (é›»å•†æœå°‹ã€æ–‡æª”åˆ†æã€å¤šåª’é«”å•ç­”)\n",
    "\n",
    "âš ï¸ æŒ‘æˆ°:\n",
    "- éœ€è¦è™•ç†æ›´è¤‡é›œçš„è³‡æ–™é¡å‹èˆ‡å‘é‡ç©ºé–“\n",
    "- æ¨¡å‹è¤‡é›œåº¦å¢åŠ ï¼Œèª¿è©¦å›°é›£\n",
    "- è©•ä¼°æŒ‡æ¨™è¨­è¨ˆæ›´åŠ è¤‡é›œ\n",
    "\n",
    "ğŸ“Š æŠ€èƒ½æ”¶ç©«: å¤šæ¨¡æ…‹æ¨¡å‹æ‡‰ç”¨ã€è¦–è¦ºç‰¹å¾µæå–ã€è·¨æ¨¡æ…‹æª¢ç´¢\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯: æ™ºèƒ½å®¢æœã€å•†å“æœå°‹ã€é†«ç™‚å½±åƒå•ç­”\n",
    "```\n",
    "\n",
    "**ğŸ”¥ é¸é … D: Gradio WebUI æ•´åˆ (F1 - Production-Ready Interface)**\n",
    "```\n",
    "âœ… å„ªå‹¢:\n",
    "- å°‡æ‰€æœ‰æŠ€èƒ½æ•´åˆæˆå¯ç”¨çš„ç”¢å“ä»‹é¢\n",
    "- å­¸ç¿’å‰ç«¯æ•´åˆèˆ‡ä½¿ç”¨è€…é«”é©—è¨­è¨ˆ\n",
    "- å¯ç«‹å³å±•ç¤ºå­¸ç¿’æˆæœï¼Œæˆå°±æ„Ÿå¼·\n",
    "\n",
    "âš ï¸ æŒ‘æˆ°:\n",
    "- å‰ç«¯æŠ€èƒ½éœ€æ±‚ (é›–ç„¶ Gradio ç°¡åŒ–äº†å¾ˆå¤š)\n",
    "- éœ€è¦è€ƒæ…®ä¸¦ç™¼ã€å®‰å…¨æ€§ç­‰ç”Ÿç”¢ç’°å¢ƒå•é¡Œ\n",
    "- æ•´åˆè¤‡é›œåº¦é«˜ï¼Œå¯èƒ½é‡åˆ°å„ç¨®ç›¸å®¹æ€§å•é¡Œ\n",
    "\n",
    "ğŸ“Š æŠ€èƒ½æ”¶ç©«: å…¨æ£§é–‹ç™¼ã€UI/UX è¨­è¨ˆã€ç³»çµ±æ•´åˆ\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯: ä¼æ¥­å…§éƒ¨å·¥å…·ã€ç”¢å“åŸå‹ã€æŠ€è¡“å±•ç¤º\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **ğŸ’¡ æˆ‘çš„å»ºè­°å„ªå…ˆåº**\n",
    "\n",
    "**ğŸ¥‡ é¦–é¸ï¼šé¸é … A - å¤šä»£ç†å”ä½œç³»çµ± (E4)**\n",
    "- **ç†ç”±**ï¼šèƒ½å¤ æœ‰æ©Ÿæ•´åˆå‰é¢å­¸åˆ°çš„æ‰€æœ‰æŠ€èƒ½ (RAG + Tools + Fine-tuning)\n",
    "- **å­¸ç¿’åƒ¹å€¼**ï¼šç³»çµ±æ€§æ€ç¶­ã€æ¶æ§‹è¨­è¨ˆã€è¤‡é›œå•é¡Œåˆ†è§£\n",
    "- **å¯¦ç”¨æ€§**ï¼šå¯ç›´æ¥ç”¨æ–¼è‡ªå‹•åŒ–ç ”ç©¶ã€å…§å®¹å‰µä½œç­‰å¯¦éš›å ´æ™¯\n",
    "- **æŠ€èƒ½é€²éš**ï¼šå¾å–®é»æŠ€è¡“èµ°å‘ç³»çµ±å·¥ç¨‹æ€ç¶­\n",
    "\n",
    "**ğŸ¥ˆ æ¬¡é¸ï¼šé¸é … C - å¤šæ¨¡æ…‹ RAG (E2)**  \n",
    "- **ç†ç”±**ï¼šåœ¨ RAG åŸºç¤ä¸Šè‡ªç„¶å»¶ä¼¸ï¼ŒæŠ€è¡“æŒ‘æˆ°é©ä¸­\n",
    "- **å­¸ç¿’åƒ¹å€¼**ï¼šå¤šæ¨¡æ…‹ AI æ˜¯æœªä¾†è¶¨å‹¢ï¼Œå€¼å¾—æŠ•è³‡\n",
    "- **å·®ç•°åŒ–**ï¼šç›¸å°å°‘è¦‹çš„æŠ€èƒ½ï¼Œå…·æœ‰ç«¶çˆ­å„ªå‹¢\n",
    "\n",
    "**ğŸ¥‰ ç¬¬ä¸‰ï¼šé¸é … D - WebUI æ•´åˆ (F1)**\n",
    "- **ç†ç”±**ï¼šæ•´åˆå±•ç¤ºï¼Œé©—è­‰æ‰€æœ‰æŠ€èƒ½çš„å¯ç”¨æ€§\n",
    "- **æ™‚æ©Ÿè€ƒé‡**ï¼šå»ºè­°åœ¨å®Œæˆæ›´å¤šæ ¸å¿ƒæŠ€èƒ½å¾Œå†é€²è¡Œ\n",
    "\n",
    "**ğŸ¯ æ‚¨åå¥½å“ªå€‹é¸é …ï¼Ÿ**\n",
    "\n",
    "è«‹å‘Šè¨´æˆ‘æ‚¨æƒ³è¦å„ªå…ˆå­¸ç¿’å“ªå€‹æ–¹å‘ï¼Œæˆ‘å°‡ç«‹å³æº–å‚™å°æ‡‰çš„è©³ç´° notebook å…§å®¹ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
