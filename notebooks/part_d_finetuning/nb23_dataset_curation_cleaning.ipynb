{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c459ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb23_dataset_curation_cleaning.ipynb\n",
    "# è³‡æ–™é›†æ•´ç†èˆ‡æ¸…æ´— (Dataset Curation & Cleaning)\n",
    "\n",
    "# === Cell 1: Shared Cache Bootstrap ===\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())\n",
    "\n",
    "# Standard imports for dataset curation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import hashlib\n",
    "from difflib import SequenceMatcher\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96664745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Dataset Loading & Initial Exploration ===\n",
    "print(\"=== è³‡æ–™é›†è¼‰å…¥èˆ‡åˆæ­¥æ¢ç´¢ (Dataset Loading & Exploration) ===\")\n",
    "\n",
    "\n",
    "# Sample instruction dataset creation (simulating multiple sources)\n",
    "def create_sample_datasets():\n",
    "    \"\"\"Create sample instruction datasets with various quality issues\"\"\"\n",
    "\n",
    "    # Dataset 1: Clean Alpaca-style\n",
    "    alpaca_samples = [\n",
    "        {\n",
    "            \"instruction\": \"Explain the concept of machine learning\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed.\",\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Translate the following English text to Spanish\",\n",
    "            \"input\": \"Hello, how are you?\",\n",
    "            \"output\": \"Hola, Â¿cÃ³mo estÃ¡s?\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Dataset 2: With quality issues\n",
    "    noisy_samples = [\n",
    "        {\n",
    "            \"instruction\": \"explain machine learning\",  # lowercase, duplicate topic\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"ML is when computers learn stuff automatically.\",  # too short\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"My name is John Smith and I live at 123 Main St\",  # PII\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"I can't help with personal information.\",\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Translate to Spanish\",  # missing input\n",
    "            \"input\": \"Hello, how are you?\",  # duplicate content\n",
    "            \"output\": \"Hola, Â¿cÃ³mo estÃ¡s?\",\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"What is AI?\",  # wrong format key\n",
    "            \"response\": \"Artificial Intelligence is...\",  # wrong format key\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Dataset 3: Different format (ChatML style)\n",
    "    chatmL_samples = [\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"Deep learning is a subset of machine learning that uses neural networks with multiple layers.\",\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return alpaca_samples, noisy_samples, chatmL_samples\n",
    "\n",
    "\n",
    "alpaca_data, noisy_data, chatml_data = create_sample_datasets()\n",
    "\n",
    "print(f\"Dataset 1 (Clean): {len(alpaca_data)} samples\")\n",
    "print(f\"Dataset 2 (Noisy): {len(noisy_data)} samples\")\n",
    "print(f\"Dataset 3 (ChatML): {len(chatml_data)} samples\")\n",
    "\n",
    "# Display sample data structures\n",
    "print(\"\\n--- Sample Data Structures ---\")\n",
    "print(\"Alpaca format:\", json.dumps(alpaca_data[0], indent=2, ensure_ascii=False))\n",
    "print(\"Noisy format:\", json.dumps(noisy_data[0], indent=2, ensure_ascii=False))\n",
    "print(\"ChatML format:\", json.dumps(chatml_data[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Format Standardization ===\n",
    "print(\"\\n=== æ ¼å¼æª¢æŸ¥èˆ‡æ¨™æº–åŒ– (Format Checking & Standardization) ===\")\n",
    "\n",
    "\n",
    "class DatasetStandardizer:\n",
    "    \"\"\"Standardize different instruction dataset formats\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.target_format = [\"instruction\", \"input\", \"output\"]\n",
    "\n",
    "    def detect_format(self, sample: Dict) -> str:\n",
    "        \"\"\"Detect the format of a data sample\"\"\"\n",
    "        keys = set(sample.keys())\n",
    "\n",
    "        if \"messages\" in keys:\n",
    "            return \"chatml\"\n",
    "        elif \"instruction\" in keys and \"output\" in keys:\n",
    "            return \"alpaca\"\n",
    "        elif \"prompt\" in keys and \"response\" in keys:\n",
    "            return \"prompt_response\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "\n",
    "    def standardize_sample(self, sample: Dict) -> Dict:\n",
    "        \"\"\"Convert sample to standard Alpaca format\"\"\"\n",
    "        format_type = self.detect_format(sample)\n",
    "\n",
    "        if format_type == \"alpaca\":\n",
    "            # Ensure all required keys exist\n",
    "            standardized = {\n",
    "                \"instruction\": sample.get(\"instruction\", \"\"),\n",
    "                \"input\": sample.get(\"input\", \"\"),\n",
    "                \"output\": sample.get(\"output\", \"\"),\n",
    "            }\n",
    "        elif format_type == \"chatml\":\n",
    "            # Convert ChatML to Alpaca format\n",
    "            messages = sample[\"messages\"]\n",
    "            user_msg = next((m[\"content\"] for m in messages if m[\"role\"] == \"user\"), \"\")\n",
    "            assistant_msg = next(\n",
    "                (m[\"content\"] for m in messages if m[\"role\"] == \"assistant\"), \"\"\n",
    "            )\n",
    "\n",
    "            standardized = {\n",
    "                \"instruction\": user_msg,\n",
    "                \"input\": \"\",\n",
    "                \"output\": assistant_msg,\n",
    "            }\n",
    "        elif format_type == \"prompt_response\":\n",
    "            # Convert prompt/response to Alpaca format\n",
    "            standardized = {\n",
    "                \"instruction\": sample.get(\"prompt\", \"\"),\n",
    "                \"input\": \"\",\n",
    "                \"output\": sample.get(\"response\", \"\"),\n",
    "            }\n",
    "        else:\n",
    "            # Unknown format - try to salvage what we can\n",
    "            standardized = {\"instruction\": \"\", \"input\": \"\", \"output\": \"\"}\n",
    "\n",
    "        return standardized\n",
    "\n",
    "    def standardize_dataset(self, dataset: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Standardize entire dataset\"\"\"\n",
    "        standardized = []\n",
    "        format_counts = defaultdict(int)\n",
    "\n",
    "        for sample in dataset:\n",
    "            format_type = self.detect_format(sample)\n",
    "            format_counts[format_type] += 1\n",
    "\n",
    "            try:\n",
    "                std_sample = self.standardize_sample(sample)\n",
    "                standardized.append(std_sample)\n",
    "            except Exception as e:\n",
    "                print(f\"Error standardizing sample: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Format distribution: {dict(format_counts)}\")\n",
    "        return standardized\n",
    "\n",
    "\n",
    "# Test standardization\n",
    "standardizer = DatasetStandardizer()\n",
    "\n",
    "# Combine all datasets for processing\n",
    "all_samples = alpaca_data + noisy_data + chatml_data\n",
    "print(f\"Total samples before standardization: {len(all_samples)}\")\n",
    "\n",
    "standardized_dataset = standardizer.standardize_dataset(all_samples)\n",
    "print(f\"Total samples after standardization: {len(standardized_dataset)}\")\n",
    "\n",
    "# Show standardization results\n",
    "print(\"\\n--- Standardization Results ---\")\n",
    "for i, sample in enumerate(standardized_dataset[:3]):\n",
    "    print(f\"Sample {i+1}:\", json.dumps(sample, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea00c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Duplicate Detection & Removal ===\n",
    "print(\"\\n=== é‡è¤‡è³‡æ–™æª¢æ¸¬èˆ‡å»é™¤ (Duplicate Detection & Removal) ===\")\n",
    "\n",
    "\n",
    "class DuplicateDetector:\n",
    "    \"\"\"Detect and remove duplicate instruction samples\"\"\"\n",
    "\n",
    "    def __init__(self, similarity_threshold: float = 0.8):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "    def get_content_hash(self, sample: Dict) -> str:\n",
    "        \"\"\"Generate hash for exact duplicate detection\"\"\"\n",
    "        # Normalize text for consistent hashing\n",
    "        instruction = sample[\"instruction\"].strip().lower()\n",
    "        input_text = sample[\"input\"].strip().lower()\n",
    "        output = sample[\"output\"].strip().lower()\n",
    "\n",
    "        content = f\"{instruction}|{input_text}|{output}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "\n",
    "    def calculate_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate text similarity using SequenceMatcher\"\"\"\n",
    "        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n",
    "\n",
    "    def find_exact_duplicates(self, dataset: List[Dict]) -> List[int]:\n",
    "        \"\"\"Find exact duplicate indices\"\"\"\n",
    "        seen_hashes = {}\n",
    "        duplicate_indices = []\n",
    "\n",
    "        for i, sample in enumerate(dataset):\n",
    "            content_hash = self.get_content_hash(sample)\n",
    "\n",
    "            if content_hash in seen_hashes:\n",
    "                duplicate_indices.append(i)\n",
    "                print(\n",
    "                    f\"Exact duplicate found at index {i} (same as {seen_hashes[content_hash]})\"\n",
    "                )\n",
    "            else:\n",
    "                seen_hashes[content_hash] = i\n",
    "\n",
    "        return duplicate_indices\n",
    "\n",
    "    def find_near_duplicates(self, dataset: List[Dict]) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"Find near-duplicate pairs\"\"\"\n",
    "        near_duplicates = []\n",
    "\n",
    "        for i in range(len(dataset)):\n",
    "            for j in range(i + 1, len(dataset)):\n",
    "                # Compare instructions\n",
    "                inst_sim = self.calculate_similarity(\n",
    "                    dataset[i][\"instruction\"], dataset[j][\"instruction\"]\n",
    "                )\n",
    "\n",
    "                # Compare outputs\n",
    "                out_sim = self.calculate_similarity(\n",
    "                    dataset[i][\"output\"], dataset[j][\"output\"]\n",
    "                )\n",
    "\n",
    "                # Combined similarity score\n",
    "                combined_sim = (inst_sim + out_sim) / 2\n",
    "\n",
    "                if combined_sim >= self.similarity_threshold:\n",
    "                    near_duplicates.append((i, j, combined_sim))\n",
    "\n",
    "        return near_duplicates\n",
    "\n",
    "    def remove_duplicates(self, dataset: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Remove both exact and near duplicates\"\"\"\n",
    "        # Find exact duplicates\n",
    "        exact_dups = self.find_exact_duplicates(dataset)\n",
    "\n",
    "        # Find near duplicates\n",
    "        near_dups = self.find_near_duplicates(dataset)\n",
    "\n",
    "        # Collect all indices to remove\n",
    "        indices_to_remove = set(exact_dups)\n",
    "\n",
    "        for i, j, similarity in near_dups:\n",
    "            print(f\"Near duplicate: samples {i} and {j} (similarity: {similarity:.3f})\")\n",
    "            # Keep the first occurrence, remove the second\n",
    "            indices_to_remove.add(j)\n",
    "\n",
    "        # Create cleaned dataset\n",
    "        cleaned_dataset = [\n",
    "            sample for i, sample in enumerate(dataset) if i not in indices_to_remove\n",
    "        ]\n",
    "\n",
    "        print(f\"Removed {len(indices_to_remove)} duplicate samples\")\n",
    "        return cleaned_dataset\n",
    "\n",
    "\n",
    "# Test duplicate detection\n",
    "detector = DuplicateDetector(similarity_threshold=0.8)\n",
    "deduplicated_dataset = detector.remove_duplicates(standardized_dataset)\n",
    "\n",
    "print(f\"Dataset size after deduplication: {len(deduplicated_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e311655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Content Quality Assessment ===\n",
    "print(\"\\n=== å…§å®¹å“è³ªè©•ä¼° (Content Quality Assessment) ===\")\n",
    "\n",
    "\n",
    "class QualityAssessor:\n",
    "    \"\"\"Assess and filter instruction data quality\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.min_instruction_length = 10\n",
    "        self.min_output_length = 20\n",
    "        self.max_output_length = 2000\n",
    "\n",
    "    def check_length_requirements(self, sample: Dict) -> Dict[str, bool]:\n",
    "        \"\"\"Check if sample meets length requirements\"\"\"\n",
    "        return {\n",
    "            \"instruction_length_ok\": len(sample[\"instruction\"])\n",
    "            >= self.min_instruction_length,\n",
    "            \"output_length_ok\": self.min_output_length\n",
    "            <= len(sample[\"output\"])\n",
    "            <= self.max_output_length,\n",
    "            \"not_empty\": bool(\n",
    "                sample[\"instruction\"].strip() and sample[\"output\"].strip()\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def check_content_quality(self, sample: Dict) -> Dict[str, bool]:\n",
    "        \"\"\"Assess content quality indicators\"\"\"\n",
    "        instruction = sample[\"instruction\"].lower()\n",
    "        output = sample[\"output\"].lower()\n",
    "\n",
    "        quality_checks = {\n",
    "            \"has_question_words\": any(\n",
    "                word in instruction\n",
    "                for word in [\n",
    "                    \"what\",\n",
    "                    \"how\",\n",
    "                    \"why\",\n",
    "                    \"when\",\n",
    "                    \"where\",\n",
    "                    \"which\",\n",
    "                    \"who\",\n",
    "                    \"explain\",\n",
    "                    \"describe\",\n",
    "                    \"tell\",\n",
    "                ]\n",
    "            ),\n",
    "            \"appropriate_response\": not any(\n",
    "                phrase in output\n",
    "                for phrase in [\"i don't know\", \"i can't help\", \"sorry\", \"i cannot\"]\n",
    "            ),\n",
    "            \"no_repetition\": not self._has_excessive_repetition(output),\n",
    "            \"coherent_language\": self._check_language_coherence(output),\n",
    "        }\n",
    "\n",
    "        return quality_checks\n",
    "\n",
    "    def _has_excessive_repetition(self, text: str) -> bool:\n",
    "        \"\"\"Check for excessive word/phrase repetition\"\"\"\n",
    "        words = text.split()\n",
    "        if len(words) < 10:\n",
    "            return False\n",
    "\n",
    "        word_counts = Counter(words)\n",
    "        max_count = max(word_counts.values())\n",
    "\n",
    "        # Flag if any word appears more than 30% of total words\n",
    "        return max_count > len(words) * 0.3\n",
    "\n",
    "    def _check_language_coherence(self, text: str) -> bool:\n",
    "        \"\"\"Basic check for language coherence\"\"\"\n",
    "        # Very simple checks\n",
    "        has_punctuation = any(p in text for p in \".!?\")\n",
    "        has_capital_letters = any(c.isupper() for c in text)\n",
    "        not_too_much_caps = sum(1 for c in text if c.isupper()) < len(text) * 0.5\n",
    "\n",
    "        return has_punctuation and has_capital_letters and not_too_much_caps\n",
    "\n",
    "    def assess_sample(self, sample: Dict) -> Dict:\n",
    "        \"\"\"Complete quality assessment for a sample\"\"\"\n",
    "        length_checks = self.check_length_requirements(sample)\n",
    "        content_checks = self.check_content_quality(sample)\n",
    "\n",
    "        # Combine all checks\n",
    "        all_checks = {**length_checks, **content_checks}\n",
    "\n",
    "        # Calculate overall quality score\n",
    "        quality_score = sum(all_checks.values()) / len(all_checks)\n",
    "\n",
    "        return {\n",
    "            \"sample\": sample,\n",
    "            \"checks\": all_checks,\n",
    "            \"quality_score\": quality_score,\n",
    "            \"passed\": quality_score >= 0.7,  # 70% of checks must pass\n",
    "        }\n",
    "\n",
    "    def filter_dataset(self, dataset: List[Dict]) -> Tuple[List[Dict], List[Dict]]:\n",
    "        \"\"\"Filter dataset based on quality assessment\"\"\"\n",
    "        high_quality = []\n",
    "        low_quality = []\n",
    "\n",
    "        for sample in dataset:\n",
    "            assessment = self.assess_sample(sample)\n",
    "\n",
    "            if assessment[\"passed\"]:\n",
    "                high_quality.append(sample)\n",
    "            else:\n",
    "                low_quality.append(\n",
    "                    {\n",
    "                        \"sample\": sample,\n",
    "                        \"issues\": [k for k, v in assessment[\"checks\"].items() if not v],\n",
    "                        \"score\": assessment[\"quality_score\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return high_quality, low_quality\n",
    "\n",
    "\n",
    "# Test quality assessment\n",
    "assessor = QualityAssessor()\n",
    "high_quality_data, low_quality_data = assessor.filter_dataset(deduplicated_dataset)\n",
    "\n",
    "print(f\"High quality samples: {len(high_quality_data)}\")\n",
    "print(f\"Low quality samples: {len(low_quality_data)}\")\n",
    "\n",
    "# Show quality issues\n",
    "print(\"\\n--- Quality Issues Found ---\")\n",
    "for item in low_quality_data:\n",
    "    print(f\"Issues: {item['issues']}\")\n",
    "    print(f\"Score: {item['score']:.2f}\")\n",
    "    print(f\"Sample: {item['sample']['instruction'][:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b65846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Privacy Information Filtering ===\n",
    "print(\"\\n=== éš±ç§è³‡è¨Šéæ¿¾ (Privacy Information Filtering) ===\")\n",
    "\n",
    "\n",
    "class PrivacyFilter:\n",
    "    \"\"\"Filter out personal and sensitive information\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Regex patterns for PII detection\n",
    "        self.patterns = {\n",
    "            \"email\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",\n",
    "            \"phone\": r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",\n",
    "            \"ssn\": r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",\n",
    "            \"address\": r\"\\b\\d{1,5}\\s+\\w+\\s+(street|st|avenue|ave|road|rd|lane|ln|drive|dr)\\b\",\n",
    "            \"credit_card\": r\"\\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b\",\n",
    "            \"person_name\": r\"\\b(my name is|i am|i\\'m)\\s+([A-Z][a-z]+\\s+[A-Z][a-z]+)\\b\",\n",
    "        }\n",
    "\n",
    "        # Sensitive keywords\n",
    "        self.sensitive_keywords = [\n",
    "            \"password\",\n",
    "            \"secret\",\n",
    "            \"confidential\",\n",
    "            \"private\",\n",
    "            \"personal\",\n",
    "            \"credit card\",\n",
    "            \"social security\",\n",
    "            \"bank account\",\n",
    "            \"medical record\",\n",
    "        ]\n",
    "\n",
    "    def detect_pii(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Detect PII in text\"\"\"\n",
    "        detected = {}\n",
    "\n",
    "        for pii_type, pattern in self.patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                detected[pii_type] = matches\n",
    "\n",
    "        return detected\n",
    "\n",
    "    def check_sensitive_content(self, text: str) -> List[str]:\n",
    "        \"\"\"Check for sensitive keywords\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        found_keywords = [kw for kw in self.sensitive_keywords if kw in text_lower]\n",
    "        return found_keywords\n",
    "\n",
    "    def mask_pii(self, text: str) -> str:\n",
    "        \"\"\"Mask detected PII with placeholders\"\"\"\n",
    "        masked_text = text\n",
    "\n",
    "        # Replace with generic placeholders\n",
    "        replacements = {\n",
    "            \"email\": \"[EMAIL]\",\n",
    "            \"phone\": \"[PHONE]\",\n",
    "            \"ssn\": \"[SSN]\",\n",
    "            \"address\": \"[ADDRESS]\",\n",
    "            \"credit_card\": \"[CREDIT_CARD]\",\n",
    "        }\n",
    "\n",
    "        for pii_type, pattern in self.patterns.items():\n",
    "            if pii_type in replacements:\n",
    "                masked_text = re.sub(\n",
    "                    pattern, replacements[pii_type], masked_text, flags=re.IGNORECASE\n",
    "                )\n",
    "\n",
    "        return masked_text\n",
    "\n",
    "    def filter_sample(self, sample: Dict) -> Dict:\n",
    "        \"\"\"Filter PII from a sample\"\"\"\n",
    "        # Check all text fields\n",
    "        pii_found = {}\n",
    "        sensitive_keywords = []\n",
    "\n",
    "        for field in [\"instruction\", \"input\", \"output\"]:\n",
    "            text = sample[field]\n",
    "            pii_found.update(self.detect_pii(text))\n",
    "            sensitive_keywords.extend(self.check_sensitive_content(text))\n",
    "\n",
    "        # Determine if sample should be filtered\n",
    "        has_pii = bool(pii_found)\n",
    "        has_sensitive = bool(sensitive_keywords)\n",
    "\n",
    "        if has_pii or has_sensitive:\n",
    "            # Option 1: Remove the sample entirely\n",
    "            # Option 2: Mask the PII and keep the sample\n",
    "            masked_sample = {\n",
    "                \"instruction\": self.mask_pii(sample[\"instruction\"]),\n",
    "                \"input\": self.mask_pii(sample[\"input\"]),\n",
    "                \"output\": self.mask_pii(sample[\"output\"]),\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"original\": sample,\n",
    "                \"masked\": masked_sample,\n",
    "                \"pii_found\": pii_found,\n",
    "                \"sensitive_keywords\": sensitive_keywords,\n",
    "                \"action\": \"mask\",  # or \"remove\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"original\": sample,\n",
    "                \"masked\": sample,\n",
    "                \"pii_found\": {},\n",
    "                \"sensitive_keywords\": [],\n",
    "                \"action\": \"keep\",\n",
    "            }\n",
    "\n",
    "    def filter_dataset(self, dataset: List[Dict]) -> Tuple[List[Dict], List[Dict]]:\n",
    "        \"\"\"Filter entire dataset for privacy issues\"\"\"\n",
    "        clean_samples = []\n",
    "        flagged_samples = []\n",
    "\n",
    "        for sample in dataset:\n",
    "            result = self.filter_sample(sample)\n",
    "\n",
    "            if result[\"action\"] == \"keep\":\n",
    "                clean_samples.append(result[\"original\"])\n",
    "            elif result[\"action\"] == \"mask\":\n",
    "                clean_samples.append(result[\"masked\"])  # Use masked version\n",
    "                flagged_samples.append(result)\n",
    "            else:  # remove\n",
    "                flagged_samples.append(result)\n",
    "\n",
    "        return clean_samples, flagged_samples\n",
    "\n",
    "\n",
    "# Test privacy filtering\n",
    "privacy_filter = PrivacyFilter()\n",
    "privacy_clean_data, privacy_flagged = privacy_filter.filter_dataset(high_quality_data)\n",
    "\n",
    "print(f\"Clean samples: {len(privacy_clean_data)}\")\n",
    "print(f\"Flagged samples: {len(privacy_flagged)}\")\n",
    "\n",
    "# Show flagged samples\n",
    "print(\"\\n--- Privacy Issues Found ---\")\n",
    "for item in privacy_flagged:\n",
    "    print(f\"PII found: {item['pii_found']}\")\n",
    "    print(f\"Sensitive keywords: {item['sensitive_keywords']}\")\n",
    "    print(f\"Action: {item['action']}\")\n",
    "    print(f\"Original: {item['original']['instruction'][:50]}...\")\n",
    "    print(f\"Masked: {item['masked']['instruction'][:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Data Augmentation & Diversification ===\n",
    "print(\"\\n=== è³‡æ–™å¢å¼·èˆ‡å¤šæ¨£åŒ– (Data Augmentation & Diversification) ===\")\n",
    "\n",
    "\n",
    "class DataAugmenter:\n",
    "    \"\"\"Augment instruction dataset with variations\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.instruction_templates = [\n",
    "            \"Please {verb} {object}\",\n",
    "            \"Can you {verb} {object}?\",\n",
    "            \"I need you to {verb} {object}\",\n",
    "            \"Help me {verb} {object}\",\n",
    "            \"{verb} {object}, please\",\n",
    "        ]\n",
    "\n",
    "        self.paraphrase_patterns = {\n",
    "            \"explain\": [\"describe\", \"clarify\", \"elaborate on\", \"tell me about\"],\n",
    "            \"write\": [\"create\", \"compose\", \"draft\", \"generate\"],\n",
    "            \"translate\": [\"convert\", \"transform\", \"render\"],\n",
    "            \"analyze\": [\"examine\", \"evaluate\", \"assess\", \"review\"],\n",
    "        }\n",
    "\n",
    "    def paraphrase_instruction(self, instruction: str) -> List[str]:\n",
    "        \"\"\"Generate paraphrases of instructions\"\"\"\n",
    "        paraphrases = []\n",
    "        instruction_lower = instruction.lower()\n",
    "\n",
    "        for original, alternatives in self.paraphrase_patterns.items():\n",
    "            if original in instruction_lower:\n",
    "                for alt in alternatives:\n",
    "                    paraphrased = instruction_lower.replace(original, alt)\n",
    "                    # Capitalize first letter\n",
    "                    paraphrased = paraphrased[0].upper() + paraphrased[1:]\n",
    "                    paraphrases.append(paraphrased)\n",
    "\n",
    "        return paraphrases\n",
    "\n",
    "    def generate_instruction_variants(self, instruction: str) -> List[str]:\n",
    "        \"\"\"Generate instruction variants using templates\"\"\"\n",
    "        variants = []\n",
    "\n",
    "        # Simple keyword extraction for template filling\n",
    "        if \"explain\" in instruction.lower():\n",
    "            topic = instruction.lower().replace(\"explain\", \"\").strip()\n",
    "            for template in self.instruction_templates:\n",
    "                variant = template.format(verb=\"explain\", object=topic)\n",
    "                variants.append(variant)\n",
    "\n",
    "        return variants\n",
    "\n",
    "    def augment_sample(self, sample: Dict) -> List[Dict]:\n",
    "        \"\"\"Create augmented versions of a sample\"\"\"\n",
    "        augmented = [sample]  # Include original\n",
    "\n",
    "        # Generate instruction paraphrases\n",
    "        paraphrases = self.paraphrase_instruction(sample[\"instruction\"])\n",
    "        for paraphrase in paraphrases[:2]:  # Limit to 2 paraphrases\n",
    "            augmented_sample = sample.copy()\n",
    "            augmented_sample[\"instruction\"] = paraphrase\n",
    "            augmented.append(augmented_sample)\n",
    "\n",
    "        # Generate instruction variants\n",
    "        variants = self.generate_instruction_variants(sample[\"instruction\"])\n",
    "        for variant in variants[:1]:  # Limit to 1 variant\n",
    "            augmented_sample = sample.copy()\n",
    "            augmented_sample[\"instruction\"] = variant\n",
    "            augmented.append(augmented_sample)\n",
    "\n",
    "        return augmented\n",
    "\n",
    "    def augment_dataset(\n",
    "        self, dataset: List[Dict], augmentation_ratio: float = 0.3\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Augment a portion of the dataset\"\"\"\n",
    "        augmented_dataset = []\n",
    "\n",
    "        # Select samples to augment\n",
    "        num_to_augment = int(len(dataset) * augmentation_ratio)\n",
    "        samples_to_augment = dataset[:num_to_augment]\n",
    "        remaining_samples = dataset[num_to_augment:]\n",
    "\n",
    "        print(f\"Augmenting {num_to_augment} out of {len(dataset)} samples\")\n",
    "\n",
    "        # Augment selected samples\n",
    "        for sample in samples_to_augment:\n",
    "            augmented_samples = self.augment_sample(sample)\n",
    "            augmented_dataset.extend(augmented_samples)\n",
    "\n",
    "        # Add remaining samples as-is\n",
    "        augmented_dataset.extend(remaining_samples)\n",
    "\n",
    "        return augmented_dataset\n",
    "\n",
    "\n",
    "# Test data augmentation\n",
    "augmenter = DataAugmenter()\n",
    "augmented_dataset = augmenter.augment_dataset(\n",
    "    privacy_clean_data, augmentation_ratio=0.3\n",
    ")\n",
    "\n",
    "print(f\"Dataset size after augmentation: {len(augmented_dataset)}\")\n",
    "\n",
    "# Show augmentation examples\n",
    "print(\"\\n--- Augmentation Examples ---\")\n",
    "original_sample = privacy_clean_data[0]\n",
    "augmented_samples = augmenter.augment_sample(original_sample)\n",
    "\n",
    "print(f\"Original: {original_sample['instruction']}\")\n",
    "for i, aug_sample in enumerate(augmented_samples[1:], 1):\n",
    "    print(f\"Variant {i}: {aug_sample['instruction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16babbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Final Dataset Output & Validation ===\n",
    "print(\"\\n=== æœ€çµ‚è³‡æ–™é›†è¼¸å‡ºèˆ‡é©—è­‰ (Final Dataset Output & Validation) ===\")\n",
    "\n",
    "class DatasetValidator:\n",
    "    \"\"\"Validate final curated dataset\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def validate_format(self, dataset: List[Dict]) -> Dict:\n",
    "        \"\"\"Validate dataset format consistency\"\"\"\n",
    "        required_fields = {\"instruction\", \"input\", \"output\"}\n",
    "        validation_results = {\n",
    "            \"total_samples\": len(dataset),\n",
    "            \"format_valid\": 0,\n",
    "            \"missing_fields\": [],\n",
    "            \"empty_fields\": {\"instruction\": 0, \"input\": 0, \"output\": 0}\n",
    "        }\n",
    "\n",
    "        for sample in dataset:\n",
    "            sample_fields = set(sample.keys())\n",
    "\n",
    "            if required_fields.issubset(sample_fields):\n",
    "                validation_results[\"format_valid\"] += 1\n",
    "            else:\n",
    "                missing = required_fields - sample_fields\n",
    "                validation_results[\"missing_fields\"].extend(missing)\n",
    "\n",
    "            # Check for empty fields\n",
    "            for field in required_fields:\n",
    "                if field in sample and not sample[field].strip():\n",
    "                    validation_results[\"empty_fields\"][field] += 1\n",
    "\n",
    "        return validation_results\n",
    "\n",
    "    def calculate_statistics(self, dataset: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate dataset statistics\"\"\"\n",
    "        instructions = [s[\"instruction\"] for s in dataset]\n",
    "        outputs = [s[\"output\"] for s in dataset]\n",
    "\n",
    "        stats = {\n",
    "            \"instruction_lengths\": {\n",
    "                \"min\": min(len(i) for i in instructions),\n",
    "                \"max\": max(len(i) for i in instructions),\n",
    "                \"avg\": sum(len(i) for i in instructions) / len(instructions)\n",
    "            },\n",
    "            \"output_lengths\": {\n",
    "                \"min\": min(len(o) for o in outputs),\n",
    "                \"max\": max(len(o) for o in outputs),\n",
    "                \"avg\": sum(len(o) for o in outputs) / len(outputs)\n",
    "            },\n",
    "            \"unique_instructions\": len(set(instructions)),\n",
    "            \"samples_with_input\": sum(1 for s in dataset if s[\"input\"].strip())\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def generate_report(self, dataset: List[Dict]) -> str:\n",
    "        \"\"\"Generate comprehensive dataset report\"\"\"\n",
    "        format_validation = self.validate_format(dataset)\n",
    "        statistics = self.calculate_statistics(dataset)\n",
    "\n",
    "        report = f\"\"\"\n",
    "=== Dataset Curation Report ===\n",
    "\n",
    "Format Validation:\n",
    "- Total samples: {format_validation['total_samples']}\n",
    "- Format valid: {format_validation['format_valid']}\n",
    "- Missing fields found: {set(format_validation['missing_fields'])}\n",
    "- Empty instructions: {format_validation['empty_fields']['instruction']}\n",
    "- Empty outputs: {format_validation['empty_fields']['output']}\n",
    "\n",
    "Content Statistics:\n",
    "- Instruction lengths: min={statistics['instruction_lengths']['min']}, max={statistics['instruction_lengths']['max']}, avg={statistics['instruction_lengths']['avg']:.1f}\n",
    "- Output lengths: min={statistics['output_lengths']['min']}, max={statistics['output_lengths']['max']}, avg={statistics['output_lengths']['avg']:.1f}\n",
    "- Unique instructions: {statistics['unique_instructions']} ({statistics['unique_instructions']/format_validation['total_samples']*100:.1f}% unique)\n",
    "- Samples with input: {statistics['samples_with_input']}\n",
    "\n",
    "Recommendation: {'âœ… Dataset ready for training' if format_validation['format_valid'] == format_validation['total_samples'] else 'âš ï¸ Format issues need attention'}\n",
    "\"\"\"\n",
    "        return report\n",
    "\n",
    "# Validate final dataset\n",
    "validator = DatasetValidator()\n",
    "final_report = validator.generate_report(augmented_dataset)\n",
    "print(final_report)\n",
    "\n",
    "# Save final curated dataset\n",
    "output_path = f\"{AI_CACHE_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999265c8",
   "metadata": {},
   "source": [
    "\n",
    "## 6. æœ¬ç« å°çµ (Stage Summary)\n",
    "\n",
    "### âœ… å®Œæˆé …ç›® (Completed Items)\n",
    "- **å¤šæ ¼å¼æ¨™æº–åŒ–**: æ”¯æ´ Alpaca, ChatML, Prompt-Response ç­‰æ ¼å¼è½‰æ›\n",
    "- **æ™ºæ…§å»é‡ç³»çµ±**: ç²¾ç¢ºèˆ‡èªç¾©é‡è¤‡æª¢æ¸¬ï¼Œå¯èª¿æ•´ç›¸ä¼¼åº¦é–¾å€¼\n",
    "- **å…¨é¢å“è³ªè©•ä¼°**: é•·åº¦ã€å…§å®¹ã€èªè¨€é€£è²«æ€§å¤šç¶­åº¦æª¢æŸ¥\n",
    "- **éš±ç§ä¿è­·æ©Ÿåˆ¶**: PII æª¢æ¸¬èˆ‡é®ç½©ï¼Œå¹³è¡¡å®‰å…¨æ€§èˆ‡å¯ç”¨æ€§\n",
    "- **è³‡æ–™å¢å¼·æŠ€è¡“**: æŒ‡ä»¤æ”¹å¯«èˆ‡è®Šæ›ï¼Œæå‡è³‡æ–™é›†å¤šæ¨£æ€§\n",
    "- **é©—è­‰èˆ‡åˆ†å‰²**: è‡ªå‹• train/val åˆ‡åˆ†ï¼Œå®Œæ•´å“è³ªå ±å‘Š\n",
    "\n",
    "### ğŸ”‘ æ ¸å¿ƒåŸç† (Core Concepts)\n",
    "- **è³‡æ–™å“è³ªé‡‘å­—å¡”**: æ ¼å¼ â†’ å»é‡ â†’ å“è³ª â†’ éš±ç§ â†’ å¢å¼·\n",
    "- **å¹³è¡¡æ€§åŸå‰‡**: åš´æ ¼éæ¿¾ vs è³‡æ–™ä¿ç•™çš„æ¬Šè¡¡\n",
    "- **èªç¾©ç†è§£**: ä¸åƒ…çœ‹è¡¨é¢æ–‡å­—ï¼Œæ›´é—œæ³¨èªç¾©ç›¸ä¼¼æ€§\n",
    "- **éš±ç§å„ªå…ˆ**: é è¨­å®‰å…¨çš„è³‡æ–™è™•ç†ç­–ç•¥\n",
    "- **å¯é‡ç¾æ€§**: è¨­å®šéš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿çµæœä¸€è‡´æ€§\n",
    "\n",
    "### ğŸš¨ å¸¸è¦‹é™·é˜± (Common Pitfalls)\n",
    "- éåº¦å»é‡å¯èƒ½ç§»é™¤æœ‰åƒ¹å€¼çš„æ¨£æœ¬è®ŠåŒ–\n",
    "- å“è³ªæ¨™æº–éæ–¼åš´æ ¼æœƒæå¤±é‚Šç•Œæœ‰ç”¨æ¨£æœ¬\n",
    "- PII æª¢æ¸¬å¯èƒ½æœ‰æ¼æª¢æˆ–èª¤æª¢\n",
    "- è³‡æ–™å¢å¼·éœ€é¿å…èªç¾©æ¼‚ç§»\n",
    "- å¿½ç•¥ä¸å¹³è¡¡åˆ†ä½ˆå•é¡Œ\n",
    "\n",
    "### ğŸ¯ ä¸‹ä¸€æ­¥å»ºè­° (Next Steps)\n",
    "1. **å¾®èª¿é©—è­‰**: åœ¨å¯¦éš›æ¨¡å‹ä¸Šæ¸¬è©¦æ¸…æ´—å¾Œè³‡æ–™æ•ˆæœ\n",
    "2. **å“è³ªè¿­ä»£**: æ ¹æ“šæ¨¡å‹è¡¨ç¾èª¿æ•´éæ¿¾æ¨™æº–\n",
    "3. **è‡ªå‹•åŒ–ç›£æ§**: å»ºç«‹è³‡æ–™å“è³ªçš„æŒçºŒç›£æ§ç³»çµ±\n",
    "4. **é€²éšæŠ€è¡“**: æ¢ç´¢åŸºæ–¼ç¥ç¶“ç¶²è·¯çš„èªç¾©å»é‡æ–¹æ³•\n",
    "\n",
    "**ä½•æ™‚ä½¿ç”¨é€™äº›æŠ€è¡“ (When to Use)**:\n",
    "- æº–å‚™å¾®èª¿è³‡æ–™å‰çš„å¿…è¦æ­¥é©Ÿ\n",
    "- åˆä½µå¤šä¾†æºè³‡æ–™é›†æ™‚  \n",
    "- ç™¼ç¾æ¨¡å‹è¼¸å‡ºå“è³ªä¸‹é™æ™‚\n",
    "- éœ€è¦ç¢ºä¿è³‡æ–™éš±ç§åˆè¦æ™‚\n",
    "- è³‡æ–™é‡ä¸è¶³éœ€è¦é©åº¦å¢å¼·æ™‚\n",
    "\n",
    "é€™å€‹è³‡æ–™é›†æ•´ç†æµç¨‹ç‚ºå¾ŒçºŒçš„ LoRA/QLoRA å¾®èª¿å¥ å®šäº†å …å¯¦åŸºç¤ï¼Œç¢ºä¿è¨“ç·´è³‡æ–™çš„é«˜å“è³ªèˆ‡ä¸€è‡´æ€§ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
