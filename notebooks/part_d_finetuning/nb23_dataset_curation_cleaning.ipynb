{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c459ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb23_dataset_curation_cleaning.ipynb\n",
    "# 資料集整理與清洗 (Dataset Curation & Cleaning)\n",
    "\n",
    "# === Cell 1: Shared Cache Bootstrap ===\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())\n",
    "\n",
    "# Standard imports for dataset curation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import hashlib\n",
    "from difflib import SequenceMatcher\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96664745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Dataset Loading & Initial Exploration ===\n",
    "print(\"=== 資料集載入與初步探索 (Dataset Loading & Exploration) ===\")\n",
    "\n",
    "\n",
    "# Sample instruction dataset creation (simulating multiple sources)\n",
    "def create_sample_datasets():\n",
    "    \"\"\"Create sample instruction datasets with various quality issues\"\"\"\n",
    "\n",
    "    # Dataset 1: Clean Alpaca-style\n",
    "    alpaca_samples = [\n",
    "        {\n",
    "            \"instruction\": \"Explain the concept of machine learning\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed.\",\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Translate the following English text to Spanish\",\n",
    "            \"input\": \"Hello, how are you?\",\n",
    "            \"output\": \"Hola, ¿cómo estás?\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Dataset 2: With quality issues\n",
    "    noisy_samples = [\n",
    "        {\n",
    "            \"instruction\": \"explain machine learning\",  # lowercase, duplicate topic\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"ML is when computers learn stuff automatically.\",  # too short\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"My name is John Smith and I live at 123 Main St\",  # PII\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"I can't help with personal information.\",\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Translate to Spanish\",  # missing input\n",
    "            \"input\": \"Hello, how are you?\",  # duplicate content\n",
    "            \"output\": \"Hola, ¿cómo estás?\",\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"What is AI?\",  # wrong format key\n",
    "            \"response\": \"Artificial Intelligence is...\",  # wrong format key\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Dataset 3: Different format (ChatML style)\n",
    "    chatmL_samples = [\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"Deep learning is a subset of machine learning that uses neural networks with multiple layers.\",\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return alpaca_samples, noisy_samples, chatmL_samples\n",
    "\n",
    "\n",
    "alpaca_data, noisy_data, chatml_data = create_sample_datasets()\n",
    "\n",
    "print(f\"Dataset 1 (Clean): {len(alpaca_data)} samples\")\n",
    "print(f\"Dataset 2 (Noisy): {len(noisy_data)} samples\")\n",
    "print(f\"Dataset 3 (ChatML): {len(chatml_data)} samples\")\n",
    "\n",
    "# Display sample data structures\n",
    "print(\"\\n--- Sample Data Structures ---\")\n",
    "print(\"Alpaca format:\", json.dumps(alpaca_data[0], indent=2, ensure_ascii=False))\n",
    "print(\"Noisy format:\", json.dumps(noisy_data[0], indent=2, ensure_ascii=False))\n",
    "print(\"ChatML format:\", json.dumps(chatml_data[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Format Standardization ===\n",
    "print(\"\\n=== 格式檢查與標準化 (Format Checking & Standardization) ===\")\n",
    "\n",
    "\n",
    "class DatasetStandardizer:\n",
    "    \"\"\"Standardize different instruction dataset formats\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.target_format = [\"instruction\", \"input\", \"output\"]\n",
    "\n",
    "    def detect_format(self, sample: Dict) -> str:\n",
    "        \"\"\"Detect the format of a data sample\"\"\"\n",
    "        keys = set(sample.keys())\n",
    "\n",
    "        if \"messages\" in keys:\n",
    "            return \"chatml\"\n",
    "        elif \"instruction\" in keys and \"output\" in keys:\n",
    "            return \"alpaca\"\n",
    "        elif \"prompt\" in keys and \"response\" in keys:\n",
    "            return \"prompt_response\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "\n",
    "    def standardize_sample(self, sample: Dict) -> Dict:\n",
    "        \"\"\"Convert sample to standard Alpaca format\"\"\"\n",
    "        format_type = self.detect_format(sample)\n",
    "\n",
    "        if format_type == \"alpaca\":\n",
    "            # Ensure all required keys exist\n",
    "            standardized = {\n",
    "                \"instruction\": sample.get(\"instruction\", \"\"),\n",
    "                \"input\": sample.get(\"input\", \"\"),\n",
    "                \"output\": sample.get(\"output\", \"\"),\n",
    "            }\n",
    "        elif format_type == \"chatml\":\n",
    "            # Convert ChatML to Alpaca format\n",
    "            messages = sample[\"messages\"]\n",
    "            user_msg = next((m[\"content\"] for m in messages if m[\"role\"] == \"user\"), \"\")\n",
    "            assistant_msg = next(\n",
    "                (m[\"content\"] for m in messages if m[\"role\"] == \"assistant\"), \"\"\n",
    "            )\n",
    "\n",
    "            standardized = {\n",
    "                \"instruction\": user_msg,\n",
    "                \"input\": \"\",\n",
    "                \"output\": assistant_msg,\n",
    "            }\n",
    "        elif format_type == \"prompt_response\":\n",
    "            # Convert prompt/response to Alpaca format\n",
    "            standardized = {\n",
    "                \"instruction\": sample.get(\"prompt\", \"\"),\n",
    "                \"input\": \"\",\n",
    "                \"output\": sample.get(\"response\", \"\"),\n",
    "            }\n",
    "        else:\n",
    "            # Unknown format - try to salvage what we can\n",
    "            standardized = {\"instruction\": \"\", \"input\": \"\", \"output\": \"\"}\n",
    "\n",
    "        return standardized\n",
    "\n",
    "    def standardize_dataset(self, dataset: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Standardize entire dataset\"\"\"\n",
    "        standardized = []\n",
    "        format_counts = defaultdict(int)\n",
    "\n",
    "        for sample in dataset:\n",
    "            format_type = self.detect_format(sample)\n",
    "            format_counts[format_type] += 1\n",
    "\n",
    "            try:\n",
    "                std_sample = self.standardize_sample(sample)\n",
    "                standardized.append(std_sample)\n",
    "            except Exception as e:\n",
    "                print(f\"Error standardizing sample: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Format distribution: {dict(format_counts)}\")\n",
    "        return standardized\n",
    "\n",
    "\n",
    "# Test standardization\n",
    "standardizer = DatasetStandardizer()\n",
    "\n",
    "# Combine all datasets for processing\n",
    "all_samples = alpaca_data + noisy_data + chatml_data\n",
    "print(f\"Total samples before standardization: {len(all_samples)}\")\n",
    "\n",
    "standardized_dataset = standardizer.standardize_dataset(all_samples)\n",
    "print(f\"Total samples after standardization: {len(standardized_dataset)}\")\n",
    "\n",
    "# Show standardization results\n",
    "print(\"\\n--- Standardization Results ---\")\n",
    "for i, sample in enumerate(standardized_dataset[:3]):\n",
    "    print(f\"Sample {i+1}:\", json.dumps(sample, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea00c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Duplicate Detection & Removal ===\n",
    "print(\"\\n=== 重複資料檢測與去除 (Duplicate Detection & Removal) ===\")\n",
    "\n",
    "\n",
    "class DuplicateDetector:\n",
    "    \"\"\"Detect and remove duplicate instruction samples\"\"\"\n",
    "\n",
    "    def __init__(self, similarity_threshold: float = 0.8):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "    def get_content_hash(self, sample: Dict) -> str:\n",
    "        \"\"\"Generate hash for exact duplicate detection\"\"\"\n",
    "        # Normalize text for consistent hashing\n",
    "        instruction = sample[\"instruction\"].strip().lower()\n",
    "        input_text = sample[\"input\"].strip().lower()\n",
    "        output = sample[\"output\"].strip().lower()\n",
    "\n",
    "        content = f\"{instruction}|{input_text}|{output}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "\n",
    "    def calculate_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate text similarity using SequenceMatcher\"\"\"\n",
    "        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n",
    "\n",
    "    def find_exact_duplicates(self, dataset: List[Dict]) -> List[int]:\n",
    "        \"\"\"Find exact duplicate indices\"\"\"\n",
    "        seen_hashes = {}\n",
    "        duplicate_indices = []\n",
    "\n",
    "        for i, sample in enumerate(dataset):\n",
    "            content_hash = self.get_content_hash(sample)\n",
    "\n",
    "            if content_hash in seen_hashes:\n",
    "                duplicate_indices.append(i)\n",
    "                print(\n",
    "                    f\"Exact duplicate found at index {i} (same as {seen_hashes[content_hash]})\"\n",
    "                )\n",
    "            else:\n",
    "                seen_hashes[content_hash] = i\n",
    "\n",
    "        return duplicate_indices\n",
    "\n",
    "    def find_near_duplicates(self, dataset: List[Dict]) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"Find near-duplicate pairs\"\"\"\n",
    "        near_duplicates = []\n",
    "\n",
    "        for i in range(len(dataset)):\n",
    "            for j in range(i + 1, len(dataset)):\n",
    "                # Compare instructions\n",
    "                inst_sim = self.calculate_similarity(\n",
    "                    dataset[i][\"instruction\"], dataset[j][\"instruction\"]\n",
    "                )\n",
    "\n",
    "                # Compare outputs\n",
    "                out_sim = self.calculate_similarity(\n",
    "                    dataset[i][\"output\"], dataset[j][\"output\"]\n",
    "                )\n",
    "\n",
    "                # Combined similarity score\n",
    "                combined_sim = (inst_sim + out_sim) / 2\n",
    "\n",
    "                if combined_sim >= self.similarity_threshold:\n",
    "                    near_duplicates.append((i, j, combined_sim))\n",
    "\n",
    "        return near_duplicates\n",
    "\n",
    "    def remove_duplicates(self, dataset: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Remove both exact and near duplicates\"\"\"\n",
    "        # Find exact duplicates\n",
    "        exact_dups = self.find_exact_duplicates(dataset)\n",
    "\n",
    "        # Find near duplicates\n",
    "        near_dups = self.find_near_duplicates(dataset)\n",
    "\n",
    "        # Collect all indices to remove\n",
    "        indices_to_remove = set(exact_dups)\n",
    "\n",
    "        for i, j, similarity in near_dups:\n",
    "            print(f\"Near duplicate: samples {i} and {j} (similarity: {similarity:.3f})\")\n",
    "            # Keep the first occurrence, remove the second\n",
    "            indices_to_remove.add(j)\n",
    "\n",
    "        # Create cleaned dataset\n",
    "        cleaned_dataset = [\n",
    "            sample for i, sample in enumerate(dataset) if i not in indices_to_remove\n",
    "        ]\n",
    "\n",
    "        print(f\"Removed {len(indices_to_remove)} duplicate samples\")\n",
    "        return cleaned_dataset\n",
    "\n",
    "\n",
    "# Test duplicate detection\n",
    "detector = DuplicateDetector(similarity_threshold=0.8)\n",
    "deduplicated_dataset = detector.remove_duplicates(standardized_dataset)\n",
    "\n",
    "print(f\"Dataset size after deduplication: {len(deduplicated_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e311655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Content Quality Assessment ===\n",
    "print(\"\\n=== 內容品質評估 (Content Quality Assessment) ===\")\n",
    "\n",
    "\n",
    "class QualityAssessor:\n",
    "    \"\"\"Assess and filter instruction data quality\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.min_instruction_length = 10\n",
    "        self.min_output_length = 20\n",
    "        self.max_output_length = 2000\n",
    "\n",
    "    def check_length_requirements(self, sample: Dict) -> Dict[str, bool]:\n",
    "        \"\"\"Check if sample meets length requirements\"\"\"\n",
    "        return {\n",
    "            \"instruction_length_ok\": len(sample[\"instruction\"])\n",
    "            >= self.min_instruction_length,\n",
    "            \"output_length_ok\": self.min_output_length\n",
    "            <= len(sample[\"output\"])\n",
    "            <= self.max_output_length,\n",
    "            \"not_empty\": bool(\n",
    "                sample[\"instruction\"].strip() and sample[\"output\"].strip()\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def check_content_quality(self, sample: Dict) -> Dict[str, bool]:\n",
    "        \"\"\"Assess content quality indicators\"\"\"\n",
    "        instruction = sample[\"instruction\"].lower()\n",
    "        output = sample[\"output\"].lower()\n",
    "\n",
    "        quality_checks = {\n",
    "            \"has_question_words\": any(\n",
    "                word in instruction\n",
    "                for word in [\n",
    "                    \"what\",\n",
    "                    \"how\",\n",
    "                    \"why\",\n",
    "                    \"when\",\n",
    "                    \"where\",\n",
    "                    \"which\",\n",
    "                    \"who\",\n",
    "                    \"explain\",\n",
    "                    \"describe\",\n",
    "                    \"tell\",\n",
    "                ]\n",
    "            ),\n",
    "            \"appropriate_response\": not any(\n",
    "                phrase in output\n",
    "                for phrase in [\"i don't know\", \"i can't help\", \"sorry\", \"i cannot\"]\n",
    "            ),\n",
    "            \"no_repetition\": not self._has_excessive_repetition(output),\n",
    "            \"coherent_language\": self._check_language_coherence(output),\n",
    "        }\n",
    "\n",
    "        return quality_checks\n",
    "\n",
    "    def _has_excessive_repetition(self, text: str) -> bool:\n",
    "        \"\"\"Check for excessive word/phrase repetition\"\"\"\n",
    "        words = text.split()\n",
    "        if len(words) < 10:\n",
    "            return False\n",
    "\n",
    "        word_counts = Counter(words)\n",
    "        max_count = max(word_counts.values())\n",
    "\n",
    "        # Flag if any word appears more than 30% of total words\n",
    "        return max_count > len(words) * 0.3\n",
    "\n",
    "    def _check_language_coherence(self, text: str) -> bool:\n",
    "        \"\"\"Basic check for language coherence\"\"\"\n",
    "        # Very simple checks\n",
    "        has_punctuation = any(p in text for p in \".!?\")\n",
    "        has_capital_letters = any(c.isupper() for c in text)\n",
    "        not_too_much_caps = sum(1 for c in text if c.isupper()) < len(text) * 0.5\n",
    "\n",
    "        return has_punctuation and has_capital_letters and not_too_much_caps\n",
    "\n",
    "    def assess_sample(self, sample: Dict) -> Dict:\n",
    "        \"\"\"Complete quality assessment for a sample\"\"\"\n",
    "        length_checks = self.check_length_requirements(sample)\n",
    "        content_checks = self.check_content_quality(sample)\n",
    "\n",
    "        # Combine all checks\n",
    "        all_checks = {**length_checks, **content_checks}\n",
    "\n",
    "        # Calculate overall quality score\n",
    "        quality_score = sum(all_checks.values()) / len(all_checks)\n",
    "\n",
    "        return {\n",
    "            \"sample\": sample,\n",
    "            \"checks\": all_checks,\n",
    "            \"quality_score\": quality_score,\n",
    "            \"passed\": quality_score >= 0.7,  # 70% of checks must pass\n",
    "        }\n",
    "\n",
    "    def filter_dataset(self, dataset: List[Dict]) -> Tuple[List[Dict], List[Dict]]:\n",
    "        \"\"\"Filter dataset based on quality assessment\"\"\"\n",
    "        high_quality = []\n",
    "        low_quality = []\n",
    "\n",
    "        for sample in dataset:\n",
    "            assessment = self.assess_sample(sample)\n",
    "\n",
    "            if assessment[\"passed\"]:\n",
    "                high_quality.append(sample)\n",
    "            else:\n",
    "                low_quality.append(\n",
    "                    {\n",
    "                        \"sample\": sample,\n",
    "                        \"issues\": [k for k, v in assessment[\"checks\"].items() if not v],\n",
    "                        \"score\": assessment[\"quality_score\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return high_quality, low_quality\n",
    "\n",
    "\n",
    "# Test quality assessment\n",
    "assessor = QualityAssessor()\n",
    "high_quality_data, low_quality_data = assessor.filter_dataset(deduplicated_dataset)\n",
    "\n",
    "print(f\"High quality samples: {len(high_quality_data)}\")\n",
    "print(f\"Low quality samples: {len(low_quality_data)}\")\n",
    "\n",
    "# Show quality issues\n",
    "print(\"\\n--- Quality Issues Found ---\")\n",
    "for item in low_quality_data:\n",
    "    print(f\"Issues: {item['issues']}\")\n",
    "    print(f\"Score: {item['score']:.2f}\")\n",
    "    print(f\"Sample: {item['sample']['instruction'][:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b65846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Privacy Information Filtering ===\n",
    "print(\"\\n=== 隱私資訊過濾 (Privacy Information Filtering) ===\")\n",
    "\n",
    "\n",
    "class PrivacyFilter:\n",
    "    \"\"\"Filter out personal and sensitive information\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Regex patterns for PII detection\n",
    "        self.patterns = {\n",
    "            \"email\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",\n",
    "            \"phone\": r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",\n",
    "            \"ssn\": r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",\n",
    "            \"address\": r\"\\b\\d{1,5}\\s+\\w+\\s+(street|st|avenue|ave|road|rd|lane|ln|drive|dr)\\b\",\n",
    "            \"credit_card\": r\"\\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b\",\n",
    "            \"person_name\": r\"\\b(my name is|i am|i\\'m)\\s+([A-Z][a-z]+\\s+[A-Z][a-z]+)\\b\",\n",
    "        }\n",
    "\n",
    "        # Sensitive keywords\n",
    "        self.sensitive_keywords = [\n",
    "            \"password\",\n",
    "            \"secret\",\n",
    "            \"confidential\",\n",
    "            \"private\",\n",
    "            \"personal\",\n",
    "            \"credit card\",\n",
    "            \"social security\",\n",
    "            \"bank account\",\n",
    "            \"medical record\",\n",
    "        ]\n",
    "\n",
    "    def detect_pii(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Detect PII in text\"\"\"\n",
    "        detected = {}\n",
    "\n",
    "        for pii_type, pattern in self.patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                detected[pii_type] = matches\n",
    "\n",
    "        return detected\n",
    "\n",
    "    def check_sensitive_content(self, text: str) -> List[str]:\n",
    "        \"\"\"Check for sensitive keywords\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        found_keywords = [kw for kw in self.sensitive_keywords if kw in text_lower]\n",
    "        return found_keywords\n",
    "\n",
    "    def mask_pii(self, text: str) -> str:\n",
    "        \"\"\"Mask detected PII with placeholders\"\"\"\n",
    "        masked_text = text\n",
    "\n",
    "        # Replace with generic placeholders\n",
    "        replacements = {\n",
    "            \"email\": \"[EMAIL]\",\n",
    "            \"phone\": \"[PHONE]\",\n",
    "            \"ssn\": \"[SSN]\",\n",
    "            \"address\": \"[ADDRESS]\",\n",
    "            \"credit_card\": \"[CREDIT_CARD]\",\n",
    "        }\n",
    "\n",
    "        for pii_type, pattern in self.patterns.items():\n",
    "            if pii_type in replacements:\n",
    "                masked_text = re.sub(\n",
    "                    pattern, replacements[pii_type], masked_text, flags=re.IGNORECASE\n",
    "                )\n",
    "\n",
    "        return masked_text\n",
    "\n",
    "    def filter_sample(self, sample: Dict) -> Dict:\n",
    "        \"\"\"Filter PII from a sample\"\"\"\n",
    "        # Check all text fields\n",
    "        pii_found = {}\n",
    "        sensitive_keywords = []\n",
    "\n",
    "        for field in [\"instruction\", \"input\", \"output\"]:\n",
    "            text = sample[field]\n",
    "            pii_found.update(self.detect_pii(text))\n",
    "            sensitive_keywords.extend(self.check_sensitive_content(text))\n",
    "\n",
    "        # Determine if sample should be filtered\n",
    "        has_pii = bool(pii_found)\n",
    "        has_sensitive = bool(sensitive_keywords)\n",
    "\n",
    "        if has_pii or has_sensitive:\n",
    "            # Option 1: Remove the sample entirely\n",
    "            # Option 2: Mask the PII and keep the sample\n",
    "            masked_sample = {\n",
    "                \"instruction\": self.mask_pii(sample[\"instruction\"]),\n",
    "                \"input\": self.mask_pii(sample[\"input\"]),\n",
    "                \"output\": self.mask_pii(sample[\"output\"]),\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"original\": sample,\n",
    "                \"masked\": masked_sample,\n",
    "                \"pii_found\": pii_found,\n",
    "                \"sensitive_keywords\": sensitive_keywords,\n",
    "                \"action\": \"mask\",  # or \"remove\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"original\": sample,\n",
    "                \"masked\": sample,\n",
    "                \"pii_found\": {},\n",
    "                \"sensitive_keywords\": [],\n",
    "                \"action\": \"keep\",\n",
    "            }\n",
    "\n",
    "    def filter_dataset(self, dataset: List[Dict]) -> Tuple[List[Dict], List[Dict]]:\n",
    "        \"\"\"Filter entire dataset for privacy issues\"\"\"\n",
    "        clean_samples = []\n",
    "        flagged_samples = []\n",
    "\n",
    "        for sample in dataset:\n",
    "            result = self.filter_sample(sample)\n",
    "\n",
    "            if result[\"action\"] == \"keep\":\n",
    "                clean_samples.append(result[\"original\"])\n",
    "            elif result[\"action\"] == \"mask\":\n",
    "                clean_samples.append(result[\"masked\"])  # Use masked version\n",
    "                flagged_samples.append(result)\n",
    "            else:  # remove\n",
    "                flagged_samples.append(result)\n",
    "\n",
    "        return clean_samples, flagged_samples\n",
    "\n",
    "\n",
    "# Test privacy filtering\n",
    "privacy_filter = PrivacyFilter()\n",
    "privacy_clean_data, privacy_flagged = privacy_filter.filter_dataset(high_quality_data)\n",
    "\n",
    "print(f\"Clean samples: {len(privacy_clean_data)}\")\n",
    "print(f\"Flagged samples: {len(privacy_flagged)}\")\n",
    "\n",
    "# Show flagged samples\n",
    "print(\"\\n--- Privacy Issues Found ---\")\n",
    "for item in privacy_flagged:\n",
    "    print(f\"PII found: {item['pii_found']}\")\n",
    "    print(f\"Sensitive keywords: {item['sensitive_keywords']}\")\n",
    "    print(f\"Action: {item['action']}\")\n",
    "    print(f\"Original: {item['original']['instruction'][:50]}...\")\n",
    "    print(f\"Masked: {item['masked']['instruction'][:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Data Augmentation & Diversification ===\n",
    "print(\"\\n=== 資料增強與多樣化 (Data Augmentation & Diversification) ===\")\n",
    "\n",
    "\n",
    "class DataAugmenter:\n",
    "    \"\"\"Augment instruction dataset with variations\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.instruction_templates = [\n",
    "            \"Please {verb} {object}\",\n",
    "            \"Can you {verb} {object}?\",\n",
    "            \"I need you to {verb} {object}\",\n",
    "            \"Help me {verb} {object}\",\n",
    "            \"{verb} {object}, please\",\n",
    "        ]\n",
    "\n",
    "        self.paraphrase_patterns = {\n",
    "            \"explain\": [\"describe\", \"clarify\", \"elaborate on\", \"tell me about\"],\n",
    "            \"write\": [\"create\", \"compose\", \"draft\", \"generate\"],\n",
    "            \"translate\": [\"convert\", \"transform\", \"render\"],\n",
    "            \"analyze\": [\"examine\", \"evaluate\", \"assess\", \"review\"],\n",
    "        }\n",
    "\n",
    "    def paraphrase_instruction(self, instruction: str) -> List[str]:\n",
    "        \"\"\"Generate paraphrases of instructions\"\"\"\n",
    "        paraphrases = []\n",
    "        instruction_lower = instruction.lower()\n",
    "\n",
    "        for original, alternatives in self.paraphrase_patterns.items():\n",
    "            if original in instruction_lower:\n",
    "                for alt in alternatives:\n",
    "                    paraphrased = instruction_lower.replace(original, alt)\n",
    "                    # Capitalize first letter\n",
    "                    paraphrased = paraphrased[0].upper() + paraphrased[1:]\n",
    "                    paraphrases.append(paraphrased)\n",
    "\n",
    "        return paraphrases\n",
    "\n",
    "    def generate_instruction_variants(self, instruction: str) -> List[str]:\n",
    "        \"\"\"Generate instruction variants using templates\"\"\"\n",
    "        variants = []\n",
    "\n",
    "        # Simple keyword extraction for template filling\n",
    "        if \"explain\" in instruction.lower():\n",
    "            topic = instruction.lower().replace(\"explain\", \"\").strip()\n",
    "            for template in self.instruction_templates:\n",
    "                variant = template.format(verb=\"explain\", object=topic)\n",
    "                variants.append(variant)\n",
    "\n",
    "        return variants\n",
    "\n",
    "    def augment_sample(self, sample: Dict) -> List[Dict]:\n",
    "        \"\"\"Create augmented versions of a sample\"\"\"\n",
    "        augmented = [sample]  # Include original\n",
    "\n",
    "        # Generate instruction paraphrases\n",
    "        paraphrases = self.paraphrase_instruction(sample[\"instruction\"])\n",
    "        for paraphrase in paraphrases[:2]:  # Limit to 2 paraphrases\n",
    "            augmented_sample = sample.copy()\n",
    "            augmented_sample[\"instruction\"] = paraphrase\n",
    "            augmented.append(augmented_sample)\n",
    "\n",
    "        # Generate instruction variants\n",
    "        variants = self.generate_instruction_variants(sample[\"instruction\"])\n",
    "        for variant in variants[:1]:  # Limit to 1 variant\n",
    "            augmented_sample = sample.copy()\n",
    "            augmented_sample[\"instruction\"] = variant\n",
    "            augmented.append(augmented_sample)\n",
    "\n",
    "        return augmented\n",
    "\n",
    "    def augment_dataset(\n",
    "        self, dataset: List[Dict], augmentation_ratio: float = 0.3\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Augment a portion of the dataset\"\"\"\n",
    "        augmented_dataset = []\n",
    "\n",
    "        # Select samples to augment\n",
    "        num_to_augment = int(len(dataset) * augmentation_ratio)\n",
    "        samples_to_augment = dataset[:num_to_augment]\n",
    "        remaining_samples = dataset[num_to_augment:]\n",
    "\n",
    "        print(f\"Augmenting {num_to_augment} out of {len(dataset)} samples\")\n",
    "\n",
    "        # Augment selected samples\n",
    "        for sample in samples_to_augment:\n",
    "            augmented_samples = self.augment_sample(sample)\n",
    "            augmented_dataset.extend(augmented_samples)\n",
    "\n",
    "        # Add remaining samples as-is\n",
    "        augmented_dataset.extend(remaining_samples)\n",
    "\n",
    "        return augmented_dataset\n",
    "\n",
    "\n",
    "# Test data augmentation\n",
    "augmenter = DataAugmenter()\n",
    "augmented_dataset = augmenter.augment_dataset(\n",
    "    privacy_clean_data, augmentation_ratio=0.3\n",
    ")\n",
    "\n",
    "print(f\"Dataset size after augmentation: {len(augmented_dataset)}\")\n",
    "\n",
    "# Show augmentation examples\n",
    "print(\"\\n--- Augmentation Examples ---\")\n",
    "original_sample = privacy_clean_data[0]\n",
    "augmented_samples = augmenter.augment_sample(original_sample)\n",
    "\n",
    "print(f\"Original: {original_sample['instruction']}\")\n",
    "for i, aug_sample in enumerate(augmented_samples[1:], 1):\n",
    "    print(f\"Variant {i}: {aug_sample['instruction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16babbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Final Dataset Output & Validation ===\n",
    "print(\"\\n=== 最終資料集輸出與驗證 (Final Dataset Output & Validation) ===\")\n",
    "\n",
    "class DatasetValidator:\n",
    "    \"\"\"Validate final curated dataset\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def validate_format(self, dataset: List[Dict]) -> Dict:\n",
    "        \"\"\"Validate dataset format consistency\"\"\"\n",
    "        required_fields = {\"instruction\", \"input\", \"output\"}\n",
    "        validation_results = {\n",
    "            \"total_samples\": len(dataset),\n",
    "            \"format_valid\": 0,\n",
    "            \"missing_fields\": [],\n",
    "            \"empty_fields\": {\"instruction\": 0, \"input\": 0, \"output\": 0}\n",
    "        }\n",
    "\n",
    "        for sample in dataset:\n",
    "            sample_fields = set(sample.keys())\n",
    "\n",
    "            if required_fields.issubset(sample_fields):\n",
    "                validation_results[\"format_valid\"] += 1\n",
    "            else:\n",
    "                missing = required_fields - sample_fields\n",
    "                validation_results[\"missing_fields\"].extend(missing)\n",
    "\n",
    "            # Check for empty fields\n",
    "            for field in required_fields:\n",
    "                if field in sample and not sample[field].strip():\n",
    "                    validation_results[\"empty_fields\"][field] += 1\n",
    "\n",
    "        return validation_results\n",
    "\n",
    "    def calculate_statistics(self, dataset: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate dataset statistics\"\"\"\n",
    "        instructions = [s[\"instruction\"] for s in dataset]\n",
    "        outputs = [s[\"output\"] for s in dataset]\n",
    "\n",
    "        stats = {\n",
    "            \"instruction_lengths\": {\n",
    "                \"min\": min(len(i) for i in instructions),\n",
    "                \"max\": max(len(i) for i in instructions),\n",
    "                \"avg\": sum(len(i) for i in instructions) / len(instructions)\n",
    "            },\n",
    "            \"output_lengths\": {\n",
    "                \"min\": min(len(o) for o in outputs),\n",
    "                \"max\": max(len(o) for o in outputs),\n",
    "                \"avg\": sum(len(o) for o in outputs) / len(outputs)\n",
    "            },\n",
    "            \"unique_instructions\": len(set(instructions)),\n",
    "            \"samples_with_input\": sum(1 for s in dataset if s[\"input\"].strip())\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def generate_report(self, dataset: List[Dict]) -> str:\n",
    "        \"\"\"Generate comprehensive dataset report\"\"\"\n",
    "        format_validation = self.validate_format(dataset)\n",
    "        statistics = self.calculate_statistics(dataset)\n",
    "\n",
    "        report = f\"\"\"\n",
    "=== Dataset Curation Report ===\n",
    "\n",
    "Format Validation:\n",
    "- Total samples: {format_validation['total_samples']}\n",
    "- Format valid: {format_validation['format_valid']}\n",
    "- Missing fields found: {set(format_validation['missing_fields'])}\n",
    "- Empty instructions: {format_validation['empty_fields']['instruction']}\n",
    "- Empty outputs: {format_validation['empty_fields']['output']}\n",
    "\n",
    "Content Statistics:\n",
    "- Instruction lengths: min={statistics['instruction_lengths']['min']}, max={statistics['instruction_lengths']['max']}, avg={statistics['instruction_lengths']['avg']:.1f}\n",
    "- Output lengths: min={statistics['output_lengths']['min']}, max={statistics['output_lengths']['max']}, avg={statistics['output_lengths']['avg']:.1f}\n",
    "- Unique instructions: {statistics['unique_instructions']} ({statistics['unique_instructions']/format_validation['total_samples']*100:.1f}% unique)\n",
    "- Samples with input: {statistics['samples_with_input']}\n",
    "\n",
    "Recommendation: {'✅ Dataset ready for training' if format_validation['format_valid'] == format_validation['total_samples'] else '⚠️ Format issues need attention'}\n",
    "\"\"\"\n",
    "        return report\n",
    "\n",
    "# Validate final dataset\n",
    "validator = DatasetValidator()\n",
    "final_report = validator.generate_report(augmented_dataset)\n",
    "print(final_report)\n",
    "\n",
    "# Save final curated dataset\n",
    "output_path = f\"{AI_CACHE_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999265c8",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 本章小結 (Stage Summary)\n",
    "\n",
    "### ✅ 完成項目 (Completed Items)\n",
    "- **多格式標準化**: 支援 Alpaca, ChatML, Prompt-Response 等格式轉換\n",
    "- **智慧去重系統**: 精確與語義重複檢測，可調整相似度閾值\n",
    "- **全面品質評估**: 長度、內容、語言連貫性多維度檢查\n",
    "- **隱私保護機制**: PII 檢測與遮罩，平衡安全性與可用性\n",
    "- **資料增強技術**: 指令改寫與變換，提升資料集多樣性\n",
    "- **驗證與分割**: 自動 train/val 切分，完整品質報告\n",
    "\n",
    "### 🔑 核心原理 (Core Concepts)\n",
    "- **資料品質金字塔**: 格式 → 去重 → 品質 → 隱私 → 增強\n",
    "- **平衡性原則**: 嚴格過濾 vs 資料保留的權衡\n",
    "- **語義理解**: 不僅看表面文字，更關注語義相似性\n",
    "- **隱私優先**: 預設安全的資料處理策略\n",
    "- **可重現性**: 設定隨機種子，確保結果一致性\n",
    "\n",
    "### 🚨 常見陷阱 (Common Pitfalls)\n",
    "- 過度去重可能移除有價值的樣本變化\n",
    "- 品質標準過於嚴格會損失邊界有用樣本\n",
    "- PII 檢測可能有漏檢或誤檢\n",
    "- 資料增強需避免語義漂移\n",
    "- 忽略不平衡分佈問題\n",
    "\n",
    "### 🎯 下一步建議 (Next Steps)\n",
    "1. **微調驗證**: 在實際模型上測試清洗後資料效果\n",
    "2. **品質迭代**: 根據模型表現調整過濾標準\n",
    "3. **自動化監控**: 建立資料品質的持續監控系統\n",
    "4. **進階技術**: 探索基於神經網路的語義去重方法\n",
    "\n",
    "**何時使用這些技術 (When to Use)**:\n",
    "- 準備微調資料前的必要步驟\n",
    "- 合併多來源資料集時  \n",
    "- 發現模型輸出品質下降時\n",
    "- 需要確保資料隱私合規時\n",
    "- 資料量不足需要適度增強時\n",
    "\n",
    "這個資料集整理流程為後續的 LoRA/QLoRA 微調奠定了堅實基礎，確保訓練資料的高品質與一致性！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
