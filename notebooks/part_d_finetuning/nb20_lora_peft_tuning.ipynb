{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d613d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb20_lora_peft_tuning.ipynb\n",
    "# LoRA (Low-Rank Adaptation) å¾®èª¿å¯¦æˆ°\n",
    "\n",
    "# Cell 1: Environment Setup and Shared Cache\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\"\n",
    "    )\n",
    "\n",
    "# Install required packages\n",
    "# !pip install transformers==4.36.0 peft==0.7.1 datasets==2.14.0 accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: LoRA Theory and Configuration\n",
    "\"\"\"\n",
    "LoRA (Low-Rank Adaptation) åŸç†ï¼š\n",
    "- åŸå§‹æ¬Šé‡çŸ©é™£ W âˆˆ R^(dÃ—k) ä¿æŒå‡çµ\n",
    "- æ·»åŠ ä½ç§©åˆ†è§£ï¼šÎ”W = BAï¼Œå…¶ä¸­ B âˆˆ R^(dÃ—r), A âˆˆ R^(rÃ—k)\n",
    "- r << min(d,k)ï¼Œå¤§å¹…æ¸›å°‘å¯è¨“ç·´åƒæ•¸\n",
    "- å‰å‘å‚³æ’­ï¼šh = (W + Î±/r * BA)x\n",
    "\n",
    "é—œéµåƒæ•¸ï¼š\n",
    "- r (rank): ä½ç§©ç¶­åº¦ï¼Œé€šå¸¸ 4-64ï¼Œè¶Šå¤§è¡¨é”èƒ½åŠ›è¶Šå¼·ä½†åƒæ•¸è¶Šå¤š\n",
    "- Î± (alpha): ç¸®æ”¾å› å­ï¼Œæ§åˆ¶ LoRA è²¢ç»åº¦\n",
    "- target_modules: è¦æ‡‰ç”¨ LoRA çš„æ¨¡çµ„ï¼ˆé€šå¸¸æ˜¯ attention çš„ q,k,v,oï¼‰\n",
    "\"\"\"\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# LoRA configuration - conservative settings for stable training\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # rank - balance between performance and efficiency\n",
    "    lora_alpha=32,  # scaling factor (typically 2*r)\n",
    "    target_modules=[  # apply LoRA to attention layers\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",  # MLP layers for better coverage\n",
    "    ],\n",
    "    lora_dropout=0.1,  # dropout for regularization\n",
    "    bias=\"none\",  # don't train bias terms\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "print(\"LoRA Config:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d08faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Base Model Loading with Low-VRAM Settings\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"  # Good Chinese performance\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, trust_remote_code=True, cache_dir=os.environ[\"TRANSFORMERS_CACHE\"]\n",
    ")\n",
    "\n",
    "# Add padding token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with memory optimization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # automatic device placement\n",
    "    torch_dtype=torch.float16,  # half precision\n",
    "    load_in_4bit=True,  # 4-bit quantization for VRAM efficiency\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=os.environ[\"TRANSFORMERS_CACHE\"],\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Chinese Instruction Dataset Preparation\n",
    "\"\"\"\n",
    "å‰µå»ºä¸­æ–‡æŒ‡ä»¤æ•¸æ“šé›† - åŒ…å«æ—¥å¸¸å°è©±ã€çŸ¥è­˜å•ç­”ã€ä»»å‹™æŒ‡ä»¤ç­‰\n",
    "æ ¼å¼ï¼š{\"instruction\": \"å•é¡Œ\", \"output\": \"å›ç­”\"}\n",
    "\"\"\"\n",
    "\n",
    "# Sample Chinese instruction data - mix of conversation, QA, and tasks\n",
    "chinese_instructions = [\n",
    "    # Daily conversation\n",
    "    {\n",
    "        \"instruction\": \"ä½ å¥½ï¼Œä½ æ˜¯èª°ï¼Ÿ\",\n",
    "        \"output\": \"ä½ å¥½ï¼æˆ‘æ˜¯ä¸€å€‹AIåŠ©æ‰‹ï¼Œå¾ˆé«˜èˆˆç‚ºæ‚¨æœå‹™ã€‚æˆ‘å¯ä»¥å›ç­”å•é¡Œã€å”åŠ©ä»»å‹™ï¼Œæˆ–èˆ‡æ‚¨èŠå¤©ã€‚æœ‰ä»€éº¼æˆ‘å¯ä»¥å¹«åŠ©æ‚¨çš„å—ï¼Ÿ\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"ä»Šå¤©å¤©æ°£å¦‚ä½•ï¼Ÿ\",\n",
    "        \"output\": \"å¾ˆæŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•ç²å–å³æ™‚å¤©æ°£è³‡è¨Šã€‚å»ºè­°æ‚¨æŸ¥çœ‹å¤©æ°£æ‡‰ç”¨ç¨‹å¼æˆ–æ°£è±¡ç¶²ç«™ä¾†äº†è§£ç•¶å‰å¤©æ°£ç‹€æ³ã€‚\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"æ¨è–¦ä¸€äº›å¥½çœ‹çš„é›»å½±\",\n",
    "        \"output\": \"æˆ‘æ¨è–¦å¹¾éƒ¨ä¸åŒé¡å‹çš„å„ªç§€é›»å½±ï¼šã€Šè‚–ç”³å…‹çš„æ•‘è´–ã€‹ï¼ˆåŠ‡æƒ…ï¼‰ã€ã€Šæ˜Ÿéš›æ•ˆæ‡‰ã€‹ï¼ˆç§‘å¹»ï¼‰ã€ã€Šåƒèˆ‡åƒå°‹ã€‹ï¼ˆå‹•ç•«ï¼‰ã€ã€Šå¯„ç”ŸèŸ²ã€‹ï¼ˆé©šæ‚šï¼‰ã€‚æ‚¨åå¥½å“ªç¨®é¡å‹å‘¢ï¼Ÿ\",\n",
    "    },\n",
    "    # Knowledge QA\n",
    "    {\n",
    "        \"instruction\": \"ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ\",\n",
    "        \"output\": \"äººå·¥æ™ºæ…§ï¼ˆAIï¼‰æ˜¯æŒ‡è®“æ©Ÿå™¨æ¨¡æ“¬äººé¡æ™ºèƒ½çš„æŠ€è¡“ï¼ŒåŒ…æ‹¬å­¸ç¿’ã€æ¨ç†ã€æ„ŸçŸ¥ã€æ±ºç­–ç­‰èƒ½åŠ›ã€‚ä¸»è¦åˆ†æ”¯æœ‰æ©Ÿå™¨å­¸ç¿’ã€æ·±åº¦å­¸ç¿’ã€è‡ªç„¶èªè¨€è™•ç†ã€é›»è…¦è¦–è¦ºç­‰ã€‚\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"å°ç£çš„é¦–éƒ½æ˜¯å“ªè£¡ï¼Ÿ\",\n",
    "        \"output\": \"å°ç£çš„é¦–éƒ½æ˜¯å°åŒ—å¸‚ã€‚å°åŒ—æ˜¯å°ç£çš„æ”¿æ²»ã€ç¶“æ¿Ÿã€æ–‡åŒ–ä¸­å¿ƒï¼Œä¹Ÿæ˜¯æœ€å¤§çš„éƒ½å¸‚ã€‚\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’\",\n",
    "        \"output\": \"æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯ï¼Œè®“é›»è…¦é€šéæ•¸æ“šè‡ªå‹•å­¸ç¿’å’Œæ”¹é€²ï¼Œè€Œä¸éœ€è¦æ˜ç¢ºç·¨ç¨‹ã€‚ä¸»è¦é¡å‹åŒ…æ‹¬ç›£ç£å­¸ç¿’ã€ç„¡ç›£ç£å­¸ç¿’å’Œå¼·åŒ–å­¸ç¿’ã€‚\",\n",
    "    },\n",
    "    # Task instructions\n",
    "    {\n",
    "        \"instruction\": \"å¹«æˆ‘å¯«ä¸€å°æ„Ÿè¬ä¿¡\",\n",
    "        \"output\": \"è¦ªæ„›çš„[æ”¶ä»¶äººå§“å]ï¼š\\n\\næ„Ÿè¬æ‚¨åœ¨[å…·é«”äº‹ä»¶]ä¸­çµ¦äºˆçš„å¹«åŠ©å’Œæ”¯æŒã€‚æ‚¨çš„[å…·é«”å¹«åŠ©å…§å®¹]å°æˆ‘ä¾†èªªæ„ç¾©é‡å¤§ï¼Œè®“æˆ‘èƒ½å¤ [é”æˆçš„çµæœ]ã€‚\\n\\nå†æ¬¡è¡¨é”æˆ‘çš„èª æ‘¯è¬æ„ï¼ŒæœŸå¾…æœªä¾†æœ‰æ©Ÿæœƒå›å ±æ‚¨çš„æ©æƒ…ã€‚\\n\\næ­¤è‡´\\næ•¬ç¦®\\n\\n[æ‚¨çš„å§“å]\\n[æ—¥æœŸ]\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"ç¸½çµä»¥ä¸‹æ–‡å­—çš„é‡é»\",\n",
    "        \"output\": \"å¾ˆæŠ±æ­‰ï¼Œæ‚¨æ²’æœ‰æä¾›éœ€è¦ç¸½çµçš„æ–‡å­—å…§å®¹ã€‚è«‹æä¾›å…·é«”çš„æ–‡å­—ææ–™ï¼Œæˆ‘å°‡ç‚ºæ‚¨æå–é‡é»å’Œé—œéµè³‡è¨Šã€‚\",\n",
    "    },\n",
    "    {\"instruction\": \"ç¿»è­¯ï¼šHello, how are you?\", \"output\": \"ä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ\"},\n",
    "    {\n",
    "        \"instruction\": \"ç”¨Pythonå¯«ä¸€å€‹è¨ˆç®—éšä¹˜çš„å‡½æ•¸\",\n",
    "        \"output\": \"```python\\ndef factorial(n):\\n    if n < 0:\\n        return None  # è² æ•¸æ²’æœ‰éšä¹˜\\n    elif n == 0 or n == 1:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)\\n\\n# ä½¿ç”¨ç¯„ä¾‹\\nprint(factorial(5))  # è¼¸å‡º: 120\\n```\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Extend dataset with more samples (you can add more or load from file)\n",
    "extended_data = chinese_instructions * 5  # Repeat for more training data\n",
    "\n",
    "\n",
    "# Format data for training (Chat template format)\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format instruction-response pair using chat template\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "# Convert to formatted dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "formatted_data = [format_instruction(ex) for ex in extended_data]\n",
    "train_dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(\"\\nSample formatted text:\")\n",
    "print(train_dataset[0][\"text\"][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: LoRA Model Setup and Wrapping\n",
    "\"\"\"\n",
    "ä½¿ç”¨ PEFT å°‡ LoRA é©é…å™¨åŒ…è£åˆ°åŸºç¤æ¨¡å‹ä¸Š\n",
    "åªæœ‰ LoRA åƒæ•¸æœƒè¢«è¨“ç·´ï¼ŒåŸºç¤æ¨¡å‹ä¿æŒå‡çµ\n",
    "\"\"\"\n",
    "\n",
    "# Wrap model with LoRA\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "# Print trainable parameters info\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Print number of trainable parameters\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "\n",
    "    print(f\"Trainable params: {trainable_params:,} || All params: {all_param:,}\")\n",
    "    print(f\"Trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
    "\n",
    "\n",
    "print_trainable_parameters(peft_model)\n",
    "\n",
    "# Verify LoRA modules are added\n",
    "print(\"\\nLoRA modules added:\")\n",
    "for name, module in peft_model.named_modules():\n",
    "    if \"lora\" in name.lower():\n",
    "        print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa32a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training Configuration with Memory Optimization\n",
    "\"\"\"\n",
    "è¨“ç·´åƒæ•¸è¨­å®šï¼Œé‡å°ä½é¡¯å­˜ç’°å¢ƒå„ªåŒ–\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Tokenization function for training\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text for training\"\"\"\n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # Will be handled by data collator\n",
    "        max_length=512,  # Reasonable length for efficiency\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # For causal LM, labels are same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    "    pad_to_multiple_of=8,  # For efficiency\n",
    ")\n",
    "\n",
    "# Training arguments with memory optimization\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{AI_CACHE_ROOT}/lora_checkpoints\",\n",
    "    # Training schedule\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Small batch size for memory\n",
    "    gradient_accumulation_steps=8,  # Effective batch size = 1*8 = 8\n",
    "    # Optimization\n",
    "    learning_rate=2e-4,  # Higher LR often works well for LoRA\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    # Memory optimization\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=True,  # Trade compute for memory\n",
    "    fp16=True,  # Half precision training\n",
    "    # Logging and saving\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,  # Keep only recent checkpoints\n",
    "    # Misc\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,  # Disable wandb/tensorboard\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Output dir: {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab28ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Loop Execution\n",
    "\"\"\"\n",
    "åŸ·è¡Œ LoRA å¾®èª¿è¨“ç·´\n",
    "ç›£æ§æå¤±ä¸‹é™å’Œè¨˜æ†¶é«”ä½¿ç”¨\n",
    "\"\"\"\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Check memory before training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n",
    "\n",
    "print(\"\\nğŸš€ Starting LoRA fine-tuning...\")\n",
    "print(\"This may take 10-30 minutes depending on your hardware.\")\n",
    "\n",
    "# Start training\n",
    "try:\n",
    "    training_result = trainer.train()\n",
    "    print(\"\\nâœ… Training completed successfully!\")\n",
    "    print(f\"Final train loss: {training_result.training_loss:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Training failed: {e}\")\n",
    "    print(\"Try reducing batch size or sequence length if OOM occurs.\")\n",
    "\n",
    "# Check memory after training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory after training: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Model Saving and Loading Verification\n",
    "\"\"\"\n",
    "ä¿å­˜ LoRA é©é…å™¨ä¸¦é©—è­‰è¼‰å…¥\n",
    "\"\"\"\n",
    "\n",
    "# Save LoRA adapter (only saves the small adapter weights)\n",
    "lora_save_path = f\"{AI_CACHE_ROOT}/lora_adapters/qwen2.5-7b-chinese-instruct\"\n",
    "peft_model.save_pretrained(lora_save_path)\n",
    "\n",
    "print(f\"âœ… LoRA adapter saved to: {lora_save_path}\")\n",
    "\n",
    "# Check saved files\n",
    "import os\n",
    "\n",
    "saved_files = os.listdir(lora_save_path)\n",
    "print(f\"Saved files: {saved_files}\")\n",
    "\n",
    "# Calculate adapter size\n",
    "total_size = sum(\n",
    "    os.path.getsize(os.path.join(lora_save_path, f)) for f in saved_files\n",
    ") / (\n",
    "    1024 * 1024\n",
    ")  # Convert to MB\n",
    "print(f\"Adapter size: {total_size:.2f} MB\")\n",
    "\n",
    "# Test loading the adapter\n",
    "print(\"\\nğŸ”„ Testing adapter loading...\")\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model again (simulate fresh start)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=os.environ[\"TRANSFORMERS_CACHE\"],\n",
    ")\n",
    "\n",
    "# Load and apply LoRA adapter\n",
    "loaded_model = PeftModel.from_pretrained(base_model, lora_save_path)\n",
    "print(\"âœ… LoRA adapter loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c45e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Before/After Comparison\n",
    "\"\"\"\n",
    "æ¯”è¼ƒå¾®èª¿å‰å¾Œçš„ä¸­æ–‡å›æ‡‰å“è³ª\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_response(model, prompt, max_length=100):\n",
    "    \"\"\"Generate response using the model\"\"\"\n",
    "    # Format as chat\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode response (remove input prompt)\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
    "    )\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"ä½ å¥½ï¼Œè«‹è‡ªæˆ‘ä»‹ç´¹ä¸€ä¸‹\",\n",
    "    \"ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ\",\n",
    "    \"å¹«æˆ‘å¯«ä¸€é¦–é—œæ–¼æ˜¥å¤©çš„è©©\",\n",
    "    \"Python å’Œ Java æœ‰ä»€éº¼å·®åˆ¥ï¼Ÿ\",\n",
    "]\n",
    "\n",
    "print(\"ğŸ“Š å¾®èª¿å‰å¾Œæ•ˆæœæ¯”è¼ƒ\\n\")\n",
    "\n",
    "# Compare base model vs fine-tuned model\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"æ¸¬è©¦ {i}: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Original model response\n",
    "    try:\n",
    "        original_response = generate_response(model, prompt, max_length=80)\n",
    "        print(f\"åŸå§‹æ¨¡å‹: {original_response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"åŸå§‹æ¨¡å‹: [Error] {e}\")\n",
    "\n",
    "    # Fine-tuned model response\n",
    "    try:\n",
    "        finetuned_response = generate_response(loaded_model, prompt, max_length=80)\n",
    "        print(f\"å¾®èª¿æ¨¡å‹: {finetuned_response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"å¾®èª¿æ¨¡å‹: [Error] {e}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed265daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Smoke Test and Summary\n",
    "\"\"\"\n",
    "é©—æ”¶æ¸¬è©¦ï¼šç¢ºä¿æ‰€æœ‰çµ„ä»¶æ­£å¸¸å·¥ä½œ\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def smoke_test():\n",
    "    \"\"\"Simple smoke test for LoRA fine-tuning\"\"\"\n",
    "    tests = []\n",
    "\n",
    "    # Test 1: LoRA config is valid\n",
    "    try:\n",
    "        assert lora_config.r > 0, \"LoRA rank should be positive\"\n",
    "        assert len(lora_config.target_modules) > 0, \"Should have target modules\"\n",
    "        tests.append(\"âœ… LoRA config valid\")\n",
    "    except Exception as e:\n",
    "        tests.append(f\"âŒ LoRA config failed: {e}\")\n",
    "\n",
    "    # Test 2: Model has LoRA parameters\n",
    "    try:\n",
    "        lora_params = sum(\n",
    "            1 for name, _ in peft_model.named_parameters() if \"lora\" in name\n",
    "        )\n",
    "        assert lora_params > 0, \"Should have LoRA parameters\"\n",
    "        tests.append(f\"âœ… LoRA parameters added ({lora_params} tensors)\")\n",
    "    except Exception as e:\n",
    "        tests.append(f\"âŒ LoRA parameters failed: {e}\")\n",
    "\n",
    "    # Test 3: Adapter can be saved and loaded\n",
    "    try:\n",
    "        assert os.path.exists(lora_save_path), \"LoRA adapter should be saved\"\n",
    "        assert os.path.exists(\n",
    "            f\"{lora_save_path}/adapter_config.json\"\n",
    "        ), \"Config should exist\"\n",
    "        tests.append(\"âœ… Adapter save/load works\")\n",
    "    except Exception as e:\n",
    "        tests.append(f\"âŒ Adapter save/load failed: {e}\")\n",
    "\n",
    "    # Test 4: Model can generate text\n",
    "    try:\n",
    "        test_response = generate_response(loaded_model, \"ä½ å¥½\", max_length=20)\n",
    "        assert len(test_response) > 0, \"Should generate non-empty response\"\n",
    "        tests.append(\"âœ… Text generation works\")\n",
    "    except Exception as e:\n",
    "        tests.append(f\"âŒ Text generation failed: {e}\")\n",
    "\n",
    "    return tests\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "print(\"ğŸ§ª åŸ·è¡Œé©—æ”¶æ¸¬è©¦...\")\n",
    "test_results = smoke_test()\n",
    "for result in test_results:\n",
    "    print(result)\n",
    "\n",
    "print(\n",
    "    f\"\\nâœ… é€šéæ¸¬è©¦: {sum(1 for r in test_results if r.startswith('âœ…'))}/{len(test_results)}\"\n",
    ")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nğŸ“ˆ è¨“ç·´ç¸½çµ:\")\n",
    "print(f\"åŸºç¤æ¨¡å‹: {model_name}\")\n",
    "print(f\"LoRA rank: {lora_config.r}\")\n",
    "print(f\"è¨“ç·´æ¨£æœ¬: {len(train_dataset)}\")\n",
    "print(f\"é©é…å™¨å¤§å°: {total_size:.2f} MB\")\n",
    "print(\n",
    "    f\"å¯è¨“ç·´åƒæ•¸æ¯”ä¾‹: {100 * sum(p.numel() for p in peft_model.parameters() if p.requires_grad) / sum(p.numel() for p in peft_model.parameters()):.2f}%\"\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ¯ é‡è¦æ¦‚å¿µå›é¡§:\")\n",
    "print(\"â€¢ LoRA é€šéä½ç§©åˆ†è§£å¤§å¹…æ¸›å°‘å¯è¨“ç·´åƒæ•¸\")\n",
    "print(\"â€¢ åªéœ€ä¿å­˜å°å‹é©é…å™¨æ–‡ä»¶ï¼ŒåŸºç¤æ¨¡å‹å¯é‡ç”¨\")\n",
    "print(\"â€¢ rank åƒæ•¸æ§åˆ¶è¡¨é”èƒ½åŠ›èˆ‡æ•ˆç‡çš„å¹³è¡¡\")\n",
    "print(\"â€¢ é©åˆåœ¨æœ‰é™è³‡æºä¸‹é€²è¡Œæ¨¡å‹å€‹æ€§åŒ–\")\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å»ºè­°:\")\n",
    "print(\"â€¢ å˜—è©¦ä¸åŒçš„ rank å€¼ (4, 8, 32, 64)\")\n",
    "print(\"â€¢ æ¸¬è©¦æ›´å¤š target_modules çµ„åˆ\")\n",
    "print(\"â€¢ æ”¶é›†æ›´å¤šé«˜å“è³ªçš„ä¸­æ–‡æŒ‡ä»¤æ•¸æ“š\")\n",
    "print(\"â€¢ é€²è¡Œé‡åŒ–è©•ä¼° (ROUGE, BLEU, äººå·¥è©•åˆ†)\")\n",
    "print(\"â€¢ å­¸ç¿’ QLoRA (nb21) å¯¦ç¾æ›´ä½é¡¯å­˜å¾®èª¿\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76256209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick smoke test for LoRA fine-tuning completion\n",
    "def quick_lora_test():\n",
    "    assert \"lora_config\" in locals(), \"LoRA config should be defined\"\n",
    "    assert os.path.exists(lora_save_path), \"LoRA adapter should be saved\"\n",
    "    response = generate_response(loaded_model, \"ä½ å¥½\", max_length=10)\n",
    "    assert len(response) > 0, \"Model should generate text\"\n",
    "    print(\"âœ… LoRA fine-tuning smoke test passed!\")\n",
    "\n",
    "\n",
    "quick_lora_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1edab9",
   "metadata": {},
   "source": [
    "\n",
    "## 6. æœ¬ç« å°çµ\n",
    "\n",
    "### âœ… å®Œæˆé …ç›®\n",
    "- **LoRA åŸç†èˆ‡å¯¦ä½œ**ï¼šä½ç§©é©æ‡‰å™¨é…ç½®èˆ‡æ‡‰ç”¨\n",
    "- **PEFT åº«æ•´åˆ**ï¼šHugging Face PEFT ç„¡ç¸«æ•´åˆ\n",
    "- **ä¸­æ–‡æŒ‡ä»¤å¾®èª¿**ï¼šç¹é«”ä¸­æ–‡å°è©±èˆ‡ä»»å‹™æ•¸æ“š\n",
    "- **è¨˜æ†¶é«”å„ªåŒ–**ï¼š4-bit é‡åŒ– + æ¢¯åº¦æª¢æŸ¥é»\n",
    "- **æ•ˆæœè©•ä¼°**ï¼šå¾®èª¿å‰å¾Œä¸­æ–‡å›æ‡‰å“è³ªæ¯”è¼ƒ\n",
    "\n",
    "### ğŸ”§ æ ¸å¿ƒåŸç†è¦é»\n",
    "- **ä½ç§©åˆ†è§£**ï¼šÎ”W = BAï¼Œr << min(d,k) å¤§å¹…æ¸›å°‘åƒæ•¸\n",
    "- **åƒæ•¸æ•ˆç‡**ï¼šåƒ… 0.1-1% åƒæ•¸å¯è¨“ç·´ï¼Œé©é…å™¨æª”æ¡ˆå°\n",
    "- **æ¨¡çµ„åŒ–è¨­è¨ˆ**ï¼šé©é…å™¨å¯ç¨ç«‹ä¿å­˜ã€è¼‰å…¥ã€åˆ†äº«\n",
    "- **è¨˜æ†¶é«”å‹å–„**ï¼šæ¢¯åº¦æª¢æŸ¥é» + é‡åŒ–å¯¦ç¾ä½ VRAM å¾®èª¿\n",
    "\n",
    "### âš ï¸ å¸¸è¦‹å‘é»\n",
    "- **rank é¸æ“‡**ï¼šå¤ªå°è¡¨é”èƒ½åŠ›ä¸è¶³ï¼Œå¤ªå¤§æ¥è¿‘å…¨é‡å¾®èª¿\n",
    "- **ç›®æ¨™æ¨¡çµ„**ï¼šéœ€æ ¹æ“šæ¨¡å‹æ¶æ§‹èª¿æ•´ target_modules\n",
    "- **æ•¸æ“šæ ¼å¼**ï¼šç¢ºä¿ä½¿ç”¨æ­£ç¢ºçš„èŠå¤©æ¨¡æ¿æ ¼å¼\n",
    "- **é¡¯å­˜ç®¡ç†**ï¼šè¨“ç·´æ™‚ç›£æ§è¨˜æ†¶é«”ï¼Œé©æ™‚èª¿æ•´æ‰¹æ¬¡å¤§å°\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å»ºè­°\n",
    "1. **é€²éšæŠ€è¡“**ï¼šå­¸ç¿’ QLoRA (nb21) å¯¦ç¾ int4 é‡åŒ–å¾®èª¿\n",
    "2. **æ•¸æ“šæ“´å±•**ï¼šæ”¶é›†æ›´å¤šé«˜å“è³ªä¸­æ–‡æŒ‡ä»¤-å›æ‡‰å°\n",
    "3. **è¶…åƒèª¿å„ª**ï¼šç³»çµ±æ€§æ¸¬è©¦ä¸åŒ rankã€alphaã€å­¸ç¿’ç‡çµ„åˆ\n",
    "4. **è©•ä¼°å®Œå–„**ï¼šå»ºç«‹è‡ªå‹•åŒ–è©•ä¼°ç®¡ç·š (ROUGE, äººå·¥è©•åˆ†)\n",
    "5. **æ‡‰ç”¨æ•´åˆ**ï¼šå°‡å¾®èª¿å¾Œæ¨¡å‹æ•´åˆåˆ° RAG/Agent ç³»çµ±\n",
    "\n",
    "---\n",
    "\n",
    "**è¨˜æ†¶é«”éœ€æ±‚è©•ä¼°**ï¼š\n",
    "- 4GB VRAMï¼šå¯å¾®èª¿ 7B æ¨¡å‹ (batch_size=1)\n",
    "- 8GB VRAMï¼šå¯ç”¨æ›´å¤§æ‰¹æ¬¡æˆ–æ›´é«˜ rank\n",
    "- 12GB+ VRAMï¼šå¯å˜—è©¦ 14B æ¨¡å‹å¾®èª¿\n",
    "\n",
    "é€™å€‹ notebook æä¾›äº†å®Œæ•´çš„ LoRA å¾®èª¿æµç¨‹ï¼Œå¾ç†è«–åˆ°å¯¦ä½œï¼Œç‰¹åˆ¥é‡å°ä¸­æ–‡å ´æ™¯å’Œä½é¡¯å­˜ç’°å¢ƒå„ªåŒ–ã€‚æ‚¨æƒ³ç¹¼çºŒé€²è¡Œ QLoRA (nb21) é‚„æ˜¯å…ˆå®Œå–„ç•¶å‰çš„å¾®èª¿è©•ä¼°ï¼Ÿ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
