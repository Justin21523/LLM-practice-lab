{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d613d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb20_lora_peft_tuning.ipynb\n",
    "# LoRA (Low-Rank Adaptation) 微調實戰\n",
    "\n",
    "# Cell 1: Environment Setup and Shared Cache\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\"\n",
    "    )\n",
    "\n",
    "# Install required packages\n",
    "# !pip install transformers==4.36.0 peft==0.7.1 datasets==2.14.0 accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: LoRA Theory and Configuration\n",
    "\"\"\"\n",
    "LoRA (Low-Rank Adaptation) 原理：\n",
    "- 原始權重矩陣 W ∈ R^(d×k) 保持凍結\n",
    "- 添加低秩分解：ΔW = BA，其中 B ∈ R^(d×r), A ∈ R^(r×k)\n",
    "- r << min(d,k)，大幅減少可訓練參數\n",
    "- 前向傳播：h = (W + α/r * BA)x\n",
    "\n",
    "關鍵參數：\n",
    "- r (rank): 低秩維度，通常 4-64，越大表達能力越強但參數越多\n",
    "- α (alpha): 縮放因子，控制 LoRA 貢獻度\n",
    "- target_modules: 要應用 LoRA 的模組（通常是 attention 的 q,k,v,o）\n",
    "\"\"\"\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# LoRA configuration - conservative settings for stable training\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # rank - balance between performance and efficiency\n",
    "    lora_alpha=32,  # scaling factor (typically 2*r)\n",
    "    target_modules=[  # apply LoRA to attention layers\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",  # MLP layers for better coverage\n",
    "    ],\n",
    "    lora_dropout=0.1,  # dropout for regularization\n",
    "    bias=\"none\",  # don't train bias terms\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "print(\"LoRA Config:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d08faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Base Model Loading with Low-VRAM Settings\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"  # Good Chinese performance\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, trust_remote_code=True, cache_dir=os.environ[\"TRANSFORMERS_CACHE\"]\n",
    ")\n",
    "\n",
    "# Add padding token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with memory optimization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # automatic device placement\n",
    "    torch_dtype=torch.float16,  # half precision\n",
    "    load_in_4bit=True,  # 4-bit quantization for VRAM efficiency\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=os.environ[\"TRANSFORMERS_CACHE\"],\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Chinese Instruction Dataset Preparation\n",
    "\"\"\"\n",
    "創建中文指令數據集 - 包含日常對話、知識問答、任務指令等\n",
    "格式：{\"instruction\": \"問題\", \"output\": \"回答\"}\n",
    "\"\"\"\n",
    "\n",
    "# Sample Chinese instruction data - mix of conversation, QA, and tasks\n",
    "chinese_instructions = [\n",
    "    # Daily conversation\n",
    "    {\n",
    "        \"instruction\": \"你好，你是誰？\",\n",
    "        \"output\": \"你好！我是一個AI助手，很高興為您服務。我可以回答問題、協助任務，或與您聊天。有什麼我可以幫助您的嗎？\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"今天天氣如何？\",\n",
    "        \"output\": \"很抱歉，我無法獲取即時天氣資訊。建議您查看天氣應用程式或氣象網站來了解當前天氣狀況。\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"推薦一些好看的電影\",\n",
    "        \"output\": \"我推薦幾部不同類型的優秀電影：《肖申克的救贖》（劇情）、《星際效應》（科幻）、《千與千尋》（動畫）、《寄生蟲》（驚悚）。您偏好哪種類型呢？\",\n",
    "    },\n",
    "    # Knowledge QA\n",
    "    {\n",
    "        \"instruction\": \"什麼是人工智慧？\",\n",
    "        \"output\": \"人工智慧（AI）是指讓機器模擬人類智能的技術，包括學習、推理、感知、決策等能力。主要分支有機器學習、深度學習、自然語言處理、電腦視覺等。\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"台灣的首都是哪裡？\",\n",
    "        \"output\": \"台灣的首都是台北市。台北是台灣的政治、經濟、文化中心，也是最大的都市。\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"解釋什麼是機器學習\",\n",
    "        \"output\": \"機器學習是人工智慧的一個分支，讓電腦通過數據自動學習和改進，而不需要明確編程。主要類型包括監督學習、無監督學習和強化學習。\",\n",
    "    },\n",
    "    # Task instructions\n",
    "    {\n",
    "        \"instruction\": \"幫我寫一封感謝信\",\n",
    "        \"output\": \"親愛的[收件人姓名]：\\n\\n感謝您在[具體事件]中給予的幫助和支持。您的[具體幫助內容]對我來說意義重大，讓我能夠[達成的結果]。\\n\\n再次表達我的誠摯謝意，期待未來有機會回報您的恩情。\\n\\n此致\\n敬禮\\n\\n[您的姓名]\\n[日期]\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"總結以下文字的重點\",\n",
    "        \"output\": \"很抱歉，您沒有提供需要總結的文字內容。請提供具體的文字材料，我將為您提取重點和關鍵資訊。\",\n",
    "    },\n",
    "    {\"instruction\": \"翻譯：Hello, how are you?\", \"output\": \"你好，你好嗎？\"},\n",
    "    {\n",
    "        \"instruction\": \"用Python寫一個計算階乘的函數\",\n",
    "        \"output\": \"```python\\ndef factorial(n):\\n    if n < 0:\\n        return None  # 負數沒有階乘\\n    elif n == 0 or n == 1:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)\\n\\n# 使用範例\\nprint(factorial(5))  # 輸出: 120\\n```\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Extend dataset with more samples (you can add more or load from file)\n",
    "extended_data = chinese_instructions * 5  # Repeat for more training data\n",
    "\n",
    "\n",
    "# Format data for training (Chat template format)\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format instruction-response pair using chat template\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "# Convert to formatted dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "formatted_data = [format_instruction(ex) for ex in extended_data]\n",
    "train_dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(\"\\nSample formatted text:\")\n",
    "print(train_dataset[0][\"text\"][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: LoRA Model Setup and Wrapping\n",
    "\"\"\"\n",
    "使用 PEFT 將 LoRA 適配器包裝到基礎模型上\n",
    "只有 LoRA 參數會被訓練，基礎模型保持凍結\n",
    "\"\"\"\n",
    "\n",
    "# Wrap model with LoRA\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "# Print trainable parameters info\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Print number of trainable parameters\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "\n",
    "    print(f\"Trainable params: {trainable_params:,} || All params: {all_param:,}\")\n",
    "    print(f\"Trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
    "\n",
    "\n",
    "print_trainable_parameters(peft_model)\n",
    "\n",
    "# Verify LoRA modules are added\n",
    "print(\"\\nLoRA modules added:\")\n",
    "for name, module in peft_model.named_modules():\n",
    "    if \"lora\" in name.lower():\n",
    "        print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa32a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training Configuration with Memory Optimization\n",
    "\"\"\"\n",
    "訓練參數設定，針對低顯存環境優化\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Tokenization function for training\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text for training\"\"\"\n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # Will be handled by data collator\n",
    "        max_length=512,  # Reasonable length for efficiency\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # For causal LM, labels are same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    "    pad_to_multiple_of=8,  # For efficiency\n",
    ")\n",
    "\n",
    "# Training arguments with memory optimization\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{AI_CACHE_ROOT}/lora_checkpoints\",\n",
    "    # Training schedule\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Small batch size for memory\n",
    "    gradient_accumulation_steps=8,  # Effective batch size = 1*8 = 8\n",
    "    # Optimization\n",
    "    learning_rate=2e-4,  # Higher LR often works well for LoRA\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    # Memory optimization\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=True,  # Trade compute for memory\n",
    "    fp16=True,  # Half precision training\n",
    "    # Logging and saving\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,  # Keep only recent checkpoints\n",
    "    # Misc\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,  # Disable wandb/tensorboard\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Output dir: {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab28ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Loop Execution\n",
    "\"\"\"\n",
    "執行 LoRA 微調訓練\n",
    "監控損失下降和記憶體使用\n",
    "\"\"\"\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Check memory before training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n",
    "\n",
    "print(\"\\n🚀 Starting LoRA fine-tuning...\")\n",
    "print(\"This may take 10-30 minutes depending on your hardware.\")\n",
    "\n",
    "# Start training\n",
    "try:\n",
    "    training_result = trainer.train()\n",
    "    print(\"\\n✅ Training completed successfully!\")\n",
    "    print(f\"Final train loss: {training_result.training_loss:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Training failed: {e}\")\n",
    "    print(\"Try reducing batch size or sequence length if OOM occurs.\")\n",
    "\n",
    "# Check memory after training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory after training: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Model Saving and Loading Verification\n",
    "\"\"\"\n",
    "保存 LoRA 適配器並驗證載入\n",
    "\"\"\"\n",
    "\n",
    "# Save LoRA adapter (only saves the small adapter weights)\n",
    "lora_save_path = f\"{AI_CACHE_ROOT}/lora_adapters/qwen2.5-7b-chinese-instruct\"\n",
    "peft_model.save_pretrained(lora_save_path)\n",
    "\n",
    "print(f\"✅ LoRA adapter saved to: {lora_save_path}\")\n",
    "\n",
    "# Check saved files\n",
    "import os\n",
    "\n",
    "saved_files = os.listdir(lora_save_path)\n",
    "print(f\"Saved files: {saved_files}\")\n",
    "\n",
    "# Calculate adapter size\n",
    "total_size = sum(\n",
    "    os.path.getsize(os.path.join(lora_save_path, f)) for f in saved_files\n",
    ") / (\n",
    "    1024 * 1024\n",
    ")  # Convert to MB\n",
    "print(f\"Adapter size: {total_size:.2f} MB\")\n",
    "\n",
    "# Test loading the adapter\n",
    "print(\"\\n🔄 Testing adapter loading...\")\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model again (simulate fresh start)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=os.environ[\"TRANSFORMERS_CACHE\"],\n",
    ")\n",
    "\n",
    "# Load and apply LoRA adapter\n",
    "loaded_model = PeftModel.from_pretrained(base_model, lora_save_path)\n",
    "print(\"✅ LoRA adapter loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c45e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Before/After Comparison\n",
    "\"\"\"\n",
    "比較微調前後的中文回應品質\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_response(model, prompt, max_length=100):\n",
    "    \"\"\"Generate response using the model\"\"\"\n",
    "    # Format as chat\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode response (remove input prompt)\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
    "    )\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"你好，請自我介紹一下\",\n",
    "    \"什麼是機器學習？\",\n",
    "    \"幫我寫一首關於春天的詩\",\n",
    "    \"Python 和 Java 有什麼差別？\",\n",
    "]\n",
    "\n",
    "print(\"📊 微調前後效果比較\\n\")\n",
    "\n",
    "# Compare base model vs fine-tuned model\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"測試 {i}: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Original model response\n",
    "    try:\n",
    "        original_response = generate_response(model, prompt, max_length=80)\n",
    "        print(f\"原始模型: {original_response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"原始模型: [Error] {e}\")\n",
    "\n",
    "    # Fine-tuned model response\n",
    "    try:\n",
    "        finetuned_response = generate_response(loaded_model, prompt, max_length=80)\n",
    "        print(f\"微調模型: {finetuned_response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"微調模型: [Error] {e}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed265daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Smoke Test and Summary\n",
    "\"\"\"\n",
    "驗收測試：確保所有組件正常工作\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def smoke_test():\n",
    "    \"\"\"Simple smoke test for LoRA fine-tuning\"\"\"\n",
    "    tests = []\n",
    "\n",
    "    # Test 1: LoRA config is valid\n",
    "    try:\n",
    "        assert lora_config.r > 0, \"LoRA rank should be positive\"\n",
    "        assert len(lora_config.target_modules) > 0, \"Should have target modules\"\n",
    "        tests.append(\"✅ LoRA config valid\")\n",
    "    except Exception as e:\n",
    "        tests.append(f\"❌ LoRA config failed: {e}\")\n",
    "\n",
    "    # Test 2: Model has LoRA parameters\n",
    "    try:\n",
    "        lora_params = sum(\n",
    "            1 for name, _ in peft_model.named_parameters() if \"lora\" in name\n",
    "        )\n",
    "        assert lora_params > 0, \"Should have LoRA parameters\"\n",
    "        tests.append(f\"✅ LoRA parameters added ({lora_params} tensors)\")\n",
    "    except Exception as e:\n",
    "        tests.append(f\"❌ LoRA parameters failed: {e}\")\n",
    "\n",
    "    # Test 3: Adapter can be saved and loaded\n",
    "    try:\n",
    "        assert os.path.exists(lora_save_path), \"LoRA adapter should be saved\"\n",
    "        assert os.path.exists(\n",
    "            f\"{lora_save_path}/adapter_config.json\"\n",
    "        ), \"Config should exist\"\n",
    "        tests.append(\"✅ Adapter save/load works\")\n",
    "    except Exception as e:\n",
    "        tests.append(f\"❌ Adapter save/load failed: {e}\")\n",
    "\n",
    "    # Test 4: Model can generate text\n",
    "    try:\n",
    "        test_response = generate_response(loaded_model, \"你好\", max_length=20)\n",
    "        assert len(test_response) > 0, \"Should generate non-empty response\"\n",
    "        tests.append(\"✅ Text generation works\")\n",
    "    except Exception as e:\n",
    "        tests.append(f\"❌ Text generation failed: {e}\")\n",
    "\n",
    "    return tests\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "print(\"🧪 執行驗收測試...\")\n",
    "test_results = smoke_test()\n",
    "for result in test_results:\n",
    "    print(result)\n",
    "\n",
    "print(\n",
    "    f\"\\n✅ 通過測試: {sum(1 for r in test_results if r.startswith('✅'))}/{len(test_results)}\"\n",
    ")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n📈 訓練總結:\")\n",
    "print(f\"基礎模型: {model_name}\")\n",
    "print(f\"LoRA rank: {lora_config.r}\")\n",
    "print(f\"訓練樣本: {len(train_dataset)}\")\n",
    "print(f\"適配器大小: {total_size:.2f} MB\")\n",
    "print(\n",
    "    f\"可訓練參數比例: {100 * sum(p.numel() for p in peft_model.parameters() if p.requires_grad) / sum(p.numel() for p in peft_model.parameters()):.2f}%\"\n",
    ")\n",
    "\n",
    "print(\"\\n🎯 重要概念回顧:\")\n",
    "print(\"• LoRA 通過低秩分解大幅減少可訓練參數\")\n",
    "print(\"• 只需保存小型適配器文件，基礎模型可重用\")\n",
    "print(\"• rank 參數控制表達能力與效率的平衡\")\n",
    "print(\"• 適合在有限資源下進行模型個性化\")\n",
    "\n",
    "print(\"\\n🚀 下一步建議:\")\n",
    "print(\"• 嘗試不同的 rank 值 (4, 8, 32, 64)\")\n",
    "print(\"• 測試更多 target_modules 組合\")\n",
    "print(\"• 收集更多高品質的中文指令數據\")\n",
    "print(\"• 進行量化評估 (ROUGE, BLEU, 人工評分)\")\n",
    "print(\"• 學習 QLoRA (nb21) 實現更低顯存微調\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76256209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick smoke test for LoRA fine-tuning completion\n",
    "def quick_lora_test():\n",
    "    assert \"lora_config\" in locals(), \"LoRA config should be defined\"\n",
    "    assert os.path.exists(lora_save_path), \"LoRA adapter should be saved\"\n",
    "    response = generate_response(loaded_model, \"你好\", max_length=10)\n",
    "    assert len(response) > 0, \"Model should generate text\"\n",
    "    print(\"✅ LoRA fine-tuning smoke test passed!\")\n",
    "\n",
    "\n",
    "quick_lora_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1edab9",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 本章小結\n",
    "\n",
    "### ✅ 完成項目\n",
    "- **LoRA 原理與實作**：低秩適應器配置與應用\n",
    "- **PEFT 庫整合**：Hugging Face PEFT 無縫整合\n",
    "- **中文指令微調**：繁體中文對話與任務數據\n",
    "- **記憶體優化**：4-bit 量化 + 梯度檢查點\n",
    "- **效果評估**：微調前後中文回應品質比較\n",
    "\n",
    "### 🔧 核心原理要點\n",
    "- **低秩分解**：ΔW = BA，r << min(d,k) 大幅減少參數\n",
    "- **參數效率**：僅 0.1-1% 參數可訓練，適配器檔案小\n",
    "- **模組化設計**：適配器可獨立保存、載入、分享\n",
    "- **記憶體友善**：梯度檢查點 + 量化實現低 VRAM 微調\n",
    "\n",
    "### ⚠️ 常見坑點\n",
    "- **rank 選擇**：太小表達能力不足，太大接近全量微調\n",
    "- **目標模組**：需根據模型架構調整 target_modules\n",
    "- **數據格式**：確保使用正確的聊天模板格式\n",
    "- **顯存管理**：訓練時監控記憶體，適時調整批次大小\n",
    "\n",
    "### 🚀 下一步建議\n",
    "1. **進階技術**：學習 QLoRA (nb21) 實現 int4 量化微調\n",
    "2. **數據擴展**：收集更多高品質中文指令-回應對\n",
    "3. **超參調優**：系統性測試不同 rank、alpha、學習率組合\n",
    "4. **評估完善**：建立自動化評估管線 (ROUGE, 人工評分)\n",
    "5. **應用整合**：將微調後模型整合到 RAG/Agent 系統\n",
    "\n",
    "---\n",
    "\n",
    "**記憶體需求評估**：\n",
    "- 4GB VRAM：可微調 7B 模型 (batch_size=1)\n",
    "- 8GB VRAM：可用更大批次或更高 rank\n",
    "- 12GB+ VRAM：可嘗試 14B 模型微調\n",
    "\n",
    "這個 notebook 提供了完整的 LoRA 微調流程，從理論到實作，特別針對中文場景和低顯存環境優化。您想繼續進行 QLoRA (nb21) 還是先完善當前的微調評估？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
