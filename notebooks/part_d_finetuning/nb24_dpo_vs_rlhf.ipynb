{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e55eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Shared Cache Bootstrap ===\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())\n",
    "print(\n",
    "    f\"[GPU Memory] {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"[CPU Mode]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff04b5b9",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\n",
    "ğŸ¯ Learning Objectives (å­¸ç¿’ç›®æ¨™):\n",
    "\n",
    "1. **Preference Learning Theory (åå¥½å­¸ç¿’ç†è«–)**:\n",
    "   - DPO vs RLHF fundamental differences (DPO vs RLHF åŸºæœ¬å·®ç•°)\n",
    "   - Training complexity and stability comparison (è¨“ç·´è¤‡é›œåº¦èˆ‡ç©©å®šæ€§å°æ¯”)\n",
    "\n",
    "2. **DPO Implementation (DPO å¯¦ä½œ)**:\n",
    "   - Direct preference optimization with TRL (ä½¿ç”¨ TRL é€²è¡Œç›´æ¥åå¥½å„ªåŒ–)\n",
    "   - QLoRA + DPO for low-VRAM training (ä½ VRAM çš„ QLoRA + DPO è¨“ç·´)\n",
    "\n",
    "3. **Preference Data Construction (åå¥½è³‡æ–™æ§‹å»º)**:\n",
    "   - Create chosen/rejected pairs (å»ºç«‹ chosen/rejected é…å°)\n",
    "   - Quality assessment for preference learning (åå¥½å­¸ç¿’çš„å“è³ªè©•ä¼°)\n",
    "\n",
    "4. **Comparative Analysis (å°æ¯”åˆ†æ)**:\n",
    "   - Performance vs computational cost (æ•ˆèƒ½ vs è¨ˆç®—æˆæœ¬)\n",
    "   - When to use DPO vs RLHF (ä½•æ™‚ä½¿ç”¨ DPO vs RLHF)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“¦ Dependencies Installation\n",
    "# ============================================================================\n",
    "\n",
    "# Core ML and preference learning libraries\n",
    "# !pip install transformers>=4.36 datasets accelerate bitsandbytes>=0.41\n",
    "# !pip install trl>=0.7.0 peft>=0.7.0  # TRL for DPO training\n",
    "# !pip install torch>=2.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "import json, random, numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ§  DPO vs RLHF Theory Comparison (ç†è«–å°æ¯”)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def explain_preference_learning():\n",
    "    \"\"\"\n",
    "    Explain the fundamental differences between DPO and RLHF\n",
    "    è§£é‡‹ DPO èˆ‡ RLHF çš„åŸºæœ¬å·®ç•°\n",
    "    \"\"\"\n",
    "    comparison = {\n",
    "        \"Aspect\": [\n",
    "            \"Training Paradigm\",\n",
    "            \"Complexity\",\n",
    "            \"Stability\",\n",
    "            \"Memory Usage\",\n",
    "            \"Training Time\",\n",
    "            \"Performance\",\n",
    "        ],\n",
    "        \"DPO\": [\n",
    "            \"Direct optimization on preference pairs\",\n",
    "            \"Simple, single-stage training\",\n",
    "            \"More stable, no RL instability\",\n",
    "            \"Lower (no critic model needed)\",\n",
    "            \"Faster convergence\",\n",
    "            \"Comparable to RLHF in many tasks\",\n",
    "        ],\n",
    "        \"RLHF\": [\n",
    "            \"RL with reward model + PPO\",\n",
    "            \"Complex, multi-stage (SFTâ†’RMâ†’PPO)\",\n",
    "            \"Can be unstable due to RL\",\n",
    "            \"Higher (actor + critic + ref models)\",\n",
    "            \"Slower, more hyperparameter sensitive\",\n",
    "            \"State-of-the-art when tuned well\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(\"ğŸ“Š DPO vs RLHF Comparison (DPO vs RLHF å°æ¯”)\")\n",
    "    print(\"=\" * 80)\n",
    "    for i, aspect in enumerate(comparison[\"Aspect\"]):\n",
    "        print(f\"{aspect:20} | DPO: {comparison['DPO'][i]}\")\n",
    "        print(f\"{' ' * 20} | RLHF: {comparison['RLHF'][i]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    return comparison\n",
    "\n",
    "\n",
    "# Display the comparison\n",
    "comparison_table = explain_preference_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ—ƒï¸ Preference Dataset Preparation (åå¥½è³‡æ–™æº–å‚™)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def create_chinese_preference_dataset(num_samples: int = 100) -> Dataset:\n",
    "    \"\"\"\n",
    "    Create a synthetic Chinese preference dataset\n",
    "    å»ºç«‹åˆæˆä¸­æ–‡åå¥½è³‡æ–™é›†\n",
    "    \"\"\"\n",
    "\n",
    "    # Sample prompts for Chinese assistant evaluation\n",
    "    prompts = [\n",
    "        \"è«‹è§£é‡‹ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ\",\n",
    "        \"å¦‚ä½•å­¸ç¿’ç¨‹å¼è¨­è¨ˆï¼Ÿ\",\n",
    "        \"æ¨è–¦å¹¾æœ¬å¥½æ›¸çµ¦æˆ‘\",\n",
    "        \"è«‹å¯«ä¸€å€‹é—œæ–¼å‹æƒ…çš„çŸ­æ•…äº‹\",\n",
    "        \"å¦‚ä½•ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼ï¼Ÿ\",\n",
    "        \"è§£é‡‹é‡å­ç‰©ç†çš„åŸºæœ¬æ¦‚å¿µ\",\n",
    "        \"å¦‚ä½•æå‡å·¥ä½œæ•ˆç‡ï¼Ÿ\",\n",
    "        \"æè¿°ä½ æœ€å–œæ­¡çš„å­£ç¯€\",\n",
    "        \"å¦‚ä½•å­¸ç¿’ä¸€é–€æ–°èªè¨€ï¼Ÿ\",\n",
    "        \"è«‹çµ¦å‡ºæŠ•è³‡å»ºè­°\",\n",
    "    ]\n",
    "\n",
    "    # Generate chosen (better) and rejected (worse) responses\n",
    "    def generate_response_pair(prompt: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate chosen and rejected responses for a prompt\"\"\"\n",
    "\n",
    "        # Better response characteristics: helpful, detailed, structured\n",
    "        chosen_responses = {\n",
    "            \"è«‹è§£é‡‹ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ\": \"äººå·¥æ™ºæ…§ï¼ˆAIï¼‰æ˜¯æŒ‡è®“æ©Ÿå™¨æ¨¡æ“¬äººé¡æ™ºèƒ½çš„æŠ€è¡“ã€‚å®ƒåŒ…å«æ©Ÿå™¨å­¸ç¿’ã€æ·±åº¦å­¸ç¿’ã€è‡ªç„¶èªè¨€è™•ç†ç­‰åˆ†æ”¯ã€‚AIå¯ä»¥æ‡‰ç”¨åœ¨åœ–åƒè­˜åˆ¥ã€èªéŸ³è™•ç†ã€è‡ªå‹•é§•é§›ç­‰é ˜åŸŸï¼Œæ—¨åœ¨è§£æ±ºè¤‡é›œå•é¡Œä¸¦æå‡æ•ˆç‡ã€‚\",\n",
    "            \"å¦‚ä½•å­¸ç¿’ç¨‹å¼è¨­è¨ˆï¼Ÿ\": \"å­¸ç¿’ç¨‹å¼è¨­è¨ˆå»ºè­°æŒ‰ä»¥ä¸‹æ­¥é©Ÿï¼š1) é¸æ“‡é©åˆçš„èªè¨€ï¼ˆå¦‚Pythonï¼‰2) æŒæ¡åŸºæœ¬èªæ³•å’Œæ¦‚å¿µ 3) å¤šåšç·´ç¿’é …ç›® 4) åƒèˆ‡é–‹æºé …ç›® 5) æŒçºŒå­¸ç¿’æ–°æŠ€è¡“ã€‚å»ºè­°å¾ç°¡å–®é …ç›®é–‹å§‹ï¼Œé€æ­¥æå‡é›£åº¦ã€‚\",\n",
    "        }\n",
    "\n",
    "        # Worse response characteristics: vague, unhelpful, or inappropriate\n",
    "        rejected_responses = {\n",
    "            \"è«‹è§£é‡‹ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ\": \"AIå°±æ˜¯å¾ˆå²å®³çš„é›»è…¦æŠ€è¡“ï¼Œå¾ˆè¤‡é›œï¼Œæˆ‘ä¹Ÿä¸å¤ªæ‡‚ã€‚\",\n",
    "            \"å¦‚ä½•å­¸ç¿’ç¨‹å¼è¨­è¨ˆï¼Ÿ\": \"ç¨‹å¼è¨­è¨ˆå¾ˆé›£ï¼Œä½ ç›´æ¥å»è£œç¿’ç­å­¸æ¯”è¼ƒå¿«ã€‚\",\n",
    "        }\n",
    "\n",
    "        # Use predefined responses or generate generic ones\n",
    "        chosen = chosen_responses.get(\n",
    "            prompt, f\"é€™æ˜¯ä¸€å€‹å¾ˆå¥½çš„å•é¡Œã€‚{prompt}çš„ç­”æ¡ˆéœ€è¦å¾å¤šå€‹è§’åº¦ä¾†åˆ†æ...\"\n",
    "        )\n",
    "        rejected = rejected_responses.get(prompt, f\"ä¸çŸ¥é“ï¼Œä½ å¯ä»¥è‡ªå·±æŸ¥è³‡æ–™ã€‚\")\n",
    "\n",
    "        return chosen, rejected\n",
    "\n",
    "    # Build preference dataset\n",
    "    preference_data = []\n",
    "    for i in range(num_samples):\n",
    "        prompt = random.choice(prompts)\n",
    "        chosen, rejected = generate_response_pair(prompt)\n",
    "\n",
    "        preference_data.append(\n",
    "            {\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
    "        )\n",
    "\n",
    "    return Dataset.from_list(preference_data)\n",
    "\n",
    "\n",
    "# Create preference dataset\n",
    "print(\"ğŸ—ƒï¸ Creating Chinese Preference Dataset...\")\n",
    "preference_dataset = create_chinese_preference_dataset(num_samples=50)\n",
    "print(f\"Created {len(preference_dataset)} preference pairs\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nğŸ“ Sample Preference Pair:\")\n",
    "sample = preference_dataset[0]\n",
    "print(f\"Prompt: {sample['prompt']}\")\n",
    "print(f\"Chosen: {sample['chosen'][:100]}...\")\n",
    "print(f\"Rejected: {sample['rejected'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37966c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ”§ Model and Tokenizer Setup (æ¨¡å‹èˆ‡åˆ†è©å™¨è¨­å®š)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def setup_model_for_dpo(\n",
    "    model_name: str = \"microsoft/DialoGPT-medium\", use_4bit: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Setup model and tokenizer for DPO training\n",
    "    ç‚º DPO è¨“ç·´è¨­ç½®æ¨¡å‹èˆ‡åˆ†è©å™¨\n",
    "    \"\"\"\n",
    "\n",
    "    # Configure quantization for low VRAM\n",
    "    if use_4bit:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "    else:\n",
    "        bnb_config = None\n",
    "\n",
    "    # Load model\n",
    "    print(f\"ğŸ¤– Loading model: {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# Setup model (using smaller model for demo)\n",
    "model_name = \"microsoft/DialoGPT-medium\"  # Smaller model for demo\n",
    "model, tokenizer = setup_model_for_dpo(model_name, use_4bit=True)\n",
    "\n",
    "print(f\"âœ… Model loaded: {model_name}\")\n",
    "print(f\"ğŸ“ Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ddb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¯ DPO Training Implementation (DPO è¨“ç·´å¯¦ä½œ)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def prepare_model_for_dpo_training(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Prepare model for DPO training with LoRA\n",
    "    æº–å‚™æ¨¡å‹é€²è¡Œ DPO è¨“ç·´ï¼ˆä½¿ç”¨ LoRAï¼‰\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare model for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # LoRA configuration for DPO\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"c_attn\", \"c_proj\"],  # For DialoGPT\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "\n",
    "    # Add LoRA adapters\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Prepare model for training\n",
    "print(\"ğŸ”§ Preparing model for DPO training...\")\n",
    "model = prepare_model_for_dpo_training(model, tokenizer)\n",
    "\n",
    "\n",
    "def format_preference_data_for_dpo(dataset: Dataset, tokenizer) -> Dataset:\n",
    "    \"\"\"\n",
    "    Format preference dataset for DPO training\n",
    "    ç‚º DPO è¨“ç·´æ ¼å¼åŒ–åå¥½è³‡æ–™\n",
    "    \"\"\"\n",
    "\n",
    "    def format_sample(examples):\n",
    "        formatted = []\n",
    "        for prompt, chosen, rejected in zip(\n",
    "            examples[\"prompt\"], examples[\"chosen\"], examples[\"rejected\"]\n",
    "        ):\n",
    "            formatted.append({\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected})\n",
    "        return formatted\n",
    "\n",
    "    # Apply formatting\n",
    "    formatted_dataset = dataset.map(\n",
    "        lambda x: {\"formatted\": format_sample(x)},\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    return formatted_dataset\n",
    "\n",
    "\n",
    "# Format dataset for DPO\n",
    "print(\"ğŸ“‹ Formatting preference dataset for DPO...\")\n",
    "formatted_dataset = preference_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "\n",
    "def train_dpo_model(model, tokenizer, train_dataset, eval_dataset=None):\n",
    "    \"\"\"\n",
    "    Train model using Direct Preference Optimization\n",
    "    ä½¿ç”¨ç›´æ¥åå¥½å„ªåŒ–è¨“ç·´æ¨¡å‹\n",
    "    \"\"\"\n",
    "\n",
    "    # DPO training configuration\n",
    "    training_args = DPOConfig(\n",
    "        output_dir=\"./dpo_output\",\n",
    "        num_train_epochs=1,  # Short for demo\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=5e-6,\n",
    "        max_length=256,\n",
    "        max_prompt_length=128,\n",
    "        beta=0.1,  # DPO regularization parameter\n",
    "        logging_steps=10,\n",
    "        save_steps=50,\n",
    "        eval_steps=50,\n",
    "        warmup_ratio=0.1,\n",
    "        remove_unused_columns=False,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_drop_last=True,\n",
    "        bf16=True if torch.cuda.is_bf16_supported() else False,\n",
    "        fp16=True if not torch.cuda.is_bf16_supported() else False,\n",
    "    )\n",
    "\n",
    "    # Initialize DPO trainer\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        beta=0.1,  # DPO regularization coefficient\n",
    "    )\n",
    "\n",
    "    print(\"ğŸš€ Starting DPO training...\")\n",
    "    try:\n",
    "        # Start training\n",
    "        train_result = dpo_trainer.train()\n",
    "        print(\"âœ… DPO training completed!\")\n",
    "\n",
    "        # Save model\n",
    "        dpo_trainer.save_model()\n",
    "        print(\"ğŸ’¾ Model saved to ./dpo_output\")\n",
    "\n",
    "        return train_result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Training failed: {e}\")\n",
    "        print(\"ğŸ’¡ This might be due to memory constraints or data formatting issues\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Note: Actual training requires more memory and time\n",
    "print(\"ğŸ“ DPO Training Setup Complete\")\n",
    "print(\"ğŸ’¡ To run actual training, uncomment the following lines:\")\n",
    "print(\n",
    "    \"# train_result = train_dpo_model(model, tokenizer, formatted_dataset['train'], formatted_dataset['test'])\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ†š RLHF vs DPO Comparison Implementation (RLHF vs DPO æ¯”è¼ƒå¯¦ä½œ)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def simulate_rlhf_workflow():\n",
    "    \"\"\"\n",
    "    Simulate RLHF workflow steps (for comparison)\n",
    "    æ¨¡æ“¬ RLHF å·¥ä½œæµç¨‹æ­¥é©Ÿï¼ˆç”¨æ–¼æ¯”è¼ƒï¼‰\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"ğŸ”„ RLHF Workflow Simulation:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    steps = [\n",
    "        (\"1. Supervised Fine-tuning (SFT)\", \"Train base model on demonstration data\"),\n",
    "        (\"2. Reward Model Training\", \"Train reward model on preference pairs\"),\n",
    "        (\"3. PPO Training\", \"Optimize policy using PPO with reward model\"),\n",
    "        (\"4. Iterative Refinement\", \"Multiple rounds of data collection and training\"),\n",
    "    ]\n",
    "\n",
    "    for step, description in steps:\n",
    "        print(f\"{step}: {description}\")\n",
    "\n",
    "    print(\"\\nğŸ“Š Resource Requirements Comparison:\")\n",
    "\n",
    "    resource_comparison = {\n",
    "        \"Training Stages\": [\"DPO: 1 stage\", \"RLHF: 3-4 stages\"],\n",
    "        \"Models Required\": [\n",
    "            \"DPO: Base + Reference\",\n",
    "            \"RLHF: Base + Reward + Actor + Critic\",\n",
    "        ],\n",
    "        \"Memory Usage\": [\"DPO: ~2x base model\", \"RLHF: ~4x base model\"],\n",
    "        \"Training Time\": [\"DPO: Hours\", \"RLHF: Days to weeks\"],\n",
    "        \"Hyperparameter Sensitivity\": [\"DPO: Low\", \"RLHF: High\"],\n",
    "        \"Implementation Complexity\": [\"DPO: Simple\", \"RLHF: Complex\"],\n",
    "    }\n",
    "\n",
    "    for aspect, comparison in resource_comparison.items():\n",
    "        print(f\"{aspect:25}: {comparison[0]} vs {comparison[1]}\")\n",
    "\n",
    "    return resource_comparison\n",
    "\n",
    "\n",
    "# Run RLHF simulation\n",
    "rlhf_comparison = simulate_rlhf_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc30dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“Š Evaluation and Comparison (è©•ä¼°èˆ‡æ¯”è¼ƒ)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def evaluate_preference_model(model, tokenizer, test_prompts: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate model responses for preference learning effectiveness\n",
    "    è©•ä¼°æ¨¡å‹å›æ‡‰çš„åå¥½å­¸ç¿’æ•ˆæœ\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"ğŸ“Š Evaluating model responses...\")\n",
    "\n",
    "    results = {\n",
    "        \"prompts\": [],\n",
    "        \"responses\": [],\n",
    "        \"response_lengths\": [],\n",
    "        \"helpfulness_scores\": [],\n",
    "    }\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for prompt in test_prompts:\n",
    "        # Generate response\n",
    "        inputs = tokenizer.encode(\n",
    "            prompt, return_tensors=\"pt\", max_length=128, truncation=True\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = response[len(prompt) :].strip()\n",
    "\n",
    "        # Simple helpfulness scoring (length and content diversity)\n",
    "        helpfulness_score = min(\n",
    "            len(response.split()) / 20, 1.0\n",
    "        )  # Normalize by word count\n",
    "\n",
    "        results[\"prompts\"].append(prompt)\n",
    "        results[\"responses\"].append(response)\n",
    "        results[\"response_lengths\"].append(len(response))\n",
    "        results[\"helpfulness_scores\"].append(helpfulness_score)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test prompts for evaluation\n",
    "test_prompts = [\n",
    "    \"è«‹è§£é‡‹æ©Ÿå™¨å­¸ç¿’çš„åŸºæœ¬æ¦‚å¿µ\",\n",
    "    \"å¦‚ä½•æå‡å­¸ç¿’æ•ˆç‡ï¼Ÿ\",\n",
    "    \"æ¨è–¦ä¸€äº›å¥½ç”¨çš„ç¨‹å¼å·¥å…·\",\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Running model evaluation...\")\n",
    "try:\n",
    "    eval_results = evaluate_preference_model(model, tokenizer, test_prompts)\n",
    "\n",
    "    print(\"\\nğŸ“‹ Evaluation Results:\")\n",
    "    for i, (prompt, response, score) in enumerate(\n",
    "        zip(\n",
    "            eval_results[\"prompts\"],\n",
    "            eval_results[\"responses\"],\n",
    "            eval_results[\"helpfulness_scores\"],\n",
    "        )\n",
    "    ):\n",
    "        print(f\"\\n{i+1}. Prompt: {prompt}\")\n",
    "        print(f\"   Response: {response[:100]}...\")\n",
    "        print(f\"   Helpfulness Score: {score:.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Evaluation failed: {e}\")\n",
    "    print(\"ğŸ’¡ This is expected with the demo setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“ˆ Best Practices and Recommendations (æœ€ä½³å¯¦å‹™èˆ‡å»ºè­°)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def provide_dpo_rlhf_recommendations():\n",
    "    \"\"\"\n",
    "    Provide practical recommendations for choosing between DPO and RLHF\n",
    "    æä¾›é¸æ“‡ DPO æˆ– RLHF çš„å¯¦ç”¨å»ºè­°\n",
    "    \"\"\"\n",
    "\n",
    "    recommendations = {\n",
    "        \"Use DPO When\": [\n",
    "            \"Limited computational resources (ä½¿ç”¨æœ‰é™è¨ˆç®—è³‡æºæ™‚)\",\n",
    "            \"Need stable and reproducible training (éœ€è¦ç©©å®šå¯é‡ç¾çš„è¨“ç·´æ™‚)\",\n",
    "            \"Working with smaller models (<7B parameters) (ä½¿ç”¨è¼ƒå°æ¨¡å‹æ™‚)\",\n",
    "            \"Preference data is high-quality and sufficient (åå¥½è³‡æ–™å“è³ªé«˜ä¸”å……è¶³æ™‚)\",\n",
    "            \"Quick iteration and experimentation needed (éœ€è¦å¿«é€Ÿè¿­ä»£å¯¦é©—æ™‚)\",\n",
    "        ],\n",
    "        \"Use RLHF When\": [\n",
    "            \"Maximum performance is critical (æœ€å¤§æ•ˆèƒ½è‡³é—œé‡è¦æ™‚)\",\n",
    "            \"Have abundant computational resources (æœ‰å……è¶³è¨ˆç®—è³‡æºæ™‚)\",\n",
    "            \"Working with large models (>13B parameters) (ä½¿ç”¨å¤§å‹æ¨¡å‹æ™‚)\",\n",
    "            \"Can invest in reward model engineering (å¯æŠ•è³‡çå‹µæ¨¡å‹å·¥ç¨‹æ™‚)\",\n",
    "            \"Long-term production deployment planned (è¨ˆåŠƒé•·æœŸç”Ÿç”¢éƒ¨ç½²æ™‚)\",\n",
    "        ],\n",
    "        \"Hybrid Approaches\": [\n",
    "            \"Start with DPO for rapid prototyping (å¾ DPO é–‹å§‹å¿«é€ŸåŸå‹è¨­è¨ˆ)\",\n",
    "            \"Use DPO findings to inform RLHF design (ç”¨ DPO ç™¼ç¾ä¾†æŒ‡å° RLHF è¨­è¨ˆ)\",\n",
    "            \"Apply DPO for domain adaptation, RLHF for final tuning (ç”¨ DPO åšé ˜åŸŸé©æ‡‰ï¼ŒRLHF åšæœ€çµ‚èª¿å„ª)\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(\"ğŸ¯ DPO vs RLHF Selection Guide\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for category, items in recommendations.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  â€¢ {item}\")\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# Display recommendations\n",
    "recommendations = provide_dpo_rlhf_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ† Chapter Summary and Key Takeaways (ç« ç¯€ç¸½çµèˆ‡é—œéµè¦é»)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def summarize_dpo_rlhf_learning():\n",
    "    \"\"\"\n",
    "    Summarize key learnings from DPO vs RLHF comparison\n",
    "    ç¸½çµ DPO vs RLHF æ¯”è¼ƒçš„é—œéµå­¸ç¿’é»\n",
    "    \"\"\"\n",
    "\n",
    "    summary = {\n",
    "        \"Key Concepts Learned\": [\n",
    "            \"DPO simplifies preference learning by removing RL complexity (DPO é€šéç§»é™¤ RL è¤‡é›œæ€§ç°¡åŒ–åå¥½å­¸ç¿’)\",\n",
    "            \"Preference data quality is crucial for both approaches (åå¥½è³‡æ–™å“è³ªå°å…©ç¨®æ–¹æ³•éƒ½è‡³é—œé‡è¦)\",\n",
    "            \"Resource requirements differ significantly between DPO and RLHF (DPO å’Œ RLHF çš„è³‡æºéœ€æ±‚å·®ç•°å¾ˆå¤§)\",\n",
    "            \"Training stability varies between the two methods (å…©ç¨®æ–¹æ³•çš„è¨“ç·´ç©©å®šæ€§ä¸åŒ)\",\n",
    "        ],\n",
    "        \"Practical Skills Gained\": [\n",
    "            \"Setting up DPO training with TRL and LoRA (ä½¿ç”¨ TRL å’Œ LoRA è¨­ç½® DPO è¨“ç·´)\",\n",
    "            \"Creating and formatting preference datasets (å»ºç«‹å’Œæ ¼å¼åŒ–åå¥½è³‡æ–™é›†)\",\n",
    "            \"Comparing training paradigms for preference learning (æ¯”è¼ƒåå¥½å­¸ç¿’çš„è¨“ç·´ç¯„å¼)\",\n",
    "            \"Evaluating preference-trained models (è©•ä¼°åå¥½è¨“ç·´çš„æ¨¡å‹)\",\n",
    "        ],\n",
    "        \"Common Pitfalls\": [\n",
    "            \"Insufficient preference data quality can harm both methods (åå¥½è³‡æ–™å“è³ªä¸è¶³æœƒæå®³å…©ç¨®æ–¹æ³•)\",\n",
    "            \"DPO beta parameter requires careful tuning (DPO beta åƒæ•¸éœ€è¦ä»”ç´°èª¿å„ª)\",\n",
    "            \"Memory management crucial for multi-model RLHF setup (è¨˜æ†¶é«”ç®¡ç†å°å¤šæ¨¡å‹ RLHF è¨­ç½®è‡³é—œé‡è¦)\",\n",
    "            \"Evaluation metrics for preference learning need careful design (åå¥½å­¸ç¿’çš„è©•ä¼°æŒ‡æ¨™éœ€è¦ä»”ç´°è¨­è¨ˆ)\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(\"ğŸ“š Chapter 24 Learning Summary\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for category, points in summary.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for point in points:\n",
    "            print(f\"  âœ“ {point}\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Display learning summary\n",
    "learning_summary = summarize_dpo_rlhf_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ§ª Smoke Test (é©—æ”¶æ¸¬è©¦)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def run_dpo_rlhf_smoke_test():\n",
    "    \"\"\"\n",
    "    Quick smoke test to verify DPO setup and concepts\n",
    "    å¿«é€Ÿç…™éœ§æ¸¬è©¦ä»¥é©—è­‰ DPO è¨­ç½®å’Œæ¦‚å¿µ\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"ğŸ§ª Running DPO vs RLHF Smoke Test...\")\n",
    "\n",
    "    tests = []\n",
    "\n",
    "    # Test 1: Model and tokenizer loaded\n",
    "    try:\n",
    "        assert model is not None and tokenizer is not None\n",
    "        tests.append(\"âœ… Model and tokenizer loaded successfully\")\n",
    "    except:\n",
    "        tests.append(\"âŒ Model/tokenizer loading failed\")\n",
    "\n",
    "    # Test 2: Preference dataset created\n",
    "    try:\n",
    "        assert len(preference_dataset) > 0\n",
    "        assert \"prompt\" in preference_dataset.column_names\n",
    "        assert \"chosen\" in preference_dataset.column_names\n",
    "        assert \"rejected\" in preference_dataset.column_names\n",
    "        tests.append(\"âœ… Preference dataset created with correct format\")\n",
    "    except:\n",
    "        tests.append(\"âŒ Preference dataset creation failed\")\n",
    "\n",
    "    # Test 3: LoRA configuration applied\n",
    "    try:\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_percentage = trainable_params / total_params * 100\n",
    "        assert trainable_percentage < 5  # LoRA should have <5% trainable params\n",
    "        tests.append(f\"âœ… LoRA applied ({trainable_percentage:.2f}% trainable)\")\n",
    "    except:\n",
    "        tests.append(\"âŒ LoRA configuration failed\")\n",
    "\n",
    "    # Test 4: Basic generation works\n",
    "    try:\n",
    "        test_prompt = \"Hello\"\n",
    "        inputs = tokenizer.encode(test_prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(inputs, max_new_tokens=10, do_sample=False)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        assert len(response) > len(test_prompt)\n",
    "        tests.append(\"âœ… Basic text generation functional\")\n",
    "    except:\n",
    "        tests.append(\"âŒ Text generation failed\")\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nğŸ“‹ Test Results:\")\n",
    "    for test in tests:\n",
    "        print(f\"  {test}\")\n",
    "\n",
    "    success_rate = sum(1 for test in tests if test.startswith(\"âœ…\")) / len(tests)\n",
    "    print(f\"\\nğŸ¯ Success Rate: {success_rate:.1%}\")\n",
    "\n",
    "    return success_rate >= 0.75\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_passed = run_dpo_rlhf_smoke_test()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ‰ Notebook 24 Complete: DPO vs RLHF Preference Learning\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if smoke_test_passed:\n",
    "    print(\"âœ… All systems functional - ready for preference learning!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Some issues detected - review setup before proceeding\")\n",
    "\n",
    "print(\"\\nğŸ“– Next Steps:\")\n",
    "print(\"  â€¢ Try actual DPO training with more data and compute\")\n",
    "print(\"  â€¢ Experiment with different beta values in DPO\")\n",
    "print(\"  â€¢ Compare results with RLHF when resources allow\")\n",
    "print(\"  â€¢ Explore domain-specific preference learning (nb25)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899069f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ğŸ§ª Final Acceptance Test (æœ€çµ‚é©—æ”¶æ¸¬è©¦)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def final_acceptance_test():\n",
    "    \"\"\"5-line smoke test for DPO vs RLHF comparison\"\"\"\n",
    "    assert model and tokenizer, \"Model/tokenizer setup failed\"\n",
    "    assert len(preference_dataset) > 0, \"Preference dataset empty\"\n",
    "    assert (\n",
    "        sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        < sum(p.numel() for p in model.parameters()) * 0.1\n",
    "    ), \"LoRA not applied\"\n",
    "    print(\"âœ… DPO setup complete - preference learning ready!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "final_acceptance_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68bcca",
   "metadata": {},
   "source": [
    "\n",
    "## 6. æœ¬ç« å°çµ\n",
    "\n",
    "### âœ… å®Œæˆé …ç›® (Completed Items)\n",
    "- **DPO ç†è«–èˆ‡å¯¦ä½œ**ï¼šå®Œæˆ Direct Preference Optimization çš„ç†è«–èªªæ˜èˆ‡è¨“ç·´è¨­ç½®\n",
    "- **åå¥½è³‡æ–™é›†æ§‹å»º**ï¼šå»ºç«‹ä¸­æ–‡åå¥½é…å°è³‡æ–™é›†ï¼ŒåŒ…å« chosen/rejected å›æ‡‰\n",
    "- **ä½è³‡æºè¨“ç·´æ–¹æ¡ˆ**ï¼šæ•´åˆ QLoRA + DPO å¯¦ç¾ä½ VRAM åå¥½å­¸ç¿’\n",
    "- **DPO vs RLHF å°æ¯”**ï¼šå…¨é¢æ¯”è¼ƒå…©ç¨®åå¥½å­¸ç¿’æ–¹æ³•çš„å„ªç¼ºé»\n",
    "- **è©•ä¼°æ¡†æ¶å»ºç«‹**ï¼šè¨­è¨ˆåå¥½å­¸ç¿’æ•ˆæœçš„è©•ä¼°æ–¹æ³•\n",
    "\n",
    "### ğŸ§  æ ¸å¿ƒåŸç†è¦é» (Key Concepts)\n",
    "- **DPO ç°¡åŒ–å„ªå‹¢**ï¼šé€éç›´æ¥å„ªåŒ–åå¥½é…å°ï¼Œé¿å… RLHF çš„ RL è¤‡é›œæ€§èˆ‡ä¸ç©©å®šæ€§\n",
    "- **è³‡æºéœ€æ±‚å·®ç•°**ï¼šDPO åƒ…éœ€ ~2x åŸºç¤æ¨¡å‹è¨˜æ†¶é«”ï¼ŒRLHF éœ€è¦ ~4xï¼ˆactor/critic/reward/reference modelsï¼‰\n",
    "- **è¨“ç·´ç©©å®šæ€§**ï¼šDPO è¨“ç·´æ›´ç©©å®šï¼Œè¶…åƒæ•¸æ•æ„Ÿåº¦è¼ƒä½\n",
    "- **é©ç”¨å ´æ™¯å€åˆ†**ï¼šDPO é©åˆè³‡æºå—é™èˆ‡å¿«é€Ÿè¿­ä»£ï¼ŒRLHF é©åˆè¿½æ±‚æ¥µè‡´æ•ˆèƒ½\n",
    "- **Beta åƒæ•¸é‡è¦æ€§**ï¼šDPO çš„ beta åƒæ•¸æ§åˆ¶æ­£å‰‡åŒ–å¼·åº¦ï¼Œéœ€è¦ä»”ç´°èª¿å„ª\n",
    "\n",
    "### âš ï¸ å¸¸è¦‹å‘é» (Common Pitfalls)\n",
    "- **åå¥½è³‡æ–™å“è³ª**ï¼šä½å“è³ªçš„ chosen/rejected é…å°æœƒåš´é‡å½±éŸ¿å…©ç¨®æ–¹æ³•çš„æ•ˆæœ\n",
    "- **è¨˜æ†¶é«”ç®¡ç†**ï¼šå³ä½¿æ˜¯ DPO ä¹Ÿéœ€è¦è¼‰å…¥ reference modelï¼Œéœ€æ³¨æ„è¨˜æ†¶é«”é…ç½®\n",
    "- **è©•ä¼°å›°é›£æ€§**ï¼šåå¥½å­¸ç¿’çš„æ•ˆæœè©•ä¼°æ¯”å‚³çµ±ç›£ç£å­¸ç¿’æ›´å…·æŒ‘æˆ°æ€§\n",
    "- **éåº¦å„ªåŒ–é¢¨éšª**ï¼šDPO å¯èƒ½éåº¦é©æ‡‰è¨“ç·´è³‡æ–™ä¸­çš„åå¥½æ¨¡å¼\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å»ºè­° (Next Steps)\n",
    "1. **é ˜åŸŸç‰¹å®šå¾®èª¿ (nb25)**ï¼šå°‡åå¥½å­¸ç¿’æ‡‰ç”¨åˆ°ç‰¹å®šé ˜åŸŸï¼ˆé†«ç™‚/æ³•å¾‹/é‡‘èï¼‰\n",
    "2. **é€²éšè©•ä¼°æ–¹æ³•**ï¼šå¯¦ä½œæ›´è¤‡é›œçš„åå¥½å­¸ç¿’è©•ä¼°æŒ‡æ¨™\n",
    "3. **æ··åˆç­–ç•¥æ¢ç´¢**ï¼šå˜—è©¦ DPO + SFT æˆ– DPO + å°è¦æ¨¡ RLHF çš„çµ„åˆæ–¹æ³•\n",
    "4. **ç”Ÿç”¢éƒ¨ç½²æº–å‚™**ï¼šå„ªåŒ–æ¨ç†æ•ˆèƒ½ä¸¦æº–å‚™å¯¦éš›éƒ¨ç½²ç’°å¢ƒ\n",
    "\n",
    "---\n",
    "\n",
    "**éšæ®µæ€§é‡Œç¨‹ç¢‘ï¼šPart D (Fine-tuning) æ¥è¿‘å®Œæˆï¼**\n",
    "\n",
    "æˆ‘å€‘å·²ç¶“å®Œæˆäº† Fine-tuning éšæ®µçš„æ ¸å¿ƒæŠ€è¡“ï¼š\n",
    "- âœ… LoRA å¾®èª¿ (nb20)\n",
    "- âœ… QLoRA ä½ VRAM è¨“ç·´ (nb21) \n",
    "- âœ… Adapters/Prefix Tuning (nb22)\n",
    "- âœ… è³‡æ–™é›†æ•´ç†èˆ‡æ¸…æ´— (nb23)\n",
    "- âœ… **DPO vs RLHF åå¥½å­¸ç¿’ (nb24)** â† å‰›å®Œæˆ\n",
    "- ğŸ”„ é ˜åŸŸç‰¹å®šå¾®èª¿ (nb25) â† ä¸‹ä¸€å€‹\n",
    "\n",
    "å®Œæˆ nb25 å¾Œï¼Œæˆ‘å€‘å°‡é€²å…¥ **Part E: RAG Ã— Agents (é«˜éšæ‡‰ç”¨)** éšæ®µï¼Œæ•´åˆå‰é¢å­¸ç¿’çš„æ‰€æœ‰æŠ€è¡“ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
