{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e55eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Shared Cache Bootstrap ===\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())\n",
    "print(\n",
    "    f\"[GPU Memory] {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"[CPU Mode]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff04b5b9",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\n",
    "🎯 Learning Objectives (學習目標):\n",
    "\n",
    "1. **Preference Learning Theory (偏好學習理論)**:\n",
    "   - DPO vs RLHF fundamental differences (DPO vs RLHF 基本差異)\n",
    "   - Training complexity and stability comparison (訓練複雜度與穩定性對比)\n",
    "\n",
    "2. **DPO Implementation (DPO 實作)**:\n",
    "   - Direct preference optimization with TRL (使用 TRL 進行直接偏好優化)\n",
    "   - QLoRA + DPO for low-VRAM training (低 VRAM 的 QLoRA + DPO 訓練)\n",
    "\n",
    "3. **Preference Data Construction (偏好資料構建)**:\n",
    "   - Create chosen/rejected pairs (建立 chosen/rejected 配對)\n",
    "   - Quality assessment for preference learning (偏好學習的品質評估)\n",
    "\n",
    "4. **Comparative Analysis (對比分析)**:\n",
    "   - Performance vs computational cost (效能 vs 計算成本)\n",
    "   - When to use DPO vs RLHF (何時使用 DPO vs RLHF)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 📦 Dependencies Installation\n",
    "# ============================================================================\n",
    "\n",
    "# Core ML and preference learning libraries\n",
    "# !pip install transformers>=4.36 datasets accelerate bitsandbytes>=0.41\n",
    "# !pip install trl>=0.7.0 peft>=0.7.0  # TRL for DPO training\n",
    "# !pip install torch>=2.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "import json, random, numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 🧠 DPO vs RLHF Theory Comparison (理論對比)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def explain_preference_learning():\n",
    "    \"\"\"\n",
    "    Explain the fundamental differences between DPO and RLHF\n",
    "    解釋 DPO 與 RLHF 的基本差異\n",
    "    \"\"\"\n",
    "    comparison = {\n",
    "        \"Aspect\": [\n",
    "            \"Training Paradigm\",\n",
    "            \"Complexity\",\n",
    "            \"Stability\",\n",
    "            \"Memory Usage\",\n",
    "            \"Training Time\",\n",
    "            \"Performance\",\n",
    "        ],\n",
    "        \"DPO\": [\n",
    "            \"Direct optimization on preference pairs\",\n",
    "            \"Simple, single-stage training\",\n",
    "            \"More stable, no RL instability\",\n",
    "            \"Lower (no critic model needed)\",\n",
    "            \"Faster convergence\",\n",
    "            \"Comparable to RLHF in many tasks\",\n",
    "        ],\n",
    "        \"RLHF\": [\n",
    "            \"RL with reward model + PPO\",\n",
    "            \"Complex, multi-stage (SFT→RM→PPO)\",\n",
    "            \"Can be unstable due to RL\",\n",
    "            \"Higher (actor + critic + ref models)\",\n",
    "            \"Slower, more hyperparameter sensitive\",\n",
    "            \"State-of-the-art when tuned well\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(\"📊 DPO vs RLHF Comparison (DPO vs RLHF 對比)\")\n",
    "    print(\"=\" * 80)\n",
    "    for i, aspect in enumerate(comparison[\"Aspect\"]):\n",
    "        print(f\"{aspect:20} | DPO: {comparison['DPO'][i]}\")\n",
    "        print(f\"{' ' * 20} | RLHF: {comparison['RLHF'][i]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    return comparison\n",
    "\n",
    "\n",
    "# Display the comparison\n",
    "comparison_table = explain_preference_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 🗃️ Preference Dataset Preparation (偏好資料準備)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def create_chinese_preference_dataset(num_samples: int = 100) -> Dataset:\n",
    "    \"\"\"\n",
    "    Create a synthetic Chinese preference dataset\n",
    "    建立合成中文偏好資料集\n",
    "    \"\"\"\n",
    "\n",
    "    # Sample prompts for Chinese assistant evaluation\n",
    "    prompts = [\n",
    "        \"請解釋什麼是人工智慧？\",\n",
    "        \"如何學習程式設計？\",\n",
    "        \"推薦幾本好書給我\",\n",
    "        \"請寫一個關於友情的短故事\",\n",
    "        \"如何保持健康的生活方式？\",\n",
    "        \"解釋量子物理的基本概念\",\n",
    "        \"如何提升工作效率？\",\n",
    "        \"描述你最喜歡的季節\",\n",
    "        \"如何學習一門新語言？\",\n",
    "        \"請給出投資建議\",\n",
    "    ]\n",
    "\n",
    "    # Generate chosen (better) and rejected (worse) responses\n",
    "    def generate_response_pair(prompt: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate chosen and rejected responses for a prompt\"\"\"\n",
    "\n",
    "        # Better response characteristics: helpful, detailed, structured\n",
    "        chosen_responses = {\n",
    "            \"請解釋什麼是人工智慧？\": \"人工智慧（AI）是指讓機器模擬人類智能的技術。它包含機器學習、深度學習、自然語言處理等分支。AI可以應用在圖像識別、語音處理、自動駕駛等領域，旨在解決複雜問題並提升效率。\",\n",
    "            \"如何學習程式設計？\": \"學習程式設計建議按以下步驟：1) 選擇適合的語言（如Python）2) 掌握基本語法和概念 3) 多做練習項目 4) 參與開源項目 5) 持續學習新技術。建議從簡單項目開始，逐步提升難度。\",\n",
    "        }\n",
    "\n",
    "        # Worse response characteristics: vague, unhelpful, or inappropriate\n",
    "        rejected_responses = {\n",
    "            \"請解釋什麼是人工智慧？\": \"AI就是很厲害的電腦技術，很複雜，我也不太懂。\",\n",
    "            \"如何學習程式設計？\": \"程式設計很難，你直接去補習班學比較快。\",\n",
    "        }\n",
    "\n",
    "        # Use predefined responses or generate generic ones\n",
    "        chosen = chosen_responses.get(\n",
    "            prompt, f\"這是一個很好的問題。{prompt}的答案需要從多個角度來分析...\"\n",
    "        )\n",
    "        rejected = rejected_responses.get(prompt, f\"不知道，你可以自己查資料。\")\n",
    "\n",
    "        return chosen, rejected\n",
    "\n",
    "    # Build preference dataset\n",
    "    preference_data = []\n",
    "    for i in range(num_samples):\n",
    "        prompt = random.choice(prompts)\n",
    "        chosen, rejected = generate_response_pair(prompt)\n",
    "\n",
    "        preference_data.append(\n",
    "            {\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
    "        )\n",
    "\n",
    "    return Dataset.from_list(preference_data)\n",
    "\n",
    "\n",
    "# Create preference dataset\n",
    "print(\"🗃️ Creating Chinese Preference Dataset...\")\n",
    "preference_dataset = create_chinese_preference_dataset(num_samples=50)\n",
    "print(f\"Created {len(preference_dataset)} preference pairs\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n📝 Sample Preference Pair:\")\n",
    "sample = preference_dataset[0]\n",
    "print(f\"Prompt: {sample['prompt']}\")\n",
    "print(f\"Chosen: {sample['chosen'][:100]}...\")\n",
    "print(f\"Rejected: {sample['rejected'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37966c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 🔧 Model and Tokenizer Setup (模型與分詞器設定)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def setup_model_for_dpo(\n",
    "    model_name: str = \"microsoft/DialoGPT-medium\", use_4bit: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Setup model and tokenizer for DPO training\n",
    "    為 DPO 訓練設置模型與分詞器\n",
    "    \"\"\"\n",
    "\n",
    "    # Configure quantization for low VRAM\n",
    "    if use_4bit:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "    else:\n",
    "        bnb_config = None\n",
    "\n",
    "    # Load model\n",
    "    print(f\"🤖 Loading model: {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# Setup model (using smaller model for demo)\n",
    "model_name = \"microsoft/DialoGPT-medium\"  # Smaller model for demo\n",
    "model, tokenizer = setup_model_for_dpo(model_name, use_4bit=True)\n",
    "\n",
    "print(f\"✅ Model loaded: {model_name}\")\n",
    "print(f\"📏 Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ddb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 🎯 DPO Training Implementation (DPO 訓練實作)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def prepare_model_for_dpo_training(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Prepare model for DPO training with LoRA\n",
    "    準備模型進行 DPO 訓練（使用 LoRA）\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare model for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # LoRA configuration for DPO\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"c_attn\", \"c_proj\"],  # For DialoGPT\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "\n",
    "    # Add LoRA adapters\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Prepare model for training\n",
    "print(\"🔧 Preparing model for DPO training...\")\n",
    "model = prepare_model_for_dpo_training(model, tokenizer)\n",
    "\n",
    "\n",
    "def format_preference_data_for_dpo(dataset: Dataset, tokenizer) -> Dataset:\n",
    "    \"\"\"\n",
    "    Format preference dataset for DPO training\n",
    "    為 DPO 訓練格式化偏好資料\n",
    "    \"\"\"\n",
    "\n",
    "    def format_sample(examples):\n",
    "        formatted = []\n",
    "        for prompt, chosen, rejected in zip(\n",
    "            examples[\"prompt\"], examples[\"chosen\"], examples[\"rejected\"]\n",
    "        ):\n",
    "            formatted.append({\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected})\n",
    "        return formatted\n",
    "\n",
    "    # Apply formatting\n",
    "    formatted_dataset = dataset.map(\n",
    "        lambda x: {\"formatted\": format_sample(x)},\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    return formatted_dataset\n",
    "\n",
    "\n",
    "# Format dataset for DPO\n",
    "print(\"📋 Formatting preference dataset for DPO...\")\n",
    "formatted_dataset = preference_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "\n",
    "def train_dpo_model(model, tokenizer, train_dataset, eval_dataset=None):\n",
    "    \"\"\"\n",
    "    Train model using Direct Preference Optimization\n",
    "    使用直接偏好優化訓練模型\n",
    "    \"\"\"\n",
    "\n",
    "    # DPO training configuration\n",
    "    training_args = DPOConfig(\n",
    "        output_dir=\"./dpo_output\",\n",
    "        num_train_epochs=1,  # Short for demo\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=5e-6,\n",
    "        max_length=256,\n",
    "        max_prompt_length=128,\n",
    "        beta=0.1,  # DPO regularization parameter\n",
    "        logging_steps=10,\n",
    "        save_steps=50,\n",
    "        eval_steps=50,\n",
    "        warmup_ratio=0.1,\n",
    "        remove_unused_columns=False,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_drop_last=True,\n",
    "        bf16=True if torch.cuda.is_bf16_supported() else False,\n",
    "        fp16=True if not torch.cuda.is_bf16_supported() else False,\n",
    "    )\n",
    "\n",
    "    # Initialize DPO trainer\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        beta=0.1,  # DPO regularization coefficient\n",
    "    )\n",
    "\n",
    "    print(\"🚀 Starting DPO training...\")\n",
    "    try:\n",
    "        # Start training\n",
    "        train_result = dpo_trainer.train()\n",
    "        print(\"✅ DPO training completed!\")\n",
    "\n",
    "        # Save model\n",
    "        dpo_trainer.save_model()\n",
    "        print(\"💾 Model saved to ./dpo_output\")\n",
    "\n",
    "        return train_result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed: {e}\")\n",
    "        print(\"💡 This might be due to memory constraints or data formatting issues\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Note: Actual training requires more memory and time\n",
    "print(\"📝 DPO Training Setup Complete\")\n",
    "print(\"💡 To run actual training, uncomment the following lines:\")\n",
    "print(\n",
    "    \"# train_result = train_dpo_model(model, tokenizer, formatted_dataset['train'], formatted_dataset['test'])\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 🆚 RLHF vs DPO Comparison Implementation (RLHF vs DPO 比較實作)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def simulate_rlhf_workflow():\n",
    "    \"\"\"\n",
    "    Simulate RLHF workflow steps (for comparison)\n",
    "    模擬 RLHF 工作流程步驟（用於比較）\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"🔄 RLHF Workflow Simulation:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    steps = [\n",
    "        (\"1. Supervised Fine-tuning (SFT)\", \"Train base model on demonstration data\"),\n",
    "        (\"2. Reward Model Training\", \"Train reward model on preference pairs\"),\n",
    "        (\"3. PPO Training\", \"Optimize policy using PPO with reward model\"),\n",
    "        (\"4. Iterative Refinement\", \"Multiple rounds of data collection and training\"),\n",
    "    ]\n",
    "\n",
    "    for step, description in steps:\n",
    "        print(f\"{step}: {description}\")\n",
    "\n",
    "    print(\"\\n📊 Resource Requirements Comparison:\")\n",
    "\n",
    "    resource_comparison = {\n",
    "        \"Training Stages\": [\"DPO: 1 stage\", \"RLHF: 3-4 stages\"],\n",
    "        \"Models Required\": [\n",
    "            \"DPO: Base + Reference\",\n",
    "            \"RLHF: Base + Reward + Actor + Critic\",\n",
    "        ],\n",
    "        \"Memory Usage\": [\"DPO: ~2x base model\", \"RLHF: ~4x base model\"],\n",
    "        \"Training Time\": [\"DPO: Hours\", \"RLHF: Days to weeks\"],\n",
    "        \"Hyperparameter Sensitivity\": [\"DPO: Low\", \"RLHF: High\"],\n",
    "        \"Implementation Complexity\": [\"DPO: Simple\", \"RLHF: Complex\"],\n",
    "    }\n",
    "\n",
    "    for aspect, comparison in resource_comparison.items():\n",
    "        print(f\"{aspect:25}: {comparison[0]} vs {comparison[1]}\")\n",
    "\n",
    "    return resource_comparison\n",
    "\n",
    "\n",
    "# Run RLHF simulation\n",
    "rlhf_comparison = simulate_rlhf_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc30dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 📊 Evaluation and Comparison (評估與比較)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def evaluate_preference_model(model, tokenizer, test_prompts: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate model responses for preference learning effectiveness\n",
    "    評估模型回應的偏好學習效果\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"📊 Evaluating model responses...\")\n",
    "\n",
    "    results = {\n",
    "        \"prompts\": [],\n",
    "        \"responses\": [],\n",
    "        \"response_lengths\": [],\n",
    "        \"helpfulness_scores\": [],\n",
    "    }\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for prompt in test_prompts:\n",
    "        # Generate response\n",
    "        inputs = tokenizer.encode(\n",
    "            prompt, return_tensors=\"pt\", max_length=128, truncation=True\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = response[len(prompt) :].strip()\n",
    "\n",
    "        # Simple helpfulness scoring (length and content diversity)\n",
    "        helpfulness_score = min(\n",
    "            len(response.split()) / 20, 1.0\n",
    "        )  # Normalize by word count\n",
    "\n",
    "        results[\"prompts\"].append(prompt)\n",
    "        results[\"responses\"].append(response)\n",
    "        results[\"response_lengths\"].append(len(response))\n",
    "        results[\"helpfulness_scores\"].append(helpfulness_score)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test prompts for evaluation\n",
    "test_prompts = [\n",
    "    \"請解釋機器學習的基本概念\",\n",
    "    \"如何提升學習效率？\",\n",
    "    \"推薦一些好用的程式工具\",\n",
    "]\n",
    "\n",
    "print(\"🧪 Running model evaluation...\")\n",
    "try:\n",
    "    eval_results = evaluate_preference_model(model, tokenizer, test_prompts)\n",
    "\n",
    "    print(\"\\n📋 Evaluation Results:\")\n",
    "    for i, (prompt, response, score) in enumerate(\n",
    "        zip(\n",
    "            eval_results[\"prompts\"],\n",
    "            eval_results[\"responses\"],\n",
    "            eval_results[\"helpfulness_scores\"],\n",
    "        )\n",
    "    ):\n",
    "        print(f\"\\n{i+1}. Prompt: {prompt}\")\n",
    "        print(f\"   Response: {response[:100]}...\")\n",
    "        print(f\"   Helpfulness Score: {score:.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Evaluation failed: {e}\")\n",
    "    print(\"💡 This is expected with the demo setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 📈 Best Practices and Recommendations (最佳實務與建議)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def provide_dpo_rlhf_recommendations():\n",
    "    \"\"\"\n",
    "    Provide practical recommendations for choosing between DPO and RLHF\n",
    "    提供選擇 DPO 或 RLHF 的實用建議\n",
    "    \"\"\"\n",
    "\n",
    "    recommendations = {\n",
    "        \"Use DPO When\": [\n",
    "            \"Limited computational resources (使用有限計算資源時)\",\n",
    "            \"Need stable and reproducible training (需要穩定可重現的訓練時)\",\n",
    "            \"Working with smaller models (<7B parameters) (使用較小模型時)\",\n",
    "            \"Preference data is high-quality and sufficient (偏好資料品質高且充足時)\",\n",
    "            \"Quick iteration and experimentation needed (需要快速迭代實驗時)\",\n",
    "        ],\n",
    "        \"Use RLHF When\": [\n",
    "            \"Maximum performance is critical (最大效能至關重要時)\",\n",
    "            \"Have abundant computational resources (有充足計算資源時)\",\n",
    "            \"Working with large models (>13B parameters) (使用大型模型時)\",\n",
    "            \"Can invest in reward model engineering (可投資獎勵模型工程時)\",\n",
    "            \"Long-term production deployment planned (計劃長期生產部署時)\",\n",
    "        ],\n",
    "        \"Hybrid Approaches\": [\n",
    "            \"Start with DPO for rapid prototyping (從 DPO 開始快速原型設計)\",\n",
    "            \"Use DPO findings to inform RLHF design (用 DPO 發現來指導 RLHF 設計)\",\n",
    "            \"Apply DPO for domain adaptation, RLHF for final tuning (用 DPO 做領域適應，RLHF 做最終調優)\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(\"🎯 DPO vs RLHF Selection Guide\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for category, items in recommendations.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  • {item}\")\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# Display recommendations\n",
    "recommendations = provide_dpo_rlhf_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 🏆 Chapter Summary and Key Takeaways (章節總結與關鍵要點)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def summarize_dpo_rlhf_learning():\n",
    "    \"\"\"\n",
    "    Summarize key learnings from DPO vs RLHF comparison\n",
    "    總結 DPO vs RLHF 比較的關鍵學習點\n",
    "    \"\"\"\n",
    "\n",
    "    summary = {\n",
    "        \"Key Concepts Learned\": [\n",
    "            \"DPO simplifies preference learning by removing RL complexity (DPO 通過移除 RL 複雜性簡化偏好學習)\",\n",
    "            \"Preference data quality is crucial for both approaches (偏好資料品質對兩種方法都至關重要)\",\n",
    "            \"Resource requirements differ significantly between DPO and RLHF (DPO 和 RLHF 的資源需求差異很大)\",\n",
    "            \"Training stability varies between the two methods (兩種方法的訓練穩定性不同)\",\n",
    "        ],\n",
    "        \"Practical Skills Gained\": [\n",
    "            \"Setting up DPO training with TRL and LoRA (使用 TRL 和 LoRA 設置 DPO 訓練)\",\n",
    "            \"Creating and formatting preference datasets (建立和格式化偏好資料集)\",\n",
    "            \"Comparing training paradigms for preference learning (比較偏好學習的訓練範式)\",\n",
    "            \"Evaluating preference-trained models (評估偏好訓練的模型)\",\n",
    "        ],\n",
    "        \"Common Pitfalls\": [\n",
    "            \"Insufficient preference data quality can harm both methods (偏好資料品質不足會損害兩種方法)\",\n",
    "            \"DPO beta parameter requires careful tuning (DPO beta 參數需要仔細調優)\",\n",
    "            \"Memory management crucial for multi-model RLHF setup (記憶體管理對多模型 RLHF 設置至關重要)\",\n",
    "            \"Evaluation metrics for preference learning need careful design (偏好學習的評估指標需要仔細設計)\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(\"📚 Chapter 24 Learning Summary\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for category, points in summary.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for point in points:\n",
    "            print(f\"  ✓ {point}\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Display learning summary\n",
    "learning_summary = summarize_dpo_rlhf_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 🧪 Smoke Test (驗收測試)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def run_dpo_rlhf_smoke_test():\n",
    "    \"\"\"\n",
    "    Quick smoke test to verify DPO setup and concepts\n",
    "    快速煙霧測試以驗證 DPO 設置和概念\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"🧪 Running DPO vs RLHF Smoke Test...\")\n",
    "\n",
    "    tests = []\n",
    "\n",
    "    # Test 1: Model and tokenizer loaded\n",
    "    try:\n",
    "        assert model is not None and tokenizer is not None\n",
    "        tests.append(\"✅ Model and tokenizer loaded successfully\")\n",
    "    except:\n",
    "        tests.append(\"❌ Model/tokenizer loading failed\")\n",
    "\n",
    "    # Test 2: Preference dataset created\n",
    "    try:\n",
    "        assert len(preference_dataset) > 0\n",
    "        assert \"prompt\" in preference_dataset.column_names\n",
    "        assert \"chosen\" in preference_dataset.column_names\n",
    "        assert \"rejected\" in preference_dataset.column_names\n",
    "        tests.append(\"✅ Preference dataset created with correct format\")\n",
    "    except:\n",
    "        tests.append(\"❌ Preference dataset creation failed\")\n",
    "\n",
    "    # Test 3: LoRA configuration applied\n",
    "    try:\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_percentage = trainable_params / total_params * 100\n",
    "        assert trainable_percentage < 5  # LoRA should have <5% trainable params\n",
    "        tests.append(f\"✅ LoRA applied ({trainable_percentage:.2f}% trainable)\")\n",
    "    except:\n",
    "        tests.append(\"❌ LoRA configuration failed\")\n",
    "\n",
    "    # Test 4: Basic generation works\n",
    "    try:\n",
    "        test_prompt = \"Hello\"\n",
    "        inputs = tokenizer.encode(test_prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(inputs, max_new_tokens=10, do_sample=False)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        assert len(response) > len(test_prompt)\n",
    "        tests.append(\"✅ Basic text generation functional\")\n",
    "    except:\n",
    "        tests.append(\"❌ Text generation failed\")\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n📋 Test Results:\")\n",
    "    for test in tests:\n",
    "        print(f\"  {test}\")\n",
    "\n",
    "    success_rate = sum(1 for test in tests if test.startswith(\"✅\")) / len(tests)\n",
    "    print(f\"\\n🎯 Success Rate: {success_rate:.1%}\")\n",
    "\n",
    "    return success_rate >= 0.75\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_passed = run_dpo_rlhf_smoke_test()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🎉 Notebook 24 Complete: DPO vs RLHF Preference Learning\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if smoke_test_passed:\n",
    "    print(\"✅ All systems functional - ready for preference learning!\")\n",
    "else:\n",
    "    print(\"⚠️  Some issues detected - review setup before proceeding\")\n",
    "\n",
    "print(\"\\n📖 Next Steps:\")\n",
    "print(\"  • Try actual DPO training with more data and compute\")\n",
    "print(\"  • Experiment with different beta values in DPO\")\n",
    "print(\"  • Compare results with RLHF when resources allow\")\n",
    "print(\"  • Explore domain-specific preference learning (nb25)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899069f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 🧪 Final Acceptance Test (最終驗收測試)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def final_acceptance_test():\n",
    "    \"\"\"5-line smoke test for DPO vs RLHF comparison\"\"\"\n",
    "    assert model and tokenizer, \"Model/tokenizer setup failed\"\n",
    "    assert len(preference_dataset) > 0, \"Preference dataset empty\"\n",
    "    assert (\n",
    "        sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        < sum(p.numel() for p in model.parameters()) * 0.1\n",
    "    ), \"LoRA not applied\"\n",
    "    print(\"✅ DPO setup complete - preference learning ready!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "final_acceptance_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68bcca",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 本章小結\n",
    "\n",
    "### ✅ 完成項目 (Completed Items)\n",
    "- **DPO 理論與實作**：完成 Direct Preference Optimization 的理論說明與訓練設置\n",
    "- **偏好資料集構建**：建立中文偏好配對資料集，包含 chosen/rejected 回應\n",
    "- **低資源訓練方案**：整合 QLoRA + DPO 實現低 VRAM 偏好學習\n",
    "- **DPO vs RLHF 對比**：全面比較兩種偏好學習方法的優缺點\n",
    "- **評估框架建立**：設計偏好學習效果的評估方法\n",
    "\n",
    "### 🧠 核心原理要點 (Key Concepts)\n",
    "- **DPO 簡化優勢**：透過直接優化偏好配對，避免 RLHF 的 RL 複雜性與不穩定性\n",
    "- **資源需求差異**：DPO 僅需 ~2x 基礎模型記憶體，RLHF 需要 ~4x（actor/critic/reward/reference models）\n",
    "- **訓練穩定性**：DPO 訓練更穩定，超參數敏感度較低\n",
    "- **適用場景區分**：DPO 適合資源受限與快速迭代，RLHF 適合追求極致效能\n",
    "- **Beta 參數重要性**：DPO 的 beta 參數控制正則化強度，需要仔細調優\n",
    "\n",
    "### ⚠️ 常見坑點 (Common Pitfalls)\n",
    "- **偏好資料品質**：低品質的 chosen/rejected 配對會嚴重影響兩種方法的效果\n",
    "- **記憶體管理**：即使是 DPO 也需要載入 reference model，需注意記憶體配置\n",
    "- **評估困難性**：偏好學習的效果評估比傳統監督學習更具挑戰性\n",
    "- **過度優化風險**：DPO 可能過度適應訓練資料中的偏好模式\n",
    "\n",
    "### 🚀 下一步建議 (Next Steps)\n",
    "1. **領域特定微調 (nb25)**：將偏好學習應用到特定領域（醫療/法律/金融）\n",
    "2. **進階評估方法**：實作更複雜的偏好學習評估指標\n",
    "3. **混合策略探索**：嘗試 DPO + SFT 或 DPO + 小規模 RLHF 的組合方法\n",
    "4. **生產部署準備**：優化推理效能並準備實際部署環境\n",
    "\n",
    "---\n",
    "\n",
    "**階段性里程碑：Part D (Fine-tuning) 接近完成！**\n",
    "\n",
    "我們已經完成了 Fine-tuning 階段的核心技術：\n",
    "- ✅ LoRA 微調 (nb20)\n",
    "- ✅ QLoRA 低 VRAM 訓練 (nb21) \n",
    "- ✅ Adapters/Prefix Tuning (nb22)\n",
    "- ✅ 資料集整理與清洗 (nb23)\n",
    "- ✅ **DPO vs RLHF 偏好學習 (nb24)** ← 剛完成\n",
    "- 🔄 領域特定微調 (nb25) ← 下一個\n",
    "\n",
    "完成 nb25 後，我們將進入 **Part E: RAG × Agents (高階應用)** 階段，整合前面學習的所有技術！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
