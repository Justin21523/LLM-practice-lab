{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7459a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb32_fastapi_docker_deploy.ipynb\n",
    "# FastAPI + Docker 部署 - 生產級 LLM API 服務\n",
    "\n",
    "## 1. 環境初始化 & 共享快取設置\n",
    "\n",
    "# === Shared Cache Bootstrap ===\n",
    "import os, pathlib, torch, sys\n",
    "from typing import Optional, List, Dict, Any\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "for k, v in paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec39453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "pip install fastapi[all] uvicorn[standard] python-multipart\n",
    "pip install docker python-dotenv pydantic-settings\n",
    "pip install prometheus-client structlog\n",
    "pip install httpx pytest-asyncio\n",
    "\n",
    "# app/models.py - Pydantic 模型定義\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class MessageRole(str, Enum):\n",
    "    SYSTEM = \"system\"\n",
    "    USER = \"user\"\n",
    "    ASSISTANT = \"assistant\"\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    role: MessageRole\n",
    "    content: str\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class GenerateRequest(BaseModel):\n",
    "    prompt: str = Field(..., description=\"Input prompt for generation\")\n",
    "    max_tokens: int = Field(512, ge=1, le=4096)\n",
    "    temperature: float = Field(0.7, ge=0.0, le=2.0)\n",
    "    top_p: float = Field(0.9, ge=0.0, le=1.0)\n",
    "    model_id: Optional[str] = Field(\"Qwen/Qwen2.5-7B-Instruct\", description=\"Model identifier\")\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    messages: List[ChatMessage]\n",
    "    max_tokens: int = Field(512, ge=1, le=4096)\n",
    "    temperature: float = Field(0.7, ge=0.0, le=2.0)\n",
    "    stream: bool = Field(False, description=\"Enable streaming response\")\n",
    "    use_tools: bool = Field(False, description=\"Enable tool calling\")\n",
    "\n",
    "class RetrieveRequest(BaseModel):\n",
    "    query: str = Field(..., description=\"Search query\")\n",
    "    top_k: int = Field(5, ge=1, le=20)\n",
    "    collection_name: Optional[str] = Field(\"default\", description=\"Vector collection name\")\n",
    "\n",
    "class GenerateResponse(BaseModel):\n",
    "    text: str\n",
    "    model_id: str\n",
    "    tokens_used: int\n",
    "    generation_time: float\n",
    "    metadata: Dict[str, Any] = {}\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    message: ChatMessage\n",
    "    model_id: str\n",
    "    tokens_used: int\n",
    "    generation_time: float\n",
    "    tool_calls: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "class RetrieveResponse(BaseModel):\n",
    "    results: List[Dict[str, Any]]\n",
    "    query: str\n",
    "    total_found: int\n",
    "    search_time: float\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    status: str\n",
    "    timestamp: str\n",
    "    model_loaded: bool\n",
    "    gpu_available: bool\n",
    "    memory_usage: Dict[str, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app/core/llm_service.py - LLM 服務管理\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import structlog\n",
    "from typing import List, Dict, Any, Optional\n",
    "import time\n",
    "import psutil\n",
    "import threading\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "\n",
    "class LLMService:\n",
    "    \"\"\"Unified LLM service with model management and generation\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.current_model_id = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model_lock = threading.Lock()\n",
    "\n",
    "    async def load_model(self, model_id: str, force_reload: bool = False) -> bool:\n",
    "        \"\"\"Load or switch model with optimized configuration\"\"\"\n",
    "        if self.current_model_id == model_id and not force_reload:\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "            with self.model_lock:\n",
    "                logger.info(\"Loading model\", model_id=model_id, device=self.device)\n",
    "\n",
    "                # Clear previous model\n",
    "                if self.model is not None:\n",
    "                    del self.model\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # Configure quantization for low VRAM\n",
    "                quantization_config = None\n",
    "                if torch.cuda.is_available():\n",
    "                    quantization_config = BitsAndBytesConfig(\n",
    "                        load_in_4bit=True,\n",
    "                        bnb_4bit_compute_dtype=torch.float16,\n",
    "                        bnb_4bit_use_double_quant=True,\n",
    "                        bnb_4bit_quant_type=\"nf4\",\n",
    "                    )\n",
    "\n",
    "                # Load tokenizer and model\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    model_id,\n",
    "                    trust_remote_code=True,\n",
    "                    cache_dir=os.environ.get(\"TRANSFORMERS_CACHE\"),\n",
    "                )\n",
    "\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_id,\n",
    "                    quantization_config=quantization_config,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    cache_dir=os.environ.get(\"TRANSFORMERS_CACHE\"),\n",
    "                )\n",
    "\n",
    "                # Set pad token if missing\n",
    "                if self.tokenizer.pad_token is None:\n",
    "                    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "                self.current_model_id = model_id\n",
    "                logger.info(\"Model loaded successfully\", model_id=model_id)\n",
    "                return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to load model\", model_id=model_id, error=str(e))\n",
    "            return False\n",
    "\n",
    "    async def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_tokens: int = 512,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Generate text with the loaded model\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise ValueError(\"No model loaded\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=2048,\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Generate with optimized parameters\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    do_sample=temperature > 0,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            # Decode output\n",
    "            generated_text = self.tokenizer.decode(\n",
    "                outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            generation_time = time.time() - start_time\n",
    "            tokens_used = outputs[0].shape[1] - inputs.input_ids.shape[1]\n",
    "\n",
    "            return {\n",
    "                \"text\": generated_text.strip(),\n",
    "                \"tokens_used\": tokens_used,\n",
    "                \"generation_time\": generation_time,\n",
    "                \"model_id\": self.current_model_id,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Generation failed\", error=str(e))\n",
    "            raise\n",
    "\n",
    "    async def chat_completion(\n",
    "        self,\n",
    "        messages: List[ChatMessage],\n",
    "        max_tokens: int = 512,\n",
    "        temperature: float = 0.7,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Chat completion with message formatting\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise ValueError(\"No model loaded\")\n",
    "\n",
    "        # Format messages for chat model\n",
    "        formatted_prompt = self._format_chat_messages(messages)\n",
    "\n",
    "        result = await self.generate_text(\n",
    "            formatted_prompt, max_tokens=max_tokens, temperature=temperature\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"message\": ChatMessage(role=MessageRole.ASSISTANT, content=result[\"text\"]),\n",
    "            \"tokens_used\": result[\"tokens_used\"],\n",
    "            \"generation_time\": result[\"generation_time\"],\n",
    "            \"model_id\": result[\"model_id\"],\n",
    "        }\n",
    "\n",
    "    def _format_chat_messages(self, messages: List[ChatMessage]) -> str:\n",
    "        \"\"\"Format messages for instruction-tuned models\"\"\"\n",
    "        formatted = \"\"\n",
    "        for msg in messages:\n",
    "            if msg.role == MessageRole.SYSTEM:\n",
    "                formatted += f\"System: {msg.content}\\n\"\n",
    "            elif msg.role == MessageRole.USER:\n",
    "                formatted += f\"User: {msg.content}\\n\"\n",
    "            elif msg.role == MessageRole.ASSISTANT:\n",
    "                formatted += f\"Assistant: {msg.content}\\n\"\n",
    "\n",
    "        formatted += \"Assistant: \"\n",
    "        return formatted\n",
    "\n",
    "    def get_model_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current model information and system stats\"\"\"\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        gpu_memory = 0\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.memory_allocated(0) / 1e9\n",
    "\n",
    "        return {\n",
    "            \"model_loaded\": self.model is not None,\n",
    "            \"current_model\": self.current_model_id,\n",
    "            \"device\": self.device,\n",
    "            \"system_memory_gb\": memory_info.total / 1e9,\n",
    "            \"memory_used_gb\": memory_info.used / 1e9,\n",
    "            \"gpu_memory_gb\": gpu_memory,\n",
    "        }\n",
    "\n",
    "\n",
    "# Global service instance\n",
    "llm_service = LLMService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app/main.py - FastAPI 應用主體\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.middleware.gzip import GZipMiddleware\n",
    "from contextlib import asynccontextmanager\n",
    "import structlog\n",
    "from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST\n",
    "from fastapi.responses import Response\n",
    "import time\n",
    "\n",
    "# Import our models and services\n",
    "from .models import *\n",
    "from .core.llm_service import llm_service\n",
    "\n",
    "# Configure structured logging\n",
    "structlog.configure(\n",
    "    processors=[\n",
    "        structlog.stdlib.filter_by_level,\n",
    "        structlog.stdlib.add_logger_name,\n",
    "        structlog.stdlib.add_log_level,\n",
    "        structlog.stdlib.PositionalArgumentsFormatter(),\n",
    "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "        structlog.processors.StackInfoRenderer(),\n",
    "        structlog.processors.format_exc_info,\n",
    "        structlog.processors.UnicodeDecoder(),\n",
    "        structlog.processors.JSONRenderer(),\n",
    "    ],\n",
    "    context_class=dict,\n",
    "    logger_factory=structlog.stdlib.LoggerFactory(),\n",
    "    wrapper_class=structlog.stdlib.BoundLogger,\n",
    "    cache_logger_on_first_use=True,\n",
    ")\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "# Prometheus metrics\n",
    "REQUEST_COUNT = Counter(\n",
    "    \"api_requests_total\", \"Total API requests\", [\"method\", \"endpoint\", \"status\"]\n",
    ")\n",
    "REQUEST_DURATION = Histogram(\n",
    "    \"api_request_duration_seconds\", \"Request duration\", [\"method\", \"endpoint\"]\n",
    ")\n",
    "GENERATION_DURATION = Histogram(\n",
    "    \"llm_generation_duration_seconds\", \"LLM generation time\"\n",
    ")\n",
    "TOKENS_GENERATED = Counter(\"llm_tokens_generated_total\", \"Total tokens generated\")\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Application lifespan management\"\"\"\n",
    "    # Startup\n",
    "    logger.info(\"Starting FastAPI LLM service\")\n",
    "\n",
    "    # Preload default model\n",
    "    default_model = os.getenv(\"DEFAULT_MODEL\", \"Qwen/Qwen2.5-7B-Instruct\")\n",
    "    success = await llm_service.load_model(default_model)\n",
    "    if not success:\n",
    "        logger.warning(\"Failed to preload default model\", model=default_model)\n",
    "\n",
    "    yield\n",
    "\n",
    "    # Shutdown\n",
    "    logger.info(\"Shutting down FastAPI LLM service\")\n",
    "\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"LLM API Service\",\n",
    "    description=\"Production-ready LLM API with RAG and Agent capabilities\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan,\n",
    ")\n",
    "\n",
    "# Middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Configure appropriately for production\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "app.add_middleware(GZipMiddleware, minimum_size=1000)\n",
    "\n",
    "\n",
    "# Dependency for request timing\n",
    "async def track_request_metrics(request):\n",
    "    start_time = time.time()\n",
    "    yield\n",
    "    duration = time.time() - start_time\n",
    "    REQUEST_DURATION.labels(method=request.method, endpoint=request.url.path).observe(\n",
    "        duration\n",
    "    )\n",
    "\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def metrics_middleware(request, call_next):\n",
    "    \"\"\"Track request metrics\"\"\"\n",
    "    start_time = time.time()\n",
    "    response = await call_next(request)\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    REQUEST_COUNT.labels(\n",
    "        method=request.method, endpoint=request.url.path, status=response.status_code\n",
    "    ).inc()\n",
    "\n",
    "    REQUEST_DURATION.labels(method=request.method, endpoint=request.url.path).observe(\n",
    "        duration\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/health\", response_model=HealthResponse)\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    model_info = llm_service.get_model_info()\n",
    "\n",
    "    return HealthResponse(\n",
    "        status=\"healthy\" if model_info[\"model_loaded\"] else \"degraded\",\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        model_loaded=model_info[\"model_loaded\"],\n",
    "        gpu_available=torch.cuda.is_available(),\n",
    "        memory_usage={\n",
    "            \"system_memory_gb\": model_info[\"system_memory_gb\"],\n",
    "            \"memory_used_gb\": model_info[\"memory_used_gb\"],\n",
    "            \"gpu_memory_gb\": model_info[\"gpu_memory_gb\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "# Prometheus metrics endpoint\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint\"\"\"\n",
    "    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)\n",
    "\n",
    "\n",
    "# Text generation endpoint\n",
    "@app.post(\"/generate\", response_model=GenerateResponse)\n",
    "async def generate_text(request: GenerateRequest):\n",
    "    \"\"\"Generate text from prompt\"\"\"\n",
    "    logger.info(\n",
    "        \"Generate request\", prompt_length=len(request.prompt), model_id=request.model_id\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Load model if different from current\n",
    "        if llm_service.current_model_id != request.model_id:\n",
    "            success = await llm_service.load_model(request.model_id)\n",
    "            if not success:\n",
    "                raise HTTPException(\n",
    "                    status_code=500, detail=f\"Failed to load model: {request.model_id}\"\n",
    "                )\n",
    "\n",
    "        # Generate text\n",
    "        start_time = time.time()\n",
    "        result = await llm_service.generate_text(\n",
    "            prompt=request.prompt,\n",
    "            max_tokens=request.max_tokens,\n",
    "            temperature=request.temperature,\n",
    "            top_p=request.top_p,\n",
    "        )\n",
    "\n",
    "        # Track metrics\n",
    "        GENERATION_DURATION.observe(result[\"generation_time\"])\n",
    "        TOKENS_GENERATED.inc(result[\"tokens_used\"])\n",
    "\n",
    "        return GenerateResponse(\n",
    "            text=result[\"text\"],\n",
    "            model_id=result[\"model_id\"],\n",
    "            tokens_used=result[\"tokens_used\"],\n",
    "            generation_time=result[\"generation_time\"],\n",
    "            metadata={\n",
    "                \"prompt_length\": len(request.prompt),\n",
    "                \"temperature\": request.temperature,\n",
    "                \"top_p\": request.top_p,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Generation failed\", error=str(e))\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "# Chat completion endpoint\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat_completion(request: ChatRequest):\n",
    "    \"\"\"Chat completion with message history\"\"\"\n",
    "    logger.info(\"Chat request\", message_count=len(request.messages))\n",
    "\n",
    "    try:\n",
    "        # Generate response\n",
    "        result = await llm_service.chat_completion(\n",
    "            messages=request.messages,\n",
    "            max_tokens=request.max_tokens,\n",
    "            temperature=request.temperature,\n",
    "        )\n",
    "\n",
    "        # Track metrics\n",
    "        GENERATION_DURATION.observe(result[\"generation_time\"])\n",
    "        TOKENS_GENERATED.inc(result[\"tokens_used\"])\n",
    "\n",
    "        return ChatResponse(\n",
    "            message=result[\"message\"],\n",
    "            model_id=result[\"model_id\"],\n",
    "            tokens_used=result[\"tokens_used\"],\n",
    "            generation_time=result[\"generation_time\"],\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Chat completion failed\", error=str(e))\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "# Model management endpoints\n",
    "@app.post(\"/models/load\")\n",
    "async def load_model(model_id: str):\n",
    "    \"\"\"Load a specific model\"\"\"\n",
    "    logger.info(\"Model load request\", model_id=model_id)\n",
    "\n",
    "    success = await llm_service.load_model(model_id, force_reload=True)\n",
    "    if not success:\n",
    "        raise HTTPException(status_code=500, detail=f\"Failed to load model: {model_id}\")\n",
    "\n",
    "    return {\"status\": \"success\", \"model_id\": model_id}\n",
    "\n",
    "\n",
    "@app.get(\"/models/current\")\n",
    "async def get_current_model():\n",
    "    \"\"\"Get current model information\"\"\"\n",
    "    model_info = llm_service.get_model_info()\n",
    "    return model_info\n",
    "\n",
    "\n",
    "# Simple RAG endpoint (placeholder for integration with existing RAG service)\n",
    "@app.post(\"/retrieve\", response_model=RetrieveResponse)\n",
    "async def retrieve_documents(request: RetrieveRequest):\n",
    "    \"\"\"Document retrieval endpoint (integrate with existing RAG service)\"\"\"\n",
    "    # This would integrate with the RAG service from nb26\n",
    "    # For now, return a placeholder response\n",
    "\n",
    "    return RetrieveResponse(\n",
    "        results=[], query=request.query, total_found=0, search_time=0.0\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(\n",
    "        \"app.main:app\",\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        reload=True,\n",
    "        log_config=None,  # Use our structured logging\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Docker 配置文件\n",
    "\n",
    "```dockerfile\n",
    "# Dockerfile\n",
    "FROM nvidia/cuda:11.8-devel-ubuntu22.04\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "ENV AI_CACHE_ROOT=/app/cache\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3.10 \\\n",
    "    python3-pip \\\n",
    "    python3-venv \\\n",
    "    git \\\n",
    "    curl \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Create app directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Create cache directory\n",
    "RUN mkdir -p /app/cache\n",
    "\n",
    "# Copy requirements first for better caching\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip3 install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Create non-root user\n",
    "RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app\n",
    "USER appuser\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Run the application\n",
    "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"1\"]\n",
    "```\n",
    "\n",
    "```yaml\n",
    "# docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  llm-api:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - AI_CACHE_ROOT=/app/cache\n",
    "      - DEFAULT_MODEL=Qwen/Qwen2.5-7B-Instruct\n",
    "      - LOG_LEVEL=INFO\n",
    "    volumes:\n",
    "      # Mount cache directory to persist models\n",
    "      - ./cache:/app/cache\n",
    "      # Mount for development (optional)\n",
    "      - .:/app\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 60s\n",
    "    restart: unless-stopped\n",
    "\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "    command:\n",
    "      - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "      - '--storage.tsdb.path=/prometheus'\n",
    "      - '--web.console.libraries=/etc/prometheus/console_libraries'\n",
    "      - '--web.console.templates=/etc/prometheus/consoles'\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "    volumes:\n",
    "      - grafana-storage:/var/lib/grafana\n",
    "      - ./monitoring/dashboards:/etc/grafana/provisioning/dashboards\n",
    "      - ./monitoring/datasources:/etc/grafana/provisioning/datasources\n",
    "\n",
    "volumes:\n",
    "  grafana-storage:\n",
    "```\n",
    "\n",
    "```txt\n",
    "# requirements.txt\n",
    "fastapi[all]==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "transformers==4.36.0\n",
    "torch==2.1.0\n",
    "accelerate==0.24.0\n",
    "bitsandbytes==0.41.0\n",
    "pydantic==2.5.0\n",
    "pydantic-settings==2.1.0\n",
    "python-multipart==0.0.6\n",
    "python-dotenv==1.0.0\n",
    "structlog==23.2.0\n",
    "prometheus-client==0.19.0\n",
    "psutil==5.9.6\n",
    "httpx==0.25.0\n",
    "pytest-asyncio==0.21.0\n",
    "```\n",
    "\n",
    "## 5. 監控配置\n",
    "\n",
    "```yaml\n",
    "# monitoring/prometheus.yml\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "  evaluation_interval: 15s\n",
    "\n",
    "rule_files:\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: 'llm-api'\n",
    "    static_configs:\n",
    "      - targets: ['llm-api:8000']\n",
    "    metrics_path: '/metrics'\n",
    "    scrape_interval: 5s\n",
    "```\n",
    "\n",
    "```bash\n",
    "# deployment/deploy.sh\n",
    "#!/bin/bash\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"🚀 Deploying LLM API Service...\"\n",
    "\n",
    "# Check if Docker is running\n",
    "if ! docker info > /dev/null 2>&1; then\n",
    "    echo \"❌ Docker is not running. Please start Docker first.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Check for NVIDIA Docker support (if using GPU)\n",
    "if command -v nvidia-docker &> /dev/null; then\n",
    "    echo \"✅ NVIDIA Docker support detected\"\n",
    "else\n",
    "    echo \"⚠️  NVIDIA Docker not found. GPU acceleration will not be available.\"\n",
    "fi\n",
    "\n",
    "# Create necessary directories\n",
    "mkdir -p cache monitoring/dashboards monitoring/datasources\n",
    "\n",
    "# Copy environment file if it doesn't exist\n",
    "if [ ! -f .env ]; then\n",
    "    cp .env.example .env\n",
    "    echo \"📝 Created .env file. Please update with your settings.\"\n",
    "fi\n",
    "\n",
    "# Build and start services\n",
    "echo \"🔨 Building Docker images...\"\n",
    "docker-compose build\n",
    "\n",
    "echo \"🚀 Starting services...\"\n",
    "docker-compose up -d\n",
    "\n",
    "# Wait for health check\n",
    "echo \"⏳ Waiting for services to be healthy...\"\n",
    "sleep 30\n",
    "\n",
    "# Check service health\n",
    "if curl -f http://localhost:8000/health > /dev/null 2>&1; then\n",
    "    echo \"✅ LLM API service is healthy!\"\n",
    "    echo \"🌐 API available at: http://localhost:8000\"\n",
    "    echo \"📊 Prometheus available at: http://localhost:9090\"\n",
    "    echo \"📈 Grafana available at: http://localhost:3000 (admin/admin)\"\n",
    "else\n",
    "    echo \"❌ Service health check failed. Check logs with: docker-compose logs\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"🎉 Deployment completed successfully!\"\n",
    "```\n",
    "\n",
    "## 6. 測試套件"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
