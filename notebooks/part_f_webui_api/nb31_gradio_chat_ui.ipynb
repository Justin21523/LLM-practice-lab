{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18176f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb31_gradio_chat_ui.ipynb\n",
    "# Gradio 聊天介面 - 整合 LLM + RAG + Function Calling\n",
    "\n",
    "# === Cell 1: Shared Cache Bootstrap ===\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "for k, v in paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache] Root:\", AI_CACHE_ROOT)\n",
    "print(\n",
    "    \"[GPU]\",\n",
    "    torch.cuda.is_available(),\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Dependencies Installation ===\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for Gradio chat UI\"\"\"\n",
    "    packages = [\n",
    "        \"gradio>=4.0.0\",\n",
    "        \"transformers>=4.35.0\",\n",
    "        \"accelerate\",\n",
    "        \"bitsandbytes\",\n",
    "        \"sentence-transformers\",\n",
    "        \"faiss-cpu\",\n",
    "        \"PyPDF2\",\n",
    "        \"langchain-community\",\n",
    "        \"opencc-python-reimplemented\",\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"✅ Installed: {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"❌ Failed to install: {package}\")\n",
    "\n",
    "\n",
    "# Uncomment to install packages\n",
    "# install_packages()\n",
    "\n",
    "import gradio as gr\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e95c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: LLM Adapter (Multi-Backend Support) ===\n",
    "class LLMAdapter:\n",
    "    \"\"\"Unified LLM interface supporting multiple backends\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backend: str = \"transformers\",\n",
    "        model_id: str = \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.backend = backend\n",
    "        self.model_id = model_id\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._load_model(**kwargs)\n",
    "\n",
    "    def _load_model(self, **kwargs):\n",
    "        \"\"\"Load model based on backend type\"\"\"\n",
    "        if self.backend == \"transformers\":\n",
    "            self._load_transformers_model(**kwargs)\n",
    "        elif self.backend == \"ollama\":\n",
    "            self._load_ollama_model(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backend: {self.backend}\")\n",
    "\n",
    "    def _load_transformers_model(self, **kwargs):\n",
    "        \"\"\"Load model using transformers library\"\"\"\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        import torch\n",
    "\n",
    "        # Default settings for low VRAM\n",
    "        default_kwargs = {\n",
    "            \"device_map\": \"auto\",\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"load_in_4bit\": True if torch.cuda.is_available() else False,\n",
    "            \"trust_remote_code\": True,\n",
    "        }\n",
    "        default_kwargs.update(kwargs)\n",
    "\n",
    "        try:\n",
    "            print(f\"Loading {self.model_id} with transformers...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_id,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=os.environ.get(\"TRANSFORMERS_CACHE\"),\n",
    "            )\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_id,\n",
    "                cache_dir=os.environ.get(\"TRANSFORMERS_CACHE\"),\n",
    "                **default_kwargs,\n",
    "            )\n",
    "\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            print(f\"✅ Model loaded: {self.model_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load model: {e}\")\n",
    "            # Fallback to CPU\n",
    "            if \"cuda\" in str(e).lower():\n",
    "                print(\"🔄 Falling back to CPU...\")\n",
    "                default_kwargs.update({\"device_map\": \"cpu\", \"load_in_4bit\": False})\n",
    "                self._load_transformers_model(**default_kwargs)\n",
    "\n",
    "    def _load_ollama_model(self, **kwargs):\n",
    "        \"\"\"Load model using Ollama (placeholder)\"\"\"\n",
    "        print(f\"📝 Ollama backend: {self.model_id} (requires ollama server)\")\n",
    "        # Implementation would connect to local ollama server\n",
    "\n",
    "    def generate(\n",
    "        self, messages: List[Dict[str, str]], max_tokens: int = 512, **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"Generate response from messages\"\"\"\n",
    "        if self.backend == \"transformers\":\n",
    "            return self._generate_transformers(messages, max_tokens, **kwargs)\n",
    "        elif self.backend == \"ollama\":\n",
    "            return self._generate_ollama(messages, max_tokens, **kwargs)\n",
    "        else:\n",
    "            return \"Error: Unsupported backend\"\n",
    "\n",
    "    def _generate_transformers(\n",
    "        self, messages: List[Dict[str, str]], max_tokens: int, **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"Generate using transformers\"\"\"\n",
    "        try:\n",
    "            # Format messages for chat template\n",
    "            if hasattr(self.tokenizer, \"apply_chat_template\"):\n",
    "                prompt = self.tokenizer.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "            else:\n",
    "                # Fallback formatting\n",
    "                prompt = \"\"\n",
    "                for msg in messages:\n",
    "                    if msg[\"role\"] == \"system\":\n",
    "                        prompt += f\"System: {msg['content']}\\n\"\n",
    "                    elif msg[\"role\"] == \"user\":\n",
    "                        prompt += f\"User: {msg['content']}\\n\"\n",
    "                    elif msg[\"role\"] == \"assistant\":\n",
    "                        prompt += f\"Assistant: {msg['content']}\\n\"\n",
    "                prompt += \"Assistant: \"\n",
    "\n",
    "            # Tokenize and generate\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, return_tensors=\"pt\", truncation=True, max_length=2048\n",
    "            )\n",
    "\n",
    "            if torch.cuda.is_available() and hasattr(self.model, \"device\"):\n",
    "                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "            # Decode response\n",
    "            response = self.tokenizer.decode(\n",
    "                outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            return response.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Generation error: {str(e)}\"\n",
    "\n",
    "    def _generate_ollama(\n",
    "        self, messages: List[Dict[str, str]], max_tokens: int, **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"Generate using Ollama (placeholder)\"\"\"\n",
    "        return f\"Ollama response for {len(messages)} messages (placeholder)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1dd345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: RAG System Core ===\n",
    "class RAGSystem:\n",
    "    \"\"\"Retrieval-Augmented Generation system with document processing\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str = \"BAAI/bge-m3\", chunk_size: int = 512):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.embedder = None\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "        self.chunks = []\n",
    "        self._load_embedder()\n",
    "\n",
    "    def _load_embedder(self):\n",
    "        \"\"\"Load sentence transformer for embeddings\"\"\"\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "\n",
    "            self.embedder = SentenceTransformer(\n",
    "                self.embedding_model, cache_folder=os.environ.get(\"TRANSFORMERS_CACHE\")\n",
    "            )\n",
    "            print(f\"✅ Loaded embedder: {self.embedding_model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load embedder: {e}\")\n",
    "\n",
    "    def add_documents(self, files) -> str:\n",
    "        \"\"\"Add uploaded documents to knowledge base\"\"\"\n",
    "        if not files:\n",
    "            return \"No files uploaded\"\n",
    "\n",
    "        added_docs = []\n",
    "        for file in files:\n",
    "            try:\n",
    "                content = self._extract_text(file)\n",
    "                if content:\n",
    "                    self.documents.append(\n",
    "                        {\n",
    "                            \"filename\": file.name,\n",
    "                            \"content\": content,\n",
    "                            \"timestamp\": datetime.now().isoformat(),\n",
    "                        }\n",
    "                    )\n",
    "                    added_docs.append(file.name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file.name}: {e}\")\n",
    "\n",
    "        if added_docs:\n",
    "            self._build_index()\n",
    "            return f\"✅ Added {len(added_docs)} documents: {', '.join(added_docs)}\"\n",
    "        else:\n",
    "            return \"❌ No documents could be processed\"\n",
    "\n",
    "    def _extract_text(self, file) -> str:\n",
    "        \"\"\"Extract text from uploaded file\"\"\"\n",
    "        if file.name.endswith(\".pdf\"):\n",
    "            return self._extract_pdf_text(file)\n",
    "        elif file.name.endswith((\".txt\", \".md\")):\n",
    "            return file.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _extract_pdf_text(self, file) -> str:\n",
    "        \"\"\"Extract text from PDF file\"\"\"\n",
    "        try:\n",
    "            import PyPDF2\n",
    "\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"PDF extraction error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks\"\"\"\n",
    "        # Simple chunking - could use more sophisticated methods\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for word in words:\n",
    "            if current_length + len(word) > self.chunk_size and current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [word]\n",
    "                current_length = len(word)\n",
    "            else:\n",
    "                current_chunk.append(word)\n",
    "                current_length += len(word) + 1  # +1 for space\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"Build FAISS index from documents\"\"\"\n",
    "        if not self.embedder or not self.documents:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import faiss\n",
    "            import numpy as np\n",
    "\n",
    "            # Create chunks from all documents\n",
    "            all_chunks = []\n",
    "            for doc in self.documents:\n",
    "                chunks = self._chunk_text(doc[\"content\"])\n",
    "                for chunk in chunks:\n",
    "                    all_chunks.append(\n",
    "                        {\n",
    "                            \"text\": chunk,\n",
    "                            \"source\": doc[\"filename\"],\n",
    "                            \"timestamp\": doc[\"timestamp\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            self.chunks = all_chunks\n",
    "\n",
    "            # Generate embeddings\n",
    "            texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "            embeddings = self.embedder.encode(texts, show_progress_bar=True)\n",
    "\n",
    "            # Build FAISS index\n",
    "            dimension = embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatIP(dimension)  # Inner product for similarity\n",
    "\n",
    "            # Normalize embeddings for cosine similarity\n",
    "            faiss.normalize_L2(embeddings.astype(np.float32))\n",
    "            self.index.add(embeddings.astype(np.float32))\n",
    "\n",
    "            print(f\"✅ Built index with {len(all_chunks)} chunks\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Index building error: {e}\")\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant chunks for query\"\"\"\n",
    "        if not self.embedder or not self.index or not self.chunks:\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            import faiss\n",
    "            import numpy as np\n",
    "\n",
    "            # Encode query\n",
    "            query_embedding = self.embedder.encode([query])\n",
    "            faiss.normalize_L2(query_embedding.astype(np.float32))\n",
    "\n",
    "            # Search\n",
    "            scores, indices = self.index.search(\n",
    "                query_embedding.astype(np.float32), top_k\n",
    "            )\n",
    "\n",
    "            # Format results\n",
    "            results = []\n",
    "            for score, idx in zip(scores[0], indices[0]):\n",
    "                if idx < len(self.chunks):\n",
    "                    chunk = self.chunks[idx]\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"text\": chunk[\"text\"],\n",
    "                            \"source\": chunk[\"source\"],\n",
    "                            \"score\": float(score),\n",
    "                            \"timestamp\": chunk[\"timestamp\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Retrieval error: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93619e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Function Calling Tools ===\n",
    "class ToolManager:\n",
    "    \"\"\"Manage function calling tools\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tools = {\n",
    "            \"calculator\": self.calculator,\n",
    "            \"datetime\": self.get_datetime,\n",
    "            \"web_search\": self.web_search_placeholder,\n",
    "        }\n",
    "\n",
    "    def calculator(self, expression: str) -> str:\n",
    "        \"\"\"Simple calculator tool\"\"\"\n",
    "        try:\n",
    "            # Safe evaluation - only allow basic math\n",
    "            allowed_chars = set(\"0123456789+-*/.()% \")\n",
    "            if not all(c in allowed_chars for c in expression):\n",
    "                return \"Error: Invalid characters in expression\"\n",
    "\n",
    "            result = eval(expression)\n",
    "            return f\"Result: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Calculation error: {str(e)}\"\n",
    "\n",
    "    def get_datetime(self, format_type: str = \"default\") -> str:\n",
    "        \"\"\"Get current date and time\"\"\"\n",
    "        now = datetime.now()\n",
    "        if format_type == \"date\":\n",
    "            return now.strftime(\"%Y-%m-%d\")\n",
    "        elif format_type == \"time\":\n",
    "            return now.strftime(\"%H:%M:%S\")\n",
    "        else:\n",
    "            return now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def web_search_placeholder(self, query: str) -> str:\n",
    "        \"\"\"Placeholder for web search (could integrate DuckDuckGo)\"\"\"\n",
    "        return f\"Web search placeholder for: {query}\"\n",
    "\n",
    "    def parse_tool_call(self, text: str) -> Tuple[str, Optional[Dict[str, Any]]]:\n",
    "        \"\"\"Parse tool calls from LLM response\"\"\"\n",
    "        # Simple pattern matching for tool calls\n",
    "        # Format: [TOOL:tool_name(arg1=value1, arg2=value2)]\n",
    "        import re\n",
    "\n",
    "        pattern = r\"\\[TOOL:(\\w+)\\(([^)]*)\\)\\]\"\n",
    "        match = re.search(pattern, text)\n",
    "\n",
    "        if match:\n",
    "            tool_name = match.group(1)\n",
    "            args_str = match.group(2)\n",
    "\n",
    "            # Parse arguments\n",
    "            args = {}\n",
    "            if args_str:\n",
    "                for arg_pair in args_str.split(\",\"):\n",
    "                    if \"=\" in arg_pair:\n",
    "                        key, value = arg_pair.split(\"=\", 1)\n",
    "                        args[key.strip()] = value.strip().strip(\"\\\"'\")\n",
    "\n",
    "            return tool_name, args\n",
    "\n",
    "        return \"\", None\n",
    "\n",
    "    def execute_tool(self, tool_name: str, args: Dict[str, Any]) -> str:\n",
    "        \"\"\"Execute a tool with given arguments\"\"\"\n",
    "        if tool_name not in self.tools:\n",
    "            return f\"Unknown tool: {tool_name}\"\n",
    "\n",
    "        try:\n",
    "            tool_func = self.tools[tool_name]\n",
    "            return tool_func(**args)\n",
    "        except Exception as e:\n",
    "            return f\"Tool execution error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a681a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Conversation Memory Manager ===\n",
    "class ConversationMemory:\n",
    "    \"\"\"Manage conversation history and context\"\"\"\n",
    "\n",
    "    def __init__(self, max_history: int = 10):\n",
    "        self.max_history = max_history\n",
    "        self.conversations = {}  # user_id -> conversation history\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def add_message(self, user_id: str, role: str, content: str):\n",
    "        \"\"\"Add message to conversation history\"\"\"\n",
    "        with self.lock:\n",
    "            if user_id not in self.conversations:\n",
    "                self.conversations[user_id] = []\n",
    "\n",
    "            self.conversations[user_id].append(\n",
    "                {\n",
    "                    \"role\": role,\n",
    "                    \"content\": content,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Keep only recent messages\n",
    "            if (\n",
    "                len(self.conversations[user_id]) > self.max_history * 2\n",
    "            ):  # *2 for user+assistant pairs\n",
    "                self.conversations[user_id] = self.conversations[user_id][\n",
    "                    -self.max_history * 2 :\n",
    "                ]\n",
    "\n",
    "    def get_conversation(self, user_id: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get conversation history for LLM context\"\"\"\n",
    "        with self.lock:\n",
    "            if user_id not in self.conversations:\n",
    "                return []\n",
    "\n",
    "            # Return only role and content for LLM\n",
    "            return [\n",
    "                {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n",
    "                for msg in self.conversations[user_id]\n",
    "            ]\n",
    "\n",
    "    def clear_conversation(self, user_id: str):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        with self.lock:\n",
    "            if user_id in self.conversations:\n",
    "                del self.conversations[user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b030f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Gradio Chat UI Core Logic ===\n",
    "class ChatbotCore:\n",
    "    \"\"\"Core chatbot logic integrating LLM, RAG, and tools\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.rag = RAGSystem()\n",
    "        self.tools = ToolManager()\n",
    "        self.memory = ConversationMemory()\n",
    "        self.current_model = None\n",
    "        self.system_prompt = \"\"\"You are a helpful AI assistant. You can use tools by calling them in the format [TOOL:tool_name(arg1=value1, arg2=value2)].\n",
    "\n",
    "Available tools:\n",
    "- calculator(expression=\"math expression\") - for calculations\n",
    "- datetime(format_type=\"default\"|\"date\"|\"time\") - get current date/time\n",
    "- web_search(query=\"search terms\") - search the web\n",
    "\n",
    "You can also answer questions based on uploaded documents if available.\"\"\"\n",
    "\n",
    "    def load_model(self, model_name: str, backend: str = \"transformers\") -> str:\n",
    "        \"\"\"Load or switch LLM model\"\"\"\n",
    "        try:\n",
    "            if self.current_model == f\"{backend}:{model_name}\":\n",
    "                return f\"✅ Model already loaded: {model_name}\"\n",
    "\n",
    "            self.llm = LLMAdapter(backend=backend, model_id=model_name)\n",
    "            self.current_model = f\"{backend}:{model_name}\"\n",
    "            return f\"✅ Loaded model: {model_name} ({backend})\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"❌ Failed to load model: {str(e)}\"\n",
    "\n",
    "    def process_message(\n",
    "        self, message: str, user_id: str = \"default\", use_rag: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"Process user message and return response\"\"\"\n",
    "        if not self.llm:\n",
    "            return \"❌ No model loaded. Please select a model first.\"\n",
    "\n",
    "        try:\n",
    "            # Build conversation context\n",
    "            messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "\n",
    "            # Add conversation history\n",
    "            history = self.memory.get_conversation(user_id)\n",
    "            messages.extend(history)\n",
    "\n",
    "            # Add RAG context if enabled and available\n",
    "            if use_rag and self.rag.chunks:\n",
    "                relevant_docs = self.rag.retrieve(message, top_k=3)\n",
    "                if relevant_docs:\n",
    "                    rag_context = \"\\n\\nRelevant information from documents:\\n\"\n",
    "                    for doc in relevant_docs:\n",
    "                        rag_context += (\n",
    "                            f\"- {doc['text'][:200]}... (from {doc['source']})\\n\"\n",
    "                        )\n",
    "\n",
    "                    message_with_rag = message + rag_context\n",
    "                else:\n",
    "                    message_with_rag = message\n",
    "            else:\n",
    "                message_with_rag = message\n",
    "\n",
    "            # Add current user message\n",
    "            messages.append({\"role\": \"user\", \"content\": message_with_rag})\n",
    "\n",
    "            # Generate response\n",
    "            response = self.llm.generate(messages, max_tokens=512)\n",
    "\n",
    "            # Check for tool calls\n",
    "            tool_name, tool_args = self.tools.parse_tool_call(response)\n",
    "            if tool_name and tool_args is not None:\n",
    "                tool_result = self.tools.execute_tool(tool_name, tool_args)\n",
    "                response += f\"\\n\\nTool result: {tool_result}\"\n",
    "\n",
    "            # Save conversation\n",
    "            self.memory.add_message(user_id, \"user\", message)\n",
    "            self.memory.add_message(user_id, \"assistant\", response)\n",
    "\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"❌ Error processing message: {str(e)}\"\n",
    "\n",
    "    def upload_documents(self, files) -> str:\n",
    "        \"\"\"Handle document upload\"\"\"\n",
    "        return self.rag.add_documents(files)\n",
    "\n",
    "    def clear_conversation(self, user_id: str = \"default\") -> str:\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.memory.clear_conversation(user_id)\n",
    "        return \"✅ Conversation cleared\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1081af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Gradio WebUI Setup and Launch ===\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create and return Gradio interface\"\"\"\n",
    "\n",
    "    # Initialize chatbot core\n",
    "    chatbot = ChatbotCore()\n",
    "\n",
    "    # Model options\n",
    "    model_options = [\n",
    "        \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"microsoft/DialoGPT-medium\",\n",
    "        \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    ]\n",
    "\n",
    "    def load_model_wrapper(model_name, backend):\n",
    "        return chatbot.load_model(model_name, backend)\n",
    "\n",
    "    def chat_response(message, history, use_rag):\n",
    "        if not message.strip():\n",
    "            return history, \"\"\n",
    "\n",
    "        # Generate response\n",
    "        response = chatbot.process_message(message, use_rag=use_rag)\n",
    "\n",
    "        # Update history\n",
    "        history.append([message, response])\n",
    "        return history, \"\"\n",
    "\n",
    "    def upload_files_wrapper(files):\n",
    "        return chatbot.upload_documents(files)\n",
    "\n",
    "    def clear_chat_wrapper():\n",
    "        result = chatbot.clear_conversation()\n",
    "        return [], result\n",
    "\n",
    "    # Create Gradio interface\n",
    "    with gr.Blocks(title=\"LLM Chat Assistant\", theme=gr.themes.Soft()) as iface:\n",
    "\n",
    "        gr.Markdown(\"# 🤖 LLM Chat Assistant\")\n",
    "        gr.Markdown(\"Multi-model chatbot with RAG and function calling capabilities\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                # Model selection\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\"### Model Configuration\")\n",
    "                    model_dropdown = gr.Dropdown(\n",
    "                        choices=model_options,\n",
    "                        value=model_options[0],\n",
    "                        label=\"Select Model\",\n",
    "                        interactive=True,\n",
    "                    )\n",
    "                    backend_radio = gr.Radio(\n",
    "                        choices=[\"transformers\", \"ollama\"],\n",
    "                        value=\"transformers\",\n",
    "                        label=\"Backend\",\n",
    "                    )\n",
    "                    load_btn = gr.Button(\"Load Model\", variant=\"primary\")\n",
    "                    model_status = gr.Textbox(\n",
    "                        label=\"Model Status\", value=\"No model loaded\", interactive=False\n",
    "                    )\n",
    "\n",
    "                # Document upload\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\"### Knowledge Base\")\n",
    "                    file_upload = gr.Files(\n",
    "                        label=\"Upload Documents (PDF, TXT, MD)\",\n",
    "                        file_types=[\".pdf\", \".txt\", \".md\"],\n",
    "                    )\n",
    "                    upload_btn = gr.Button(\"Add to Knowledge Base\")\n",
    "                    upload_status = gr.Textbox(label=\"Upload Status\", interactive=False)\n",
    "\n",
    "                # Settings\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\"### Settings\")\n",
    "                    use_rag_checkbox = gr.Checkbox(\n",
    "                        label=\"Enable RAG (use uploaded documents)\", value=True\n",
    "                    )\n",
    "                    clear_btn = gr.Button(\"Clear Conversation\", variant=\"secondary\")\n",
    "                    clear_status = gr.Textbox(label=\"Action Status\", interactive=False)\n",
    "\n",
    "            with gr.Column(scale=3):\n",
    "                # Chat interface\n",
    "                gr.Markdown(\"### Chat\")\n",
    "                chatbot_display = gr.Chatbot(\n",
    "                    label=\"Conversation\", height=500, show_copy_button=True\n",
    "                )\n",
    "\n",
    "                with gr.Row():\n",
    "                    message_input = gr.Textbox(\n",
    "                        label=\"Message\",\n",
    "                        placeholder=\"Type your message here...\",\n",
    "                        lines=2,\n",
    "                        scale=4,\n",
    "                    )\n",
    "                    send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "\n",
    "        # Examples\n",
    "        gr.Markdown(\"### 💡 Example Messages\")\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                [\"Hello! How can you help me?\"],\n",
    "                [\"Calculate 15 * 23 + 45\"],\n",
    "                [\"What's the current date and time?\"],\n",
    "                ['[TOOL:calculator(expression=\"2+2*3\")]'],\n",
    "                [\"Summarize the uploaded documents\"],\n",
    "            ],\n",
    "            inputs=message_input,\n",
    "        )\n",
    "\n",
    "        # Event handlers\n",
    "        load_btn.click(\n",
    "            load_model_wrapper,\n",
    "            inputs=[model_dropdown, backend_radio],\n",
    "            outputs=model_status,\n",
    "        )\n",
    "\n",
    "        upload_btn.click(\n",
    "            upload_files_wrapper, inputs=file_upload, outputs=upload_status\n",
    "        )\n",
    "\n",
    "        send_btn.click(\n",
    "            chat_response,\n",
    "            inputs=[message_input, chatbot_display, use_rag_checkbox],\n",
    "            outputs=[chatbot_display, message_input],\n",
    "        )\n",
    "\n",
    "        message_input.submit(\n",
    "            chat_response,\n",
    "            inputs=[message_input, chatbot_display, use_rag_checkbox],\n",
    "            outputs=[chatbot_display, message_input],\n",
    "        )\n",
    "\n",
    "        clear_btn.click(clear_chat_wrapper, outputs=[chatbot_display, clear_status])\n",
    "\n",
    "    return iface\n",
    "\n",
    "\n",
    "# Launch interface\n",
    "if __name__ == \"__main__\":\n",
    "    interface = create_gradio_interface()\n",
    "\n",
    "    # Launch with public link for sharing (optional)\n",
    "    interface.launch(\n",
    "        server_name=\"0.0.0.0\",  # Allow external connections\n",
    "        server_port=7860,  # Default Gradio port\n",
    "        share=False,  # Set to True for public ngrok link\n",
    "        debug=True,\n",
    "        show_error=True,\n",
    "    )\n",
    "\n",
    "print(\"🚀 Gradio interface ready! Run the cell above to launch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae22558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9: Smoke Test ===\n",
    "def test_gradio_components():\n",
    "    \"\"\"Quick test of core components\"\"\"\n",
    "    print(\"🧪 Testing Gradio Chat UI Components...\")\n",
    "\n",
    "    # Test LLM Adapter\n",
    "    try:\n",
    "        adapter = LLMAdapter(\n",
    "            backend=\"transformers\", model_id=\"microsoft/DialoGPT-medium\"\n",
    "        )\n",
    "        test_response = adapter.generate(\n",
    "            [{\"role\": \"user\", \"content\": \"Hello\"}], max_tokens=10\n",
    "        )\n",
    "        print(f\"✅ LLM Adapter: {test_response[:50]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ LLM Adapter: {e}\")\n",
    "\n",
    "    # Test RAG System\n",
    "    try:\n",
    "        rag = RAGSystem()\n",
    "        print(f\"✅ RAG System initialized with embedder: {rag.embedding_model}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ RAG System: {e}\")\n",
    "\n",
    "    # Test Tool Manager\n",
    "    try:\n",
    "        tools = ToolManager()\n",
    "        calc_result = tools.calculator(\"2+2\")\n",
    "        time_result = tools.get_datetime(\"time\")\n",
    "        print(f\"✅ Tools: Calculator={calc_result}, Time={time_result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Tool Manager: {e}\")\n",
    "\n",
    "    # Test Memory\n",
    "    try:\n",
    "        memory = ConversationMemory()\n",
    "        memory.add_message(\"test\", \"user\", \"Hello\")\n",
    "        memory.add_message(\"test\", \"assistant\", \"Hi there!\")\n",
    "        history = memory.get_conversation(\"test\")\n",
    "        print(f\"✅ Memory: {len(history)} messages stored\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Memory: {e}\")\n",
    "\n",
    "    print(\"🏁 Component tests completed!\")\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "test_gradio_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd597db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 10: Usage Guide and Deployment Notes ===\n",
    "print(\n",
    "    \"\"\"\n",
    "📋 使用指南 (Usage Guide)\n",
    "===========================\n",
    "\n",
    "🔧 設定步驟 (Setup Steps):\n",
    "1. 執行環境初始化 Cell (共享快取設定)\n",
    "2. 安裝依賴套件 (取消註解 install_packages() 並執行)\n",
    "3. 啟動 Gradio 介面 (執行 Cell 8)\n",
    "4. 在瀏覽器中開啟顯示的 URL (通常是 http://localhost:7860)\n",
    "\n",
    "💬 功能特色 (Features):\n",
    "✅ 多模型支援 - 支援 Qwen, DialoGPT, Zephyr 等\n",
    "✅ RAG 文件問答 - 上傳 PDF/TXT/MD 檔案進行知識庫問答\n",
    "✅ 工具調用 - 計算機、日期時間、網路搜尋 (占位符)\n",
    "✅ 會話記憶 - 保持多輪對話上下文\n",
    "✅ 低資源友善 - 支援 4bit 量化、CPU fallback\n",
    "\n",
    "🛠️ 工具使用方式 (Tool Usage):\n",
    "- 計算: \"Calculate 15 * 23\" 或 \"[TOOL:calculator(expression=\\\"15*23\\\")]\"\n",
    "- 日期: \"What's the date?\" 或 \"[TOOL:datetime(format_type=\\\"date\\\")]\"\n",
    "- 時間: \"Current time?\" 或 \"[TOOL:datetime(format_type=\\\"time\\\")]\"\n",
    "\n",
    "📁 RAG 文件上傳 (Document Upload):\n",
    "1. 點擊 \"Upload Documents\" 區域\n",
    "2. 選擇 PDF、TXT 或 MD 檔案\n",
    "3. 點擊 \"Add to Knowledge Base\"\n",
    "4. 開啟 \"Enable RAG\" 選項\n",
    "5. 詢問文件相關問題\n",
    "\n",
    "⚙️ 模型配置建議 (Model Configuration):\n",
    "- 8GB VRAM: Qwen2.5-1.5B-Instruct (4bit)\n",
    "- 12GB VRAM: Qwen2.5-7B-Instruct (4bit)\n",
    "- 16GB+ VRAM: Qwen2.5-7B-Instruct (fp16)\n",
    "- CPU only: DialoGPT-medium\n",
    "\n",
    "🚨 常見問題排解 (Troubleshooting):\n",
    "- CUDA OOM → 改用更小模型或開啟 4bit 量化\n",
    "- 載入失敗 → 檢查網路連線與 HuggingFace 可用性\n",
    "- RAG 無回應 → 確認文件已成功上傳並建立索引\n",
    "- 工具不執行 → 檢查工具調用格式是否正確\n",
    "\n",
    "🌐 部署選項 (Deployment Options):\n",
    "1. 本地部署: 直接執行 notebook，訪問 localhost:7860\n",
    "2. 區網分享: 設定 server_name=\"0.0.0.0\" 開放區網訪問\n",
    "3. 公開分享: 設定 share=True 生成 ngrok 公開連結 (開發測試用)\n",
    "4. 容器化: 可包裝為 Docker 容器進行部署\n",
    "\n",
    "🔒 安全建議 (Security Recommendations):\n",
    "- 生產環境請勿開啟 share=True\n",
    "- 設定適當的網路防火牆規則\n",
    "- 考慮加入身份驗證機制\n",
    "- 定期更新依賴套件版本\n",
    "\n",
    "📈 效能優化 (Performance Optimization):\n",
    "- 使用 GGUF 量化模型 (llama.cpp backend)\n",
    "- 啟用 gradient checkpointing\n",
    "- 調整 chunk_size 與 max_history 參數\n",
    "- 考慮使用 Redis 做分散式會話儲存\n",
    "\n",
    "🔄 擴展建議 (Extension Ideas):\n",
    "- 整合更多工具 (天氣、股價、新聞)\n",
    "- 加入語音輸入/輸出 (Whisper/TTS)\n",
    "- 支援圖片上傳與多模態對話\n",
    "- 加入對話導出/匯入功能\n",
    "- 實作使用者角色與權限管理\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29519f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 最小驗收測試 (5行內) ===\n",
    "interface = create_gradio_interface()\n",
    "chatbot = ChatbotCore()\n",
    "status = chatbot.load_model(\"microsoft/DialoGPT-medium\")\n",
    "response = chatbot.process_message(\"Hello, test message\", use_rag=False)\n",
    "print(\n",
    "    f\"✅ Interface created, model loaded: {status[:20]}..., response: {response[:30]}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a9201",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 本章小結\n",
    "\n",
    "### 🎯 完成項目 (Completed Items)\n",
    "\n",
    "* **整合式聊天介面** - 建立支援多模型切換的 Gradio WebUI\n",
    "* **RAG 文件問答系統** - PDF/TXT/MD 文件上傳與 FAISS 檢索整合\n",
    "* **工具調用功能** - 計算機、日期時間、網路搜尋工具整合\n",
    "* **會話記憶管理** - 多用戶對話歷史保存與上下文維護\n",
    "* **低資源優化** - 4bit 量化、CPU fallback、記憶體優化策略\n",
    "* **統一模型介面** - LLMAdapter 支援 transformers/ollama 多後端\n",
    "* **使用者友善設計** - 響應式介面、範例提示、狀態顯示\n",
    "\n",
    "### 🧠 核心原理要點 (Core Concepts)\n",
    "\n",
    "* **模組化架構設計 (Modular Architecture)** - 各組件獨立可測試、可重用\n",
    "* **統一抽象介面 (Unified Abstraction)** - LLMAdapter 隔離不同後端實作細節\n",
    "* **記憶體高效管理 (Memory Efficiency)** - 4bit 量化、gradient checkpointing、動態卸載\n",
    "* **RAG 檢索增強 (Retrieval-Augmented Generation)** - 文件向量化、相似度搜尋、上下文注入\n",
    "* **工具調用協議 (Function Calling Protocol)** - 結構化工具調用與結果處理\n",
    "* **會話狀態管理 (Session State Management)** - 多用戶隔離、歷史截斷、記憶體控制\n",
    "\n",
    "### ⚠️ 常見陷阱 (Common Pitfalls)\n",
    "\n",
    "* **記憶體溢出** - 大模型 + 長對話歷史 → 使用 4bit 量化與歷史截斷\n",
    "* **工具調用格式錯誤** - LLM 輸出格式不一致 → 改進解析邏輯與提示工程\n",
    "* **RAG 檢索品質** - 分塊策略影響檢索效果 → 調整 chunk_size 與重排器\n",
    "* **並發安全問題** - 多用戶同時訪問 → 使用 threading.Lock 保護共享狀態\n",
    "* **模型載入失敗** - 網路或硬體限制 → 提供多層次 fallback 機制\n",
    "\n",
    "### 🚀 下一步建議 (Next Steps)\n",
    "\n",
    "**立即可做:**\n",
    "* 整合更多開源工具（天氣 API、新聞抓取、檔案操作）\n",
    "* 加入語音輸入/輸出功能（Whisper ASR + TTS）\n",
    "* 實作對話匯出/匯入與分享功能\n",
    "\n",
    "**進階擴展:**\n",
    "* 遷移到 FastAPI 後端以支援更多客戶端\n",
    "* 加入使用者身份驗證與權限管理\n",
    "* 實作分散式部署與負載平衡\n",
    "* 整合向量資料庫（Qdrant/Weaviate）以取代 FAISS\n",
    "\n",
    "**生產準備:**\n",
    "* 容器化部署（Docker + docker-compose）\n",
    "* 監控與日誌收集（Prometheus + Grafana）\n",
    "* 自動化測試與 CI/CD 流程\n",
    "* 安全強化與性能調優\n",
    "\n",
    "---\n",
    "\n",
    "**🎯 階段 F 總結 - WebUI & API 部署整合**\n",
    "\n",
    "我們已完成 **Part F** 的第一本重要 notebook，建立了一個功能完整的聊天介面。下一步可以選擇：\n",
    "\n",
    "1. **nb32_fastapi_docker_deploy.ipynb** - 完成 API 後端與容器化部署\n",
    "2. **回到 Part D 微調系列** - 實作 LoRA/QLoRA 客製化模型  \n",
    "3. **回到 Part E 進階應用** - 多代理協作與自動化流程\n",
    "\n",
    "**建議優先順序：** 先完成 nb32 API 部署以形成完整的部署方案，再回到核心技術深化。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
