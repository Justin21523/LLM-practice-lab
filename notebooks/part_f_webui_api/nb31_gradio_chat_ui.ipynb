{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18176f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb31_gradio_chat_ui.ipynb\n",
    "# Gradio èŠå¤©ä»‹é¢ - æ•´åˆ LLM + RAG + Function Calling\n",
    "\n",
    "# === Cell 1: Shared Cache Bootstrap ===\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "for k, v in paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache] Root:\", AI_CACHE_ROOT)\n",
    "print(\n",
    "    \"[GPU]\",\n",
    "    torch.cuda.is_available(),\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Dependencies Installation ===\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for Gradio chat UI\"\"\"\n",
    "    packages = [\n",
    "        \"gradio>=4.0.0\",\n",
    "        \"transformers>=4.35.0\",\n",
    "        \"accelerate\",\n",
    "        \"bitsandbytes\",\n",
    "        \"sentence-transformers\",\n",
    "        \"faiss-cpu\",\n",
    "        \"PyPDF2\",\n",
    "        \"langchain-community\",\n",
    "        \"opencc-python-reimplemented\",\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"âœ… Installed: {package}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"âŒ Failed to install: {package}\")\n",
    "\n",
    "\n",
    "# Uncomment to install packages\n",
    "# install_packages()\n",
    "\n",
    "import gradio as gr\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e95c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: LLM Adapter (Multi-Backend Support) ===\n",
    "class LLMAdapter:\n",
    "    \"\"\"Unified LLM interface supporting multiple backends\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backend: str = \"transformers\",\n",
    "        model_id: str = \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.backend = backend\n",
    "        self.model_id = model_id\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._load_model(**kwargs)\n",
    "\n",
    "    def _load_model(self, **kwargs):\n",
    "        \"\"\"Load model based on backend type\"\"\"\n",
    "        if self.backend == \"transformers\":\n",
    "            self._load_transformers_model(**kwargs)\n",
    "        elif self.backend == \"ollama\":\n",
    "            self._load_ollama_model(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backend: {self.backend}\")\n",
    "\n",
    "    def _load_transformers_model(self, **kwargs):\n",
    "        \"\"\"Load model using transformers library\"\"\"\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        import torch\n",
    "\n",
    "        # Default settings for low VRAM\n",
    "        default_kwargs = {\n",
    "            \"device_map\": \"auto\",\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"load_in_4bit\": True if torch.cuda.is_available() else False,\n",
    "            \"trust_remote_code\": True,\n",
    "        }\n",
    "        default_kwargs.update(kwargs)\n",
    "\n",
    "        try:\n",
    "            print(f\"Loading {self.model_id} with transformers...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_id,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=os.environ.get(\"TRANSFORMERS_CACHE\"),\n",
    "            )\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_id,\n",
    "                cache_dir=os.environ.get(\"TRANSFORMERS_CACHE\"),\n",
    "                **default_kwargs,\n",
    "            )\n",
    "\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            print(f\"âœ… Model loaded: {self.model_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to load model: {e}\")\n",
    "            # Fallback to CPU\n",
    "            if \"cuda\" in str(e).lower():\n",
    "                print(\"ğŸ”„ Falling back to CPU...\")\n",
    "                default_kwargs.update({\"device_map\": \"cpu\", \"load_in_4bit\": False})\n",
    "                self._load_transformers_model(**default_kwargs)\n",
    "\n",
    "    def _load_ollama_model(self, **kwargs):\n",
    "        \"\"\"Load model using Ollama (placeholder)\"\"\"\n",
    "        print(f\"ğŸ“ Ollama backend: {self.model_id} (requires ollama server)\")\n",
    "        # Implementation would connect to local ollama server\n",
    "\n",
    "    def generate(\n",
    "        self, messages: List[Dict[str, str]], max_tokens: int = 512, **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"Generate response from messages\"\"\"\n",
    "        if self.backend == \"transformers\":\n",
    "            return self._generate_transformers(messages, max_tokens, **kwargs)\n",
    "        elif self.backend == \"ollama\":\n",
    "            return self._generate_ollama(messages, max_tokens, **kwargs)\n",
    "        else:\n",
    "            return \"Error: Unsupported backend\"\n",
    "\n",
    "    def _generate_transformers(\n",
    "        self, messages: List[Dict[str, str]], max_tokens: int, **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"Generate using transformers\"\"\"\n",
    "        try:\n",
    "            # Format messages for chat template\n",
    "            if hasattr(self.tokenizer, \"apply_chat_template\"):\n",
    "                prompt = self.tokenizer.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "            else:\n",
    "                # Fallback formatting\n",
    "                prompt = \"\"\n",
    "                for msg in messages:\n",
    "                    if msg[\"role\"] == \"system\":\n",
    "                        prompt += f\"System: {msg['content']}\\n\"\n",
    "                    elif msg[\"role\"] == \"user\":\n",
    "                        prompt += f\"User: {msg['content']}\\n\"\n",
    "                    elif msg[\"role\"] == \"assistant\":\n",
    "                        prompt += f\"Assistant: {msg['content']}\\n\"\n",
    "                prompt += \"Assistant: \"\n",
    "\n",
    "            # Tokenize and generate\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, return_tensors=\"pt\", truncation=True, max_length=2048\n",
    "            )\n",
    "\n",
    "            if torch.cuda.is_available() and hasattr(self.model, \"device\"):\n",
    "                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "            # Decode response\n",
    "            response = self.tokenizer.decode(\n",
    "                outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            return response.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Generation error: {str(e)}\"\n",
    "\n",
    "    def _generate_ollama(\n",
    "        self, messages: List[Dict[str, str]], max_tokens: int, **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"Generate using Ollama (placeholder)\"\"\"\n",
    "        return f\"Ollama response for {len(messages)} messages (placeholder)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1dd345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: RAG System Core ===\n",
    "class RAGSystem:\n",
    "    \"\"\"Retrieval-Augmented Generation system with document processing\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str = \"BAAI/bge-m3\", chunk_size: int = 512):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.embedder = None\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "        self.chunks = []\n",
    "        self._load_embedder()\n",
    "\n",
    "    def _load_embedder(self):\n",
    "        \"\"\"Load sentence transformer for embeddings\"\"\"\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "\n",
    "            self.embedder = SentenceTransformer(\n",
    "                self.embedding_model, cache_folder=os.environ.get(\"TRANSFORMERS_CACHE\")\n",
    "            )\n",
    "            print(f\"âœ… Loaded embedder: {self.embedding_model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to load embedder: {e}\")\n",
    "\n",
    "    def add_documents(self, files) -> str:\n",
    "        \"\"\"Add uploaded documents to knowledge base\"\"\"\n",
    "        if not files:\n",
    "            return \"No files uploaded\"\n",
    "\n",
    "        added_docs = []\n",
    "        for file in files:\n",
    "            try:\n",
    "                content = self._extract_text(file)\n",
    "                if content:\n",
    "                    self.documents.append(\n",
    "                        {\n",
    "                            \"filename\": file.name,\n",
    "                            \"content\": content,\n",
    "                            \"timestamp\": datetime.now().isoformat(),\n",
    "                        }\n",
    "                    )\n",
    "                    added_docs.append(file.name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file.name}: {e}\")\n",
    "\n",
    "        if added_docs:\n",
    "            self._build_index()\n",
    "            return f\"âœ… Added {len(added_docs)} documents: {', '.join(added_docs)}\"\n",
    "        else:\n",
    "            return \"âŒ No documents could be processed\"\n",
    "\n",
    "    def _extract_text(self, file) -> str:\n",
    "        \"\"\"Extract text from uploaded file\"\"\"\n",
    "        if file.name.endswith(\".pdf\"):\n",
    "            return self._extract_pdf_text(file)\n",
    "        elif file.name.endswith((\".txt\", \".md\")):\n",
    "            return file.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _extract_pdf_text(self, file) -> str:\n",
    "        \"\"\"Extract text from PDF file\"\"\"\n",
    "        try:\n",
    "            import PyPDF2\n",
    "\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"PDF extraction error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks\"\"\"\n",
    "        # Simple chunking - could use more sophisticated methods\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for word in words:\n",
    "            if current_length + len(word) > self.chunk_size and current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [word]\n",
    "                current_length = len(word)\n",
    "            else:\n",
    "                current_chunk.append(word)\n",
    "                current_length += len(word) + 1  # +1 for space\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"Build FAISS index from documents\"\"\"\n",
    "        if not self.embedder or not self.documents:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import faiss\n",
    "            import numpy as np\n",
    "\n",
    "            # Create chunks from all documents\n",
    "            all_chunks = []\n",
    "            for doc in self.documents:\n",
    "                chunks = self._chunk_text(doc[\"content\"])\n",
    "                for chunk in chunks:\n",
    "                    all_chunks.append(\n",
    "                        {\n",
    "                            \"text\": chunk,\n",
    "                            \"source\": doc[\"filename\"],\n",
    "                            \"timestamp\": doc[\"timestamp\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            self.chunks = all_chunks\n",
    "\n",
    "            # Generate embeddings\n",
    "            texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "            embeddings = self.embedder.encode(texts, show_progress_bar=True)\n",
    "\n",
    "            # Build FAISS index\n",
    "            dimension = embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatIP(dimension)  # Inner product for similarity\n",
    "\n",
    "            # Normalize embeddings for cosine similarity\n",
    "            faiss.normalize_L2(embeddings.astype(np.float32))\n",
    "            self.index.add(embeddings.astype(np.float32))\n",
    "\n",
    "            print(f\"âœ… Built index with {len(all_chunks)} chunks\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Index building error: {e}\")\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant chunks for query\"\"\"\n",
    "        if not self.embedder or not self.index or not self.chunks:\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            import faiss\n",
    "            import numpy as np\n",
    "\n",
    "            # Encode query\n",
    "            query_embedding = self.embedder.encode([query])\n",
    "            faiss.normalize_L2(query_embedding.astype(np.float32))\n",
    "\n",
    "            # Search\n",
    "            scores, indices = self.index.search(\n",
    "                query_embedding.astype(np.float32), top_k\n",
    "            )\n",
    "\n",
    "            # Format results\n",
    "            results = []\n",
    "            for score, idx in zip(scores[0], indices[0]):\n",
    "                if idx < len(self.chunks):\n",
    "                    chunk = self.chunks[idx]\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"text\": chunk[\"text\"],\n",
    "                            \"source\": chunk[\"source\"],\n",
    "                            \"score\": float(score),\n",
    "                            \"timestamp\": chunk[\"timestamp\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Retrieval error: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93619e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Function Calling Tools ===\n",
    "class ToolManager:\n",
    "    \"\"\"Manage function calling tools\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tools = {\n",
    "            \"calculator\": self.calculator,\n",
    "            \"datetime\": self.get_datetime,\n",
    "            \"web_search\": self.web_search_placeholder,\n",
    "        }\n",
    "\n",
    "    def calculator(self, expression: str) -> str:\n",
    "        \"\"\"Simple calculator tool\"\"\"\n",
    "        try:\n",
    "            # Safe evaluation - only allow basic math\n",
    "            allowed_chars = set(\"0123456789+-*/.()% \")\n",
    "            if not all(c in allowed_chars for c in expression):\n",
    "                return \"Error: Invalid characters in expression\"\n",
    "\n",
    "            result = eval(expression)\n",
    "            return f\"Result: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Calculation error: {str(e)}\"\n",
    "\n",
    "    def get_datetime(self, format_type: str = \"default\") -> str:\n",
    "        \"\"\"Get current date and time\"\"\"\n",
    "        now = datetime.now()\n",
    "        if format_type == \"date\":\n",
    "            return now.strftime(\"%Y-%m-%d\")\n",
    "        elif format_type == \"time\":\n",
    "            return now.strftime(\"%H:%M:%S\")\n",
    "        else:\n",
    "            return now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def web_search_placeholder(self, query: str) -> str:\n",
    "        \"\"\"Placeholder for web search (could integrate DuckDuckGo)\"\"\"\n",
    "        return f\"Web search placeholder for: {query}\"\n",
    "\n",
    "    def parse_tool_call(self, text: str) -> Tuple[str, Optional[Dict[str, Any]]]:\n",
    "        \"\"\"Parse tool calls from LLM response\"\"\"\n",
    "        # Simple pattern matching for tool calls\n",
    "        # Format: [TOOL:tool_name(arg1=value1, arg2=value2)]\n",
    "        import re\n",
    "\n",
    "        pattern = r\"\\[TOOL:(\\w+)\\(([^)]*)\\)\\]\"\n",
    "        match = re.search(pattern, text)\n",
    "\n",
    "        if match:\n",
    "            tool_name = match.group(1)\n",
    "            args_str = match.group(2)\n",
    "\n",
    "            # Parse arguments\n",
    "            args = {}\n",
    "            if args_str:\n",
    "                for arg_pair in args_str.split(\",\"):\n",
    "                    if \"=\" in arg_pair:\n",
    "                        key, value = arg_pair.split(\"=\", 1)\n",
    "                        args[key.strip()] = value.strip().strip(\"\\\"'\")\n",
    "\n",
    "            return tool_name, args\n",
    "\n",
    "        return \"\", None\n",
    "\n",
    "    def execute_tool(self, tool_name: str, args: Dict[str, Any]) -> str:\n",
    "        \"\"\"Execute a tool with given arguments\"\"\"\n",
    "        if tool_name not in self.tools:\n",
    "            return f\"Unknown tool: {tool_name}\"\n",
    "\n",
    "        try:\n",
    "            tool_func = self.tools[tool_name]\n",
    "            return tool_func(**args)\n",
    "        except Exception as e:\n",
    "            return f\"Tool execution error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a681a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Conversation Memory Manager ===\n",
    "class ConversationMemory:\n",
    "    \"\"\"Manage conversation history and context\"\"\"\n",
    "\n",
    "    def __init__(self, max_history: int = 10):\n",
    "        self.max_history = max_history\n",
    "        self.conversations = {}  # user_id -> conversation history\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def add_message(self, user_id: str, role: str, content: str):\n",
    "        \"\"\"Add message to conversation history\"\"\"\n",
    "        with self.lock:\n",
    "            if user_id not in self.conversations:\n",
    "                self.conversations[user_id] = []\n",
    "\n",
    "            self.conversations[user_id].append(\n",
    "                {\n",
    "                    \"role\": role,\n",
    "                    \"content\": content,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Keep only recent messages\n",
    "            if (\n",
    "                len(self.conversations[user_id]) > self.max_history * 2\n",
    "            ):  # *2 for user+assistant pairs\n",
    "                self.conversations[user_id] = self.conversations[user_id][\n",
    "                    -self.max_history * 2 :\n",
    "                ]\n",
    "\n",
    "    def get_conversation(self, user_id: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get conversation history for LLM context\"\"\"\n",
    "        with self.lock:\n",
    "            if user_id not in self.conversations:\n",
    "                return []\n",
    "\n",
    "            # Return only role and content for LLM\n",
    "            return [\n",
    "                {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n",
    "                for msg in self.conversations[user_id]\n",
    "            ]\n",
    "\n",
    "    def clear_conversation(self, user_id: str):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        with self.lock:\n",
    "            if user_id in self.conversations:\n",
    "                del self.conversations[user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b030f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Gradio Chat UI Core Logic ===\n",
    "class ChatbotCore:\n",
    "    \"\"\"Core chatbot logic integrating LLM, RAG, and tools\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.rag = RAGSystem()\n",
    "        self.tools = ToolManager()\n",
    "        self.memory = ConversationMemory()\n",
    "        self.current_model = None\n",
    "        self.system_prompt = \"\"\"You are a helpful AI assistant. You can use tools by calling them in the format [TOOL:tool_name(arg1=value1, arg2=value2)].\n",
    "\n",
    "Available tools:\n",
    "- calculator(expression=\"math expression\") - for calculations\n",
    "- datetime(format_type=\"default\"|\"date\"|\"time\") - get current date/time\n",
    "- web_search(query=\"search terms\") - search the web\n",
    "\n",
    "You can also answer questions based on uploaded documents if available.\"\"\"\n",
    "\n",
    "    def load_model(self, model_name: str, backend: str = \"transformers\") -> str:\n",
    "        \"\"\"Load or switch LLM model\"\"\"\n",
    "        try:\n",
    "            if self.current_model == f\"{backend}:{model_name}\":\n",
    "                return f\"âœ… Model already loaded: {model_name}\"\n",
    "\n",
    "            self.llm = LLMAdapter(backend=backend, model_id=model_name)\n",
    "            self.current_model = f\"{backend}:{model_name}\"\n",
    "            return f\"âœ… Loaded model: {model_name} ({backend})\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"âŒ Failed to load model: {str(e)}\"\n",
    "\n",
    "    def process_message(\n",
    "        self, message: str, user_id: str = \"default\", use_rag: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"Process user message and return response\"\"\"\n",
    "        if not self.llm:\n",
    "            return \"âŒ No model loaded. Please select a model first.\"\n",
    "\n",
    "        try:\n",
    "            # Build conversation context\n",
    "            messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "\n",
    "            # Add conversation history\n",
    "            history = self.memory.get_conversation(user_id)\n",
    "            messages.extend(history)\n",
    "\n",
    "            # Add RAG context if enabled and available\n",
    "            if use_rag and self.rag.chunks:\n",
    "                relevant_docs = self.rag.retrieve(message, top_k=3)\n",
    "                if relevant_docs:\n",
    "                    rag_context = \"\\n\\nRelevant information from documents:\\n\"\n",
    "                    for doc in relevant_docs:\n",
    "                        rag_context += (\n",
    "                            f\"- {doc['text'][:200]}... (from {doc['source']})\\n\"\n",
    "                        )\n",
    "\n",
    "                    message_with_rag = message + rag_context\n",
    "                else:\n",
    "                    message_with_rag = message\n",
    "            else:\n",
    "                message_with_rag = message\n",
    "\n",
    "            # Add current user message\n",
    "            messages.append({\"role\": \"user\", \"content\": message_with_rag})\n",
    "\n",
    "            # Generate response\n",
    "            response = self.llm.generate(messages, max_tokens=512)\n",
    "\n",
    "            # Check for tool calls\n",
    "            tool_name, tool_args = self.tools.parse_tool_call(response)\n",
    "            if tool_name and tool_args is not None:\n",
    "                tool_result = self.tools.execute_tool(tool_name, tool_args)\n",
    "                response += f\"\\n\\nTool result: {tool_result}\"\n",
    "\n",
    "            # Save conversation\n",
    "            self.memory.add_message(user_id, \"user\", message)\n",
    "            self.memory.add_message(user_id, \"assistant\", response)\n",
    "\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"âŒ Error processing message: {str(e)}\"\n",
    "\n",
    "    def upload_documents(self, files) -> str:\n",
    "        \"\"\"Handle document upload\"\"\"\n",
    "        return self.rag.add_documents(files)\n",
    "\n",
    "    def clear_conversation(self, user_id: str = \"default\") -> str:\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.memory.clear_conversation(user_id)\n",
    "        return \"âœ… Conversation cleared\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1081af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Gradio WebUI Setup and Launch ===\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create and return Gradio interface\"\"\"\n",
    "\n",
    "    # Initialize chatbot core\n",
    "    chatbot = ChatbotCore()\n",
    "\n",
    "    # Model options\n",
    "    model_options = [\n",
    "        \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"microsoft/DialoGPT-medium\",\n",
    "        \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    ]\n",
    "\n",
    "    def load_model_wrapper(model_name, backend):\n",
    "        return chatbot.load_model(model_name, backend)\n",
    "\n",
    "    def chat_response(message, history, use_rag):\n",
    "        if not message.strip():\n",
    "            return history, \"\"\n",
    "\n",
    "        # Generate response\n",
    "        response = chatbot.process_message(message, use_rag=use_rag)\n",
    "\n",
    "        # Update history\n",
    "        history.append([message, response])\n",
    "        return history, \"\"\n",
    "\n",
    "    def upload_files_wrapper(files):\n",
    "        return chatbot.upload_documents(files)\n",
    "\n",
    "    def clear_chat_wrapper():\n",
    "        result = chatbot.clear_conversation()\n",
    "        return [], result\n",
    "\n",
    "    # Create Gradio interface\n",
    "    with gr.Blocks(title=\"LLM Chat Assistant\", theme=gr.themes.Soft()) as iface:\n",
    "\n",
    "        gr.Markdown(\"# ğŸ¤– LLM Chat Assistant\")\n",
    "        gr.Markdown(\"Multi-model chatbot with RAG and function calling capabilities\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                # Model selection\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\"### Model Configuration\")\n",
    "                    model_dropdown = gr.Dropdown(\n",
    "                        choices=model_options,\n",
    "                        value=model_options[0],\n",
    "                        label=\"Select Model\",\n",
    "                        interactive=True,\n",
    "                    )\n",
    "                    backend_radio = gr.Radio(\n",
    "                        choices=[\"transformers\", \"ollama\"],\n",
    "                        value=\"transformers\",\n",
    "                        label=\"Backend\",\n",
    "                    )\n",
    "                    load_btn = gr.Button(\"Load Model\", variant=\"primary\")\n",
    "                    model_status = gr.Textbox(\n",
    "                        label=\"Model Status\", value=\"No model loaded\", interactive=False\n",
    "                    )\n",
    "\n",
    "                # Document upload\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\"### Knowledge Base\")\n",
    "                    file_upload = gr.Files(\n",
    "                        label=\"Upload Documents (PDF, TXT, MD)\",\n",
    "                        file_types=[\".pdf\", \".txt\", \".md\"],\n",
    "                    )\n",
    "                    upload_btn = gr.Button(\"Add to Knowledge Base\")\n",
    "                    upload_status = gr.Textbox(label=\"Upload Status\", interactive=False)\n",
    "\n",
    "                # Settings\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\"### Settings\")\n",
    "                    use_rag_checkbox = gr.Checkbox(\n",
    "                        label=\"Enable RAG (use uploaded documents)\", value=True\n",
    "                    )\n",
    "                    clear_btn = gr.Button(\"Clear Conversation\", variant=\"secondary\")\n",
    "                    clear_status = gr.Textbox(label=\"Action Status\", interactive=False)\n",
    "\n",
    "            with gr.Column(scale=3):\n",
    "                # Chat interface\n",
    "                gr.Markdown(\"### Chat\")\n",
    "                chatbot_display = gr.Chatbot(\n",
    "                    label=\"Conversation\", height=500, show_copy_button=True\n",
    "                )\n",
    "\n",
    "                with gr.Row():\n",
    "                    message_input = gr.Textbox(\n",
    "                        label=\"Message\",\n",
    "                        placeholder=\"Type your message here...\",\n",
    "                        lines=2,\n",
    "                        scale=4,\n",
    "                    )\n",
    "                    send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "\n",
    "        # Examples\n",
    "        gr.Markdown(\"### ğŸ’¡ Example Messages\")\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                [\"Hello! How can you help me?\"],\n",
    "                [\"Calculate 15 * 23 + 45\"],\n",
    "                [\"What's the current date and time?\"],\n",
    "                ['[TOOL:calculator(expression=\"2+2*3\")]'],\n",
    "                [\"Summarize the uploaded documents\"],\n",
    "            ],\n",
    "            inputs=message_input,\n",
    "        )\n",
    "\n",
    "        # Event handlers\n",
    "        load_btn.click(\n",
    "            load_model_wrapper,\n",
    "            inputs=[model_dropdown, backend_radio],\n",
    "            outputs=model_status,\n",
    "        )\n",
    "\n",
    "        upload_btn.click(\n",
    "            upload_files_wrapper, inputs=file_upload, outputs=upload_status\n",
    "        )\n",
    "\n",
    "        send_btn.click(\n",
    "            chat_response,\n",
    "            inputs=[message_input, chatbot_display, use_rag_checkbox],\n",
    "            outputs=[chatbot_display, message_input],\n",
    "        )\n",
    "\n",
    "        message_input.submit(\n",
    "            chat_response,\n",
    "            inputs=[message_input, chatbot_display, use_rag_checkbox],\n",
    "            outputs=[chatbot_display, message_input],\n",
    "        )\n",
    "\n",
    "        clear_btn.click(clear_chat_wrapper, outputs=[chatbot_display, clear_status])\n",
    "\n",
    "    return iface\n",
    "\n",
    "\n",
    "# Launch interface\n",
    "if __name__ == \"__main__\":\n",
    "    interface = create_gradio_interface()\n",
    "\n",
    "    # Launch with public link for sharing (optional)\n",
    "    interface.launch(\n",
    "        server_name=\"0.0.0.0\",  # Allow external connections\n",
    "        server_port=7860,  # Default Gradio port\n",
    "        share=False,  # Set to True for public ngrok link\n",
    "        debug=True,\n",
    "        show_error=True,\n",
    "    )\n",
    "\n",
    "print(\"ğŸš€ Gradio interface ready! Run the cell above to launch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae22558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9: Smoke Test ===\n",
    "def test_gradio_components():\n",
    "    \"\"\"Quick test of core components\"\"\"\n",
    "    print(\"ğŸ§ª Testing Gradio Chat UI Components...\")\n",
    "\n",
    "    # Test LLM Adapter\n",
    "    try:\n",
    "        adapter = LLMAdapter(\n",
    "            backend=\"transformers\", model_id=\"microsoft/DialoGPT-medium\"\n",
    "        )\n",
    "        test_response = adapter.generate(\n",
    "            [{\"role\": \"user\", \"content\": \"Hello\"}], max_tokens=10\n",
    "        )\n",
    "        print(f\"âœ… LLM Adapter: {test_response[:50]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ LLM Adapter: {e}\")\n",
    "\n",
    "    # Test RAG System\n",
    "    try:\n",
    "        rag = RAGSystem()\n",
    "        print(f\"âœ… RAG System initialized with embedder: {rag.embedding_model}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ RAG System: {e}\")\n",
    "\n",
    "    # Test Tool Manager\n",
    "    try:\n",
    "        tools = ToolManager()\n",
    "        calc_result = tools.calculator(\"2+2\")\n",
    "        time_result = tools.get_datetime(\"time\")\n",
    "        print(f\"âœ… Tools: Calculator={calc_result}, Time={time_result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Tool Manager: {e}\")\n",
    "\n",
    "    # Test Memory\n",
    "    try:\n",
    "        memory = ConversationMemory()\n",
    "        memory.add_message(\"test\", \"user\", \"Hello\")\n",
    "        memory.add_message(\"test\", \"assistant\", \"Hi there!\")\n",
    "        history = memory.get_conversation(\"test\")\n",
    "        print(f\"âœ… Memory: {len(history)} messages stored\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Memory: {e}\")\n",
    "\n",
    "    print(\"ğŸ Component tests completed!\")\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "test_gradio_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd597db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 10: Usage Guide and Deployment Notes ===\n",
    "print(\n",
    "    \"\"\"\n",
    "ğŸ“‹ ä½¿ç”¨æŒ‡å— (Usage Guide)\n",
    "===========================\n",
    "\n",
    "ğŸ”§ è¨­å®šæ­¥é©Ÿ (Setup Steps):\n",
    "1. åŸ·è¡Œç’°å¢ƒåˆå§‹åŒ– Cell (å…±äº«å¿«å–è¨­å®š)\n",
    "2. å®‰è£ä¾è³´å¥—ä»¶ (å–æ¶ˆè¨»è§£ install_packages() ä¸¦åŸ·è¡Œ)\n",
    "3. å•Ÿå‹• Gradio ä»‹é¢ (åŸ·è¡Œ Cell 8)\n",
    "4. åœ¨ç€è¦½å™¨ä¸­é–‹å•Ÿé¡¯ç¤ºçš„ URL (é€šå¸¸æ˜¯ http://localhost:7860)\n",
    "\n",
    "ğŸ’¬ åŠŸèƒ½ç‰¹è‰² (Features):\n",
    "âœ… å¤šæ¨¡å‹æ”¯æ´ - æ”¯æ´ Qwen, DialoGPT, Zephyr ç­‰\n",
    "âœ… RAG æ–‡ä»¶å•ç­” - ä¸Šå‚³ PDF/TXT/MD æª”æ¡ˆé€²è¡ŒçŸ¥è­˜åº«å•ç­”\n",
    "âœ… å·¥å…·èª¿ç”¨ - è¨ˆç®—æ©Ÿã€æ—¥æœŸæ™‚é–“ã€ç¶²è·¯æœå°‹ (å ä½ç¬¦)\n",
    "âœ… æœƒè©±è¨˜æ†¶ - ä¿æŒå¤šè¼ªå°è©±ä¸Šä¸‹æ–‡\n",
    "âœ… ä½è³‡æºå‹å–„ - æ”¯æ´ 4bit é‡åŒ–ã€CPU fallback\n",
    "\n",
    "ğŸ› ï¸ å·¥å…·ä½¿ç”¨æ–¹å¼ (Tool Usage):\n",
    "- è¨ˆç®—: \"Calculate 15 * 23\" æˆ– \"[TOOL:calculator(expression=\\\"15*23\\\")]\"\n",
    "- æ—¥æœŸ: \"What's the date?\" æˆ– \"[TOOL:datetime(format_type=\\\"date\\\")]\"\n",
    "- æ™‚é–“: \"Current time?\" æˆ– \"[TOOL:datetime(format_type=\\\"time\\\")]\"\n",
    "\n",
    "ğŸ“ RAG æ–‡ä»¶ä¸Šå‚³ (Document Upload):\n",
    "1. é»æ“Š \"Upload Documents\" å€åŸŸ\n",
    "2. é¸æ“‡ PDFã€TXT æˆ– MD æª”æ¡ˆ\n",
    "3. é»æ“Š \"Add to Knowledge Base\"\n",
    "4. é–‹å•Ÿ \"Enable RAG\" é¸é …\n",
    "5. è©¢å•æ–‡ä»¶ç›¸é—œå•é¡Œ\n",
    "\n",
    "âš™ï¸ æ¨¡å‹é…ç½®å»ºè­° (Model Configuration):\n",
    "- 8GB VRAM: Qwen2.5-1.5B-Instruct (4bit)\n",
    "- 12GB VRAM: Qwen2.5-7B-Instruct (4bit)\n",
    "- 16GB+ VRAM: Qwen2.5-7B-Instruct (fp16)\n",
    "- CPU only: DialoGPT-medium\n",
    "\n",
    "ğŸš¨ å¸¸è¦‹å•é¡Œæ’è§£ (Troubleshooting):\n",
    "- CUDA OOM â†’ æ”¹ç”¨æ›´å°æ¨¡å‹æˆ–é–‹å•Ÿ 4bit é‡åŒ–\n",
    "- è¼‰å…¥å¤±æ•— â†’ æª¢æŸ¥ç¶²è·¯é€£ç·šèˆ‡ HuggingFace å¯ç”¨æ€§\n",
    "- RAG ç„¡å›æ‡‰ â†’ ç¢ºèªæ–‡ä»¶å·²æˆåŠŸä¸Šå‚³ä¸¦å»ºç«‹ç´¢å¼•\n",
    "- å·¥å…·ä¸åŸ·è¡Œ â†’ æª¢æŸ¥å·¥å…·èª¿ç”¨æ ¼å¼æ˜¯å¦æ­£ç¢º\n",
    "\n",
    "ğŸŒ éƒ¨ç½²é¸é … (Deployment Options):\n",
    "1. æœ¬åœ°éƒ¨ç½²: ç›´æ¥åŸ·è¡Œ notebookï¼Œè¨ªå• localhost:7860\n",
    "2. å€ç¶²åˆ†äº«: è¨­å®š server_name=\"0.0.0.0\" é–‹æ”¾å€ç¶²è¨ªå•\n",
    "3. å…¬é–‹åˆ†äº«: è¨­å®š share=True ç”Ÿæˆ ngrok å…¬é–‹é€£çµ (é–‹ç™¼æ¸¬è©¦ç”¨)\n",
    "4. å®¹å™¨åŒ–: å¯åŒ…è£ç‚º Docker å®¹å™¨é€²è¡Œéƒ¨ç½²\n",
    "\n",
    "ğŸ”’ å®‰å…¨å»ºè­° (Security Recommendations):\n",
    "- ç”Ÿç”¢ç’°å¢ƒè«‹å‹¿é–‹å•Ÿ share=True\n",
    "- è¨­å®šé©ç•¶çš„ç¶²è·¯é˜²ç«ç‰†è¦å‰‡\n",
    "- è€ƒæ…®åŠ å…¥èº«ä»½é©—è­‰æ©Ÿåˆ¶\n",
    "- å®šæœŸæ›´æ–°ä¾è³´å¥—ä»¶ç‰ˆæœ¬\n",
    "\n",
    "ğŸ“ˆ æ•ˆèƒ½å„ªåŒ– (Performance Optimization):\n",
    "- ä½¿ç”¨ GGUF é‡åŒ–æ¨¡å‹ (llama.cpp backend)\n",
    "- å•Ÿç”¨ gradient checkpointing\n",
    "- èª¿æ•´ chunk_size èˆ‡ max_history åƒæ•¸\n",
    "- è€ƒæ…®ä½¿ç”¨ Redis åšåˆ†æ•£å¼æœƒè©±å„²å­˜\n",
    "\n",
    "ğŸ”„ æ“´å±•å»ºè­° (Extension Ideas):\n",
    "- æ•´åˆæ›´å¤šå·¥å…· (å¤©æ°£ã€è‚¡åƒ¹ã€æ–°è)\n",
    "- åŠ å…¥èªéŸ³è¼¸å…¥/è¼¸å‡º (Whisper/TTS)\n",
    "- æ”¯æ´åœ–ç‰‡ä¸Šå‚³èˆ‡å¤šæ¨¡æ…‹å°è©±\n",
    "- åŠ å…¥å°è©±å°å‡º/åŒ¯å…¥åŠŸèƒ½\n",
    "- å¯¦ä½œä½¿ç”¨è€…è§’è‰²èˆ‡æ¬Šé™ç®¡ç†\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29519f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === æœ€å°é©—æ”¶æ¸¬è©¦ (5è¡Œå…§) ===\n",
    "interface = create_gradio_interface()\n",
    "chatbot = ChatbotCore()\n",
    "status = chatbot.load_model(\"microsoft/DialoGPT-medium\")\n",
    "response = chatbot.process_message(\"Hello, test message\", use_rag=False)\n",
    "print(\n",
    "    f\"âœ… Interface created, model loaded: {status[:20]}..., response: {response[:30]}...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a9201",
   "metadata": {},
   "source": [
    "\n",
    "## 6. æœ¬ç« å°çµ\n",
    "\n",
    "### ğŸ¯ å®Œæˆé …ç›® (Completed Items)\n",
    "\n",
    "* **æ•´åˆå¼èŠå¤©ä»‹é¢** - å»ºç«‹æ”¯æ´å¤šæ¨¡å‹åˆ‡æ›çš„ Gradio WebUI\n",
    "* **RAG æ–‡ä»¶å•ç­”ç³»çµ±** - PDF/TXT/MD æ–‡ä»¶ä¸Šå‚³èˆ‡ FAISS æª¢ç´¢æ•´åˆ\n",
    "* **å·¥å…·èª¿ç”¨åŠŸèƒ½** - è¨ˆç®—æ©Ÿã€æ—¥æœŸæ™‚é–“ã€ç¶²è·¯æœå°‹å·¥å…·æ•´åˆ\n",
    "* **æœƒè©±è¨˜æ†¶ç®¡ç†** - å¤šç”¨æˆ¶å°è©±æ­·å²ä¿å­˜èˆ‡ä¸Šä¸‹æ–‡ç¶­è­·\n",
    "* **ä½è³‡æºå„ªåŒ–** - 4bit é‡åŒ–ã€CPU fallbackã€è¨˜æ†¶é«”å„ªåŒ–ç­–ç•¥\n",
    "* **çµ±ä¸€æ¨¡å‹ä»‹é¢** - LLMAdapter æ”¯æ´ transformers/ollama å¤šå¾Œç«¯\n",
    "* **ä½¿ç”¨è€…å‹å–„è¨­è¨ˆ** - éŸ¿æ‡‰å¼ä»‹é¢ã€ç¯„ä¾‹æç¤ºã€ç‹€æ…‹é¡¯ç¤º\n",
    "\n",
    "### ğŸ§  æ ¸å¿ƒåŸç†è¦é» (Core Concepts)\n",
    "\n",
    "* **æ¨¡çµ„åŒ–æ¶æ§‹è¨­è¨ˆ (Modular Architecture)** - å„çµ„ä»¶ç¨ç«‹å¯æ¸¬è©¦ã€å¯é‡ç”¨\n",
    "* **çµ±ä¸€æŠ½è±¡ä»‹é¢ (Unified Abstraction)** - LLMAdapter éš”é›¢ä¸åŒå¾Œç«¯å¯¦ä½œç´°ç¯€\n",
    "* **è¨˜æ†¶é«”é«˜æ•ˆç®¡ç† (Memory Efficiency)** - 4bit é‡åŒ–ã€gradient checkpointingã€å‹•æ…‹å¸è¼‰\n",
    "* **RAG æª¢ç´¢å¢å¼· (Retrieval-Augmented Generation)** - æ–‡ä»¶å‘é‡åŒ–ã€ç›¸ä¼¼åº¦æœå°‹ã€ä¸Šä¸‹æ–‡æ³¨å…¥\n",
    "* **å·¥å…·èª¿ç”¨å”è­° (Function Calling Protocol)** - çµæ§‹åŒ–å·¥å…·èª¿ç”¨èˆ‡çµæœè™•ç†\n",
    "* **æœƒè©±ç‹€æ…‹ç®¡ç† (Session State Management)** - å¤šç”¨æˆ¶éš”é›¢ã€æ­·å²æˆªæ–·ã€è¨˜æ†¶é«”æ§åˆ¶\n",
    "\n",
    "### âš ï¸ å¸¸è¦‹é™·é˜± (Common Pitfalls)\n",
    "\n",
    "* **è¨˜æ†¶é«”æº¢å‡º** - å¤§æ¨¡å‹ + é•·å°è©±æ­·å² â†’ ä½¿ç”¨ 4bit é‡åŒ–èˆ‡æ­·å²æˆªæ–·\n",
    "* **å·¥å…·èª¿ç”¨æ ¼å¼éŒ¯èª¤** - LLM è¼¸å‡ºæ ¼å¼ä¸ä¸€è‡´ â†’ æ”¹é€²è§£æé‚è¼¯èˆ‡æç¤ºå·¥ç¨‹\n",
    "* **RAG æª¢ç´¢å“è³ª** - åˆ†å¡Šç­–ç•¥å½±éŸ¿æª¢ç´¢æ•ˆæœ â†’ èª¿æ•´ chunk_size èˆ‡é‡æ’å™¨\n",
    "* **ä¸¦ç™¼å®‰å…¨å•é¡Œ** - å¤šç”¨æˆ¶åŒæ™‚è¨ªå• â†’ ä½¿ç”¨ threading.Lock ä¿è­·å…±äº«ç‹€æ…‹\n",
    "* **æ¨¡å‹è¼‰å…¥å¤±æ•—** - ç¶²è·¯æˆ–ç¡¬é«”é™åˆ¶ â†’ æä¾›å¤šå±¤æ¬¡ fallback æ©Ÿåˆ¶\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å»ºè­° (Next Steps)\n",
    "\n",
    "**ç«‹å³å¯åš:**\n",
    "* æ•´åˆæ›´å¤šé–‹æºå·¥å…·ï¼ˆå¤©æ°£ APIã€æ–°èæŠ“å–ã€æª”æ¡ˆæ“ä½œï¼‰\n",
    "* åŠ å…¥èªéŸ³è¼¸å…¥/è¼¸å‡ºåŠŸèƒ½ï¼ˆWhisper ASR + TTSï¼‰\n",
    "* å¯¦ä½œå°è©±åŒ¯å‡º/åŒ¯å…¥èˆ‡åˆ†äº«åŠŸèƒ½\n",
    "\n",
    "**é€²éšæ“´å±•:**\n",
    "* é·ç§»åˆ° FastAPI å¾Œç«¯ä»¥æ”¯æ´æ›´å¤šå®¢æˆ¶ç«¯\n",
    "* åŠ å…¥ä½¿ç”¨è€…èº«ä»½é©—è­‰èˆ‡æ¬Šé™ç®¡ç†\n",
    "* å¯¦ä½œåˆ†æ•£å¼éƒ¨ç½²èˆ‡è² è¼‰å¹³è¡¡\n",
    "* æ•´åˆå‘é‡è³‡æ–™åº«ï¼ˆQdrant/Weaviateï¼‰ä»¥å–ä»£ FAISS\n",
    "\n",
    "**ç”Ÿç”¢æº–å‚™:**\n",
    "* å®¹å™¨åŒ–éƒ¨ç½²ï¼ˆDocker + docker-composeï¼‰\n",
    "* ç›£æ§èˆ‡æ—¥èªŒæ”¶é›†ï¼ˆPrometheus + Grafanaï¼‰\n",
    "* è‡ªå‹•åŒ–æ¸¬è©¦èˆ‡ CI/CD æµç¨‹\n",
    "* å®‰å…¨å¼·åŒ–èˆ‡æ€§èƒ½èª¿å„ª\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ¯ éšæ®µ F ç¸½çµ - WebUI & API éƒ¨ç½²æ•´åˆ**\n",
    "\n",
    "æˆ‘å€‘å·²å®Œæˆ **Part F** çš„ç¬¬ä¸€æœ¬é‡è¦ notebookï¼Œå»ºç«‹äº†ä¸€å€‹åŠŸèƒ½å®Œæ•´çš„èŠå¤©ä»‹é¢ã€‚ä¸‹ä¸€æ­¥å¯ä»¥é¸æ“‡ï¼š\n",
    "\n",
    "1. **nb32_fastapi_docker_deploy.ipynb** - å®Œæˆ API å¾Œç«¯èˆ‡å®¹å™¨åŒ–éƒ¨ç½²\n",
    "2. **å›åˆ° Part D å¾®èª¿ç³»åˆ—** - å¯¦ä½œ LoRA/QLoRA å®¢è£½åŒ–æ¨¡å‹  \n",
    "3. **å›åˆ° Part E é€²éšæ‡‰ç”¨** - å¤šä»£ç†å”ä½œèˆ‡è‡ªå‹•åŒ–æµç¨‹\n",
    "\n",
    "**å»ºè­°å„ªå…ˆé †åºï¼š** å…ˆå®Œæˆ nb32 API éƒ¨ç½²ä»¥å½¢æˆå®Œæ•´çš„éƒ¨ç½²æ–¹æ¡ˆï¼Œå†å›åˆ°æ ¸å¿ƒæŠ€è¡“æ·±åŒ–ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
