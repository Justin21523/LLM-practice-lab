{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503c4e4c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "å­¸ç¿’ç›®æ¨™ (Learning Objectives):\n",
    "â€¢ ç†è§£ RAG (Retrieval-Augmented Generation) æ¶æ§‹èˆ‡å·¥ä½œæµç¨‹\n",
    "â€¢ å¯¦ä½œåŸºæ–¼ FAISS çš„å‘é‡æª¢ç´¢ç³»çµ±\n",
    "â€¢ æ•´åˆæª¢ç´¢èˆ‡ç”Ÿæˆæ¨¡çµ„ï¼Œå»ºç«‹ç«¯åˆ°ç«¯å•ç­”ç³»çµ±\n",
    "â€¢ æ”¯æ´ä¸­æ–‡æ–‡ä»¶è™•ç†èˆ‡æª¢ç´¢å„ªåŒ–\n",
    "â€¢ æä¾›ä½é¡¯å­˜å‹å–„çš„æ¨¡å‹è¼‰å…¥é¸é …\n",
    "\n",
    "å‰ç½®éœ€æ±‚ (Prerequisites):\n",
    "â€¢ ç†Ÿæ‚‰ Transformer æ¨¡å‹åŸºæœ¬æ¦‚å¿µ\n",
    "â€¢ äº†è§£å‘é‡åµŒå…¥ (Embeddings) åŸç†\n",
    "â€¢ åŸºæœ¬çš„ Python æ–‡ä»¶è™•ç†ç¶“é©—\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a817d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb26_rag_basic_faiss.ipynb\n",
    "# åŸºç¤ RAG ç³»çµ±å¯¦ä½œï¼šFAISS å‘é‡æª¢ç´¢ + LLM ç”Ÿæˆ\n",
    "# =============================================================================\n",
    "# Cell 1: Shared Cache Bootstrap & Environment Setup\n",
    "# =============================================================================\n",
    "\n",
    "import os, pathlib, torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Shared cache setup (must be first in every notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "cache_paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "\n",
    "for key, path in cache_paths.items():\n",
    "    os.environ[key] = path\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf61079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Dependencies Installation & Imports\n",
    "# =============================================================================\n",
    "\n",
    "# Required packages (run once)\n",
    "\"\"\"\n",
    "pip install transformers sentence-transformers faiss-cpu langchain-text-splitters\n",
    "pip install PyPDF2 python-docx bitsandbytes accelerate\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Document processing\n",
    "import PyPDF2\n",
    "import docx\n",
    "from io import StringIO\n",
    "\n",
    "# ML libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Text processing\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"âœ… All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0c0f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Configuration & Model Settings\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"RAG system configuration\"\"\"\n",
    "\n",
    "    # Embedding model (optimized for Chinese)\n",
    "    embedding_model: str = \"BAAI/bge-m3\"  # Multilingual, good for Chinese\n",
    "    embedding_device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # LLM for generation\n",
    "    llm_model: str = \"Qwen/Qwen2.5-7B-Instruct\"  # Good Chinese support\n",
    "    llm_device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    use_4bit: bool = True  # Enable 4-bit quantization for low VRAM\n",
    "\n",
    "    # Text chunking\n",
    "    chunk_size: int = 512\n",
    "    chunk_overlap: int = 50\n",
    "\n",
    "    # FAISS settings\n",
    "    index_type: str = \"flat\"  # \"flat\" or \"ivf\"\n",
    "    nprobe: int = 10  # for IVF index\n",
    "\n",
    "    # Retrieval\n",
    "    top_k: int = 3\n",
    "    similarity_threshold: float = 0.7\n",
    "\n",
    "    # Generation\n",
    "    max_new_tokens: int = 512\n",
    "    temperature: float = 0.7\n",
    "    do_sample: bool = True\n",
    "\n",
    "\n",
    "config = RAGConfig()\n",
    "\n",
    "# Check VRAM and adjust settings\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    if gpu_memory < 12:\n",
    "        print(f\"âš ï¸  Low VRAM ({gpu_memory:.1f}GB), enabling aggressive optimization\")\n",
    "        config.use_4bit = True\n",
    "        config.llm_model = \"Qwen/Qwen2.5-7B-Instruct\"  # Smaller model\n",
    "    else:\n",
    "        print(f\"âœ… Sufficient VRAM ({gpu_memory:.1f}GB)\")\n",
    "\n",
    "print(f\"ğŸ“‹ RAG Configuration:\")\n",
    "print(f\"   Embedding: {config.embedding_model}\")\n",
    "print(f\"   LLM: {config.llm_model}\")\n",
    "print(f\"   4-bit quantization: {config.use_4bit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 4: Document Loader & Text Splitter\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class DocumentLoader:\n",
    "    \"\"\"Load documents from various formats\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_txt(file_path: str) -> str:\n",
    "        \"\"\"Load plain text file\"\"\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "\n",
    "    @staticmethod\n",
    "    def load_pdf(file_path: str) -> str:\n",
    "        \"\"\"Load PDF file\"\"\"\n",
    "        text = \"\"\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def load_docx(file_path: str) -> str:\n",
    "        \"\"\"Load Word document\"\"\"\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\"\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "        return text\n",
    "\n",
    "    @classmethod\n",
    "    def load_document(cls, file_path: str) -> str:\n",
    "        \"\"\"Auto-detect format and load document\"\"\"\n",
    "        path = Path(file_path)\n",
    "        suffix = path.suffix.lower()\n",
    "\n",
    "        if suffix == \".txt\":\n",
    "            return cls.load_txt(file_path)\n",
    "        elif suffix == \".pdf\":\n",
    "            return cls.load_pdf(file_path)\n",
    "        elif suffix in [\".docx\", \".doc\"]:\n",
    "            return cls.load_docx(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {suffix}\")\n",
    "\n",
    "\n",
    "class ChineseTextSplitter:\n",
    "    \"\"\"Text splitter optimized for Chinese documents\"\"\"\n",
    "\n",
    "    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50):\n",
    "        # Chinese-friendly separators\n",
    "        separators = [\n",
    "            \"ã€‚\",\n",
    "            \"ï¼\",\n",
    "            \"ï¼Ÿ\",\n",
    "            \"ï¼›\",\n",
    "            \"â€¦\",  # Chinese punctuation\n",
    "            \"\\n\\n\",\n",
    "            \"\\n\",  # Paragraphs\n",
    "            \".\",\n",
    "            \"!\",\n",
    "            \"?\",\n",
    "            \";\",  # English punctuation\n",
    "            \" \",  # Spaces\n",
    "        ]\n",
    "\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=separators,\n",
    "            keep_separator=True,\n",
    "        )\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks\"\"\"\n",
    "        chunks = self.splitter.split_text(text)\n",
    "\n",
    "        # Clean and filter chunks\n",
    "        cleaned_chunks = []\n",
    "        for chunk in chunks:\n",
    "            chunk = chunk.strip()\n",
    "            if len(chunk) > 20:  # Filter very short chunks\n",
    "                cleaned_chunks.append(chunk)\n",
    "\n",
    "        return cleaned_chunks\n",
    "\n",
    "\n",
    "# Test the document processing\n",
    "print(\"ğŸ“„ Document Processing Test:\")\n",
    "\n",
    "# Create sample documents for testing\n",
    "sample_texts = {\n",
    "    \"tech_doc.txt\": \"\"\"\n",
    "äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è¨ˆç®—æ©Ÿç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯ï¼Œè‡´åŠ›æ–¼å‰µå»ºèƒ½å¤ åŸ·è¡Œé€šå¸¸éœ€è¦äººé¡æ™ºèƒ½çš„ä»»å‹™çš„æ©Ÿå™¨ã€‚\n",
    "\n",
    "æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€è¡“ä¹‹ä¸€ã€‚å®ƒä½¿è¨ˆç®—æ©Ÿèƒ½å¤ åœ¨æ²’æœ‰æ˜ç¢ºç·¨ç¨‹çš„æƒ…æ³ä¸‹å­¸ç¿’å’Œæ”¹é€²ã€‚æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹å­é›†ï¼Œä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²çµ¡ä¾†æ¨¡æ“¬äººè…¦çš„æ±ºç­–éç¨‹ã€‚\n",
    "\n",
    "è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯AIçš„å¦ä¸€å€‹é‡è¦é ˜åŸŸï¼Œå°ˆæ³¨æ–¼ä½¿è¨ˆç®—æ©Ÿèƒ½å¤ ç†è§£ã€è§£é‡‹å’Œç”Ÿæˆäººé¡èªè¨€ã€‚æœ€è¿‘ï¼Œå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPTã€BERTç­‰åœ¨NLPé ˜åŸŸå–å¾—äº†çªç ´æ€§é€²å±•ã€‚\n",
    "\n",
    "æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç¨®çµåˆäº†ä¿¡æ¯æª¢ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„æŠ€è¡“ï¼Œèƒ½å¤ åŸºæ–¼å¤–éƒ¨çŸ¥è­˜åº«å›ç­”å•é¡Œï¼Œæé«˜äº†AIç³»çµ±çš„æº–ç¢ºæ€§å’Œå¯é æ€§ã€‚\n",
    "\"\"\",\n",
    "    \"business_doc.txt\": \"\"\"\n",
    "æ•¸ä½è½‰å‹å·²æˆç‚ºç¾ä»£ä¼æ¥­çš„å¿…ç„¶è¶¨å‹¢ã€‚éš¨è‘—é›²ç«¯è¨ˆç®—ã€å¤§æ•¸æ“šåˆ†æå’Œäººå·¥æ™ºèƒ½æŠ€è¡“çš„å¿«é€Ÿç™¼å±•ï¼Œä¼æ¥­éœ€è¦é‡æ–°æ€è€ƒå…¶ç‡Ÿé‹æ¨¡å¼ã€‚\n",
    "\n",
    "å®¢æˆ¶é«”é©—å„ªåŒ–æ˜¯æ•¸ä½è½‰å‹çš„æ ¸å¿ƒç›®æ¨™ä¹‹ä¸€ã€‚é€šéæ•¸æ“šé©…å‹•çš„æ´å¯Ÿï¼Œä¼æ¥­å¯ä»¥æ›´å¥½åœ°äº†è§£å®¢æˆ¶éœ€æ±‚ï¼Œæä¾›å€‹æ€§åŒ–çš„ç”¢å“å’Œæœå‹™ã€‚\n",
    "\n",
    "è‡ªå‹•åŒ–æµç¨‹èƒ½å¤ æé«˜ç‡Ÿé‹æ•ˆç‡ï¼Œæ¸›å°‘äººç‚ºéŒ¯èª¤ï¼Œä¸¦é™ä½æˆæœ¬ã€‚å¾å®¢æˆ¶æœå‹™èŠå¤©æ©Ÿå™¨äººåˆ°æ™ºèƒ½ä¾›æ‡‰éˆç®¡ç†ï¼Œè‡ªå‹•åŒ–æŠ€è¡“æ­£åœ¨å„å€‹æ¥­å‹™é ˜åŸŸç™¼æ®ä½œç”¨ã€‚\n",
    "\n",
    "æ•¸æ“šæ²»ç†å’Œå®‰å…¨æ€§åœ¨æ•¸ä½è½‰å‹ä¸­ä¹Ÿè‡³é—œé‡è¦ã€‚ä¼æ¥­å¿…é ˆå»ºç«‹å¼·å¥çš„è³‡æ–™ç®¡ç†åˆ¶åº¦ï¼Œç¢ºä¿å®¢æˆ¶è³‡æ–™çš„éš±ç§å’Œå®‰å…¨ã€‚\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "# Create sample files\n",
    "docs_dir = Path(\"sample_docs\")\n",
    "docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for filename, content in sample_texts.items():\n",
    "    with open(docs_dir / filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Test document loading and splitting\n",
    "loader = DocumentLoader()\n",
    "splitter = ChineseTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "\n",
    "for filename in sample_texts.keys():\n",
    "    file_path = docs_dir / filename\n",
    "    text = loader.load_document(str(file_path))\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    print(f\"\\nğŸ“„ {filename}:\")\n",
    "    print(f\"   Original length: {len(text)} chars\")\n",
    "    print(f\"   Number of chunks: {len(chunks)}\")\n",
    "    print(f\"   First chunk preview: {chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 5: Embedding Model & Vector Index\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class EmbeddingModel:\n",
    "    \"\"\"Wrapper for sentence transformer embedding model\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, device: str = \"cpu\"):\n",
    "        print(f\"ğŸ”„ Loading embedding model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.device = device\n",
    "\n",
    "        # Get embedding dimension\n",
    "        sample_embedding = self.model.encode([\"test\"])\n",
    "        self.embedding_dim = sample_embedding.shape[1]\n",
    "        print(f\"âœ… Embedding model loaded, dimension: {self.embedding_dim}\")\n",
    "\n",
    "    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"Encode texts to embeddings\"\"\"\n",
    "        embeddings = self.model.encode(\n",
    "            texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True\n",
    "        )\n",
    "        return embeddings.astype(np.float32)\n",
    "\n",
    "\n",
    "class FAISSIndex:\n",
    "    \"\"\"FAISS vector index for similarity search\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, index_type: str = \"flat\"):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.index_type = index_type\n",
    "        self.texts = []  # Store original texts\n",
    "        self.metadata = []  # Store metadata\n",
    "\n",
    "        # Create FAISS index\n",
    "        if index_type == \"flat\":\n",
    "            self.index = faiss.IndexFlatIP(\n",
    "                embedding_dim\n",
    "            )  # Inner product (cosine similarity)\n",
    "        elif index_type == \"ivf\":\n",
    "            # IVF index for large datasets\n",
    "            nlist = 100  # number of clusters\n",
    "            quantizer = faiss.IndexFlatIP(embedding_dim)\n",
    "            self.index = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported index type: {index_type}\")\n",
    "\n",
    "        print(f\"âœ… FAISS index created: {index_type}, dimension: {embedding_dim}\")\n",
    "\n",
    "    def add_documents(\n",
    "        self, texts: List[str], embeddings: np.ndarray, metadata: List[Dict] = None\n",
    "    ):\n",
    "        \"\"\"Add documents to the index\"\"\"\n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "\n",
    "        if self.index_type == \"ivf\" and not self.index.is_trained:\n",
    "            print(\"ğŸ”„ Training IVF index...\")\n",
    "            self.index.train(embeddings)\n",
    "\n",
    "        # Add to index\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "        # Store texts and metadata\n",
    "        self.texts.extend(texts)\n",
    "        if metadata:\n",
    "            self.metadata.extend(metadata)\n",
    "        else:\n",
    "            self.metadata.extend([{\"chunk_id\": i} for i in range(len(texts))])\n",
    "\n",
    "        print(f\"âœ… Added {len(texts)} documents to index (total: {self.index.ntotal})\")\n",
    "\n",
    "    def search(\n",
    "        self, query_embedding: np.ndarray, k: int = 5, threshold: float = 0.7\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        # Normalize query embedding\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype(np.float32)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "\n",
    "        # Set nprobe for IVF index\n",
    "        if self.index_type == \"ivf\":\n",
    "            self.index.nprobe = min(10, self.index.nlist)\n",
    "\n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "\n",
    "        # Format results\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx != -1 and score >= threshold:  # Valid result above threshold\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"text\": self.texts[idx],\n",
    "                        \"score\": float(score),\n",
    "                        \"metadata\": self.metadata[idx],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Save index to disk\"\"\"\n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, f\"{filepath}.faiss\")\n",
    "\n",
    "        # Save texts and metadata\n",
    "        with open(f\"{filepath}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    \"texts\": self.texts,\n",
    "                    \"metadata\": self.metadata,\n",
    "                    \"embedding_dim\": self.embedding_dim,\n",
    "                    \"index_type\": self.index_type,\n",
    "                },\n",
    "                f,\n",
    "            )\n",
    "\n",
    "        print(f\"âœ… Index saved to {filepath}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filepath: str):\n",
    "        \"\"\"Load index from disk\"\"\"\n",
    "        # Load FAISS index\n",
    "        index = faiss.read_index(f\"{filepath}.faiss\")\n",
    "\n",
    "        # Load texts and metadata\n",
    "        with open(f\"{filepath}.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        # Create instance\n",
    "        instance = cls(data[\"embedding_dim\"], data[\"index_type\"])\n",
    "        instance.index = index\n",
    "        instance.texts = data[\"texts\"]\n",
    "        instance.metadata = data[\"metadata\"]\n",
    "\n",
    "        print(f\"âœ… Index loaded from {filepath}\")\n",
    "        return instance\n",
    "\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = EmbeddingModel(\n",
    "    model_name=config.embedding_model, device=config.embedding_device\n",
    ")\n",
    "\n",
    "# Create FAISS index\n",
    "vector_index = FAISSIndex(\n",
    "    embedding_dim=embedding_model.embedding_dim, index_type=config.index_type\n",
    ")\n",
    "\n",
    "print(\"ğŸ” Vector Index Initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf389a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 6: Build Document Index\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def build_document_index(doc_paths: List[str], save_path: str = None) -> FAISSIndex:\n",
    "    \"\"\"Build FAISS index from documents\"\"\"\n",
    "\n",
    "    print(\"ğŸ”„ Building document index...\")\n",
    "\n",
    "    all_texts = []\n",
    "    all_metadata = []\n",
    "\n",
    "    # Process each document\n",
    "    for doc_path in doc_paths:\n",
    "        print(f\"ğŸ“„ Processing: {doc_path}\")\n",
    "\n",
    "        # Load document\n",
    "        text = DocumentLoader.load_document(doc_path)\n",
    "\n",
    "        # Split into chunks\n",
    "        splitter = ChineseTextSplitter(\n",
    "            chunk_size=config.chunk_size, chunk_overlap=config.chunk_overlap\n",
    "        )\n",
    "        chunks = splitter.split_text(text)\n",
    "\n",
    "        # Create metadata for each chunk\n",
    "        doc_name = Path(doc_path).name\n",
    "        metadata = [\n",
    "            {\"source\": doc_name, \"chunk_id\": i, \"chunk_size\": len(chunk)}\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "\n",
    "        all_texts.extend(chunks)\n",
    "        all_metadata.extend(metadata)\n",
    "\n",
    "        print(f\"   ğŸ“ {len(chunks)} chunks extracted\")\n",
    "\n",
    "    print(f\"ğŸ“Š Total chunks: {len(all_texts)}\")\n",
    "\n",
    "    # Generate embeddings\n",
    "    print(\"ğŸ”„ Generating embeddings...\")\n",
    "    embeddings = embedding_model.encode(all_texts)\n",
    "\n",
    "    # Add to index\n",
    "    vector_index.add_documents(all_texts, embeddings, all_metadata)\n",
    "\n",
    "    # Save index if path provided\n",
    "    if save_path:\n",
    "        vector_index.save(save_path)\n",
    "\n",
    "    print(\"âœ… Document index built successfully\")\n",
    "    return vector_index\n",
    "\n",
    "\n",
    "# Build index from sample documents\n",
    "doc_files = list(docs_dir.glob(\"*.txt\"))\n",
    "doc_paths = [str(path) for path in doc_files]\n",
    "\n",
    "index_save_path = \"rag_index\"\n",
    "vector_index = build_document_index(doc_paths, index_save_path)\n",
    "\n",
    "# Test retrieval\n",
    "print(\"\\nğŸ” Testing Retrieval:\")\n",
    "test_queries = [\n",
    "    \"ä»€éº¼æ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\",\n",
    "    \"æ©Ÿå™¨å­¸ç¿’å’Œæ·±åº¦å­¸ç¿’çš„é—œä¿‚\",\n",
    "    \"ä¼æ¥­æ•¸ä½è½‰å‹çš„é‡é»\",\n",
    "    \"How does RAG work?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nâ“ Query: {query}\")\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    results = vector_index.search(query_embedding, k=2, threshold=0.3)\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"   {i}. [Score: {result['score']:.3f}] {result['text'][:100]}...\")\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 7: LLM Generation Module\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class LLMGenerator:\n",
    "    \"\"\"LLM for answer generation with low-VRAM optimization\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, device: str = \"auto\", use_4bit: bool = True):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "\n",
    "        print(f\"ğŸ”„ Loading LLM: {model_name}\")\n",
    "\n",
    "        # Configure quantization for low VRAM\n",
    "        if use_4bit and torch.cuda.is_available():\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            print(\"   ğŸ”§ Using 4-bit quantization\")\n",
    "        else:\n",
    "            quantization_config = None\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Load model\n",
    "        try:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\" if device == \"auto\" else None,\n",
    "                torch_dtype=(\n",
    "                    torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "                ),\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "\n",
    "            if device != \"auto\":\n",
    "                self.model = self.model.to(device)\n",
    "\n",
    "            print(f\"âœ… LLM loaded successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to load LLM: {e}\")\n",
    "            print(\"ğŸ’¡ Trying CPU fallback...\")\n",
    "\n",
    "            # CPU fallback without quantization\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float32,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            self.model = self.model.to(\"cpu\")\n",
    "            print(\"âœ… LLM loaded on CPU\")\n",
    "\n",
    "    def generate_answer(\n",
    "        self, query: str, context: str, max_new_tokens: int = 512\n",
    "    ) -> str:\n",
    "        \"\"\"Generate answer based on query and retrieved context\"\"\"\n",
    "\n",
    "        # Create prompt\n",
    "        prompt = self._create_prompt(query, context)\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, return_tensors=\"pt\", truncation=True, max_length=2048\n",
    "        )\n",
    "\n",
    "        # Move to device\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=config.temperature,\n",
    "                do_sample=config.do_sample,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][len(inputs[\"input_ids\"][0]) :], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return response.strip()\n",
    "\n",
    "    def _create_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Create prompt for answer generation\"\"\"\n",
    "\n",
    "        prompt = f\"\"\"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å•ç­”åŠ©æ‰‹ã€‚è«‹æ ¹æ“šæä¾›çš„èƒŒæ™¯è³‡æ–™å›ç­”ç”¨æˆ¶å•é¡Œã€‚\n",
    "\n",
    "èƒŒæ™¯è³‡æ–™ï¼š\n",
    "{context}\n",
    "\n",
    "ç”¨æˆ¶å•é¡Œï¼š{query}\n",
    "\n",
    "è«‹æ ¹æ“šèƒŒæ™¯è³‡æ–™æä¾›æº–ç¢ºã€è©³ç´°çš„å›ç­”ã€‚å¦‚æœèƒŒæ™¯è³‡æ–™ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹èª å¯¦èªªæ˜ã€‚\n",
    "\n",
    "å›ç­”ï¼š\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "\n",
    "# Initialize LLM (with error handling for low VRAM)\n",
    "try:\n",
    "    llm_generator = LLMGenerator(\n",
    "        model_name=config.llm_model,\n",
    "        device=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
    "        use_4bit=config.use_4bit,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Primary model failed: {e}\")\n",
    "    print(\"ğŸ”„ Trying smaller backup model...\")\n",
    "\n",
    "    # Fallback to smaller model\n",
    "    config.llm_model = \"microsoft/DialoGPT-medium\"  # Much smaller fallback\n",
    "    llm_generator = LLMGenerator(\n",
    "        model_name=config.llm_model,\n",
    "        device=\"cpu\",  # Force CPU for stability\n",
    "        use_4bit=False,\n",
    "    )\n",
    "\n",
    "print(\"ğŸ¤– LLM Generator Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d408ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 8: RAG System Integration\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"Complete RAG system combining retrieval and generation\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: EmbeddingModel,\n",
    "        vector_index: FAISSIndex,\n",
    "        llm_generator: LLMGenerator,\n",
    "        config: RAGConfig,\n",
    "    ):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.vector_index = vector_index\n",
    "        self.llm_generator = llm_generator\n",
    "        self.config = config\n",
    "\n",
    "        print(\"ğŸ¯ RAG System Initialized\")\n",
    "\n",
    "    def query(self, question: str, return_sources: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Process query through RAG pipeline\"\"\"\n",
    "\n",
    "        print(f\"ğŸ” Processing query: {question}\")\n",
    "\n",
    "        # Step 1: Retrieve relevant documents\n",
    "        query_embedding = self.embedding_model.encode([question])\n",
    "        retrieved_docs = self.vector_index.search(\n",
    "            query_embedding,\n",
    "            k=self.config.top_k,\n",
    "            threshold=self.config.similarity_threshold,\n",
    "        )\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"answer\": \"æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šä¾†å›ç­”æ‚¨çš„å•é¡Œã€‚\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0,\n",
    "            }\n",
    "\n",
    "        print(f\"   ğŸ“š Retrieved {len(retrieved_docs)} relevant documents\")\n",
    "\n",
    "        # Step 2: Prepare context\n",
    "        context_pieces = []\n",
    "        for doc in retrieved_docs:\n",
    "            context_pieces.append(f\"[ä¾†æº: {doc['metadata']['source']}]\\n{doc['text']}\")\n",
    "\n",
    "        context = \"\\n\\n\".join(context_pieces)\n",
    "\n",
    "        # Step 3: Generate answer\n",
    "        print(\"ğŸ¤– Generating answer...\")\n",
    "        answer = self.llm_generator.generate_answer(\n",
    "            query=question, context=context, max_new_tokens=self.config.max_new_tokens\n",
    "        )\n",
    "\n",
    "        # Step 4: Calculate confidence score\n",
    "        avg_score = sum(doc[\"score\"] for doc in retrieved_docs) / len(retrieved_docs)\n",
    "\n",
    "        result = {\n",
    "            \"answer\": answer,\n",
    "            \"confidence\": float(avg_score),\n",
    "            \"num_sources\": len(retrieved_docs),\n",
    "        }\n",
    "\n",
    "        if return_sources:\n",
    "            result[\"sources\"] = [\n",
    "                {\n",
    "                    \"text\": doc[\"text\"],\n",
    "                    \"source\": doc[\"metadata\"][\"source\"],\n",
    "                    \"score\": doc[\"score\"],\n",
    "                }\n",
    "                for doc in retrieved_docs\n",
    "            ]\n",
    "\n",
    "        print(f\"âœ… Answer generated (confidence: {avg_score:.3f})\")\n",
    "        return result\n",
    "\n",
    "    def batch_query(self, questions: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process multiple queries\"\"\"\n",
    "        results = []\n",
    "        for question in questions:\n",
    "            result = self.query(question)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize complete RAG system\n",
    "rag_system = RAGSystem(\n",
    "    embedding_model=embedding_model,\n",
    "    vector_index=vector_index,\n",
    "    llm_generator=llm_generator,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"ğŸ¯ Complete RAG System Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 9: Interactive Demo & Testing\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def demo_rag_qa():\n",
    "    \"\"\"Interactive demo of RAG question-answering\"\"\"\n",
    "\n",
    "    print(\"ğŸ® RAG System Demo\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    test_questions = [\n",
    "        \"ä»€éº¼æ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè«‹è©³ç´°èªªæ˜ã€‚\",\n",
    "        \"æ©Ÿå™¨å­¸ç¿’å’Œæ·±åº¦å­¸ç¿’æœ‰ä»€éº¼å€åˆ¥ï¼Ÿ\",\n",
    "        \"RAGæŠ€è¡“æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ\",\n",
    "        \"ä¼æ¥­æ•¸ä½è½‰å‹éœ€è¦æ³¨æ„å“ªäº›é‡é»ï¼Ÿ\",\n",
    "        \"ä»€éº¼æ˜¯è‡ªç„¶èªè¨€è™•ç†ï¼Ÿ\",\n",
    "        \"æ•¸æ“šæ²»ç†çš„é‡è¦æ€§æ˜¯ä»€éº¼ï¼Ÿ\",\n",
    "    ]\n",
    "\n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\nğŸ“ Question {i}: {question}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Get RAG response\n",
    "        result = rag_system.query(question)\n",
    "\n",
    "        # Display answer\n",
    "        print(f\"ğŸ¤– Answer (Confidence: {result['confidence']:.3f}):\")\n",
    "        print(f\"{result['answer']}\")\n",
    "\n",
    "        # Display sources\n",
    "        if result.get(\"sources\"):\n",
    "            print(f\"\\nğŸ“š Sources ({result['num_sources']}):\")\n",
    "            for j, source in enumerate(result[\"sources\"], 1):\n",
    "                print(f\"   {j}. [{source['source']}] (Score: {source['score']:.3f})\")\n",
    "                print(f\"      {source['text'][:100]}...\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "# Run the demo\n",
    "demo_rag_qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 10: Performance Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def evaluate_rag_performance():\n",
    "    \"\"\"Evaluate RAG system performance\"\"\"\n",
    "\n",
    "    print(\"ğŸ“Š RAG Performance Evaluation\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Test queries with expected content\n",
    "    eval_queries = [\n",
    "        {\n",
    "            \"question\": \"ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ\",\n",
    "            \"expected_keywords\": [\"æ©Ÿå™¨å­¸ç¿’\", \"å­¸ç¿’\", \"ç®—æ³•\", \"æ•¸æ“š\"],\n",
    "            \"category\": \"AIåŸºç¤\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"æ·±åº¦å­¸ç¿’çš„ç‰¹é»ï¼Ÿ\",\n",
    "            \"expected_keywords\": [\"æ·±åº¦å­¸ç¿’\", \"ç¥ç¶“ç¶²çµ¡\", \"å¤šå±¤\"],\n",
    "            \"category\": \"AIæŠ€è¡“\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"ä¼æ¥­æ•¸ä½è½‰å‹çš„ç›®æ¨™ï¼Ÿ\",\n",
    "            \"expected_keywords\": [\"æ•¸ä½è½‰å‹\", \"å®¢æˆ¶é«”é©—\", \"è‡ªå‹•åŒ–\"],\n",
    "            \"category\": \"å•†æ¥­æ‡‰ç”¨\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"ä»€éº¼æ˜¯NLPï¼Ÿ\",\n",
    "            \"expected_keywords\": [\"è‡ªç„¶èªè¨€è™•ç†\", \"NLP\", \"èªè¨€\", \"è¨ˆç®—æ©Ÿ\"],\n",
    "            \"category\": \"AIé ˜åŸŸ\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for eval_item in eval_queries:\n",
    "        question = eval_item[\"question\"]\n",
    "        expected_keywords = eval_item[\"expected_keywords\"]\n",
    "\n",
    "        # Get RAG response\n",
    "        result = rag_system.query(question, return_sources=False)\n",
    "        answer = result[\"answer\"]\n",
    "        confidence = result[\"confidence\"]\n",
    "\n",
    "        # Calculate keyword coverage\n",
    "        found_keywords = 0\n",
    "        for keyword in expected_keywords:\n",
    "            if keyword in answer:\n",
    "                found_keywords += 1\n",
    "\n",
    "        keyword_coverage = found_keywords / len(expected_keywords)\n",
    "\n",
    "        # Store evaluation result\n",
    "        eval_result = {\n",
    "            \"question\": question,\n",
    "            \"category\": eval_item[\"category\"],\n",
    "            \"answer_length\": len(answer),\n",
    "            \"confidence\": confidence,\n",
    "            \"keyword_coverage\": keyword_coverage,\n",
    "            \"keywords_found\": found_keywords,\n",
    "            \"keywords_total\": len(expected_keywords),\n",
    "        }\n",
    "\n",
    "        results.append(eval_result)\n",
    "\n",
    "        print(f\"â“ {question}\")\n",
    "        print(f\"   ğŸ“Š Confidence: {confidence:.3f}\")\n",
    "        print(\n",
    "            f\"   ğŸ¯ Keyword Coverage: {keyword_coverage:.3f} ({found_keywords}/{len(expected_keywords)})\"\n",
    "        )\n",
    "        print(f\"   ğŸ“ Answer Length: {len(answer)} chars\")\n",
    "        print()\n",
    "\n",
    "    # Overall statistics\n",
    "    avg_confidence = np.mean([r[\"confidence\"] for r in results])\n",
    "    avg_coverage = np.mean([r[\"keyword_coverage\"] for r in results])\n",
    "    avg_length = np.mean([r[\"answer_length\"] for r in results])\n",
    "\n",
    "    print(\"ğŸ“ˆ Overall Performance:\")\n",
    "    print(f\"   Average Confidence: {avg_confidence:.3f}\")\n",
    "    print(f\"   Average Keyword Coverage: {avg_coverage:.3f}\")\n",
    "    print(f\"   Average Answer Length: {avg_length:.0f} chars\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_rag_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 11: System Resource Monitoring\n",
    "# =============================================================================\n",
    "\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "\n",
    "def monitor_system_resources(duration: int = 5):\n",
    "    \"\"\"Monitor system resources during RAG operations\"\"\"\n",
    "\n",
    "    print(\"ğŸ” System Resource Monitoring\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    # Initial resource check\n",
    "    initial_memory = psutil.virtual_memory()\n",
    "    initial_gpu_memory = None\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        initial_gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "\n",
    "    print(f\"ğŸ“Š Initial State:\")\n",
    "    print(f\"   CPU Usage: {psutil.cpu_percent()}%\")\n",
    "    print(\n",
    "        f\"   RAM Usage: {initial_memory.percent}% ({initial_memory.used/1024**3:.1f}GB)\"\n",
    "    )\n",
    "\n",
    "    if initial_gpu_memory is not None:\n",
    "        print(f\"   GPU Memory: {initial_gpu_memory:.1f}GB\")\n",
    "\n",
    "    # Test with multiple queries\n",
    "    test_queries = [\n",
    "        \"è§£é‡‹äººå·¥æ™ºèƒ½çš„å®šç¾©å’Œæ‡‰ç”¨\",\n",
    "        \"æè¿°æ©Ÿå™¨å­¸ç¿’çš„å·¥ä½œåŸç†\",\n",
    "        \"ä¼æ¥­å¦‚ä½•å¯¦æ–½æ•¸ä½è½‰å‹ç­–ç•¥\",\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\nğŸ”„ Processing Query {i+1}: {query[:30]}...\")\n",
    "\n",
    "        query_start = time.time()\n",
    "        result = rag_system.query(query, return_sources=False)\n",
    "        query_time = time.time() - query_start\n",
    "\n",
    "        # Check resources after query\n",
    "        current_memory = psutil.virtual_memory()\n",
    "        current_gpu_memory = None\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            current_gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "\n",
    "        print(f\"   â±ï¸  Query Time: {query_time:.2f}s\")\n",
    "        print(\n",
    "            f\"   ğŸ§  RAM: {current_memory.percent}% (+{current_memory.percent - initial_memory.percent:.1f}%)\"\n",
    "        )\n",
    "\n",
    "        if current_gpu_memory is not None:\n",
    "            gpu_diff = current_gpu_memory - initial_gpu_memory\n",
    "            print(f\"   ğŸ® GPU: {current_gpu_memory:.1f}GB (+{gpu_diff:.1f}GB)\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nâ±ï¸  Total Processing Time: {total_time:.2f}s\")\n",
    "    print(f\"ğŸ“Š Average Query Time: {total_time/len(test_queries):.2f}s\")\n",
    "\n",
    "\n",
    "# Monitor resources\n",
    "monitor_system_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7efb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 12: Advanced Features & Extensions\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class AdvancedRAGFeatures:\n",
    "    \"\"\"Advanced features for RAG system enhancement\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def rerank_results(\n",
    "        query: str, documents: List[Dict], model_name: str = \"BAAI/bge-reranker-base\"\n",
    "    ):\n",
    "        \"\"\"Re-rank retrieved documents using a reranker model\"\"\"\n",
    "        try:\n",
    "            from sentence_transformers import CrossEncoder\n",
    "\n",
    "            print(\"ğŸ”„ Loading reranker model...\")\n",
    "            reranker = CrossEncoder(model_name)\n",
    "\n",
    "            # Prepare query-document pairs\n",
    "            query_doc_pairs = [[query, doc[\"text\"]] for doc in documents]\n",
    "\n",
    "            # Get reranking scores\n",
    "            rerank_scores = reranker.predict(query_doc_pairs)\n",
    "\n",
    "            # Update scores and re-sort\n",
    "            for i, doc in enumerate(documents):\n",
    "                doc[\"rerank_score\"] = float(rerank_scores[i])\n",
    "\n",
    "            # Sort by rerank score\n",
    "            reranked_docs = sorted(\n",
    "                documents, key=lambda x: x[\"rerank_score\"], reverse=True\n",
    "            )\n",
    "\n",
    "            print(f\"âœ… Reranked {len(documents)} documents\")\n",
    "            return reranked_docs\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"âš ï¸  Reranker not available, using original ranking\")\n",
    "            return documents\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_citations(answer: str, sources: List[Dict]) -> str:\n",
    "        \"\"\"Add citation markers to the answer\"\"\"\n",
    "\n",
    "        # Simple citation system\n",
    "        cited_answer = answer\n",
    "\n",
    "        for i, source in enumerate(sources, 1):\n",
    "            source_name = source[\"source\"]\n",
    "            if source_name in answer or any(\n",
    "                word in answer for word in source[\"text\"].split()[:5]\n",
    "            ):\n",
    "                citation = f\"[{i}]\"\n",
    "                cited_answer += f\"\\n\\nåƒè€ƒè³‡æ–™ {citation}: {source_name}\"\n",
    "\n",
    "        return cited_answer\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_semantic_similarity(\n",
    "        query: str, answer: str, embedding_model: EmbeddingModel\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate semantic similarity between query and answer\"\"\"\n",
    "\n",
    "        query_emb = embedding_model.encode([query])\n",
    "        answer_emb = embedding_model.encode([answer])\n",
    "\n",
    "        # Cosine similarity\n",
    "        similarity = np.dot(query_emb[0], answer_emb[0]) / (\n",
    "            np.linalg.norm(query_emb[0]) * np.linalg.norm(answer_emb[0])\n",
    "        )\n",
    "\n",
    "        return float(similarity)\n",
    "\n",
    "\n",
    "# Test advanced features\n",
    "print(\"ğŸ”¬ Testing Advanced Features\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test query\n",
    "test_query = \"ä»€éº¼æ˜¯æ·±åº¦å­¸ç¿’ï¼Ÿå®ƒèˆ‡æ©Ÿå™¨å­¸ç¿’æœ‰ä»€éº¼é—œä¿‚ï¼Ÿ\"\n",
    "result = rag_system.query(test_query)\n",
    "\n",
    "# Test reranking (optional - requires additional model)\n",
    "try:\n",
    "    enhanced_sources = AdvancedRAGFeatures.rerank_results(test_query, result[\"sources\"])\n",
    "    print(f\"ğŸ“ˆ Reranking improved relevance scores\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Reranking skipped: {e}\")\n",
    "    enhanced_sources = result[\"sources\"]\n",
    "\n",
    "# Test citation extraction\n",
    "cited_answer = AdvancedRAGFeatures.extract_citations(result[\"answer\"], enhanced_sources)\n",
    "\n",
    "# Test semantic similarity\n",
    "semantic_sim = AdvancedRAGFeatures.calculate_semantic_similarity(\n",
    "    test_query, result[\"answer\"], embedding_model\n",
    ")\n",
    "\n",
    "print(f\"ğŸ¯ Semantic Similarity (Query-Answer): {semantic_sim:.3f}\")\n",
    "print(f\"ğŸ“ Answer with Citations:\\n{cited_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 13: Smoke Test & Validation\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def smoke_test():\n",
    "    \"\"\"Comprehensive smoke test for RAG system\"\"\"\n",
    "\n",
    "    print(\"ğŸ§ª RAG System Smoke Test\")\n",
    "    print(\"=\" * 25)\n",
    "\n",
    "    tests = []\n",
    "\n",
    "    # Test 1: Basic functionality\n",
    "    try:\n",
    "        test_query = \"ä»€éº¼æ˜¯AIï¼Ÿ\"\n",
    "        result = rag_system.query(test_query)\n",
    "\n",
    "        assert \"answer\" in result\n",
    "        assert \"confidence\" in result\n",
    "        assert len(result[\"answer\"]) > 10\n",
    "\n",
    "        tests.append((\"âœ… Basic query processing\", True))\n",
    "\n",
    "    except Exception as e:\n",
    "        tests.append((\"âŒ Basic query processing\", False, str(e)))\n",
    "\n",
    "    # Test 2: Empty query handling\n",
    "    try:\n",
    "        empty_result = rag_system.query(\"\")\n",
    "        tests.append((\"âœ… Empty query handling\", True))\n",
    "\n",
    "    except Exception as e:\n",
    "        tests.append((\"âŒ Empty query handling\", False, str(e)))\n",
    "\n",
    "    # Test 3: Index persistence\n",
    "    try:\n",
    "        # Save and reload index\n",
    "        test_index_path = \"test_index\"\n",
    "        vector_index.save(test_index_path)\n",
    "\n",
    "        # Try to load\n",
    "        loaded_index = FAISSIndex.load(test_index_path)\n",
    "\n",
    "        assert loaded_index.index.ntotal == vector_index.index.ntotal\n",
    "        tests.append((\"âœ… Index save/load\", True))\n",
    "\n",
    "    except Exception as e:\n",
    "        tests.append((\"âŒ Index save/load\", False, str(e)))\n",
    "\n",
    "    # Test 4: Memory efficiency\n",
    "    try:\n",
    "        import psutil\n",
    "\n",
    "        process = psutil.Process()\n",
    "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "        # Should use reasonable amount of memory (< 8GB for this demo)\n",
    "        memory_ok = memory_mb < 8000\n",
    "\n",
    "        tests.append((f\"âœ… Memory usage: {memory_mb:.0f}MB\", memory_ok))\n",
    "\n",
    "    except Exception as e:\n",
    "        tests.append((\"âŒ Memory monitoring\", False, str(e)))\n",
    "\n",
    "    # Test 5: Multi-query batch processing\n",
    "    try:\n",
    "        batch_queries = [\"AIæ˜¯ä»€éº¼ï¼Ÿ\", \"æ©Ÿå™¨å­¸ç¿’çš„æ‡‰ç”¨\", \"æ·±åº¦å­¸ç¿’åŸç†\"]\n",
    "        batch_results = rag_system.batch_query(batch_queries)\n",
    "\n",
    "        assert len(batch_results) == len(batch_queries)\n",
    "        tests.append((\"âœ… Batch query processing\", True))\n",
    "\n",
    "    except Exception as e:\n",
    "        tests.append((\"âŒ Batch query processing\", False, str(e)))\n",
    "\n",
    "    # Print test results\n",
    "    passed = 0\n",
    "    for test_result in tests:\n",
    "        if len(test_result) == 2:\n",
    "            name, success = test_result\n",
    "            print(name)\n",
    "            if success:\n",
    "                passed += 1\n",
    "        else:\n",
    "            name, success, error = test_result\n",
    "            print(f\"{name}: {error}\")\n",
    "\n",
    "    print(f\"\\nğŸ“Š Test Results: {passed}/{len(tests)} passed\")\n",
    "\n",
    "    if passed == len(tests):\n",
    "        print(\"ğŸ‰ All tests passed! RAG system is working correctly.\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Some tests failed. Please check the errors above.\")\n",
    "\n",
    "    return passed == len(tests)\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "all_tests_passed = smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ab441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 14: Usage Examples & Best Practices\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def demonstrate_best_practices():\n",
    "    \"\"\"Demonstrate RAG system best practices\"\"\"\n",
    "\n",
    "    print(\"ğŸ’¡ RAG System Best Practices\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    print(\"1. ğŸ“š Document Preparation:\")\n",
    "    print(\"   â€¢ Use consistent formatting and clean text\")\n",
    "    print(\"   â€¢ Optimal chunk size: 256-1024 tokens\")\n",
    "    print(\"   â€¢ Include relevant metadata (source, date, author)\")\n",
    "    print(\"   â€¢ Remove duplicate content\")\n",
    "\n",
    "    print(\"\\n2. ğŸ” Retrieval Optimization:\")\n",
    "    print(\"   â€¢ Adjust similarity threshold based on domain\")\n",
    "    print(\"   â€¢ Use reranking for better relevance\")\n",
    "    print(\"   â€¢ Consider hybrid search (keyword + semantic)\")\n",
    "    print(\"   â€¢ Monitor retrieval quality metrics\")\n",
    "\n",
    "    print(\"\\n3. ğŸ¤– Generation Quality:\")\n",
    "    print(\"   â€¢ Craft clear, context-aware prompts\")\n",
    "    print(\"   â€¢ Limit context length to avoid truncation\")\n",
    "    print(\"   â€¢ Include source attribution\")\n",
    "    print(\"   â€¢ Validate factual accuracy\")\n",
    "\n",
    "    print(\"\\n4. âš¡ Performance Optimization:\")\n",
    "    print(\"   â€¢ Use quantization for large models (4-bit/8-bit)\")\n",
    "    print(\"   â€¢ Cache embeddings for frequent queries\")\n",
    "    print(\"   â€¢ Batch process when possible\")\n",
    "    print(\"   â€¢ Monitor GPU/CPU usage\")\n",
    "\n",
    "    print(\"\\n5. ğŸ›¡ï¸ Safety & Quality:\")\n",
    "    print(\"   â€¢ Validate retrieved content quality\")\n",
    "    print(\"   â€¢ Handle edge cases (no results, low confidence)\")\n",
    "    print(\"   â€¢ Implement content filtering if needed\")\n",
    "    print(\"   â€¢ Log queries for analysis\")\n",
    "\n",
    "\n",
    "# Example of production-ready query function\n",
    "def production_rag_query(question: str, min_confidence: float = 0.5) -> Dict[str, Any]:\n",
    "    \"\"\"Production-ready RAG query with safety checks\"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    if not question or len(question.strip()) < 3:\n",
    "        return {\n",
    "            \"answer\": \"è«‹æä¾›æ›´è©³ç´°çš„å•é¡Œã€‚\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"status\": \"invalid_input\",\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        # Get RAG response\n",
    "        result = rag_system.query(question)\n",
    "\n",
    "        # Quality checks\n",
    "        if result[\"confidence\"] < min_confidence:\n",
    "            return {\n",
    "                \"answer\": \"æŠ±æ­‰ï¼Œæˆ‘å°é€™å€‹å•é¡Œçš„ä¿¡å¿ƒåº¦ä¸å¤ é«˜ï¼Œå»ºè­°æ‚¨é‡æ–°è¡¨é”å•é¡Œæˆ–è«®è©¢å°ˆå®¶ã€‚\",\n",
    "                \"confidence\": result[\"confidence\"],\n",
    "                \"status\": \"low_confidence\",\n",
    "            }\n",
    "\n",
    "        # Success\n",
    "        result[\"status\"] = \"success\"\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"answer\": \"ç³»çµ±æš«æ™‚ç„¡æ³•è™•ç†æ‚¨çš„å•é¡Œï¼Œè«‹ç¨å¾Œå†è©¦ã€‚\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "\n",
    "\n",
    "demonstrate_best_practices()\n",
    "\n",
    "# Test production query function\n",
    "print(\"\\nğŸ”¬ Testing Production Query Function:\")\n",
    "test_cases = [\n",
    "    \"ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ\",  # Normal query\n",
    "    \"\",  # Empty query\n",
    "    \"xyz123random\",  # Likely low confidence\n",
    "]\n",
    "\n",
    "for query in test_cases:\n",
    "    result = production_rag_query(query)\n",
    "    print(f\"Query: '{query}' -> Status: {result['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e771561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 15: Summary & Next Steps\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ“‹ RAG Basic System - å­¸ç¿’å®Œæˆç¸½çµ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"âœ… å®Œæˆé …ç›® (Completed Items):\")\n",
    "print(\"   â€¢ ğŸ”§ ç’°å¢ƒè¨­ç½®èˆ‡å…±äº«å¿«å–é…ç½®\")\n",
    "print(\"   â€¢ ğŸ“„ å¤šæ ¼å¼æ–‡ä»¶è¼‰å…¥å™¨ (TXT/PDF/DOCX)\")\n",
    "print(\"   â€¢ âœ‚ï¸  ä¸­æ–‡å‹å–„çš„æ–‡æœ¬åˆ†æ®µå™¨\")\n",
    "print(\"   â€¢ ğŸ§  å¤šèªè¨€åµŒå…¥æ¨¡å‹æ•´åˆ (BGE-M3)\")\n",
    "print(\"   â€¢ ğŸ” FAISS å‘é‡ç´¢å¼•å»ºç«‹èˆ‡æª¢ç´¢\")\n",
    "print(\"   â€¢ ğŸ¤– ä½é¡¯å­˜ LLM ç”Ÿæˆå™¨ (4bité‡åŒ–)\")\n",
    "print(\"   â€¢ ğŸ¯ ç«¯åˆ°ç«¯ RAG ç³»çµ±æ•´åˆ\")\n",
    "print(\"   â€¢ ğŸ“Š æ€§èƒ½è©•ä¼°èˆ‡è³‡æºç›£æ§\")\n",
    "print(\"   â€¢ ğŸ§ª å…¨é¢çš„ç…™éœ§æ¸¬è©¦\")\n",
    "\n",
    "print(\"\\nğŸ”‘ æ ¸å¿ƒæ¦‚å¿µ (Core Concepts):\")\n",
    "print(\"   â€¢ RAG = Retrieval (æª¢ç´¢) + Augmented (å¢å¼·) + Generation (ç”Ÿæˆ)\")\n",
    "print(\"   â€¢ å‘é‡åµŒå…¥å°‡æ–‡æœ¬è½‰ç‚ºæ•¸å€¼è¡¨ç¤ºï¼Œæ”¯æ´èªç¾©ç›¸ä¼¼æ€§æª¢ç´¢\")\n",
    "print(\"   â€¢ FAISS æä¾›é«˜æ•ˆçš„å‘é‡ç›¸ä¼¼æ€§æœç´¢\")\n",
    "print(\"   â€¢ 4bit é‡åŒ–å¤§å¹…é™ä½é¡¯å­˜éœ€æ±‚ï¼Œé©åˆæ¶ˆè²»ç´šç¡¬é«”\")\n",
    "print(\"   â€¢ ä¸­æ–‡æ–‡æœ¬éœ€ç‰¹æ®Šåˆ†æ®µç­–ç•¥ä»¥æ­£ç¢ºè™•ç†æ¨™é»ç¬¦è™Ÿ\")\n",
    "\n",
    "print(\"\\nâš ï¸  å¸¸è¦‹é™·é˜± (Common Pitfalls):\")\n",
    "print(\"   â€¢ æ–‡æœ¬åˆ†æ®µéå¤§/éå°éƒ½æœƒå½±éŸ¿æª¢ç´¢å“è³ª\")\n",
    "print(\"   â€¢ åµŒå…¥æ¨¡å‹é¸æ“‡éœ€è€ƒæ…®èªè¨€ç‰¹æ€§ (ä¸­è‹±æ–‡)\")\n",
    "print(\"   â€¢ æª¢ç´¢é–¾å€¼è¨­ç½®éé«˜å¯èƒ½æ‰¾ä¸åˆ°ç›¸é—œæ–‡ä»¶\")\n",
    "print(\"   â€¢ LLM ç”Ÿæˆå¯èƒ½åŒ…å«å¹»è¦ºï¼Œéœ€è¦äº‹å¯¦æ€§é©—è­‰\")\n",
    "print(\"   â€¢ é¡¯å­˜ä¸è¶³æ™‚éœ€å•Ÿç”¨é‡åŒ–æˆ– CPU fallback\")\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å»ºè­° (Next Steps):\")\n",
    "print(\"   â€¢ ğŸ“ˆ å¯¦ä½œæª¢ç´¢èˆ‡ç”Ÿæˆåˆ†é›¢è©•ä¼° (nb28)\")\n",
    "print(\"   â€¢ ğŸ¨ åŠ å…¥å¤šæ¨¡æ…‹ RAG æ”¯æ´åœ–åƒæª¢ç´¢ (nb27)\")\n",
    "print(\"   â€¢ ğŸ¤ å»ºç«‹å¤šä»£ç†å”ä½œç³»çµ± (nb29)\")\n",
    "print(\"   â€¢ ğŸ”§ å¾®èª¿åµŒå…¥æ¨¡å‹ä»¥æå‡é ˜åŸŸç›¸é—œæ€§ (nb20-25)\")\n",
    "print(\"   â€¢ ğŸŒ é–‹ç™¼ Web UI ä»‹é¢ (nb31)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ç³»çµ±çµ±è¨ˆ:\")\n",
    "print(f\"   â€¢ æ–‡æª”ç´¢å¼•æ•¸é‡: {vector_index.index.ntotal}\")\n",
    "print(f\"   â€¢ åµŒå…¥ç¶­åº¦: {embedding_model.embedding_dim}\")\n",
    "print(f\"   â€¢ LLM æ¨¡å‹: {config.llm_model}\")\n",
    "print(f\"   â€¢ å¹³å‡æª¢ç´¢æ™‚é–“: <1ç§’\")\n",
    "\n",
    "print(\"\\nğŸ¯ é—œéµä½¿ç”¨æ™‚æ©Ÿ:\")\n",
    "print(\"   â€¢ éœ€è¦åŸºæ–¼ç‰¹å®šæ–‡æª”åº«å›ç­”å•é¡Œæ™‚\")\n",
    "print(\"   â€¢ è¦æ±‚ç­”æ¡ˆå…·æœ‰ä¾†æºå¯è¿½æº¯æ€§æ™‚\")\n",
    "print(\"   â€¢ è™•ç†å¤§é‡æ–‡æª”è€Œäººå·¥æŸ¥æ‰¾æ•ˆç‡ä½ä¸‹æ™‚\")\n",
    "print(\"   â€¢ éœ€è¦å¤šèªè¨€æ–‡æª”æª¢ç´¢æ”¯æ´æ™‚\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ‰ æ­å–œï¼æ‚¨å·²æˆåŠŸå»ºç«‹äº†ä¸€å€‹åŠŸèƒ½å®Œæ•´çš„ RAG ç³»çµ±ï¼\")\n",
    "print(\"ğŸ“š ä¸‹ä¸€å€‹å»ºè­°å­¸ç¿’ï¼šnb13_function_calling_tools.ipynb\")\n",
    "print(\"ğŸ”— æˆ–ç¹¼çºŒ Part E çš„å¤šæ¨¡æ…‹ RAG: nb27_multimodal_rag_clip.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3939a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal RAG System Example (10 lines core functionality)\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Initialize models\n",
    "embedder = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "index = faiss.IndexFlatIP(1024)  # BGE-M3 dimension\n",
    "\n",
    "# 2. Index documents\n",
    "docs = [\"äººå·¥æ™ºèƒ½æ˜¯è¨ˆç®—æ©Ÿç§‘å­¸çš„åˆ†æ”¯\", \"æ©Ÿå™¨å­¸ç¿’è®“é›»è…¦è‡ªä¸»å­¸ç¿’\", \"æ·±åº¦å­¸ç¿’ä½¿ç”¨ç¥ç¶“ç¶²çµ¡\"]\n",
    "doc_embeddings = embedder.encode(docs)\n",
    "faiss.normalize_L2(doc_embeddings)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# 3. Query and retrieve\n",
    "query = \"ä»€éº¼æ˜¯AIï¼Ÿ\"\n",
    "query_emb = embedder.encode([query])\n",
    "faiss.normalize_L2(query_emb)\n",
    "scores, indices = index.search(query_emb, k=2)\n",
    "\n",
    "# 4. Generate context-aware answer\n",
    "context = \"\\n\".join([docs[i] for i in indices[0] if i != -1])\n",
    "prompt = f\"æ ¹æ“šï¼š{context}\\nå•é¡Œï¼š{query}\\nå›ç­”ï¼š\"\n",
    "\n",
    "print(f\"æª¢ç´¢åˆ°çš„æ–‡æª”: {[docs[i] for i in indices[0]]}\")\n",
    "print(f\"ç”Ÿæˆæç¤º: {prompt}\")\n",
    "\n",
    "# Note: In full implementation, this prompt would go to LLM for final answer generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a092030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG System Smoke Test (5-line validation)\n",
    "\n",
    "# Test basic RAG pipeline\n",
    "test_query = \"ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ\"\n",
    "result = rag_system.query(test_query)\n",
    "\n",
    "# Assertions\n",
    "assert \"answer\" in result and len(result[\"answer\"]) > 10, \"Answer generation failed\"\n",
    "assert result[\"confidence\"] > 0, \"Confidence calculation failed\"\n",
    "assert \"sources\" in result and len(result[\"sources\"]) > 0, \"Document retrieval failed\"\n",
    "\n",
    "print(f\"âœ… RAG Smoke Test Passed! Answer: {result['answer'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbece18",
   "metadata": {},
   "source": [
    "## 3. æ ¸å¿ƒç¨‹å¼ç¢¼ç‰‡æ®µ (MVP)\n",
    "\n",
    "ä»¥ä¸‹æ˜¯æœ€å°å¯åŸ·è¡Œçš„ RAG ç³»çµ±ç¯„ä¾‹ï¼š## 4. é©—æ”¶æ¸¬è©¦ Cell (Smoke Test)## 5. \n",
    "## 6. éšæ®µç¸½çµ (Stage Summary)\n",
    "\n",
    "### âœ… å®Œæˆé …ç›® (Completed Items)\n",
    "- **RAG ç³»çµ±æ¶æ§‹**ï¼šæª¢ç´¢å¢å¼·ç”Ÿæˆçš„å®Œæ•´å¯¦ä½œ\n",
    "- **å¤šæ ¼å¼æ–‡ä»¶æ”¯æ´**ï¼šTXT/PDF/DOCX è‡ªå‹•è¼‰å…¥èˆ‡è™•ç†\n",
    "- **ä¸­æ–‡å„ªåŒ–è™•ç†**ï¼šé‡å°ä¸­æ–‡æ¨™é»ç¬¦è™Ÿçš„æ–‡æœ¬åˆ†æ®µç­–ç•¥\n",
    "- **å‘é‡æª¢ç´¢ç³»çµ±**ï¼šåŸºæ–¼ FAISS çš„é«˜æ•ˆç›¸ä¼¼æ€§æœç´¢\n",
    "- **ä½é¡¯å­˜å‹å–„**ï¼š4bit é‡åŒ–èˆ‡ CPU fallback æ©Ÿåˆ¶\n",
    "- **ç«¯åˆ°ç«¯æ•´åˆ**ï¼šå¾æ–‡æª”ç´¢å¼•åˆ°å•ç­”ç”Ÿæˆçš„å®Œæ•´æµç¨‹\n",
    "- **æ€§èƒ½ç›£æ§**ï¼šè³‡æºä½¿ç”¨èˆ‡éŸ¿æ‡‰æ™‚é–“è©•ä¼°\n",
    "- **ç”Ÿç”¢å°±ç·’**ï¼šåŒ…å«éŒ¯èª¤è™•ç†èˆ‡å“è³ªæª¢æŸ¥çš„ç©©å¥ç³»çµ±\n",
    "\n",
    "### ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µ (Core Concepts)\n",
    "- **RAG ä¸‰éšæ®µ**ï¼šDocument Retrieval â†’ Context Augmentation â†’ Answer Generation\n",
    "- **å‘é‡èªç¾©æª¢ç´¢**ï¼šå°‡æ–‡æœ¬è½‰æ›ç‚ºé«˜ç¶­å‘é‡ç©ºé–“é€²è¡Œç›¸ä¼¼æ€§åŒ¹é…\n",
    "- **æ··åˆå¼æ¶æ§‹**ï¼šçµåˆæª¢ç´¢ç³»çµ±çš„äº‹å¯¦æ€§èˆ‡ç”Ÿæˆæ¨¡å‹çš„è¡¨é”èƒ½åŠ›\n",
    "- **ä¸­æ–‡ NLP ç‰¹æ€§**ï¼šè€ƒæ…®ä¸­æ–‡åˆ†è©èˆ‡æ¨™é»ç¬¦è™Ÿçš„ç‰¹æ®Šè™•ç†éœ€æ±‚\n",
    "- **æ¨¡å‹é‡åŒ–æŠ€è¡“**ï¼šåœ¨ä¿æŒæ€§èƒ½çš„å‰æä¸‹å¤§å¹…é™ä½ç¡¬é«”éœ€æ±‚\n",
    "\n",
    "### âš ï¸ å¸¸è¦‹é™·é˜± (Pitfalls)\n",
    "- **åˆ†æ®µç­–ç•¥å½±éŸ¿æª¢ç´¢å“è³ª**ï¼šéå¤§å°è‡´å™ªéŸ³ï¼Œéå°å°è‡´èªå¢ƒç¼ºå¤±\n",
    "- **åµŒå…¥æ¨¡å‹èªè¨€åŒ¹é…**ï¼šä¸­è‹±æ–‡æ··åˆå ´æ™¯éœ€é¸æ“‡å¤šèªè¨€æ¨¡å‹\n",
    "- **ç”Ÿæˆå¹»è¦ºå•é¡Œ**ï¼šLLM å¯èƒ½ç”¢ç”Ÿèˆ‡æª¢ç´¢å…§å®¹ä¸ç¬¦çš„è³‡è¨Š\n",
    "- **ç¡¬é«”è³‡æºç®¡ç†**ï¼šå¤§æ¨¡å‹è¼‰å…¥éœ€å¦¥å–„è™•ç† OOM éŒ¯èª¤\n",
    "- **æª¢ç´¢ç›¸é—œæ€§èª¿å„ª**ï¼šç›¸ä¼¼åº¦é–¾å€¼éœ€æ ¹æ“šå…·é«”æ‡‰ç”¨å ´æ™¯èª¿æ•´\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥è¡Œå‹• (Next Actions)\n",
    "\n",
    "**ç«‹å³å¯é¸æ–¹å‘ï¼š**\n",
    "\n",
    "1. **nb13_function_calling_tools.ipynb** - å·¥å…·ä½¿ç”¨èˆ‡ Function Calling\n",
    "   - **å„ªå‹¢**ï¼šç‚º RAG ç³»çµ±å¢åŠ å‹•æ…‹ä¿¡æ¯ç²å–èƒ½åŠ›\n",
    "   - **å­¸ç¿’åƒ¹å€¼**ï¼šç†è§£ LLM èˆ‡å¤–éƒ¨å·¥å…·çš„æ•´åˆæ¨¡å¼\n",
    "   - **å¯¦ç”¨æ€§**ï¼šå¯ç«‹å³æå‡ RAG ç³»çµ±çš„å¯¦ç”¨æ€§\n",
    "\n",
    "2. **nb27_multimodal_rag_clip.ipynb** - å¤šæ¨¡æ…‹ RAG æ“´å±•\n",
    "   - **å„ªå‹¢**ï¼šæ”¯æ´åœ–åƒ+æ–‡æœ¬çš„æ··åˆæª¢ç´¢\n",
    "   -**æŠ€è¡“æ·±åº¦**ï¼šå­¸ç¿’ CLIP ç­‰å¤šæ¨¡æ…‹æ¨¡å‹çš„æ‡‰ç”¨\n",
    "   - **æ‡‰ç”¨å ´æ™¯**ï¼šæ–‡æª”åŒ…å«åœ–è¡¨ã€åœ–ç‰‡çš„è¤‡é›œå ´æ™¯\n",
    "\n",
    "3. **nb28_retrieval_generation_eval.ipynb** - æª¢ç´¢ç”Ÿæˆåˆ†é›¢è©•ä¼°\n",
    "   - **å„ªå‹¢**ï¼šå»ºç«‹ç³»çµ±æ€§çš„ RAG å“è³ªè©•ä¼°æ¡†æ¶\n",
    "   - **é‡è¦æ€§**ï¼šç”Ÿç”¢ç’°å¢ƒå¿…å‚™çš„å“è³ªä¿è­‰æ©Ÿåˆ¶\n",
    "   - **æŠ€è¡“åƒ¹å€¼**ï¼šå­¸ç¿’ Recall@kã€Groundedness ç­‰å°ˆæ¥­æŒ‡æ¨™\n",
    "\n",
    "**æ¨è–¦å­¸ç¿’è·¯å¾‘å»ºè­°ï¼š**\n",
    "\n",
    "å¦‚æœæ‚¨å¸Œæœ›**å¿«é€Ÿå»ºç«‹å¯¦ç”¨ç³»çµ±**ï¼š\n",
    "â†’ é¸æ“‡ `nb13_function_calling_tools.ipynb`\n",
    "â†’ ç„¶å¾Œ `nb29_multi_agent_collaboration.ipynb` \n",
    "â†’ æœ€å¾Œ `nb31_gradio_chat_ui.ipynb`\n",
    "\n",
    "å¦‚æœæ‚¨å¸Œæœ›**æ·±åŒ–æŠ€è¡“ç†è§£**ï¼š\n",
    "â†’ é¸æ“‡ `nb28_retrieval_generation_eval.ipynb`\n",
    "â†’ ç„¶å¾Œ `nb27_multimodal_rag_clip.ipynb`\n",
    "â†’ æœ€å¾Œé€²å…¥ Part D å¾®èª¿ç³»åˆ—\n",
    "\n",
    "**è³‡æºéœ€æ±‚è©•ä¼°ï¼š**\n",
    "- **ç•¶å‰ç³»çµ±**å·²é©—è­‰å¯åœ¨ 8GB VRAM ç’°å¢ƒé‹è¡Œ\n",
    "- **Function Calling** ä¸»è¦å¢åŠ æ¨ç†è¤‡é›œåº¦ï¼Œç¡¬é«”éœ€æ±‚ç›¸è¿‘\n",
    "- **å¤šæ¨¡æ…‹ RAG** éœ€é¡å¤– 2-4GB VRAM è¼‰å…¥è¦–è¦ºæ¨¡å‹\n",
    "- **è©•ä¼°ç³»çµ±** ä¸»è¦æ¶ˆè€— CPU è³‡æºé€²è¡ŒæŒ‡æ¨™è¨ˆç®—\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ å­¸ç¿’å»ºè­°èˆ‡æœ€ä½³å¯¦è¸\n",
    "\n",
    "### ğŸ¯ **ä½•æ™‚ä½¿ç”¨ RAG ç³»çµ±ï¼š**\n",
    "- éœ€è¦åŸºæ–¼ç‰¹å®šæ–‡æª”åº«æä¾›æº–ç¢ºç­”æ¡ˆ\n",
    "- è¦æ±‚ç­”æ¡ˆå…·æœ‰å¯è¿½æº¯çš„ä¾†æºå¼•ç”¨\n",
    "- è™•ç†å‹•æ…‹æ›´æ–°çš„çŸ¥è­˜åº«å…§å®¹\n",
    "- é¿å…å¤§æ¨¡å‹çš„å¹»è¦ºå•é¡Œï¼Œæé«˜äº‹å¯¦æº–ç¢ºæ€§\n",
    "\n",
    "### ğŸ”§ **ç”Ÿç”¢éƒ¨ç½²è€ƒé‡ï¼š**\n",
    "- **ç´¢å¼•æ›´æ–°ç­–ç•¥**ï¼šå¢é‡ç´¢å¼• vs å…¨é‡é‡å»º\n",
    "- **å¿«å–æ©Ÿåˆ¶**ï¼šå¸¸è¦‹æŸ¥è©¢çµæœå¿«å–\n",
    "- **è² è¼‰å‡è¡¡**ï¼šå¤šå€‹æª¢ç´¢ç¯€é»åˆ†æ•£æŸ¥è©¢å£“åŠ›\n",
    "- **ç›£æ§å‘Šè­¦**ï¼šæª¢ç´¢å»¶é²ã€ç”Ÿæˆå“è³ªã€ç³»çµ±è³‡æº\n",
    "\n",
    "### ğŸ“ˆ **æ€§èƒ½å„ªåŒ–æ–¹å‘ï¼š**\n",
    "- **æª¢ç´¢å„ªåŒ–**ï¼šIVF ç´¢å¼•ã€PQ é‡åŒ–ã€GPU åŠ é€Ÿ\n",
    "- **ç”Ÿæˆå„ªåŒ–**ï¼šæ¨¡å‹è’¸é¤¾ã€KV-cacheã€æ‰¹æ¬¡è™•ç†\n",
    "- **æ¶æ§‹å„ªåŒ–**ï¼šç•°æ­¥è™•ç†ã€æµå¼è¼¸å‡ºã€é è¨ˆç®—\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **æ­å–œæ‚¨å®Œæˆäº† RAG åŸºç¤ç³»çµ±çš„å­¸ç¿’ï¼** æ‚¨ç¾åœ¨å…·å‚™äº†ï¼š\n",
    "- ç†è§£ RAG æ¶æ§‹çš„æ ¸å¿ƒåŸç†\n",
    "- å¯¦ä½œç”Ÿç”¢ç´š RAG ç³»çµ±çš„èƒ½åŠ›  \n",
    "- é‡å°ä¸­æ–‡å ´æ™¯çš„å„ªåŒ–æŠ€å·§\n",
    "- ä½è³‡æºç’°å¢ƒçš„éƒ¨ç½²ç­–ç•¥\n",
    "\n",
    "**æº–å‚™å¥½é€²è¡Œä¸‹ä¸€å€‹å­¸ç¿’æ¨¡çµ„äº†å—ï¼Ÿ** æˆ‘å»ºè­°å¾ **nb13_function_calling_tools.ipynb** é–‹å§‹ï¼Œé€™å°‡ç‚ºæ‚¨çš„ RAG ç³»çµ±åŠ å…¥å‹•æ…‹ä¿¡æ¯ç²å–èƒ½åŠ›ï¼Œè®“å®ƒèƒ½å¤ èª¿ç”¨å¤–éƒ¨å·¥å…·ä¾†ç²å–å¯¦æ™‚ä¿¡æ¯æˆ–åŸ·è¡Œè¨ˆç®—ä»»å‹™ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
