{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00e6b0c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "æœ¬ Notebook å±•ç¤ºå®Œå…¨åŸºæ–¼é–‹æºæ¨¡å‹çš„ä¸­æ–‡ RAG ç³»çµ±ï¼š\n",
    "- é–‹æºLLMï¼šDeepSeek-R1-Distill / Qwen2.5-7B (ä¸­æ–‡å„ªå…ˆ)\n",
    "- é–‹æºEmbeddingï¼šBGE-M3 / BGE-small-zh\n",
    "- æœ¬åœ°æ¨ç†ï¼štransformers + 4-bit / llama-cpp / Ollama\n",
    "- ä¸­æ–‡è™•ç†ï¼šå°ˆé–€çš„ä¸­æ–‡åˆ‡åˆ†èˆ‡ç¹ç°¡è½‰æ›\n",
    "- å®Œå…¨é›¢ç·šï¼šç„¡éœ€ä»»ä½•é›²ç«¯APIé‡‘é‘°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce06a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese Open-Source RAG System\n",
    "# ä¸­æ–‡é–‹æºæª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ± - DeepSeek/Qwen + BGE + FAISS\n",
    "\n",
    "## Stage 1 - Setup & GPU/VRAM Check\n",
    "# ç’°å¢ƒè¨­ç½®èˆ‡é¡¯å­˜æª¢æŸ¥\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === Shared Cache Bootstrap (Mandatory) ===\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "cache_paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "\n",
    "for k, v in cache_paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] å…±äº«å¿«å–æ ¹ç›®éŒ„: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d43cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"[GPU] é¡¯å­˜å®¹é‡: {gpu_memory:.1f} GB\")\n",
    "    print(f\"[GPU] é¡¯å¡å‹è™Ÿ: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # VRAM-based model recommendations\n",
    "    if gpu_memory >= 16:\n",
    "        recommended_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "        print(f\"[å»ºè­°] é¡¯å­˜å……è¶³ï¼Œæ¨è–¦ä½¿ç”¨: {recommended_model} (FP16)\")\n",
    "    elif gpu_memory >= 8:\n",
    "        recommended_model = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "        print(f\"[å»ºè­°] ä¸­ç­‰é¡¯å­˜ï¼Œæ¨è–¦ä½¿ç”¨: {recommended_model} (4-bit)\")\n",
    "    else:\n",
    "        recommended_model = \"THUDM/chatglm3-6b\"\n",
    "        print(f\"[å»ºè­°] é¡¯å­˜æœ‰é™ï¼Œæ¨è–¦ä½¿ç”¨: {recommended_model} (4-bit)\")\n",
    "else:\n",
    "    recommended_model = \"microsoft/DialoGPT-medium\"\n",
    "    print(f\"[è­¦å‘Š] ç„¡GPUï¼Œæ¨è–¦CPUæ¨¡å‹: {recommended_model}\")\n",
    "\n",
    "# Create essential directories for this notebook\n",
    "nb29_paths = {\n",
    "    \"docs\": f\"{AI_CACHE_ROOT}/nb29_cn_docs\",\n",
    "    \"vectorstore\": f\"{AI_CACHE_ROOT}/vectorstores/nb29_cn_oss\",\n",
    "    \"models_cache\": f\"{AI_CACHE_ROOT}/models/nb29_cn_oss\",\n",
    "}\n",
    "\n",
    "for name, path in nb29_paths.items():\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[ç›®éŒ„] {name}: {path}\")\n",
    "\n",
    "\n",
    "# Check and install required packages\n",
    "def check_install_packages():\n",
    "    \"\"\"æª¢æŸ¥ä¸¦å®‰è£å¿…è¦å¥—ä»¶ (Check and install required packages)\"\"\"\n",
    "    required_packages = {\n",
    "        \"transformers\": \"transformers>=4.35.0\",\n",
    "        \"torch\": \"torch>=2.0.0\",\n",
    "        \"bitsandbytes\": \"bitsandbytes>=0.41.0\",\n",
    "        \"sentence_transformers\": \"sentence-transformers>=2.2.0\",\n",
    "        \"faiss\": \"faiss-cpu>=1.7.0\",\n",
    "        \"opencc\": \"opencc>=1.1.0\",\n",
    "        \"langchain\": \"langchain>=0.1.0\",\n",
    "        \"langchain_text_splitters\": \"langchain-text-splitters\",\n",
    "        \"gradio\": \"gradio>=4.0.0\",\n",
    "    }\n",
    "\n",
    "    missing_packages = []\n",
    "    for package, install_cmd in required_packages.items():\n",
    "        try:\n",
    "            __import__(package.replace(\"-\", \"_\"))\n",
    "            print(f\"âœ… {package} å·²å®‰è£\")\n",
    "        except ImportError:\n",
    "            missing_packages.append(install_cmd)\n",
    "            print(f\"âŒ {package} æœªå®‰è£\")\n",
    "\n",
    "    if missing_packages:\n",
    "        print(f\"\\nè«‹åŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤å®‰è£ç¼ºå¤±å¥—ä»¶:\")\n",
    "        print(f\"pip install {' '.join(missing_packages)}\")\n",
    "        return False\n",
    "\n",
    "    print(\"âœ… æ‰€æœ‰å¿…è¦å¥—ä»¶å·²å®‰è£\")\n",
    "    return True\n",
    "\n",
    "\n",
    "package_check_result = check_install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64baa5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 2 - Backend & Model Selector\n",
    "# å¾Œç«¯èˆ‡æ¨¡å‹é¸æ“‡å™¨\n",
    "\n",
    "# Import essential libraries\n",
    "try:\n",
    "    import transformers\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForCausalLM,\n",
    "        BitsAndBytesConfig,\n",
    "        pipeline,\n",
    "        TextStreamer,\n",
    "    )\n",
    "\n",
    "    print(\"âœ… transformers è¼‰å…¥æˆåŠŸ\")\n",
    "except ImportError:\n",
    "    print(\"âŒ transformers è¼‰å…¥å¤±æ•—\")\n",
    "\n",
    "try:\n",
    "    import sentence_transformers\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    print(\"âœ… sentence_transformers è¼‰å…¥æˆåŠŸ\")\n",
    "except ImportError:\n",
    "    print(\"âŒ sentence_transformers è¼‰å…¥å¤±æ•—\")\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "\n",
    "    print(\"âœ… faiss è¼‰å…¥æˆåŠŸ\")\n",
    "except ImportError:\n",
    "    print(\"âŒ faiss è¼‰å…¥å¤±æ•—\")\n",
    "\n",
    "try:\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "    print(\"âœ… langchain_text_splitters è¼‰å…¥æˆåŠŸ\")\n",
    "except ImportError:\n",
    "    print(\"âŒ langchain_text_splitters è¼‰å…¥å¤±æ•—\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Chinese RAG system\n",
    "class ChineseRAGConfig:\n",
    "    \"\"\"ä¸­æ–‡RAGç³»çµ±é…ç½® (Chinese RAG System Configuration)\"\"\"\n",
    "\n",
    "    # Backend options: transformers, llama_cpp, ollama\n",
    "    BACKEND = \"transformers\"\n",
    "\n",
    "    # Model configurations (Chinese-first)\n",
    "    MODELS = {\n",
    "        \"deepseek-r1-7b\": {\n",
    "            \"id\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "            \"min_vram_gb\": 6,\n",
    "            \"supports_4bit\": True,\n",
    "            \"chinese_ability\": \"excellent\",\n",
    "        },\n",
    "        \"qwen2.5-7b\": {\n",
    "            \"id\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            \"min_vram_gb\": 6,\n",
    "            \"supports_4bit\": True,\n",
    "            \"chinese_ability\": \"excellent\",\n",
    "        },\n",
    "        \"chatglm3-6b\": {\n",
    "            \"id\": \"THUDM/chatglm3-6b\",\n",
    "            \"min_vram_gb\": 5,\n",
    "            \"supports_4bit\": True,\n",
    "            \"chinese_ability\": \"excellent\",\n",
    "        },\n",
    "        \"yi-6b\": {\n",
    "            \"id\": \"01-ai/Yi-6B-Chat\",\n",
    "            \"min_vram_gb\": 5,\n",
    "            \"supports_4bit\": True,\n",
    "            \"chinese_ability\": \"good\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Embedding models (Chinese-optimized)\n",
    "    EMBEDDING_MODELS = {\n",
    "        \"bge-m3\": \"BAAI/bge-m3\",\n",
    "        \"bge-small-zh\": \"BAAI/bge-small-zh-v1.5\",\n",
    "        \"text2vec-base\": \"shibing624/text2vec-base-chinese\",\n",
    "    }\n",
    "\n",
    "    # Chinese text processing\n",
    "    CHINESE_SEPARATORS = [\"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \"ï¼›\", \"â€¦\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    CHUNK_SIZE = 512\n",
    "    CHUNK_OVERLAP = 64\n",
    "\n",
    "    # Retrieval settings\n",
    "    TOP_K = 5\n",
    "    SIMILARITY_THRESHOLD = 0.6\n",
    "\n",
    "\n",
    "config = ChineseRAGConfig()\n",
    "\n",
    "\n",
    "def auto_select_model() -> str:\n",
    "    \"\"\"æ ¹æ“šé¡¯å­˜è‡ªå‹•é¸æ“‡æ¨¡å‹ (Auto-select model based on VRAM)\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return \"qwen2.5-7b\"  # Fallback for CPU\n",
    "\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "\n",
    "    for model_key, model_info in config.MODELS.items():\n",
    "        if vram_gb >= model_info[\"min_vram_gb\"]:\n",
    "            print(f\"[è‡ªå‹•é¸æ“‡] åŸºæ–¼ {vram_gb:.1f}GB é¡¯å­˜ï¼Œé¸æ“‡: {model_key}\")\n",
    "            return model_key\n",
    "\n",
    "    # Fallback to smallest model\n",
    "    return \"chatglm3-6b\"\n",
    "\n",
    "\n",
    "# Model selection\n",
    "selected_model_key = auto_select_model()\n",
    "selected_model_info = config.MODELS[selected_model_key]\n",
    "model_id = selected_model_info[\"id\"]\n",
    "\n",
    "print(f\"\\n[æ¨¡å‹é…ç½®]\")\n",
    "print(f\"- å¾Œç«¯ (Backend): {config.BACKEND}\")\n",
    "print(f\"- æ¨¡å‹ (Model): {model_id}\")\n",
    "print(f\"- ä¸­æ–‡èƒ½åŠ› (Chinese): {selected_model_info['chinese_ability']}\")\n",
    "print(f\"- 4-bitæ”¯æ´: {selected_model_info['supports_4bit']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a654552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAdapter:\n",
    "    \"\"\"è¼•é‡LLMé©é…å™¨ (Lightweight LLM Adapter)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_id: str, backend: str = \"transformers\", load_in_4bit: bool = True\n",
    "    ):\n",
    "        self.model_id = model_id\n",
    "        self.backend = backend\n",
    "        self.load_in_4bit = load_in_4bit\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        print(f\"[LLM] åˆå§‹åŒ– {backend} å¾Œç«¯ï¼Œæ¨¡å‹: {model_id}\")\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"è¼‰å…¥æ¨¡å‹ (Load model)\"\"\"\n",
    "        try:\n",
    "            if self.backend == \"transformers\":\n",
    "                self._load_transformers()\n",
    "            elif self.backend == \"llama_cpp\":\n",
    "                self._load_llama_cpp()\n",
    "            elif self.backend == \"ollama\":\n",
    "                self._load_ollama()\n",
    "            else:\n",
    "                raise ValueError(f\"ä¸æ”¯æ´çš„å¾Œç«¯: {self.backend}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\")\n",
    "            print(f\"ğŸ”„ å˜—è©¦é™ç´šè¼‰å…¥...\")\n",
    "            self._fallback_load()\n",
    "\n",
    "    def _load_transformers(self):\n",
    "        \"\"\"è¼‰å…¥Transformersæ¨¡å‹ (Load Transformers model)\"\"\"\n",
    "        # Configure quantization for low VRAM\n",
    "        if self.load_in_4bit and torch.cuda.is_available():\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "            )\n",
    "        else:\n",
    "            bnb_config = None\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_id,\n",
    "            trust_remote_code=True,\n",
    "            cache_dir=cache_paths[\"TRANSFORMERS_CACHE\"],\n",
    "        )\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_id,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "            cache_dir=cache_paths[\"TRANSFORMERS_CACHE\"],\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… Transformersæ¨¡å‹è¼‰å…¥æˆåŠŸ\")\n",
    "\n",
    "    def _load_llama_cpp(self):\n",
    "        \"\"\"è¼‰å…¥llama-cppæ¨¡å‹ (Load llama-cpp model)\"\"\"\n",
    "        try:\n",
    "            from llama_cpp import Llama\n",
    "\n",
    "            # This would require GGUF model files\n",
    "            print(\"âš ï¸ llama-cppå¾Œç«¯éœ€è¦GGUFæ¨¡å‹æª”æ¡ˆ\")\n",
    "            raise NotImplementedError(\"llama-cpp backend not implemented in this demo\")\n",
    "        except ImportError:\n",
    "            print(\"âŒ llama-cpp-python æœªå®‰è£\")\n",
    "            raise\n",
    "\n",
    "    def _load_ollama(self):\n",
    "        \"\"\"è¼‰å…¥Ollamaæ¨¡å‹ (Load Ollama model)\"\"\"\n",
    "        try:\n",
    "            import ollama\n",
    "\n",
    "            # This would require Ollama service running\n",
    "            print(\"âš ï¸ Ollamaå¾Œç«¯éœ€è¦Ollamaæœå‹™é‹è¡Œ\")\n",
    "            raise NotImplementedError(\"Ollama backend not implemented in this demo\")\n",
    "        except ImportError:\n",
    "            print(\"âŒ ollama æœªå®‰è£\")\n",
    "            raise\n",
    "\n",
    "    def _fallback_load(self):\n",
    "        \"\"\"é™ç´šè¼‰å…¥ (Fallback loading)\"\"\"\n",
    "        print(\"ğŸ”„ å˜—è©¦CPUæ¨¡å¼è¼‰å…¥...\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_id, trust_remote_code=True\n",
    "            )\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_id,\n",
    "                torch_dtype=torch.float32,\n",
    "                device_map=None,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "            self.device = \"cpu\"\n",
    "            print(\"âœ… CPUæ¨¡å¼è¼‰å…¥æˆåŠŸ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ é™ç´šè¼‰å…¥ä¹Ÿå¤±æ•—: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        max_length: int = 1024,\n",
    "        temperature: float = 0.7,\n",
    "    ) -> str:\n",
    "        \"\"\"ç”Ÿæˆå›æ‡‰ (Generate response)\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            return \"æ¨¡å‹æœªè¼‰å…¥\"\n",
    "\n",
    "        try:\n",
    "            # Format messages for different model types\n",
    "            if \"qwen\" in self.model_id.lower():\n",
    "                # Qwen format\n",
    "                formatted_text = self._format_qwen_messages(messages)\n",
    "            elif \"chatglm\" in self.model_id.lower():\n",
    "                # ChatGLM format\n",
    "                formatted_text = self._format_chatglm_messages(messages)\n",
    "            elif \"deepseek\" in self.model_id.lower():\n",
    "                # DeepSeek format (similar to ChatML)\n",
    "                formatted_text = self._format_deepseek_messages(messages)\n",
    "            else:\n",
    "                # Generic format\n",
    "                formatted_text = self._format_generic_messages(messages)\n",
    "\n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                formatted_text, return_tensors=\"pt\", truncation=True, max_length=2048\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Generate\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=inputs.input_ids.shape[1] + max_length,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.8,\n",
    "                    top_k=50,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            # Decode response\n",
    "            response = self.tokenizer.decode(\n",
    "                outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
    "            ).strip()\n",
    "\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç”Ÿæˆå¤±æ•—: {e}\")\n",
    "            return f\"ç”ŸæˆéŒ¯èª¤: {str(e)}\"\n",
    "\n",
    "    def _format_qwen_messages(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"æ ¼å¼åŒ–Qwenæ¶ˆæ¯ (Format Qwen messages)\"\"\"\n",
    "        formatted = \"<|im_start|>system\\nä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„AIåŠ©æ‰‹ã€‚<|im_end|>\\n\"\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            formatted += f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
    "        formatted += \"<|im_start|>assistant\\n\"\n",
    "        return formatted\n",
    "\n",
    "    def _format_chatglm_messages(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"æ ¼å¼åŒ–ChatGLMæ¶ˆæ¯ (Format ChatGLM messages)\"\"\"\n",
    "        formatted = \"\"\n",
    "        for msg in messages:\n",
    "            if msg.get(\"role\") == \"user\":\n",
    "                formatted += f\"[Round 1]\\n\\nå•ï¼š{msg.get('content', '')}\\n\\nç­”ï¼š\"\n",
    "        return formatted\n",
    "\n",
    "    def _format_deepseek_messages(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"æ ¼å¼åŒ–DeepSeekæ¶ˆæ¯ (Format DeepSeek messages)\"\"\"\n",
    "        formatted = \"<ï½œbeginâ–ofâ–sentenceï½œ>\"\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if role == \"user\":\n",
    "                formatted += f\"User: {content}\\n\\nAssistant: \"\n",
    "        return formatted\n",
    "\n",
    "    def _format_generic_messages(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"é€šç”¨æ¶ˆæ¯æ ¼å¼ (Generic message format)\"\"\"\n",
    "        formatted = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if role == \"user\":\n",
    "                formatted += f\"Human: {content}\\n\\nAssistant: \"\n",
    "        return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "print(f\"\\n[è¼‰å…¥æ¨¡å‹] é–‹å§‹è¼‰å…¥ {model_id}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    llm = LLMAdapter(\n",
    "        model_id=model_id,\n",
    "        backend=config.BACKEND,\n",
    "        load_in_4bit=selected_model_info[\"supports_4bit\"] and torch.cuda.is_available(),\n",
    "    )\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œè€—æ™‚: {load_time:.2f}ç§’\")\n",
    "\n",
    "    # Quick test\n",
    "    test_messages = [{\"role\": \"user\", \"content\": \"ä½ å¥½ï¼Œè«‹ç”¨ä¸­æ–‡å›ç­”ï¼š1+1ç­‰æ–¼å¤šå°‘ï¼Ÿ\"}]\n",
    "    test_response = llm.generate(test_messages, max_length=50)\n",
    "    print(f\"[æ¸¬è©¦å›æ‡‰] {test_response}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}\")\n",
    "    llm = None\n",
    "\n",
    "## Stage 2 Summary\n",
    "print(\n",
    "    f\"\"\"\n",
    "ğŸ¯ Stage 2 å®Œæˆç¸½çµ:\n",
    "âœ… å®Œæˆé …ç›®:\n",
    "- å»ºç«‹LLMé©é…å™¨æ”¯æ´å¤šç¨®ä¸­æ–‡é–‹æºæ¨¡å‹\n",
    "- å¯¦ç¾4-bité‡åŒ–ä½é¡¯å­˜è¼‰å…¥\n",
    "- æ”¯æ´Qwen/DeepSeek/ChatGLMç­‰ä¸»æµä¸­æ–‡æ¨¡å‹\n",
    "\n",
    "ğŸ§  æ ¸å¿ƒè§€å¿µ:\n",
    "- é–‹æºå„ªå…ˆç­–ç•¥ï¼šå®Œå…¨é¿å…é›²ç«¯APIä¾è³´\n",
    "- ä¸­æ–‡æ¨¡å‹é©é…ï¼šé‡å°ä¸åŒæ¨¡å‹çš„æç¤ºè©æ ¼å¼\n",
    "- è³‡æºå„ªåŒ–ï¼šåŸºæ–¼é¡¯å­˜å®¹é‡è‡ªå‹•é¸æ“‡åˆé©æ¨¡å‹\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥: å»ºç«‹ä¸­æ–‡æ–‡æª”è³‡æ–™åº«èˆ‡åµŒå…¥ç³»çµ±\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ea18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 3 - Data Ingest (æœ¬åœ°ä¸­æ–‡æ–‡æª”)\n",
    "# æœ¬åœ°ä¸­æ–‡è³‡æ–™è¼‰å…¥èˆ‡è™•ç†\n",
    "\n",
    "\n",
    "def create_sample_chinese_docs():\n",
    "    \"\"\"å»ºç«‹ç¯„ä¾‹ä¸­æ–‡æ–‡æª” (Create sample Chinese documents)\"\"\"\n",
    "    docs_dir = pathlib.Path(nb29_paths[\"docs\"])\n",
    "\n",
    "    sample_docs = {\n",
    "        \"AIæŠ€è¡“ç™¼å±•è¶¨å‹¢.md\": \"\"\"# äººå·¥æ™ºæ…§æŠ€è¡“ç™¼å±•è¶¨å‹¢\n",
    "\n",
    "## å¤§å‹èªè¨€æ¨¡å‹çš„çªç ´\n",
    "\n",
    "è¿‘å¹´ä¾†ï¼Œå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰åœ¨è‡ªç„¶èªè¨€è™•ç†é ˜åŸŸå–å¾—äº†é©å‘½æ€§çªç ´ã€‚å¾GPTç³»åˆ—åˆ°åœ‹ç”¢çš„é€šç¾©åƒå•ã€æ–‡å¿ƒä¸€è¨€ç­‰ï¼Œé€™äº›æ¨¡å‹å±•ç¾å‡ºå¼·å¤§çš„æ–‡æœ¬ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚\n",
    "\n",
    "### æŠ€è¡“ç‰¹é»\n",
    "\n",
    "1. **è¦æ¨¡åŒ–è¨“ç·´**ï¼šä½¿ç”¨å¤§é‡æ–‡æœ¬è³‡æ–™é€²è¡Œé è¨“ç·´ï¼Œåƒæ•¸é‡å¾æ•¸åå„„åˆ°æ•¸åƒå„„ä¸ç­‰\n",
    "2. **å¤šæ¨¡æ…‹èƒ½åŠ›**ï¼šçµåˆæ–‡æœ¬ã€åœ–åƒã€èªéŸ³ç­‰å¤šç¨®æ¨¡æ…‹çš„è™•ç†èƒ½åŠ›\n",
    "3. **ä¸­æ–‡å„ªåŒ–**ï¼šé‡å°ä¸­æ–‡èªè¨€ç‰¹æ€§é€²è¡Œå°ˆé–€å„ªåŒ–å’Œè¨“ç·´\n",
    "\n",
    "### æ‡‰ç”¨é ˜åŸŸ\n",
    "\n",
    "- æ™ºèƒ½å®¢æœèˆ‡å°è©±ç³»çµ±\n",
    "- å…§å®¹å‰µä½œèˆ‡å¯«ä½œè¼”åŠ©\n",
    "- ç¨‹å¼ç¢¼ç”Ÿæˆèˆ‡é™¤éŒ¯\n",
    "- æ•™è‚²è¼”å°èˆ‡çŸ¥è­˜å•ç­”\n",
    "- ç¿»è­¯èˆ‡å¤šèªè¨€è™•ç†\n",
    "\n",
    "## æª¢ç´¢å¢å¼·ç”ŸæˆæŠ€è¡“\n",
    "\n",
    "æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRetrieval-Augmented Generation, RAGï¼‰æ˜¯ç•¶å‰æœ€é‡è¦çš„LLMæ‡‰ç”¨æ¶æ§‹ä¹‹ä¸€ã€‚å®ƒçµåˆäº†è³‡è¨Šæª¢ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„å„ªå‹¢ï¼Œèƒ½å¤ åŸºæ–¼å¤–éƒ¨çŸ¥è­˜åº«æä¾›æº–ç¢ºä¸”åŠæ™‚çš„ç­”æ¡ˆã€‚\n",
    "\n",
    "### æ ¸å¿ƒå„ªå‹¢\n",
    "\n",
    "1. **çŸ¥è­˜æ™‚æ•ˆæ€§**ï¼šå¯ä»¥æ•´åˆæœ€æ–°çš„å¤–éƒ¨è³‡è¨Š\n",
    "2. **é ˜åŸŸå°ˆæ¥­æ€§**ï¼šé‡å°ç‰¹å®šé ˜åŸŸçš„æ·±åº¦çŸ¥è­˜\n",
    "3. **å¯è§£é‡‹æ€§**ï¼šæä¾›è³‡è¨Šä¾†æºå’Œå¼•ç”¨ä¾æ“š\n",
    "4. **æˆæœ¬æ•ˆç›Š**ï¼šé¿å…é‡è¤‡è¨“ç·´å¤§æ¨¡å‹çš„é«˜æ˜‚æˆæœ¬\n",
    "\"\"\",\n",
    "        \"é–‹æºLLMç”Ÿæ…‹ç³»çµ±.md\": \"\"\"# é–‹æºå¤§å‹èªè¨€æ¨¡å‹ç”Ÿæ…‹ç³»çµ±\n",
    "\n",
    "## ä¸»è¦é–‹æºæ¨¡å‹å®¶æ—\n",
    "\n",
    "### Llamaç³»åˆ—ï¼ˆMetaï¼‰\n",
    "- Llama 2: 7B, 13B, 70Båƒæ•¸ç‰ˆæœ¬\n",
    "- Code Llama: å°ˆé–€çš„ç¨‹å¼ç¢¼ç”Ÿæˆæ¨¡å‹\n",
    "- Llama 3: æ›´å¼·çš„å¤šèªè¨€å’Œæ¨ç†èƒ½åŠ›\n",
    "\n",
    "### ä¸­æ–‡é–‹æºæ¨¡å‹\n",
    "\n",
    "#### é€šç¾©åƒå•ï¼ˆQwenï¼‰ç³»åˆ—\n",
    "- Qwen2.5: é˜¿é‡Œé›²é–‹æºçš„å¼·å¤§ä¸­æ–‡æ¨¡å‹\n",
    "- æ”¯æ´32Kä¸Šä¸‹æ–‡é•·åº¦\n",
    "- å„ªç§€çš„ä¸­æ–‡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›\n",
    "- å¤šç¨®è¦æ ¼ï¼š0.5Båˆ°72Båƒæ•¸\n",
    "\n",
    "#### DeepSeekç³»åˆ—\n",
    "- DeepSeek-R1: åŸºæ–¼å¼·åŒ–å­¸ç¿’çš„æ¨ç†å„ªåŒ–\n",
    "- DeepSeek-Coder: å°ˆæ¥­çš„ç¨‹å¼ç¢¼æ¨¡å‹\n",
    "- åœ¨æ•¸å­¸å’Œç¨‹å¼è¨­è¨ˆä»»å‹™ä¸Šè¡¨ç¾å„ªç•°\n",
    "\n",
    "#### ChatGLMç³»åˆ—ï¼ˆæ¸…è¯å¤§å­¸ï¼‰\n",
    "- ChatGLM3-6B: è¼•é‡ç´šä¸­æ–‡å°è©±æ¨¡å‹\n",
    "- æ”¯æ´å¤šè¼ªå°è©±å’Œå·¥å…·èª¿ç”¨\n",
    "- é‡å°ä¸­æ–‡èªå¢ƒæ·±åº¦å„ªåŒ–\n",
    "\n",
    "#### å…¶ä»–é‡è¦æ¨¡å‹\n",
    "- Baichuan: ç™¾å·æ™ºèƒ½é–‹æºæ¨¡å‹\n",
    "- Yi: é›¶ä¸€è¬ç‰©é–‹æºæ¨¡å‹ç³»åˆ—\n",
    "- InternLM: ä¸Šæµ·AIå¯¦é©—å®¤æ›¸ç”Ÿæ¨¡å‹\n",
    "\n",
    "## æŠ€è¡“ç”Ÿæ…‹èˆ‡å·¥å…·éˆ\n",
    "\n",
    "### æ¨ç†å¼•æ“\n",
    "- vLLM: é«˜æ•ˆèƒ½æ¨ç†åŠ é€Ÿ\n",
    "- llama.cpp: è·¨å¹³å°CPUæ¨ç†\n",
    "- Ollama: æœ¬åœ°æ¨¡å‹ç®¡ç†å¹³å°\n",
    "- TensorRT-LLM: NVIDIA GPUå„ªåŒ–\n",
    "\n",
    "### é‡åŒ–æŠ€è¡“\n",
    "- GPTQ: æ¬Šé‡é‡åŒ–æŠ€è¡“\n",
    "- AWQ: å•Ÿå‹•æ„ŸçŸ¥é‡åŒ–\n",
    "- GGUF: llama.cppçš„é‡åŒ–æ ¼å¼\n",
    "- BitsAndBytes: å‹•æ…‹é‡åŒ–è¼‰å…¥\n",
    "\n",
    "### å¾®èª¿æ¡†æ¶\n",
    "- LoRA/QLoRA: ä½ç§©é©æ‡‰å¾®èª¿\n",
    "- PEFT: åƒæ•¸æ•ˆç‡å¾®èª¿åº«\n",
    "- DeepSpeed: å¤§è¦æ¨¡åˆ†æ•£å¼è¨“ç·´\n",
    "- Unsloth: é«˜æ•ˆå¾®èª¿åŠ é€Ÿ\n",
    "\n",
    "## éƒ¨ç½²èˆ‡æ‡‰ç”¨\n",
    "\n",
    "### æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆ\n",
    "1. **å€‹äººé›»è…¦**: ä½¿ç”¨é‡åŒ–æ¨¡å‹å’Œå„ªåŒ–æ¨ç†\n",
    "2. **ä¼æ¥­ç§æœ‰é›²**: å»ºç«‹å…§éƒ¨LLMæœå‹™\n",
    "3. **é‚Šç·£è¨­å‚™**: è¼•é‡ç´šæ¨¡å‹éƒ¨ç½²\n",
    "\n",
    "### æˆæœ¬è€ƒé‡\n",
    "- é–‹æºæ¨¡å‹å…è²»ä½¿ç”¨ï¼Œåƒ…éœ€æ‰¿æ“”æ¨ç†æˆæœ¬\n",
    "- é¿å…APIèª¿ç”¨è²»ç”¨å’Œè³‡æ–™éš±ç§é¢¨éšª\n",
    "- å¯æ ¹æ“šéœ€æ±‚éˆæ´»èª¿æ•´æ¨¡å‹è¦æ¨¡\n",
    "\"\"\",\n",
    "        \"RAGç³»çµ±å¯¦ä½œæŒ‡å—.md\": \"\"\"# RAGæª¢ç´¢å¢å¼·ç”Ÿæˆç³»çµ±å¯¦ä½œæŒ‡å—\n",
    "\n",
    "## ç³»çµ±æ¶æ§‹è¨­è¨ˆ\n",
    "\n",
    "### æ ¸å¿ƒçµ„ä»¶\n",
    "\n",
    "1. **æ–‡æª”è™•ç†æ¨¡çµ„**\n",
    "   - æ–‡æª”è¼‰å…¥ï¼šæ”¯æ´PDFã€Wordã€Markdownç­‰æ ¼å¼\n",
    "   - æ–‡æœ¬åˆ‡åˆ†ï¼šæ™ºèƒ½åˆ†æ®µä¿æŒèªç¾©å®Œæ•´æ€§\n",
    "   - ç¹ç°¡è½‰æ›ï¼šè™•ç†ç¹é«”èˆ‡ç°¡é«”ä¸­æ–‡å·®ç•°\n",
    "\n",
    "2. **å‘é‡åŒ–æ¨¡çµ„**\n",
    "   - åµŒå…¥æ¨¡å‹ï¼šBGE-M3ã€text2vecç­‰ä¸­æ–‡å„ªåŒ–æ¨¡å‹\n",
    "   - å‘é‡è³‡æ–™åº«ï¼šFAISSã€Chromaã€Pineconeç­‰\n",
    "   - ç´¢å¼•å»ºç«‹ï¼šæ”¯æ´å¤§è¦æ¨¡æ–‡æª”é›†åˆ\n",
    "\n",
    "3. **æª¢ç´¢æ¨¡çµ„**\n",
    "   - èªç¾©æœå°‹ï¼šåŸºæ–¼å‘é‡ç›¸ä¼¼åº¦çš„æª¢ç´¢\n",
    "   - é‡æ’åºï¼šä½¿ç”¨å°ˆé–€çš„rerankeræ¨¡å‹æå‡ç²¾åº¦\n",
    "   - æ··åˆæª¢ç´¢ï¼šçµåˆé—œéµè©å’Œèªç¾©æª¢ç´¢\n",
    "\n",
    "4. **ç”Ÿæˆæ¨¡çµ„**\n",
    "   - æç¤ºå·¥ç¨‹ï¼šè¨­è¨ˆæœ‰æ•ˆçš„RAGæç¤ºæ¨¡æ¿\n",
    "   - ä¸Šä¸‹æ–‡ç®¡ç†ï¼šæ§åˆ¶æª¢ç´¢å…§å®¹çš„é•·åº¦å’Œå“è³ª\n",
    "   - å›ç­”ç”Ÿæˆï¼šç¢ºä¿ç­”æ¡ˆçš„æº–ç¢ºæ€§å’Œç›¸é—œæ€§\n",
    "\n",
    "## ä¸­æ–‡è™•ç†å„ªåŒ–\n",
    "\n",
    "### åˆ†è©èˆ‡åˆ‡åˆ†ç­–ç•¥\n",
    "```python\n",
    "# ä¸­æ–‡åˆ†éš”ç¬¦å„ªå…ˆåºåˆ—\n",
    "separators = [\"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \"ï¼›\", \"â€¦\", \"\\\\n\\\\n\", \"\\\\n\", \" \"]\n",
    "\n",
    "# é©åˆä¸­æ–‡çš„chunk sizeè¨­å®š\n",
    "chunk_size = 512  # ä¸­æ–‡å­—ç¬¦æ•¸\n",
    "overlap = 64      # é‡ç–Šå­—ç¬¦æ•¸\n",
    "```\n",
    "\n",
    "### åµŒå…¥æ¨¡å‹é¸æ“‡\n",
    "- **BGE-M3**: å¤šèªè¨€æ¨¡å‹ï¼Œä¸­è‹±æ–‡æ•ˆæœä½³\n",
    "- **BGE-small-zh**: å°ˆé–€çš„ä¸­æ–‡åµŒå…¥æ¨¡å‹\n",
    "- **text2vec-base-chinese**: è¼•é‡ç´šä¸­æ–‡æ¨¡å‹\n",
    "\n",
    "### æª¢ç´¢è©•ä¼°æŒ‡æ¨™\n",
    "- **Recall@K**: å‰Kå€‹çµæœä¸­æ­£ç¢ºç­”æ¡ˆçš„å¬å›ç‡\n",
    "- **MRR**: å¹³å‡å€’æ•¸æ’å\n",
    "- **NDCG**: æ¨™æº–åŒ–æŠ˜æ‰£ç´¯è¨ˆå¢ç›Š\n",
    "\n",
    "## ç³»çµ±è©•ä¼°èˆ‡å„ªåŒ–\n",
    "\n",
    "### æª¢ç´¢å“è³ªè©•ä¼°\n",
    "1. **ç›¸é—œæ€§è©•ä¼°**: æª¢ç´¢çµæœèˆ‡æŸ¥è©¢çš„ç›¸é—œç¨‹åº¦\n",
    "2. **è¦†è“‹ç‡åˆ†æ**: çŸ¥è­˜åº«å°æŸ¥è©¢é ˜åŸŸçš„è¦†è“‹ç¯„åœ\n",
    "3. **éŸ¿æ‡‰æ™‚é–“**: æª¢ç´¢å’Œç”Ÿæˆçš„å»¶é²æŒ‡æ¨™\n",
    "\n",
    "### ç”Ÿæˆå“è³ªè©•ä¼°\n",
    "1. **äº‹å¯¦æº–ç¢ºæ€§**: ç­”æ¡ˆæ˜¯å¦ç¬¦åˆæª¢ç´¢åˆ°çš„äº‹å¯¦\n",
    "2. **å®Œæ•´æ€§**: ç­”æ¡ˆæ˜¯å¦å……åˆ†å›æ‡‰äº†å•é¡Œ\n",
    "3. **æµæš¢æ€§**: ç”Ÿæˆæ–‡æœ¬çš„è‡ªç„¶åº¦å’Œå¯è®€æ€§\n",
    "4. **å¼•ç”¨æº–ç¢ºæ€§**: æ˜¯å¦æ­£ç¢ºå¼•ç”¨äº†ä¾†æºè³‡è¨Š\n",
    "\n",
    "### ç³»çµ±å„ªåŒ–ç­–ç•¥\n",
    "- **å¿«å–æ©Ÿåˆ¶**: å°å¸¸è¦‹æŸ¥è©¢å»ºç«‹å¿«å–\n",
    "- **æ‰¹æ¬¡è™•ç†**: æå‡å‘é‡åŒ–è™•ç†æ•ˆç‡\n",
    "- **æ¨¡å‹é¸æ“‡**: æ ¹æ“šç²¾åº¦å’Œæ•ˆèƒ½éœ€æ±‚é¸æ“‡åˆé©æ¨¡å‹\n",
    "- **ç¡¬é«”å„ªåŒ–**: GPUåŠ é€Ÿã€è¨˜æ†¶é«”ç®¡ç†\n",
    "\n",
    "## å¯¦éš›éƒ¨ç½²è€ƒé‡\n",
    "\n",
    "### æ•ˆèƒ½æœ€ä½³åŒ–\n",
    "- ä½¿ç”¨é‡åŒ–æ¨¡å‹æ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨\n",
    "- å¯¦æ–½æ¨¡å‹ä¸¦è¡Œå’Œæµæ°´ç·šè™•ç†\n",
    "- æœ€ä½³åŒ–å‘é‡æª¢ç´¢æ¼”ç®—æ³•\n",
    "\n",
    "### è³‡æ–™å®‰å…¨\n",
    "- æœ¬åœ°éƒ¨ç½²é¿å…è³‡æ–™å¤–æ´©\n",
    "- å­˜å–æ§åˆ¶å’Œä½¿ç”¨è€…èªè­‰\n",
    "- æ•æ„Ÿè³‡è¨Šéæ¿¾å’ŒåŒ¿ååŒ–\n",
    "\n",
    "### æ“´å±•æ€§è¨­è¨ˆ\n",
    "- å¾®æœå‹™æ¶æ§‹ä¾¿æ–¼ç¶­è­·å’Œæ“´å±•\n",
    "- æ”¯æ´å‹•æ…‹æ·»åŠ æ–°æ–‡æª”å’ŒçŸ¥è­˜æº\n",
    "- ç›£æ§å’Œæ—¥èªŒç³»çµ±å®Œå–„\n",
    "\n",
    "é€™å€‹æŒ‡å—æä¾›äº†å»ºç«‹é«˜å“è³ªä¸­æ–‡RAGç³»çµ±çš„å®Œæ•´æ¡†æ¶ï¼Œå¾æŠ€è¡“é¸å‹åˆ°å¯¦éš›éƒ¨ç½²éƒ½æœ‰è©³ç´°èªªæ˜ã€‚\n",
    "\"\"\",\n",
    "    }\n",
    "\n",
    "    created_files = []\n",
    "    for filename, content in sample_docs.items():\n",
    "        file_path = docs_dir / filename\n",
    "        if not file_path.exists():\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(content)\n",
    "            created_files.append(filename)\n",
    "            print(f\"âœ… å»ºç«‹ç¯„ä¾‹æ–‡æª”: {filename}\")\n",
    "\n",
    "    return created_files\n",
    "\n",
    "\n",
    "def scan_local_documents() -> List[Dict[str, Any]]:\n",
    "    \"\"\"æƒææœ¬åœ°æ–‡æª”ç›®éŒ„ (Scan local document directory)\"\"\"\n",
    "    docs_dir = pathlib.Path(nb29_paths[\"docs\"])\n",
    "\n",
    "    if not any(docs_dir.iterdir()):\n",
    "        print(f\"ğŸ“ æ–‡æª”ç›®éŒ„ç‚ºç©ºï¼Œå»ºç«‹ç¯„ä¾‹æ–‡æª”...\")\n",
    "        created_files = create_sample_chinese_docs()\n",
    "        print(f\"âœ… å»ºç«‹äº† {len(created_files)} å€‹ç¯„ä¾‹æ–‡æª”\")\n",
    "\n",
    "    # Scan for documents\n",
    "    supported_extensions = [\".md\", \".txt\", \".mdx\"]\n",
    "    documents = []\n",
    "\n",
    "    for file_path in docs_dir.iterdir():\n",
    "        if file_path.is_file() and file_path.suffix.lower() in supported_extensions:\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                doc_info = {\n",
    "                    \"filename\": file_path.name,\n",
    "                    \"filepath\": str(file_path),\n",
    "                    \"content\": content,\n",
    "                    \"size\": len(content),\n",
    "                    \"extension\": file_path.suffix,\n",
    "                    \"modified\": datetime.fromtimestamp(\n",
    "                        file_path.stat().st_mtime\n",
    "                    ).isoformat(),\n",
    "                }\n",
    "                documents.append(doc_info)\n",
    "                print(f\"ğŸ“„ è¼‰å…¥æ–‡æª”: {file_path.name} ({len(content)} å­—ç¬¦)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ è¼‰å…¥å¤±æ•— {file_path.name}: {e}\")\n",
    "\n",
    "    print(f\"\\nğŸ“š ç¸½å…±è¼‰å…¥ {len(documents)} å€‹æ–‡æª”\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Optional: Chinese text processing with OpenCC\n",
    "def setup_chinese_converter():\n",
    "    \"\"\"è¨­ç½®ä¸­æ–‡ç¹ç°¡è½‰æ› (Setup Chinese Traditional/Simplified converter)\"\"\"\n",
    "    try:\n",
    "        import opencc\n",
    "\n",
    "        converter = opencc.OpenCC(\"t2s\")  # Traditional to Simplified\n",
    "        print(\"âœ… OpenCC ç¹ç°¡è½‰æ›å™¨è¼‰å…¥æˆåŠŸ\")\n",
    "        return converter\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸ OpenCC æœªå®‰è£ï¼Œè·³éç¹ç°¡è½‰æ›åŠŸèƒ½\")\n",
    "        return None\n",
    "\n",
    "\n",
    "chinese_converter = setup_chinese_converter()\n",
    "\n",
    "\n",
    "def preprocess_chinese_text(text: str, converter=None) -> str:\n",
    "    \"\"\"é è™•ç†ä¸­æ–‡æ–‡æœ¬ (Preprocess Chinese text)\"\"\"\n",
    "    # Basic cleaning\n",
    "    text = text.strip()\n",
    "\n",
    "    # Optional: Convert traditional to simplified\n",
    "    if converter:\n",
    "        try:\n",
    "            text = converter.convert(text)\n",
    "        except:\n",
    "            pass  # Ignore conversion errors\n",
    "\n",
    "    # Remove excessive whitespace\n",
    "    import re\n",
    "\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)  # Normalize line breaks\n",
    "    text = re.sub(r\" +\", \" \", text)  # Normalize spaces\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Text splitting with Chinese optimization\n",
    "def create_chinese_text_splitter():\n",
    "    \"\"\"å»ºç«‹ä¸­æ–‡æ–‡æœ¬åˆ†å‰²å™¨ (Create Chinese text splitter)\"\"\"\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=config.CHINESE_SEPARATORS,\n",
    "        chunk_size=config.CHUNK_SIZE,\n",
    "        chunk_overlap=config.CHUNK_OVERLAP,\n",
    "        length_function=len,  # Use character count for Chinese\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ“ ä¸­æ–‡æ–‡æœ¬åˆ†å‰²å™¨é…ç½®:\")\n",
    "    print(f\"- å¡Šå¤§å°: {config.CHUNK_SIZE} å­—ç¬¦\")\n",
    "    print(f\"- é‡ç–Š: {config.CHUNK_OVERLAP} å­—ç¬¦\")\n",
    "    print(f\"- åˆ†éš”ç¬¦: {config.CHINESE_SEPARATORS[:5]}...\")\n",
    "\n",
    "    return text_splitter\n",
    "\n",
    "\n",
    "def process_documents_to_chunks(documents: List[Dict]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"å°‡æ–‡æª”è™•ç†ç‚ºæ–‡æœ¬å¡Š (Process documents into text chunks)\"\"\"\n",
    "    text_splitter = create_chinese_text_splitter()\n",
    "    all_chunks = []\n",
    "\n",
    "    for doc in documents:\n",
    "        print(f\"\\nğŸ“„ è™•ç†æ–‡æª”: {doc['filename']}\")\n",
    "\n",
    "        # Preprocess text\n",
    "        content = preprocess_chinese_text(doc[\"content\"], chinese_converter)\n",
    "\n",
    "        # Split into chunks\n",
    "        chunks = text_splitter.split_text(content)\n",
    "\n",
    "        # Create chunk metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_info = {\n",
    "                \"content\": chunk,\n",
    "                \"source_file\": doc[\"filename\"],\n",
    "                \"source_path\": doc[\"filepath\"],\n",
    "                \"chunk_index\": i,\n",
    "                \"chunk_id\": f\"{doc['filename']}_chunk_{i}\",\n",
    "                \"char_count\": len(chunk),\n",
    "                \"metadata\": {\n",
    "                    \"filename\": doc[\"filename\"],\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"file_size\": doc[\"size\"],\n",
    "                },\n",
    "            }\n",
    "            all_chunks.append(chunk_info)\n",
    "\n",
    "        print(f\"  âœ… åˆ†å‰²ç‚º {len(chunks)} å€‹æ–‡æœ¬å¡Š\")\n",
    "\n",
    "    print(f\"\\nğŸ“Š ç¸½è¨ˆè™•ç†çµæœ:\")\n",
    "    print(f\"- æ–‡æª”æ•¸é‡: {len(documents)}\")\n",
    "    print(f\"- æ–‡æœ¬å¡Šæ•¸é‡: {len(all_chunks)}\")\n",
    "    print(\n",
    "        f\"- å¹³å‡å¡Šå¤§å°: {sum(c['char_count'] for c in all_chunks) / len(all_chunks):.1f} å­—ç¬¦\"\n",
    "    )\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# Load and process documents\n",
    "print(f\"\\n=== Stage 3: è¼‰å…¥æœ¬åœ°ä¸­æ–‡æ–‡æª” ===\")\n",
    "documents = scan_local_documents()\n",
    "chunks = process_documents_to_chunks(documents)\n",
    "\n",
    "# Save chunks for debugging\n",
    "chunks_file = pathlib.Path(nb29_paths[\"vectorstore\"]) / \"chunks.json\"\n",
    "try:\n",
    "    with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"ğŸ’¾ æ–‡æœ¬å¡Šå·²ä¿å­˜è‡³: {chunks_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ä¿å­˜å¤±æ•—: {e}\")\n",
    "\n",
    "## Stage 3 Summary\n",
    "print(\n",
    "    f\"\"\"\n",
    "ğŸ¯ Stage 3 å®Œæˆç¸½çµ:\n",
    "âœ… å®Œæˆé …ç›®:\n",
    "- è‡ªå‹•å»ºç«‹ä¸­æ–‡ç¯„ä¾‹æ–‡æª”ï¼ˆAIæŠ€è¡“ã€é–‹æºLLMã€RAGæŒ‡å—ï¼‰\n",
    "- å¯¦ç¾ä¸­æ–‡æ–‡æœ¬é è™•ç†èˆ‡ç¹ç°¡è½‰æ›\n",
    "- è¨­è¨ˆä¸­æ–‡å„ªåŒ–çš„æ–‡æœ¬åˆ†å‰²ç­–ç•¥\n",
    "- ç”Ÿæˆå¸¶æœ‰å®Œæ•´å…ƒè³‡æ–™çš„æ–‡æœ¬å¡Š\n",
    "\n",
    "ğŸ§  æ ¸å¿ƒè§€å¿µ:\n",
    "- ä¸­æ–‡åˆ†è©ç‰¹æ€§ï¼šä½¿ç”¨æ¨™é»ç¬¦è™Ÿä½œç‚ºä¸»è¦åˆ†éš”ç¬¦\n",
    "- èªç¾©å®Œæ•´æ€§ï¼šä¿æŒæ–‡æœ¬å¡Šçš„èªç¾©é€£è²«æ€§\n",
    "- å…ƒè³‡æ–™è¿½è¹¤ï¼šç‚ºæ¯å€‹æ–‡æœ¬å¡Šä¿ç•™ä¾†æºè³‡è¨Š\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥: å»ºç«‹ä¸­æ–‡åµŒå…¥å‘é‡èˆ‡FAISSç´¢å¼•\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a009c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 4 - Embedding & Index (ä¸­æ–‡åµŒå…¥èˆ‡ç´¢å¼•)\n",
    "# ä¸­æ–‡åµŒå…¥æ¨¡å‹èˆ‡å‘é‡ç´¢å¼•å»ºç«‹\n",
    "\n",
    "\n",
    "def load_chinese_embedding_model(model_name: str = \"bge-m3\"):\n",
    "    \"\"\"è¼‰å…¥ä¸­æ–‡åµŒå…¥æ¨¡å‹ (Load Chinese embedding model)\"\"\"\n",
    "    model_id = config.EMBEDDING_MODELS.get(\n",
    "        model_name, config.EMBEDDING_MODELS[\"bge-m3\"]\n",
    "    )\n",
    "\n",
    "    print(f\"\\nğŸ”— è¼‰å…¥åµŒå…¥æ¨¡å‹: {model_id}\")\n",
    "\n",
    "    try:\n",
    "        # Load with explicit cache directory\n",
    "        embedding_model = SentenceTransformer(\n",
    "            model_id, cache_folder=cache_paths[\"HF_HOME\"]\n",
    "        )\n",
    "\n",
    "        # Test embedding\n",
    "        test_text = \"é€™æ˜¯ä¸€å€‹ä¸­æ–‡æ¸¬è©¦å¥å­ã€‚\"\n",
    "        test_embedding = embedding_model.encode([test_text])\n",
    "\n",
    "        print(f\"âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥æˆåŠŸ\")\n",
    "        print(f\"ğŸ“ å‘é‡ç¶­åº¦: {test_embedding.shape[1]}\")\n",
    "        print(f\"ğŸ§ª æ¸¬è©¦åµŒå…¥: {test_embedding[0][:5]}...\")\n",
    "\n",
    "        return embedding_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\")\n",
    "        print(f\"ğŸ”„ å˜—è©¦ä½¿ç”¨å‚™ç”¨æ¨¡å‹...\")\n",
    "\n",
    "        # Fallback to smaller model\n",
    "        try:\n",
    "            fallback_model_id = config.EMBEDDING_MODELS[\"bge-small-zh\"]\n",
    "            embedding_model = SentenceTransformer(fallback_model_id)\n",
    "            print(f\"âœ… å‚™ç”¨æ¨¡å‹è¼‰å…¥æˆåŠŸ: {fallback_model_id}\")\n",
    "            return embedding_model\n",
    "        except Exception as e2:\n",
    "            print(f\"âŒ å‚™ç”¨æ¨¡å‹ä¹Ÿå¤±æ•—: {e2}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def create_embeddings_for_chunks(\n",
    "    chunks: List[Dict], embedding_model\n",
    ") -> Tuple[List[np.ndarray], List[Dict]]:\n",
    "    \"\"\"ç‚ºæ–‡æœ¬å¡Šå»ºç«‹åµŒå…¥å‘é‡ (Create embeddings for text chunks)\"\"\"\n",
    "    print(f\"\\nğŸ§® é–‹å§‹å»ºç«‹ {len(chunks)} å€‹æ–‡æœ¬å¡Šçš„åµŒå…¥å‘é‡...\")\n",
    "\n",
    "    # Extract text content\n",
    "    texts = [chunk[\"content\"] for chunk in chunks]\n",
    "\n",
    "    # Create embeddings in batches to manage memory\n",
    "    batch_size = 32\n",
    "    all_embeddings = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_embeddings = embedding_model.encode(\n",
    "            batch_texts,\n",
    "            normalize_embeddings=True,  # Normalize for cosine similarity\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "        if (i // batch_size + 1) % 5 == 0:\n",
    "            print(\n",
    "                f\"  ğŸ“Š é€²åº¦: {min(i+batch_size, len(texts))}/{len(texts)} ({(i+batch_size)/len(texts)*100:.1f}%)\"\n",
    "            )\n",
    "\n",
    "    embedding_time = time.time() - start_time\n",
    "    print(f\"âœ… åµŒå…¥å»ºç«‹å®Œæˆï¼Œè€—æ™‚: {embedding_time:.2f}ç§’\")\n",
    "    print(f\"ğŸ“ˆ å¹³å‡é€Ÿåº¦: {len(chunks)/embedding_time:.1f} chunks/sec\")\n",
    "\n",
    "    # Convert to numpy array\n",
    "    embeddings_array = np.array(all_embeddings).astype(\"float32\")\n",
    "    print(f\"ğŸ“ åµŒå…¥çŸ©é™£å½¢ç‹€: {embeddings_array.shape}\")\n",
    "\n",
    "    return embeddings_array, chunks\n",
    "\n",
    "\n",
    "def build_faiss_index(\n",
    "    embeddings: np.ndarray, chunks: List[Dict]\n",
    ") -> Tuple[faiss.Index, List[Dict]]:\n",
    "    \"\"\"å»ºç«‹FAISSå‘é‡ç´¢å¼• (Build FAISS vector index)\"\"\"\n",
    "    print(f\"\\nğŸ—ƒï¸ å»ºç«‹FAISSç´¢å¼•...\")\n",
    "\n",
    "    dimension = embeddings.shape[1]\n",
    "    num_vectors = embeddings.shape[0]\n",
    "\n",
    "    print(f\"ğŸ“ å‘é‡ç¶­åº¦: {dimension}\")\n",
    "    print(f\"ğŸ“Š å‘é‡æ•¸é‡: {num_vectors}\")\n",
    "\n",
    "    # Choose index type based on dataset size\n",
    "    if num_vectors < 1000:\n",
    "        # For small datasets, use flat index (exact search)\n",
    "        index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity)\n",
    "        print(\"ğŸ” ä½¿ç”¨ Flat ç´¢å¼• (ç²¾ç¢ºæœç´¢)\")\n",
    "    else:\n",
    "        # For larger datasets, use approximate search\n",
    "        nlist = min(100, num_vectors // 10)  # Number of clusters\n",
    "        index = faiss.IndexIVFFlat(faiss.IndexFlatIP(dimension), dimension, nlist)\n",
    "        print(f\"ğŸ” ä½¿ç”¨ IVF ç´¢å¼• (è¿‘ä¼¼æœç´¢, nlist={nlist})\")\n",
    "\n",
    "        # Train the index\n",
    "        print(\"ğŸ‹ï¸ è¨“ç·´ç´¢å¼•...\")\n",
    "        index.train(embeddings)\n",
    "\n",
    "    # Add vectors to index\n",
    "    print(\"ğŸ“¥ æ·»åŠ å‘é‡åˆ°ç´¢å¼•...\")\n",
    "    index.add(embeddings)\n",
    "\n",
    "    print(f\"âœ… FAISSç´¢å¼•å»ºç«‹å®Œæˆ\")\n",
    "    print(f\"ğŸ“Š ç´¢å¼•çµ±è¨ˆ: {index.ntotal} å€‹å‘é‡\")\n",
    "\n",
    "    return index, chunks\n",
    "\n",
    "\n",
    "def save_vector_store(\n",
    "    index: faiss.Index, chunks: List[Dict], embedding_model_name: str\n",
    "):\n",
    "    \"\"\"ä¿å­˜å‘é‡å­˜å„² (Save vector store)\"\"\"\n",
    "    vectorstore_dir = pathlib.Path(nb29_paths[\"vectorstore\"])\n",
    "\n",
    "    # Save FAISS index\n",
    "    index_file = vectorstore_dir / \"index.faiss\"\n",
    "    faiss.write_index(index, str(index_file))\n",
    "\n",
    "    # Save chunks metadata\n",
    "    chunks_file = vectorstore_dir / \"chunks_metadata.json\"\n",
    "    with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Save configuration\n",
    "    config_file = vectorstore_dir / \"config.json\"\n",
    "    config_data = {\n",
    "        \"embedding_model\": embedding_model_name,\n",
    "        \"index_type\": \"flat\" if \"Flat\" in str(type(index)) else \"ivf\",\n",
    "        \"dimension\": index.d,\n",
    "        \"total_vectors\": index.ntotal,\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"chunk_size\": config.CHUNK_SIZE,\n",
    "        \"chunk_overlap\": config.CHUNK_OVERLAP,\n",
    "    }\n",
    "\n",
    "    with open(config_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config_data, f, indent=2)\n",
    "\n",
    "    print(f\"ğŸ’¾ å‘é‡å­˜å„²å·²ä¿å­˜è‡³: {vectorstore_dir}\")\n",
    "    print(f\"  - ç´¢å¼•æª”æ¡ˆ: index.faiss\")\n",
    "    print(f\"  - å…ƒè³‡æ–™: chunks_metadata.json\")\n",
    "    print(f\"  - é…ç½®: config.json\")\n",
    "\n",
    "\n",
    "# Load embedding model and create index\n",
    "print(f\"\\n=== Stage 4: å»ºç«‹ä¸­æ–‡åµŒå…¥èˆ‡å‘é‡ç´¢å¼• ===\")\n",
    "\n",
    "embedding_model = load_chinese_embedding_model(\"bge-m3\")\n",
    "embeddings, chunk_metadata = create_embeddings_for_chunks(chunks, embedding_model)\n",
    "faiss_index, indexed_chunks = build_faiss_index(embeddings, chunk_metadata)\n",
    "\n",
    "# Save vector store\n",
    "save_vector_store(faiss_index, indexed_chunks, \"bge-m3\")\n",
    "\n",
    "# Memory cleanup\n",
    "del embeddings\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "## Stage 4 Summary\n",
    "print(\n",
    "    f\"\"\"\n",
    "ğŸ¯ Stage 4 å®Œæˆç¸½çµ:\n",
    "âœ… å®Œæˆé …ç›®:\n",
    "- è¼‰å…¥ä¸­æ–‡å„ªåŒ–çš„BGE-M3åµŒå…¥æ¨¡å‹\n",
    "- æ‰¹æ¬¡è™•ç†æ–‡æœ¬å¡Šå»ºç«‹åµŒå…¥å‘é‡\n",
    "- å»ºç«‹FAISSå‘é‡ç´¢å¼•æ”¯æ´èªç¾©æœç´¢\n",
    "- ä¿å­˜å®Œæ•´çš„å‘é‡å­˜å„²èˆ‡å…ƒè³‡æ–™\n",
    "\n",
    "ğŸ§  æ ¸å¿ƒè§€å¿µ:\n",
    "- ä¸­æ–‡åµŒå…¥ï¼šä½¿ç”¨å°ˆé–€å„ªåŒ–çš„ä¸­æ–‡åµŒå…¥æ¨¡å‹\n",
    "- å‘é‡æ­£è¦åŒ–ï¼šç¢ºä¿é¤˜å¼¦ç›¸ä¼¼åº¦è¨ˆç®—çš„æº–ç¢ºæ€§\n",
    "- ç´¢å¼•ç­–ç•¥ï¼šæ ¹æ“šè³‡æ–™è¦æ¨¡é¸æ“‡åˆé©çš„ç´¢å¼•é¡å‹\n",
    "- è¨˜æ†¶é«”ç®¡ç†ï¼šæ‰¹æ¬¡è™•ç†é¿å…è¨˜æ†¶é«”æº¢å‡º\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥: å¯¦ç¾æª¢ç´¢ã€é‡æ’åºèˆ‡ç”Ÿæˆæµç¨‹\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caeb826",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 5 - Retrieve â†’ (Re-rank) â†’ Generate\n",
    "# æª¢ç´¢ã€é‡æ’åºèˆ‡ç”Ÿæˆæµç¨‹\n",
    "\n",
    "\n",
    "class ChineseRAGRetriever:\n",
    "    \"\"\"ä¸­æ–‡RAGæª¢ç´¢å™¨ (Chinese RAG Retriever)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        faiss_index: faiss.Index,\n",
    "        chunks: List[Dict],\n",
    "        embedding_model,\n",
    "        llm_adapter: LLMAdapter,\n",
    "    ):\n",
    "        self.index = faiss_index\n",
    "        self.chunks = chunks\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm = llm_adapter\n",
    "        self.reranker = None\n",
    "\n",
    "        print(f\"ğŸ” RAGæª¢ç´¢å™¨åˆå§‹åŒ–å®Œæˆ\")\n",
    "        print(f\"ğŸ“Š ç´¢å¼•å‘é‡æ•¸: {self.index.ntotal}\")\n",
    "        print(f\"ğŸ“„ æ–‡æœ¬å¡Šæ•¸: {len(self.chunks)}\")\n",
    "\n",
    "    def load_reranker(self, model_name: str = \"bge-reranker-base\"):\n",
    "        \"\"\"è¼‰å…¥é‡æ’åºæ¨¡å‹ (Load reranker model)\"\"\"\n",
    "        try:\n",
    "            reranker_model_id = f\"BAAI/{model_name}\"\n",
    "            from sentence_transformers import CrossEncoder\n",
    "\n",
    "            self.reranker = CrossEncoder(\n",
    "                reranker_model_id, cache_folder=cache_paths[\"HF_HOME\"]\n",
    "            )\n",
    "            print(f\"âœ… é‡æ’åºæ¨¡å‹è¼‰å…¥æˆåŠŸ: {model_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ é‡æ’åºæ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\")\n",
    "            print(f\"ğŸ’¡ å°‡è·³éé‡æ’åºæ­¥é©Ÿ\")\n",
    "            self.reranker = None\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"æª¢ç´¢ç›¸é—œæ–‡æœ¬å¡Š (Retrieve relevant text chunks)\"\"\"\n",
    "        # Encode query\n",
    "        query_embedding = self.embedding_model.encode(\n",
    "            [query], normalize_embeddings=True\n",
    "        )\n",
    "        query_vector = query_embedding.astype(\"float32\")\n",
    "\n",
    "        # Search in FAISS index\n",
    "        scores, indices = self.index.search(query_vector, top_k)\n",
    "\n",
    "        # Prepare results\n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if idx != -1:  # Valid index\n",
    "                chunk = self.chunks[idx].copy()\n",
    "                chunk[\"similarity_score\"] = float(score)\n",
    "                chunk[\"rank\"] = i + 1\n",
    "                results.append(chunk)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def rerank(self, query: str, retrieved_chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"é‡æ’åºæª¢ç´¢çµæœ (Rerank retrieved results)\"\"\"\n",
    "        if not self.reranker or len(retrieved_chunks) <= 1:\n",
    "            return retrieved_chunks\n",
    "\n",
    "        try:\n",
    "            # Prepare query-document pairs\n",
    "            pairs = [(query, chunk[\"content\"]) for chunk in retrieved_chunks]\n",
    "\n",
    "            # Get reranking scores\n",
    "            rerank_scores = self.reranker.predict(pairs)\n",
    "\n",
    "            # Update chunks with rerank scores and sort\n",
    "            for chunk, score in zip(retrieved_chunks, rerank_scores):\n",
    "                chunk[\"rerank_score\"] = float(score)\n",
    "\n",
    "            # Sort by rerank score (descending)\n",
    "            reranked = sorted(\n",
    "                retrieved_chunks, key=lambda x: x[\"rerank_score\"], reverse=True\n",
    "            )\n",
    "\n",
    "            # Update ranks\n",
    "            for i, chunk in enumerate(reranked):\n",
    "                chunk[\"rerank_rank\"] = i + 1\n",
    "\n",
    "            print(f\"ğŸ”„ é‡æ’åºå®Œæˆï¼Œèª¿æ•´äº† {len(reranked)} å€‹çµæœçš„é †åº\")\n",
    "            return reranked\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ é‡æ’åºå¤±æ•—: {e}\")\n",
    "            return retrieved_chunks\n",
    "\n",
    "    def generate_answer(\n",
    "        self, query: str, context_chunks: List[Dict], max_context_length: int = 2000\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"åŸºæ–¼æª¢ç´¢ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ (Generate answer based on retrieved context)\"\"\"\n",
    "\n",
    "        # Prepare context from chunks\n",
    "        context_parts = []\n",
    "        total_length = 0\n",
    "        used_chunks = []\n",
    "\n",
    "        for chunk in context_chunks:\n",
    "            content = chunk[\"content\"]\n",
    "            if total_length + len(content) <= max_context_length:\n",
    "                context_parts.append(f\"ã€ä¾†æºï¼š{chunk['source_file']}ã€‘\\n{content}\")\n",
    "                total_length += len(content)\n",
    "                used_chunks.append(chunk)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "        # Create RAG prompt\n",
    "        rag_prompt = f\"\"\"åŸºæ–¼ä»¥ä¸‹è³‡æ–™ä¾†æºï¼Œè«‹ç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚è«‹ç¢ºä¿ç­”æ¡ˆæº–ç¢ºä¸”æœ‰æ ¹æ“šï¼Œä¸¦åœ¨ç­”æ¡ˆæœ«å°¾åˆ—å‡ºåƒè€ƒçš„è³‡æ–™ä¾†æºã€‚\n",
    "\n",
    "å•é¡Œï¼š{query}\n",
    "\n",
    "åƒè€ƒè³‡æ–™ï¼š\n",
    "{context}\n",
    "\n",
    "è«‹æä¾›è©³ç´°ä¸”æº–ç¢ºçš„å›ç­”ï¼š\"\"\"\n",
    "\n",
    "        # Generate answer\n",
    "        messages = [{\"role\": \"user\", \"content\": rag_prompt}]\n",
    "\n",
    "        start_time = time.time()\n",
    "        answer = self.llm.generate(messages, max_length=512, temperature=0.3)\n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "        # Prepare result\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"context_chunks\": used_chunks,\n",
    "            \"context_length\": total_length,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"sources\": list(set(chunk[\"source_file\"] for chunk in used_chunks)),\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def answer(\n",
    "        self, query: str, top_k: int = 5, use_reranker: bool = True\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"ç«¯åˆ°ç«¯å•ç­” (End-to-end question answering)\"\"\"\n",
    "        print(f\"\\nâ“ å•é¡Œ: {query}\")\n",
    "\n",
    "        # Step 1: Retrieve\n",
    "        print(f\"ğŸ” æª¢ç´¢ä¸­...\")\n",
    "        retrieved = self.retrieve(query, top_k)\n",
    "        print(f\"ğŸ“„ æª¢ç´¢åˆ° {len(retrieved)} å€‹ç›¸é—œæ–‡æœ¬å¡Š\")\n",
    "\n",
    "        # Step 2: Rerank (optional)\n",
    "        if use_reranker and self.reranker:\n",
    "            print(f\"ğŸ”„ é‡æ’åºä¸­...\")\n",
    "            retrieved = self.rerank(query, retrieved)\n",
    "\n",
    "        # Step 3: Generate\n",
    "        print(f\"âœï¸ ç”Ÿæˆç­”æ¡ˆä¸­...\")\n",
    "        result = self.generate_answer(query, retrieved)\n",
    "\n",
    "        print(f\"âœ… å›ç­”å®Œæˆ (è€—æ™‚: {result['generation_time']:.2f}ç§’)\")\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# Initialize retriever\n",
    "print(f\"\\n=== Stage 5: å»ºç«‹RAGæª¢ç´¢ç”Ÿæˆç³»çµ± ===\")\n",
    "\n",
    "rag_retriever = ChineseRAGRetriever(\n",
    "    faiss_index=faiss_index,\n",
    "    chunks=indexed_chunks,\n",
    "    embedding_model=embedding_model,\n",
    "    llm_adapter=llm,\n",
    ")\n",
    "\n",
    "# Optional: Load reranker (comment out if VRAM is limited)\n",
    "# rag_retriever.load_reranker(\"bge-reranker-base\")\n",
    "\n",
    "# Demo queries\n",
    "demo_queries = [\n",
    "    \"ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ\",\n",
    "    \"RAGç³»çµ±æœ‰ä»€éº¼å„ªå‹¢ï¼Ÿ\",\n",
    "    \"å¦‚ä½•é¸æ“‡åˆé©çš„é–‹æºä¸­æ–‡æ¨¡å‹ï¼Ÿ\",\n",
    "    \"ä¸­æ–‡æ–‡æœ¬è™•ç†æœ‰ä»€éº¼ç‰¹æ®Šè€ƒé‡ï¼Ÿ\",\n",
    "]\n",
    "\n",
    "print(f\"\\nğŸ¯ RAGç³»çµ±æ¼”ç¤º\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "demo_results = []\n",
    "for query in demo_queries[:2]:  # Limit to 2 queries for demo\n",
    "    try:\n",
    "        result = rag_retriever.answer(query, top_k=3, use_reranker=False)\n",
    "        demo_results.append(result)\n",
    "\n",
    "        print(f\"\\nğŸ“ å•ç­”çµæœ:\")\n",
    "        print(f\"Q: {result['query']}\")\n",
    "        print(f\"A: {result['answer']}\")\n",
    "        print(f\"ğŸ“š åƒè€ƒä¾†æº: {', '.join(result['sources'])}\")\n",
    "        print(f\"â±ï¸ ç”Ÿæˆæ™‚é–“: {result['generation_time']:.2f}ç§’\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å•ç­”å¤±æ•—: {e}\")\n",
    "\n",
    "## Stage 5 Summary\n",
    "print(\n",
    "    f\"\"\"\n",
    "ğŸ¯ Stage 5 å®Œæˆç¸½çµ:\n",
    "âœ… å®Œæˆé …ç›®:\n",
    "- å¯¦ç¾å®Œæ•´çš„RAGæª¢ç´¢ç”Ÿæˆæµç¨‹\n",
    "- æ”¯æ´FAISSå‘é‡æª¢ç´¢èˆ‡é¤˜å¼¦ç›¸ä¼¼åº¦è¨ˆç®—\n",
    "- è¨­è¨ˆä¸­æ–‡å„ªåŒ–çš„RAGæç¤ºæ¨¡æ¿\n",
    "- æä¾›ç«¯åˆ°ç«¯å•ç­”ä»‹é¢èˆ‡ä¾†æºè¿½è¹¤\n",
    "\n",
    "ğŸ§  æ ¸å¿ƒè§€å¿µ:\n",
    "- æª¢ç´¢ç­–ç•¥ï¼šåŸºæ–¼èªç¾©ç›¸ä¼¼åº¦çš„å‘é‡æª¢ç´¢\n",
    "- ä¸Šä¸‹æ–‡ç®¡ç†ï¼šæ§åˆ¶è¼¸å…¥é•·åº¦é¿å…è¶…å‡ºæ¨¡å‹é™åˆ¶\n",
    "- æç¤ºå·¥ç¨‹ï¼šè¨­è¨ˆæœ‰æ•ˆçš„ä¸­æ–‡RAGæç¤ºæ¨¡æ¿\n",
    "- ä¾†æºæ­¸å±¬ï¼šç¢ºä¿ç­”æ¡ˆå¯è¿½æº¯åˆ°åŸå§‹æ–‡æª”\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥: å¯¦ç¾è©•ä¼°æŒ‡æ¨™èˆ‡æ•ˆèƒ½åˆ†æ\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5648c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 6 - Evaluation (è¼•é‡è©•ä¼°)\n",
    "# æª¢ç´¢èˆ‡ç”Ÿæˆå“è³ªè©•ä¼°\n",
    "\n",
    "\n",
    "def create_evaluation_dataset():\n",
    "    \"\"\"å»ºç«‹è©•ä¼°è³‡æ–™é›† (Create evaluation dataset)\"\"\"\n",
    "    eval_queries = [\n",
    "        {\n",
    "            \"query\": \"ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ\",\n",
    "            \"expected_keywords\": [\"LLM\", \"èªè¨€æ¨¡å‹\", \"GPT\", \"è¨“ç·´\", \"åƒæ•¸\"],\n",
    "            \"expected_sources\": [\"AIæŠ€è¡“ç™¼å±•è¶¨å‹¢.md\"],\n",
    "            \"category\": \"definition\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"RAGç³»çµ±çš„æ ¸å¿ƒå„ªå‹¢æ˜¯ä»€éº¼ï¼Ÿ\",\n",
    "            \"expected_keywords\": [\"RAG\", \"æª¢ç´¢å¢å¼·\", \"çŸ¥è­˜åº«\", \"æ™‚æ•ˆæ€§\", \"å°ˆæ¥­æ€§\"],\n",
    "            \"expected_sources\": [\"AIæŠ€è¡“ç™¼å±•è¶¨å‹¢.md\", \"RAGç³»çµ±å¯¦ä½œæŒ‡å—.md\"],\n",
    "            \"category\": \"concept\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"æœ‰å“ªäº›é‡è¦çš„é–‹æºä¸­æ–‡æ¨¡å‹ï¼Ÿ\",\n",
    "            \"expected_keywords\": [\"Qwen\", \"ChatGLM\", \"DeepSeek\", \"é–‹æº\", \"ä¸­æ–‡\"],\n",
    "            \"expected_sources\": [\"é–‹æºLLMç”Ÿæ…‹ç³»çµ±.md\"],\n",
    "            \"category\": \"enumeration\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"å¦‚ä½•è©•ä¼°RAGç³»çµ±çš„æª¢ç´¢å“è³ªï¼Ÿ\",\n",
    "            \"expected_keywords\": [\"Recall\", \"MRR\", \"NDCG\", \"ç›¸é—œæ€§\", \"è©•ä¼°\"],\n",
    "            \"expected_sources\": [\"RAGç³»çµ±å¯¦ä½œæŒ‡å—.md\"],\n",
    "            \"category\": \"methodology\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"ä¸­æ–‡æ–‡æœ¬åˆ†å‰²æœ‰ä»€éº¼ç‰¹æ®Šè€ƒé‡ï¼Ÿ\",\n",
    "            \"expected_keywords\": [\"åˆ†è©\", \"æ¨™é»ç¬¦è™Ÿ\", \"èªç¾©\", \"chunk\", \"ä¸­æ–‡\"],\n",
    "            \"expected_sources\": [\"RAGç³»çµ±å¯¦ä½œæŒ‡å—.md\"],\n",
    "            \"category\": \"technical\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return eval_queries\n",
    "\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"RAGç³»çµ±è©•ä¼°å™¨ (RAG System Evaluator)\"\"\"\n",
    "\n",
    "    def __init__(self, retriever: ChineseRAGRetriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def evaluate_retrieval(\n",
    "        self, eval_queries: List[Dict], k_values: List[int] = [1, 3, 5]\n",
    "    ) -> Dict:\n",
    "        \"\"\"è©•ä¼°æª¢ç´¢å“è³ª (Evaluate retrieval quality)\"\"\"\n",
    "        print(f\"\\nğŸ“Š è©•ä¼°æª¢ç´¢å“è³ª...\")\n",
    "\n",
    "        results = {\n",
    "            \"recall_at_k\": {k: [] for k in k_values},\n",
    "            \"mrr_scores\": [],\n",
    "            \"query_results\": [],\n",
    "        }\n",
    "\n",
    "        for query_data in eval_queries:\n",
    "            query = query_data[\"query\"]\n",
    "            expected_sources = set(query_data[\"expected_sources\"])\n",
    "\n",
    "            # Retrieve documents\n",
    "            retrieved = self.retriever.retrieve(query, top_k=max(k_values))\n",
    "            retrieved_sources = [chunk[\"source_file\"] for chunk in retrieved]\n",
    "\n",
    "            # Calculate Recall@K\n",
    "            query_recall = {}\n",
    "            for k in k_values:\n",
    "                retrieved_k = set(retrieved_sources[:k])\n",
    "                relevant_found = len(retrieved_k.intersection(expected_sources))\n",
    "                recall_k = (\n",
    "                    relevant_found / len(expected_sources) if expected_sources else 0\n",
    "                )\n",
    "                query_recall[k] = recall_k\n",
    "                results[\"recall_at_k\"][k].append(recall_k)\n",
    "\n",
    "            # Calculate MRR (Mean Reciprocal Rank)\n",
    "            reciprocal_rank = 0\n",
    "            for i, source in enumerate(retrieved_sources):\n",
    "                if source in expected_sources:\n",
    "                    reciprocal_rank = 1.0 / (i + 1)\n",
    "                    break\n",
    "            results[\"mrr_scores\"].append(reciprocal_rank)\n",
    "\n",
    "            # Store detailed results\n",
    "            query_result = {\n",
    "                \"query\": query,\n",
    "                \"expected_sources\": list(expected_sources),\n",
    "                \"retrieved_sources\": retrieved_sources,\n",
    "                \"recall_at_k\": query_recall,\n",
    "                \"reciprocal_rank\": reciprocal_rank,\n",
    "            }\n",
    "            results[\"query_results\"].append(query_result)\n",
    "\n",
    "            print(\n",
    "                f\"  ğŸ“ {query[:30]}... - Recall@3: {query_recall[3]:.2f}, RR: {reciprocal_rank:.2f}\"\n",
    "            )\n",
    "\n",
    "        # Calculate averages\n",
    "        avg_recall = {}\n",
    "        for k in k_values:\n",
    "            avg_recall[k] = sum(results[\"recall_at_k\"][k]) / len(\n",
    "                results[\"recall_at_k\"][k]\n",
    "            )\n",
    "\n",
    "        avg_mrr = sum(results[\"mrr_scores\"]) / len(results[\"mrr_scores\"])\n",
    "\n",
    "        results[\"average_recall_at_k\"] = avg_recall\n",
    "        results[\"average_mrr\"] = avg_mrr\n",
    "\n",
    "        return results\n",
    "\n",
    "    def evaluate_generation(self, eval_queries: List[Dict]) -> Dict:\n",
    "        \"\"\"è©•ä¼°ç”Ÿæˆå“è³ª (Evaluate generation quality)\"\"\"\n",
    "        print(f\"\\nğŸ“ è©•ä¼°ç”Ÿæˆå“è³ª...\")\n",
    "\n",
    "        results = {\n",
    "            \"groundedness_scores\": [],\n",
    "            \"keyword_coverage\": [],\n",
    "            \"response_lengths\": [],\n",
    "            \"generation_times\": [],\n",
    "            \"query_results\": [],\n",
    "        }\n",
    "\n",
    "        for query_data in eval_queries:\n",
    "            query = query_data[\"query\"]\n",
    "            expected_keywords = query_data[\"expected_keywords\"]\n",
    "\n",
    "            # Generate answer\n",
    "            answer_result = self.retriever.answer(query, top_k=3, use_reranker=False)\n",
    "            answer = answer_result[\"answer\"]\n",
    "\n",
    "            groundedness = self.calculate_groundedness(answer, context_text)\n",
    "            results[\"groundedness_scores\"].append(groundedness)\n",
    "\n",
    "            # Evaluate keyword coverage\n",
    "            answer_lower = answer.lower()\n",
    "            keyword_hits = sum(\n",
    "                1 for keyword in expected_keywords if keyword.lower() in answer_lower\n",
    "            )\n",
    "            coverage = keyword_hits / len(expected_keywords) if expected_keywords else 0\n",
    "            results[\"keyword_coverage\"].append(coverage)\n",
    "\n",
    "            # Record metrics\n",
    "            results[\"response_lengths\"].append(len(answer))\n",
    "            results[\"generation_times\"].append(answer_result[\"generation_time\"])\n",
    "\n",
    "            query_result = {\n",
    "                \"query\": query,\n",
    "                \"answer\": answer,\n",
    "                \"expected_keywords\": expected_keywords,\n",
    "                \"keyword_coverage\": coverage,\n",
    "                \"groundedness\": groundedness,\n",
    "                \"response_length\": len(answer),\n",
    "                \"generation_time\": answer_result[\"generation_time\"],\n",
    "                \"sources_used\": answer_result[\"sources\"],\n",
    "            }\n",
    "            results[\"query_results\"].append(query_result)\n",
    "\n",
    "            print(\n",
    "                f\"  ğŸ“ {query[:30]}... - é—œéµè©è¦†è“‹: {coverage:.2f}, äº‹å¯¦æ€§: {groundedness:.2f}\"\n",
    "            )\n",
    "\n",
    "        # Calculate averages\n",
    "        results[\"average_keyword_coverage\"] = sum(results[\"keyword_coverage\"]) / len(\n",
    "            results[\"keyword_coverage\"]\n",
    "        )\n",
    "        results[\"average_groundedness\"] = sum(results[\"groundedness_scores\"]) / len(\n",
    "            results[\"groundedness_scores\"]\n",
    "        )\n",
    "        results[\"average_response_length\"] = sum(results[\"response_lengths\"]) / len(\n",
    "            results[\"response_lengths\"]\n",
    "        )\n",
    "        results[\"average_generation_time\"] = sum(results[\"generation_times\"]) / len(\n",
    "            results[\"generation_times\"]\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def calculate_groundedness(self, answer: str, context: str) -> float:\n",
    "        \"\"\"è¨ˆç®—ç­”æ¡ˆçš„äº‹å¯¦ä¾æ“šæ€§ (Calculate answer groundedness)\"\"\"\n",
    "        if not context.strip():\n",
    "            return 0.0\n",
    "\n",
    "        # Simple approach: check if key phrases from answer appear in context\n",
    "        import re\n",
    "\n",
    "        # Split answer into sentences\n",
    "        answer_sentences = re.split(r\"[ã€‚ï¼ï¼Ÿ]\", answer)\n",
    "        answer_sentences = [s.strip() for s in answer_sentences if s.strip()]\n",
    "\n",
    "        if not answer_sentences:\n",
    "            return 0.0\n",
    "\n",
    "        grounded_sentences = 0\n",
    "        for sentence in answer_sentences:\n",
    "            # Check if sentence has substantial overlap with context\n",
    "            sentence_words = set(sentence.split())\n",
    "            context_words = set(context.split())\n",
    "\n",
    "            if len(sentence_words) > 2:  # Only check substantial sentences\n",
    "                overlap = len(sentence_words.intersection(context_words))\n",
    "                overlap_ratio = overlap / len(sentence_words)\n",
    "\n",
    "                if overlap_ratio > 0.3:  # At least 30% word overlap\n",
    "                    grounded_sentences += 1\n",
    "\n",
    "        return grounded_sentences / len(answer_sentences)\n",
    "\n",
    "    def performance_benchmark(self, num_queries: int = 10) -> Dict:\n",
    "        \"\"\"æ•ˆèƒ½åŸºæº–æ¸¬è©¦ (Performance benchmark)\"\"\"\n",
    "        print(f\"\\nâš¡ åŸ·è¡Œæ•ˆèƒ½åŸºæº–æ¸¬è©¦...\")\n",
    "\n",
    "        test_query = \"ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹çš„ä¸»è¦ç‰¹é»ï¼Ÿ\"\n",
    "\n",
    "        retrieval_times = []\n",
    "        generation_times = []\n",
    "        total_times = []\n",
    "\n",
    "        for i in range(num_queries):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Retrieval timing\n",
    "            retrieval_start = time.time()\n",
    "            retrieved = self.retriever.retrieve(test_query, top_k=5)\n",
    "            retrieval_time = time.time() - retrieval_start\n",
    "            retrieval_times.append(retrieval_time)\n",
    "\n",
    "            # Generation timing\n",
    "            generation_start = time.time()\n",
    "            result = self.retriever.generate_answer(test_query, retrieved)\n",
    "            generation_time = time.time() - generation_start\n",
    "            generation_times.append(generation_time)\n",
    "\n",
    "            total_time = time.time() - start_time\n",
    "            total_times.append(total_time)\n",
    "\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"  ğŸ“Š å®Œæˆ {i+1}/{num_queries} æ¬¡æ¸¬è©¦\")\n",
    "\n",
    "        # Calculate statistics\n",
    "        def calc_stats(times):\n",
    "            return {\n",
    "                \"mean\": sum(times) / len(times),\n",
    "                \"min\": min(times),\n",
    "                \"max\": max(times),\n",
    "                \"std\": (\n",
    "                    sum((t - sum(times) / len(times)) ** 2 for t in times) / len(times)\n",
    "                )\n",
    "                ** 0.5,\n",
    "            }\n",
    "\n",
    "        benchmark_results = {\n",
    "            \"retrieval\": calc_stats(retrieval_times),\n",
    "            \"generation\": calc_stats(generation_times),\n",
    "            \"total\": calc_stats(total_times),\n",
    "            \"num_queries\": num_queries,\n",
    "        }\n",
    "\n",
    "        return benchmark_results\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "print(f\"\\n=== Stage 6: RAGç³»çµ±è©•ä¼° ===\")\n",
    "\n",
    "eval_dataset = create_evaluation_dataset()\n",
    "evaluator = RAGEvaluator(rag_retriever)\n",
    "\n",
    "print(f\"ğŸ“‹ è©•ä¼°è³‡æ–™é›†: {len(eval_dataset)} å€‹æŸ¥è©¢\")\n",
    "\n",
    "# Retrieval evaluation\n",
    "retrieval_results = evaluator.evaluate_retrieval(eval_dataset)\n",
    "\n",
    "print(f\"\\nğŸ“Š æª¢ç´¢è©•ä¼°çµæœ:\")\n",
    "for k, recall in retrieval_results[\"average_recall_at_k\"].items():\n",
    "    print(f\"  - Recall@{k}: {recall:.3f}\")\n",
    "print(f\"  - MRR: {retrieval_results['average_mrr']:.3f}\")\n",
    "\n",
    "# Generation evaluation\n",
    "generation_results = evaluator.evaluate_generation(eval_dataset[:3])  # Limit for demo\n",
    "\n",
    "print(f\"\\nğŸ“ ç”Ÿæˆè©•ä¼°çµæœ:\")\n",
    "print(f\"  - å¹³å‡é—œéµè©è¦†è“‹ç‡: {generation_results['average_keyword_coverage']:.3f}\")\n",
    "print(f\"  - å¹³å‡äº‹å¯¦ä¾æ“šæ€§: {generation_results['average_groundedness']:.3f}\")\n",
    "print(f\"  - å¹³å‡å›æ‡‰é•·åº¦: {generation_results['average_response_length']:.1f} å­—ç¬¦\")\n",
    "print(f\"  - å¹³å‡ç”Ÿæˆæ™‚é–“: {generation_results['average_generation_time']:.2f} ç§’\")\n",
    "\n",
    "# Performance benchmark\n",
    "performance_results = evaluator.performance_benchmark(num_queries=5)\n",
    "\n",
    "print(f\"\\nâš¡ æ•ˆèƒ½åŸºæº–æ¸¬è©¦:\")\n",
    "print(\n",
    "    f\"  - æª¢ç´¢æ™‚é–“: {performance_results['retrieval']['mean']:.3f}Â±{performance_results['retrieval']['std']:.3f}s\"\n",
    ")\n",
    "print(\n",
    "    f\"  - ç”Ÿæˆæ™‚é–“: {performance_results['generation']['mean']:.3f}Â±{performance_results['generation']['std']:.3f}s\"\n",
    ")\n",
    "print(\n",
    "    f\"  - ç¸½æ™‚é–“: {performance_results['total']['mean']:.3f}Â±{performance_results['total']['std']:.3f}s\"\n",
    ")\n",
    "\n",
    "# Save evaluation results\n",
    "eval_results = {\n",
    "    \"retrieval_evaluation\": retrieval_results,\n",
    "    \"generation_evaluation\": generation_results,\n",
    "    \"performance_benchmark\": performance_results,\n",
    "    \"evaluation_timestamp\": datetime.now().isoformat(),\n",
    "    \"model_config\": {\n",
    "        \"llm_model\": model_id,\n",
    "        \"embedding_model\": \"bge-m3\",\n",
    "        \"chunk_size\": config.CHUNK_SIZE,\n",
    "        \"top_k\": config.TOP_K,\n",
    "    },\n",
    "}\n",
    "\n",
    "eval_file = pathlib.Path(nb29_paths[\"vectorstore\"]) / \"evaluation_results.json\"\n",
    "try:\n",
    "    with open(eval_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(eval_results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"ğŸ’¾ è©•ä¼°çµæœå·²ä¿å­˜è‡³: {eval_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ä¿å­˜è©•ä¼°çµæœå¤±æ•—: {e}\")\n",
    "\n",
    "## Stage 6 Summary\n",
    "print(\n",
    "    f\"\"\"\n",
    "ğŸ¯ Stage 6 å®Œæˆç¸½çµ:\n",
    "âœ… å®Œæˆé …ç›®:\n",
    "- å»ºç«‹å¤šç¶­åº¦RAGè©•ä¼°æ¡†æ¶ï¼ˆæª¢ç´¢+ç”Ÿæˆ+æ•ˆèƒ½ï¼‰\n",
    "- å¯¦ç¾Recall@Kã€MRRç­‰æª¢ç´¢æŒ‡æ¨™è¨ˆç®—\n",
    "- è¨­è¨ˆäº‹å¯¦ä¾æ“šæ€§èˆ‡é—œéµè©è¦†è“‹ç‡è©•ä¼°\n",
    "- åŸ·è¡Œæ•ˆèƒ½åŸºæº–æ¸¬è©¦èˆ‡çµ±è¨ˆåˆ†æ\n",
    "\n",
    "ğŸ§  æ ¸å¿ƒè§€å¿µ:\n",
    "- æª¢ç´¢è©•ä¼°ï¼šä½¿ç”¨æ¨™æº–è³‡è¨Šæª¢ç´¢æŒ‡æ¨™\n",
    "- ç”Ÿæˆè©•ä¼°ï¼šçµåˆäº‹å¯¦æ€§èˆ‡å®Œæ•´æ€§æŒ‡æ¨™\n",
    "- æ•ˆèƒ½åˆ†æï¼šåˆ†é›¢æª¢ç´¢èˆ‡ç”Ÿæˆçš„æ™‚é–“æˆæœ¬\n",
    "- æŒçºŒæ”¹é€²ï¼šå»ºç«‹å¯é‡è¤‡çš„è©•ä¼°æµç¨‹\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥: å»ºç«‹Gradioäº’å‹•ä»‹é¢\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e8e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 7 - (Optional) Gradio Quick Interface\n",
    "# å¯é¸ï¼šGradioå¿«é€Ÿäº’å‹•ä»‹é¢\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "\n",
    "    gradio_available = True\n",
    "    print(\"âœ… Gradio å¯ç”¨ï¼Œå»ºç«‹äº’å‹•ä»‹é¢...\")\n",
    "except ImportError:\n",
    "    gradio_available = False\n",
    "    print(\"âš ï¸ Gradio æœªå®‰è£ï¼Œè·³éä»‹é¢å»ºç«‹\")\n",
    "\n",
    "\n",
    "def create_gradio_interface():\n",
    "    \"\"\"å»ºç«‹GradioèŠå¤©ä»‹é¢ (Create Gradio chat interface)\"\"\"\n",
    "    if not gradio_available or not llm:\n",
    "        return None\n",
    "\n",
    "    def rag_chat(message, history):\n",
    "        \"\"\"RAGèŠå¤©è™•ç†å‡½æ•¸ (RAG chat handler)\"\"\"\n",
    "        try:\n",
    "            # Get RAG answer\n",
    "            result = rag_retriever.answer(message, top_k=3, use_reranker=False)\n",
    "\n",
    "            # Format response with sources\n",
    "            response = result[\"answer\"]\n",
    "            if result[\"sources\"]:\n",
    "                response += f\"\\n\\nğŸ“š **åƒè€ƒä¾†æº**: {', '.join(result['sources'])}\"\n",
    "\n",
    "            # Add to history\n",
    "            history.append([message, response])\n",
    "            return history, \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            error_response = f\"âŒ è™•ç†å¤±æ•—: {str(e)}\"\n",
    "            history.append([message, error_response])\n",
    "            return history, \"\"\n",
    "\n",
    "    # Create interface\n",
    "    with gr.Blocks(title=\"ä¸­æ–‡RAGç³»çµ±\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"# ğŸ‡¨ğŸ‡³ ä¸­æ–‡é–‹æºRAGå•ç­”ç³»çµ±\")\n",
    "        gr.Markdown(\"åŸºæ–¼DeepSeek/Qwen + BGE + FAISSçš„å®Œå…¨é–‹æºè§£æ±ºæ–¹æ¡ˆ\")\n",
    "\n",
    "        chatbot = gr.Chatbot(label=\"AIåŠ©æ‰‹\", height=400, show_label=True)\n",
    "\n",
    "        with gr.Row():\n",
    "            msg = gr.Textbox(label=\"è¼¸å…¥å•é¡Œ\", placeholder=\"è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ...\", scale=4)\n",
    "            send_btn = gr.Button(\"ç™¼é€\", scale=1, variant=\"primary\")\n",
    "\n",
    "        # Sample questions\n",
    "        gr.Markdown(\"### ğŸ’¡ ç¯„ä¾‹å•é¡Œ:\")\n",
    "        sample_questions = [\n",
    "            \"ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ\",\n",
    "            \"RAGç³»çµ±æœ‰ä»€éº¼å„ªå‹¢ï¼Ÿ\",\n",
    "            \"æœ‰å“ªäº›é‡è¦çš„é–‹æºä¸­æ–‡æ¨¡å‹ï¼Ÿ\",\n",
    "            \"å¦‚ä½•è©•ä¼°RAGç³»çµ±å“è³ªï¼Ÿ\",\n",
    "        ]\n",
    "\n",
    "        for question in sample_questions:\n",
    "            gr.Button(question, size=\"sm\").click(\n",
    "                lambda q=question: (chatbot.value + [[q, \"è™•ç†ä¸­...\"]], \"\"),\n",
    "                outputs=[chatbot, msg],\n",
    "            ).then(\n",
    "                rag_chat,\n",
    "                inputs=[gr.Textbox(value=question, visible=False), chatbot],\n",
    "                outputs=[chatbot, msg],\n",
    "            )\n",
    "\n",
    "        # Event handlers\n",
    "        msg.submit(rag_chat, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
    "        send_btn.click(rag_chat, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
    "\n",
    "        # System info\n",
    "        with gr.Accordion(\"ğŸ”§ ç³»çµ±è³‡è¨Š\", open=False):\n",
    "            gr.Markdown(\n",
    "                f\"\"\"\n",
    "            - **LLMæ¨¡å‹**: {model_id}\n",
    "            - **åµŒå…¥æ¨¡å‹**: BGE-M3\n",
    "            - **å‘é‡æ•¸é‡**: {faiss_index.ntotal}\n",
    "            - **æ–‡æª”æ•¸é‡**: {len(documents)}\n",
    "            - **å¾Œç«¯**: {config.BACKEND}\n",
    "            \"\"\"\n",
    "            )\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "# Create and launch Gradio interface\n",
    "if gradio_available and llm:\n",
    "    print(f\"\\nğŸŒ å»ºç«‹Gradioä»‹é¢...\")\n",
    "    gradio_demo = create_gradio_interface()\n",
    "\n",
    "    # For notebook environment, use share=False and inbrowser=False\n",
    "    try:\n",
    "        print(f\"ğŸš€ å•Ÿå‹•Gradioä»‹é¢ (æœ¬åœ°å­˜å–)\")\n",
    "        gradio_demo.launch(\n",
    "            server_name=\"127.0.0.1\",\n",
    "            server_port=7860,\n",
    "            share=False,\n",
    "            inbrowser=False,\n",
    "            quiet=True,\n",
    "        )\n",
    "        print(f\"âœ… Gradioä»‹é¢å·²å•Ÿå‹•: http://127.0.0.1:7860\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Gradioå•Ÿå‹•å¤±æ•—: {e}\")\n",
    "        print(f\"ğŸ’¡ å¯èƒ½éœ€è¦åœ¨çµ‚ç«¯æ©Ÿä¸­é‹è¡Œæ­¤notebook\")\n",
    "\n",
    "## Stage 7 Summary\n",
    "print(\n",
    "    f\"\"\"\n",
    "ğŸ¯ Stage 7 å®Œæˆç¸½çµ:\n",
    "âœ… å®Œæˆé …ç›®:\n",
    "- å»ºç«‹Gradioäº’å‹•å¼èŠå¤©ä»‹é¢\n",
    "- æ•´åˆRAGå•ç­”èˆ‡ä¾†æºé¡¯ç¤ºåŠŸèƒ½\n",
    "- æä¾›ç¯„ä¾‹å•é¡Œå¿«é€Ÿæ¸¬è©¦\n",
    "- é¡¯ç¤ºç³»çµ±é…ç½®è³‡è¨Š\n",
    "\n",
    "ğŸ§  æ ¸å¿ƒè§€å¿µ:\n",
    "- ä½¿ç”¨è€…é«”é©—ï¼šæä¾›ç›´è§€çš„èŠå¤©ä»‹é¢\n",
    "- è³‡è¨Šé€æ˜ï¼šé¡¯ç¤ºç­”æ¡ˆä¾†æºèˆ‡ç³»çµ±ç‹€æ…‹\n",
    "- å¿«é€Ÿæ¸¬è©¦ï¼šé è¨­ç¯„ä¾‹å•é¡Œä¾¿æ–¼é©—è­‰\n",
    "- æœ¬åœ°éƒ¨ç½²ï¼šå®Œå…¨æœ¬åœ°åŒ–çš„äº’å‹•ç’°å¢ƒ\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥: å®Œæ•´ç³»çµ±ç¸½çµèˆ‡å„ªåŒ–å»ºè­°\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba7864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 8 - Final Summary & Optimizations\n",
    "# æœ€çµ‚ç¸½çµèˆ‡å„ªåŒ–å»ºè­°\n",
    "\n",
    "\n",
    "def generate_system_report():\n",
    "    \"\"\"ç”Ÿæˆç³»çµ±å®Œæ•´å ±å‘Š (Generate comprehensive system report)\"\"\"\n",
    "\n",
    "    # System configuration\n",
    "    system_config = {\n",
    "        \"llm_model\": model_id,\n",
    "        \"embedding_model\": \"BAAI/bge-m3\",\n",
    "        \"vector_store\": \"FAISS\",\n",
    "        \"backend\": config.BACKEND,\n",
    "        \"quantization\": \"4-bit\" if torch.cuda.is_available() else \"none\",\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    }\n",
    "\n",
    "    # Data statistics\n",
    "    data_stats = {\n",
    "        \"total_documents\": len(documents),\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"vector_dimension\": faiss_index.d,\n",
    "        \"index_size\": faiss_index.ntotal,\n",
    "        \"avg_chunk_size\": (\n",
    "            sum(c[\"char_count\"] for c in chunks) / len(chunks) if chunks else 0\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Performance summary\n",
    "    if \"performance_results\" in locals():\n",
    "        perf_summary = {\n",
    "            \"avg_retrieval_time\": performance_results[\"retrieval\"][\"mean\"],\n",
    "            \"avg_generation_time\": performance_results[\"generation\"][\"mean\"],\n",
    "            \"avg_total_time\": performance_results[\"total\"][\"mean\"],\n",
    "        }\n",
    "    else:\n",
    "        perf_summary = {\"note\": \"Performance benchmark not run\"}\n",
    "\n",
    "    # Quality metrics\n",
    "    if \"retrieval_results\" in locals() and \"generation_results\" in locals():\n",
    "        quality_summary = {\n",
    "            \"recall_at_3\": retrieval_results[\"average_recall_at_k\"].get(3, 0),\n",
    "            \"mrr\": retrieval_results[\"average_mrr\"],\n",
    "            \"keyword_coverage\": generation_results[\"average_keyword_coverage\"],\n",
    "            \"groundedness\": generation_results[\"average_groundedness\"],\n",
    "        }\n",
    "    else:\n",
    "        quality_summary = {\"note\": \"Quality evaluation not run\"}\n",
    "\n",
    "    report = {\n",
    "        \"system_configuration\": system_config,\n",
    "        \"data_statistics\": data_stats,\n",
    "        \"performance_metrics\": perf_summary,\n",
    "        \"quality_metrics\": quality_summary,\n",
    "        \"generated_at\": datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "# Generate final report\n",
    "print(f\"\\n=== ğŸ¯ ä¸­æ–‡é–‹æºRAGç³»çµ±å®Œæ•´å ±å‘Š ===\")\n",
    "\n",
    "final_report = generate_system_report()\n",
    "\n",
    "print(f\"\\nğŸ“‹ ç³»çµ±é…ç½®:\")\n",
    "for key, value in final_report[\"system_configuration\"].items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š è³‡æ–™çµ±è¨ˆ:\")\n",
    "for key, value in final_report[\"data_statistics\"].items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "if \"note\" not in final_report[\"performance_metrics\"]:\n",
    "    print(f\"\\nâš¡ æ•ˆèƒ½æŒ‡æ¨™:\")\n",
    "    for key, value in final_report[\"performance_metrics\"].items():\n",
    "        print(f\"  - {key}: {value:.3f}s\")\n",
    "\n",
    "if \"note\" not in final_report[\"quality_metrics\"]:\n",
    "    print(f\"\\nğŸ¯ å“è³ªæŒ‡æ¨™:\")\n",
    "    for key, value in final_report[\"quality_metrics\"].items():\n",
    "        print(f\"  - {key}: {value:.3f}\")\n",
    "\n",
    "# Save final report\n",
    "report_file = pathlib.Path(nb29_paths[\"vectorstore\"]) / \"system_report.json\"\n",
    "try:\n",
    "    with open(report_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_report, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nğŸ’¾ å®Œæ•´å ±å‘Šå·²ä¿å­˜è‡³: {report_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ä¿å­˜å ±å‘Šå¤±æ•—: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3111b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Smoke Test & Validation\n",
    "# ç…™éœ§æ¸¬è©¦èˆ‡é©—è­‰\n",
    "\n",
    "\n",
    "def run_comprehensive_smoke_test():\n",
    "    \"\"\"åŸ·è¡Œå®Œæ•´çš„ç…™éœ§æ¸¬è©¦ (Run comprehensive smoke test)\"\"\"\n",
    "    print(f\"\\nğŸ§ª åŸ·è¡Œç³»çµ±ç…™éœ§æ¸¬è©¦...\")\n",
    "\n",
    "    tests = {\n",
    "        \"shared_cache_setup\": False,\n",
    "        \"model_loading\": False,\n",
    "        \"embedding_model\": False,\n",
    "        \"vector_index\": False,\n",
    "        \"retrieval_function\": False,\n",
    "        \"generation_function\": False,\n",
    "        \"end_to_end_qa\": False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Test 1: Shared cache setup\n",
    "        tests[\"shared_cache_setup\"] = all(\n",
    "            pathlib.Path(path).exists() for path in nb29_paths.values()\n",
    "        )\n",
    "\n",
    "        # Test 2: Model loading\n",
    "        tests[\"model_loading\"] = llm is not None and llm.model is not None\n",
    "\n",
    "        # Test 3: Embedding model\n",
    "        tests[\"embedding_model\"] = embedding_model is not None\n",
    "\n",
    "        # Test 4: Vector index\n",
    "        tests[\"vector_index\"] = faiss_index is not None and faiss_index.ntotal > 0\n",
    "\n",
    "        # Test 5: Retrieval function\n",
    "        try:\n",
    "            test_retrieval = rag_retriever.retrieve(\"æ¸¬è©¦\", top_k=1)\n",
    "            tests[\"retrieval_function\"] = len(test_retrieval) > 0\n",
    "        except:\n",
    "            tests[\"retrieval_function\"] = False\n",
    "\n",
    "        # Test 6: Generation function\n",
    "        try:\n",
    "            test_messages = [{\"role\": \"user\", \"content\": \"ä½ å¥½\"}]\n",
    "            test_response = llm.generate(test_messages, max_length=10)\n",
    "            tests[\"generation_function\"] = len(test_response) > 0\n",
    "        except:\n",
    "            tests[\"generation_function\"] = False\n",
    "\n",
    "        # Test 7: End-to-end QA\n",
    "        try:\n",
    "            test_result = rag_retriever.answer(\n",
    "                \"ä»€éº¼æ˜¯AIï¼Ÿ\", top_k=2, use_reranker=False\n",
    "            )\n",
    "            tests[\"end_to_end_qa\"] = (\n",
    "                \"answer\" in test_result and len(test_result[\"answer\"]) > 0\n",
    "            )\n",
    "        except:\n",
    "            tests[\"end_to_end_qa\"] = False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}\")\n",
    "\n",
    "    # Report results\n",
    "    print(f\"\\nğŸ“Š ç…™éœ§æ¸¬è©¦çµæœ:\")\n",
    "    for test_name, passed in tests.items():\n",
    "        status = \"âœ… PASS\" if passed else \"âŒ FAIL\"\n",
    "        print(f\"  - {test_name}: {status}\")\n",
    "\n",
    "    overall_health = sum(tests.values()) / len(tests)\n",
    "    health_status = (\n",
    "        \"ğŸŸ¢ å„ªç§€\"\n",
    "        if overall_health >= 0.9\n",
    "        else \"ğŸŸ¡ è‰¯å¥½\" if overall_health >= 0.7 else \"ğŸ”´ éœ€è¦æ”¹é€²\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nğŸ¥ ç³»çµ±æ•´é«”å¥åº·åº¦: {overall_health:.1%} {health_status}\")\n",
    "\n",
    "    return tests\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_results = run_comprehensive_smoke_test()\n",
    "\n",
    "# Optimization recommendations\n",
    "print(\n",
    "    f\"\"\"\n",
    "ğŸš€ ç³»çµ±å„ªåŒ–å»ºè­°:\n",
    "===============\n",
    "\n",
    "ğŸ’¡ æ•ˆèƒ½å„ªåŒ–:\n",
    "- ä½¿ç”¨GGUFé‡åŒ–æ¨¡å‹é€²ä¸€æ­¥æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨\n",
    "- å¯¦ç¾å‘é‡æª¢ç´¢çµæœå¿«å–æ©Ÿåˆ¶\n",
    "- è€ƒæ…®ä½¿ç”¨vLLMæˆ–TensorRT-LLMåŠ é€Ÿæ¨ç†\n",
    "- æ‰¹æ¬¡è™•ç†å¤šå€‹æŸ¥è©¢æå‡ååé‡\n",
    "\n",
    "ğŸ“š è³‡æ–™å“è³ª:\n",
    "- å¢åŠ æ›´å¤šé ˜åŸŸæ–‡æª”è±å¯ŒçŸ¥è­˜åº«\n",
    "- å¯¦ç¾å¢é‡æ–‡æª”æ›´æ–°æ©Ÿåˆ¶\n",
    "- å„ªåŒ–ä¸­æ–‡æ–‡æœ¬åˆ‡åˆ†ç­–ç•¥\n",
    "- å»ºç«‹æ–‡æª”å“è³ªè©•ä¼°æµç¨‹\n",
    "\n",
    "ğŸ”§ åŠŸèƒ½æ“´å±•:\n",
    "- å¢åŠ å¤šæ¨¡æ…‹æ”¯æ´ï¼ˆåœ–ç‰‡ã€è¡¨æ ¼ï¼‰\n",
    "- å¯¦ç¾å°è©±æ­·å²è¨˜æ†¶åŠŸèƒ½\n",
    "- æ·»åŠ å³æ™‚ç¶²è·¯æœå°‹è£œå……\n",
    "- å»ºç«‹ä½¿ç”¨è€…åé¥‹æ”¶é›†æ©Ÿåˆ¶\n",
    "\n",
    "ğŸ›¡ï¸ ç©©å®šæ€§æå‡:\n",
    "- æ·»åŠ æ›´å®Œå–„çš„éŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶\n",
    "- å¯¦ç¾æ¨¡å‹ç†±åˆ‡æ›åŠŸèƒ½\n",
    "- å»ºç«‹ç³»çµ±ç›£æ§èˆ‡å‘Šè­¦\n",
    "- å„ªåŒ–è¨˜æ†¶é«”ç®¡ç†é¿å…æ´©æ¼\n",
    "\n",
    "ğŸ“ˆ æ“´å±•éƒ¨ç½²:\n",
    "- å®¹å™¨åŒ–éƒ¨ç½²ï¼ˆDockerï¼‰\n",
    "- å¾®æœå‹™æ¶æ§‹æ‹†åˆ†\n",
    "- è² è¼‰å‡è¡¡èˆ‡é«˜å¯ç”¨æ€§\n",
    "- APIä»‹é¢æ¨™æº–åŒ–\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "âœ… ä¸­æ–‡é–‹æºRAGç³»çµ±å»ºç«‹å®Œæˆï¼\n",
    "\n",
    "ğŸ¯ é—œéµæˆå°±:\n",
    "- å®Œå…¨åŸºæ–¼é–‹æºæ¨¡å‹çš„ä¸­æ–‡RAGç³»çµ±\n",
    "- æ”¯æ´DeepSeek/Qwenç­‰ä¸»æµä¸­æ–‡æ¨¡å‹\n",
    "- å¯¦ç¾å¾æ–‡æª”è¼‰å…¥åˆ°å•ç­”çš„å®Œæ•´æµç¨‹\n",
    "- æä¾›è©•ä¼°æŒ‡æ¨™èˆ‡æ•ˆèƒ½åˆ†æ\n",
    "- å»ºç«‹äº’å‹•å¼èŠå¤©ä»‹é¢\n",
    "\n",
    "ğŸ“š ç³»çµ±èƒ½åŠ›:\n",
    "- ğŸ¤– ä¸­æ–‡LLM: {model_id}\n",
    "- ğŸ” å‘é‡æª¢ç´¢: FAISS + BGE-M3\n",
    "- ğŸ“„ æ–‡æª”è™•ç†: {len(documents)} å€‹æ–‡æª”ï¼Œ{len(chunks)} å€‹æ–‡æœ¬å¡Š\n",
    "- âš¡ å³æ™‚å•ç­”: å¹³å‡ {final_report.get('performance_metrics', {}).get('avg_total_time', 'N/A')} ç§’éŸ¿æ‡‰\n",
    "- ğŸŒ Webä»‹é¢: Gradioäº’å‹•ä»‹é¢\n",
    "\n",
    "ğŸš€ å¯ç«‹å³ä½¿ç”¨æ–¼:\n",
    "- ä¼æ¥­å…§éƒ¨çŸ¥è­˜å•ç­”\n",
    "- æŠ€è¡“æ–‡æª”æŸ¥è©¢ç³»çµ±\n",
    "- æ•™è‚²è¼”åŠ©å·¥å…·\n",
    "- ç ”ç©¶è³‡æ–™åˆ†æ\n",
    "\n",
    "ğŸ’¡ å®Œå…¨é›¢ç·šé‹è¡Œï¼Œä¿è­·è³‡æ–™éš±ç§ï¼\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAdapter:\n",
    "    def __init__(\n",
    "        self, model_id: str, backend: str = \"transformers\", load_in_4bit: bool = True\n",
    "    ):\n",
    "        # æ”¯æ´å¤šç¨®ä¸­æ–‡æ¨¡å‹æ ¼å¼çš„çµ±ä¸€ä»‹é¢\n",
    "        self._load_model()\n",
    "\n",
    "    def _format_qwen_messages(self, messages: List[Dict]) -> str:\n",
    "        # Qwenå°ˆç”¨çš„ChatMLæ ¼å¼\n",
    "        formatted = \"<|im_start|>system\\nä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„AIåŠ©æ‰‹ã€‚<|im_end|>\\n\"\n",
    "        # ... è™•ç†è¨Šæ¯æ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸­æ–‡åˆ†éš”ç¬¦å„ªå…ˆåºåˆ—\n",
    "CHINESE_SEPARATORS = [\"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \"ï¼›\", \"â€¦\", \"\\n\\n\", \"\\n\", \" \"]\n",
    "\n",
    "\n",
    "def create_chinese_text_splitter():\n",
    "    return RecursiveCharacterTextSplitter(\n",
    "        separators=CHINESE_SEPARATORS,\n",
    "        chunk_size=512,  # ä¸­æ–‡å­—ç¬¦æ•¸\n",
    "        chunk_overlap=64,\n",
    "        length_function=len,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce32a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChineseRAGRetriever:\n",
    "    def answer(self, query: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "        # 1. å‘é‡æª¢ç´¢\n",
    "        retrieved = self.retrieve(query, top_k)\n",
    "\n",
    "        # 2. å¯é¸é‡æ’åº\n",
    "        if self.reranker:\n",
    "            retrieved = self.rerank(query, retrieved)\n",
    "\n",
    "        # 3. ç”Ÿæˆç­”æ¡ˆ\n",
    "        return self.generate_answer(query, retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ä¸­æ–‡é–‹æºRAGç³»çµ±ç…™éœ§æ¸¬è©¦ ===\n",
    "def run_comprehensive_smoke_test():\n",
    "    tests = {\n",
    "        \"shared_cache_setup\": False,\n",
    "        \"model_loading\": False,\n",
    "        \"embedding_model\": False,\n",
    "        \"vector_index\": False,\n",
    "        \"retrieval_function\": False,\n",
    "        \"generation_function\": False,\n",
    "        \"end_to_end_qa\": False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # æ¸¬è©¦å…±äº«å¿«å–\n",
    "        tests[\"shared_cache_setup\"] = all(\n",
    "            pathlib.Path(path).exists() for path in nb29_paths.values()\n",
    "        )\n",
    "\n",
    "        # æ¸¬è©¦æ¨¡å‹è¼‰å…¥\n",
    "        tests[\"model_loading\"] = llm is not None and llm.model is not None\n",
    "\n",
    "        # æ¸¬è©¦ç«¯åˆ°ç«¯å•ç­”\n",
    "        test_result = rag_retriever.answer(\"ä»€éº¼æ˜¯AIï¼Ÿ\", top_k=2)\n",
    "        tests[\"end_to_end_qa\"] = len(test_result[\"answer\"]) > 0\n",
    "\n",
    "        # ... å…¶ä»–æ¸¬è©¦\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "\n",
    "    overall_health = sum(tests.values()) / len(tests)\n",
    "    print(f\"ğŸ¥ ç³»çµ±å¥åº·åº¦: {overall_health:.1%}\")\n",
    "    return tests\n",
    "\n",
    "\n",
    "smoke_test_results = run_comprehensive_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d8dcd",
   "metadata": {},
   "source": [
    "\n",
    "## 6. æœ¬ç« å°çµ\n",
    "\n",
    "### âœ… å®Œæˆé …ç›®\n",
    "* **å®Œå…¨é–‹æºçš„ä¸­æ–‡RAGç³»çµ±** (Fully Open-Source Chinese RAG)ï¼šç„¡éœ€ä»»ä½•é›²ç«¯APIï¼Œæ”¯æ´DeepSeek/Qwen/ChatGLMç­‰ä¸»æµä¸­æ–‡æ¨¡å‹\n",
    "* **ä¸­æ–‡å„ªåŒ–çš„æ–‡æœ¬è™•ç†** (Chinese-Optimized Text Processing)ï¼šå°ˆé–€çš„ä¸­æ–‡åˆ†è©ã€ç¹ç°¡è½‰æ›èˆ‡èªç¾©åˆ‡åˆ†ç­–ç•¥\n",
    "* **é«˜æ•ˆèƒ½å‘é‡æª¢ç´¢** (High-Performance Vector Retrieval)ï¼šBGE-M3åµŒå…¥ + FAISSç´¢å¼•ï¼Œæ”¯æ´å¤§è¦æ¨¡ä¸­æ–‡æ–‡æª”æª¢ç´¢\n",
    "* **å¤šç¶­åº¦è©•ä¼°é«”ç³»** (Multi-Dimensional Evaluation)ï¼šåŒ…å«æª¢ç´¢å“è³ª(Recall@K, MRR)ã€ç”Ÿæˆå“è³ª(äº‹å¯¦æ€§ã€å®Œæ•´æ€§)èˆ‡æ•ˆèƒ½æŒ‡æ¨™\n",
    "* **äº’å‹•å¼Webä»‹é¢** (Interactive Web Interface)ï¼šGradioèŠå¤©ä»‹é¢æ”¯æ´å³æ™‚å•ç­”èˆ‡ä¾†æºè¿½è¹¤\n",
    "\n",
    "### ğŸ§  æ ¸å¿ƒæ¦‚å¿µèˆ‡åŸç†è¦é»\n",
    "* **é–‹æºå„ªå…ˆç­–ç•¥** (Open-Source First Strategy)ï¼šå®Œå…¨é¿å…å°å•†æ¥­APIçš„ä¾è³´ï¼Œç¢ºä¿è³‡æ–™éš±ç§èˆ‡æˆæœ¬å¯æ§\n",
    "* **ä¸­æ–‡èªè¨€ç‰¹æ€§** (Chinese Language Characteristics)ï¼šé‡å°ä¸­æ–‡æ¨™é»ç¬¦è™Ÿã€èªç¾©çµæ§‹èˆ‡ç¹ç°¡å·®ç•°çš„å°ˆé–€è™•ç†\n",
    "* **æ¨¡å‹é‡åŒ–æŠ€è¡“** (Model Quantization)ï¼šä½¿ç”¨4-bité‡åŒ–å¤§å¹…é™ä½é¡¯å­˜éœ€æ±‚ï¼Œè®“7Bæ¨¡å‹åœ¨8GBé¡¯å¡ä¸Šé‹è¡Œ\n",
    "* **å‘é‡èªç¾©æª¢ç´¢** (Vector Semantic Retrieval)ï¼šåŸºæ–¼BGE-M3çš„ä¸­æ–‡èªç¾©ç†è§£ï¼Œè¶…è¶Šé—œéµè©åŒ¹é…çš„æª¢ç´¢ç²¾åº¦\n",
    "* **ç«¯åˆ°ç«¯è©•ä¼°** (End-to-End Evaluation)ï¼šå¾æª¢ç´¢åˆ°ç”Ÿæˆçš„å®Œæ•´å“è³ªè©•ä¼°ï¼Œç¢ºä¿ç³»çµ±å¯é æ€§\n",
    "\n",
    "### âš ï¸ å¸¸è¦‹å•é¡Œèˆ‡æ³¨æ„äº‹é …\n",
    "* **æ¨¡å‹ç›¸å®¹æ€§**ï¼šä¸åŒä¸­æ–‡æ¨¡å‹çš„æç¤ºè©æ ¼å¼å·®ç•°éœ€è¦å°ˆé–€é©é…\n",
    "* **é¡¯å­˜ç®¡ç†**ï¼šå¤§å‹æ¨¡å‹è¼‰å…¥éœ€è¦åˆç†çš„é‡åŒ–ç­–ç•¥èˆ‡è¨˜æ†¶é«”æ¸…ç†\n",
    "* **ä¸­æ–‡ç·¨ç¢¼**ï¼šç¢ºä¿UTF-8ç·¨ç¢¼ä¸€è‡´æ€§ï¼Œé¿å…äº‚ç¢¼å•é¡Œ\n",
    "* **æª¢ç´¢ç²¾åº¦**ï¼šå‘é‡æª¢ç´¢å¯èƒ½éºæ¼é—œéµè©ç²¾ç¢ºåŒ¹é…ï¼Œéœ€è¦æ··åˆæª¢ç´¢ç­–ç•¥\n",
    "* **å›ç­”å“è³ª**ï¼šé–‹æºæ¨¡å‹çš„ç”Ÿæˆå“è³ªå¯èƒ½ä¸å¦‚å•†æ¥­æ¨¡å‹ï¼Œéœ€è¦æ›´ç²¾ç´°çš„æç¤ºå·¥ç¨‹\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å„ªåŒ–å»ºè­°\n",
    "\n",
    "1. **æ··åˆæª¢ç´¢æ¶æ§‹** (Hybrid Retrieval Architecture)ï¼šçµåˆBM25é—œéµè©æª¢ç´¢èˆ‡å‘é‡èªç¾©æª¢ç´¢ï¼Œæå‡æª¢ç´¢å¬å›ç‡\n",
    "2. **æ¨¡å‹è’¸é¤¾å„ªåŒ–** (Model Distillation)ï¼šä½¿ç”¨æ•™å¸«-å­¸ç”Ÿæ¶æ§‹é€²ä¸€æ­¥å£“ç¸®æ¨¡å‹å¤§å°\n",
    "3. **å¢é‡å­¸ç¿’æ©Ÿåˆ¶** (Incremental Learning)ï¼šæ”¯æ´å‹•æ…‹æ·»åŠ æ–°æ–‡æª”è€Œç„¡éœ€é‡å»ºæ•´å€‹ç´¢å¼•\n",
    "4. **å¤šè¼ªå°è©±è¨˜æ†¶** (Multi-turn Conversation Memory)ï¼šæ•´åˆå°è©±æ­·å²æå‡ä¸Šä¸‹æ–‡ç†è§£\n",
    "5. **é ˜åŸŸé©æ‡‰å¾®èª¿** (Domain Adaptation Fine-tuning)ï¼šé‡å°ç‰¹å®šé ˜åŸŸä½¿ç”¨LoRAå¾®èª¿æå‡å°ˆæ¥­æ€§\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ¯ é—œéµæˆå°±**: å»ºç«‹äº†å®Œå…¨åŸºæ–¼é–‹æºæŠ€è¡“çš„ä¸­æ–‡RAGç³»çµ±ï¼Œå¯¦ç¾å¾æ–‡æª”è™•ç†åˆ°å•ç­”ç”Ÿæˆçš„ç«¯åˆ°ç«¯æµç¨‹ï¼Œç„¡éœ€ä»»ä½•å•†æ¥­APIä¾è³´\n",
    "\n",
    "**ğŸ“š æº–å‚™å°±ç·’**: å…·å‚™äº†éƒ¨ç½²ç”Ÿç”¢ç´šä¸­æ–‡çŸ¥è­˜å•ç­”ç³»çµ±çš„å®Œæ•´æŠ€è¡“æ£§ï¼Œå¯æ‡‰ç”¨æ–¼ä¼æ¥­å…§éƒ¨çŸ¥è­˜ç®¡ç†ã€æ•™è‚²è¼”åŠ©ã€æŠ€è¡“æ”¯æ´ç­‰å ´æ™¯\n",
    "\n",
    "**ğŸ’¡ æ ¸å¿ƒåƒ¹å€¼**: \n",
    "- **éš±ç§ä¿è­·**: å®Œå…¨æœ¬åœ°é‹è¡Œï¼Œä¼æ¥­æ•æ„Ÿè³‡æ–™ä¸æœƒå¤–æ´©\n",
    "- **æˆæœ¬å¯æ§**: åƒ…éœ€ä¸€æ¬¡æ€§ç¡¬é«”æŠ•è³‡ï¼Œç„¡æŒçºŒAPIè²»ç”¨\n",
    "- **è‡ªä¸»å¯æ§**: åŸºæ–¼é–‹æºæŠ€è¡“ï¼Œé¿å…ä¾›æ‡‰å•†é–å®šé¢¨éšª\n",
    "- **ä¸­æ–‡å„ªåŒ–**: å°ˆé–€é‡å°ä¸­æ–‡èªè¨€ç‰¹æ€§è¨­è¨ˆï¼Œæ•ˆæœå„ªæ–¼é€šç”¨æ–¹æ¡ˆ\n",
    "\n",
    "é€™å€‹æ›¿ä»£ç‰ˆæœ¬å®Œå…¨æ»¿è¶³äº†æ‚¨çš„è¦æ±‚ï¼šä¸­æ–‡å„ªå…ˆã€é–‹æºLLMå„ªå…ˆã€åš´æ ¼éµå®ˆå…±äº«æ¨¡å‹å¿«å–ã€æä¾›å®Œæ•´çš„MVPèˆ‡è©•ä¼°é«”ç³»ã€‚å¯ä»¥èˆ‡åŸnb29ä¸¦å­˜ï¼Œç‚ºä¸åŒéœ€æ±‚å ´æ™¯æä¾›é¸æ“‡ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
