{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00e6b0c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "本 Notebook 展示完全基於開源模型的中文 RAG 系統：\n",
    "- 開源LLM：DeepSeek-R1-Distill / Qwen2.5-7B (中文優先)\n",
    "- 開源Embedding：BGE-M3 / BGE-small-zh\n",
    "- 本地推理：transformers + 4-bit / llama-cpp / Ollama\n",
    "- 中文處理：專門的中文切分與繁簡轉換\n",
    "- 完全離線：無需任何雲端API金鑰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce06a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese Open-Source RAG System\n",
    "# 中文開源檢索增強生成系統 - DeepSeek/Qwen + BGE + FAISS\n",
    "\n",
    "## Stage 1 - Setup & GPU/VRAM Check\n",
    "# 環境設置與顯存檢查\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === Shared Cache Bootstrap (Mandatory) ===\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "cache_paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "\n",
    "for k, v in cache_paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] 共享快取根目錄: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] CUDA可用: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d43cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"[GPU] 顯存容量: {gpu_memory:.1f} GB\")\n",
    "    print(f\"[GPU] 顯卡型號: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # VRAM-based model recommendations\n",
    "    if gpu_memory >= 16:\n",
    "        recommended_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "        print(f\"[建議] 顯存充足，推薦使用: {recommended_model} (FP16)\")\n",
    "    elif gpu_memory >= 8:\n",
    "        recommended_model = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "        print(f\"[建議] 中等顯存，推薦使用: {recommended_model} (4-bit)\")\n",
    "    else:\n",
    "        recommended_model = \"THUDM/chatglm3-6b\"\n",
    "        print(f\"[建議] 顯存有限，推薦使用: {recommended_model} (4-bit)\")\n",
    "else:\n",
    "    recommended_model = \"microsoft/DialoGPT-medium\"\n",
    "    print(f\"[警告] 無GPU，推薦CPU模型: {recommended_model}\")\n",
    "\n",
    "# Create essential directories for this notebook\n",
    "nb29_paths = {\n",
    "    \"docs\": f\"{AI_CACHE_ROOT}/nb29_cn_docs\",\n",
    "    \"vectorstore\": f\"{AI_CACHE_ROOT}/vectorstores/nb29_cn_oss\",\n",
    "    \"models_cache\": f\"{AI_CACHE_ROOT}/models/nb29_cn_oss\",\n",
    "}\n",
    "\n",
    "for name, path in nb29_paths.items():\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[目錄] {name}: {path}\")\n",
    "\n",
    "\n",
    "# Check and install required packages\n",
    "def check_install_packages():\n",
    "    \"\"\"檢查並安裝必要套件 (Check and install required packages)\"\"\"\n",
    "    required_packages = {\n",
    "        \"transformers\": \"transformers>=4.35.0\",\n",
    "        \"torch\": \"torch>=2.0.0\",\n",
    "        \"bitsandbytes\": \"bitsandbytes>=0.41.0\",\n",
    "        \"sentence_transformers\": \"sentence-transformers>=2.2.0\",\n",
    "        \"faiss\": \"faiss-cpu>=1.7.0\",\n",
    "        \"opencc\": \"opencc>=1.1.0\",\n",
    "        \"langchain\": \"langchain>=0.1.0\",\n",
    "        \"langchain_text_splitters\": \"langchain-text-splitters\",\n",
    "        \"gradio\": \"gradio>=4.0.0\",\n",
    "    }\n",
    "\n",
    "    missing_packages = []\n",
    "    for package, install_cmd in required_packages.items():\n",
    "        try:\n",
    "            __import__(package.replace(\"-\", \"_\"))\n",
    "            print(f\"✅ {package} 已安裝\")\n",
    "        except ImportError:\n",
    "            missing_packages.append(install_cmd)\n",
    "            print(f\"❌ {package} 未安裝\")\n",
    "\n",
    "    if missing_packages:\n",
    "        print(f\"\\n請執行以下指令安裝缺失套件:\")\n",
    "        print(f\"pip install {' '.join(missing_packages)}\")\n",
    "        return False\n",
    "\n",
    "    print(\"✅ 所有必要套件已安裝\")\n",
    "    return True\n",
    "\n",
    "\n",
    "package_check_result = check_install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64baa5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 2 - Backend & Model Selector\n",
    "# 後端與模型選擇器\n",
    "\n",
    "# Import essential libraries\n",
    "try:\n",
    "    import transformers\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForCausalLM,\n",
    "        BitsAndBytesConfig,\n",
    "        pipeline,\n",
    "        TextStreamer,\n",
    "    )\n",
    "\n",
    "    print(\"✅ transformers 載入成功\")\n",
    "except ImportError:\n",
    "    print(\"❌ transformers 載入失敗\")\n",
    "\n",
    "try:\n",
    "    import sentence_transformers\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    print(\"✅ sentence_transformers 載入成功\")\n",
    "except ImportError:\n",
    "    print(\"❌ sentence_transformers 載入失敗\")\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "\n",
    "    print(\"✅ faiss 載入成功\")\n",
    "except ImportError:\n",
    "    print(\"❌ faiss 載入失敗\")\n",
    "\n",
    "try:\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "    print(\"✅ langchain_text_splitters 載入成功\")\n",
    "except ImportError:\n",
    "    print(\"❌ langchain_text_splitters 載入失敗\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Chinese RAG system\n",
    "class ChineseRAGConfig:\n",
    "    \"\"\"中文RAG系統配置 (Chinese RAG System Configuration)\"\"\"\n",
    "\n",
    "    # Backend options: transformers, llama_cpp, ollama\n",
    "    BACKEND = \"transformers\"\n",
    "\n",
    "    # Model configurations (Chinese-first)\n",
    "    MODELS = {\n",
    "        \"deepseek-r1-7b\": {\n",
    "            \"id\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "            \"min_vram_gb\": 6,\n",
    "            \"supports_4bit\": True,\n",
    "            \"chinese_ability\": \"excellent\",\n",
    "        },\n",
    "        \"qwen2.5-7b\": {\n",
    "            \"id\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            \"min_vram_gb\": 6,\n",
    "            \"supports_4bit\": True,\n",
    "            \"chinese_ability\": \"excellent\",\n",
    "        },\n",
    "        \"chatglm3-6b\": {\n",
    "            \"id\": \"THUDM/chatglm3-6b\",\n",
    "            \"min_vram_gb\": 5,\n",
    "            \"supports_4bit\": True,\n",
    "            \"chinese_ability\": \"excellent\",\n",
    "        },\n",
    "        \"yi-6b\": {\n",
    "            \"id\": \"01-ai/Yi-6B-Chat\",\n",
    "            \"min_vram_gb\": 5,\n",
    "            \"supports_4bit\": True,\n",
    "            \"chinese_ability\": \"good\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Embedding models (Chinese-optimized)\n",
    "    EMBEDDING_MODELS = {\n",
    "        \"bge-m3\": \"BAAI/bge-m3\",\n",
    "        \"bge-small-zh\": \"BAAI/bge-small-zh-v1.5\",\n",
    "        \"text2vec-base\": \"shibing624/text2vec-base-chinese\",\n",
    "    }\n",
    "\n",
    "    # Chinese text processing\n",
    "    CHINESE_SEPARATORS = [\"。\", \"！\", \"？\", \"；\", \"…\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    CHUNK_SIZE = 512\n",
    "    CHUNK_OVERLAP = 64\n",
    "\n",
    "    # Retrieval settings\n",
    "    TOP_K = 5\n",
    "    SIMILARITY_THRESHOLD = 0.6\n",
    "\n",
    "\n",
    "config = ChineseRAGConfig()\n",
    "\n",
    "\n",
    "def auto_select_model() -> str:\n",
    "    \"\"\"根據顯存自動選擇模型 (Auto-select model based on VRAM)\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return \"qwen2.5-7b\"  # Fallback for CPU\n",
    "\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "\n",
    "    for model_key, model_info in config.MODELS.items():\n",
    "        if vram_gb >= model_info[\"min_vram_gb\"]:\n",
    "            print(f\"[自動選擇] 基於 {vram_gb:.1f}GB 顯存，選擇: {model_key}\")\n",
    "            return model_key\n",
    "\n",
    "    # Fallback to smallest model\n",
    "    return \"chatglm3-6b\"\n",
    "\n",
    "\n",
    "# Model selection\n",
    "selected_model_key = auto_select_model()\n",
    "selected_model_info = config.MODELS[selected_model_key]\n",
    "model_id = selected_model_info[\"id\"]\n",
    "\n",
    "print(f\"\\n[模型配置]\")\n",
    "print(f\"- 後端 (Backend): {config.BACKEND}\")\n",
    "print(f\"- 模型 (Model): {model_id}\")\n",
    "print(f\"- 中文能力 (Chinese): {selected_model_info['chinese_ability']}\")\n",
    "print(f\"- 4-bit支援: {selected_model_info['supports_4bit']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a654552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAdapter:\n",
    "    \"\"\"輕量LLM適配器 (Lightweight LLM Adapter)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_id: str, backend: str = \"transformers\", load_in_4bit: bool = True\n",
    "    ):\n",
    "        self.model_id = model_id\n",
    "        self.backend = backend\n",
    "        self.load_in_4bit = load_in_4bit\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        print(f\"[LLM] 初始化 {backend} 後端，模型: {model_id}\")\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"載入模型 (Load model)\"\"\"\n",
    "        try:\n",
    "            if self.backend == \"transformers\":\n",
    "                self._load_transformers()\n",
    "            elif self.backend == \"llama_cpp\":\n",
    "                self._load_llama_cpp()\n",
    "            elif self.backend == \"ollama\":\n",
    "                self._load_ollama()\n",
    "            else:\n",
    "                raise ValueError(f\"不支援的後端: {self.backend}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 模型載入失敗: {e}\")\n",
    "            print(f\"🔄 嘗試降級載入...\")\n",
    "            self._fallback_load()\n",
    "\n",
    "    def _load_transformers(self):\n",
    "        \"\"\"載入Transformers模型 (Load Transformers model)\"\"\"\n",
    "        # Configure quantization for low VRAM\n",
    "        if self.load_in_4bit and torch.cuda.is_available():\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "            )\n",
    "        else:\n",
    "            bnb_config = None\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_id,\n",
    "            trust_remote_code=True,\n",
    "            cache_dir=cache_paths[\"TRANSFORMERS_CACHE\"],\n",
    "        )\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_id,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "            cache_dir=cache_paths[\"TRANSFORMERS_CACHE\"],\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Transformers模型載入成功\")\n",
    "\n",
    "    def _load_llama_cpp(self):\n",
    "        \"\"\"載入llama-cpp模型 (Load llama-cpp model)\"\"\"\n",
    "        try:\n",
    "            from llama_cpp import Llama\n",
    "\n",
    "            # This would require GGUF model files\n",
    "            print(\"⚠️ llama-cpp後端需要GGUF模型檔案\")\n",
    "            raise NotImplementedError(\"llama-cpp backend not implemented in this demo\")\n",
    "        except ImportError:\n",
    "            print(\"❌ llama-cpp-python 未安裝\")\n",
    "            raise\n",
    "\n",
    "    def _load_ollama(self):\n",
    "        \"\"\"載入Ollama模型 (Load Ollama model)\"\"\"\n",
    "        try:\n",
    "            import ollama\n",
    "\n",
    "            # This would require Ollama service running\n",
    "            print(\"⚠️ Ollama後端需要Ollama服務運行\")\n",
    "            raise NotImplementedError(\"Ollama backend not implemented in this demo\")\n",
    "        except ImportError:\n",
    "            print(\"❌ ollama 未安裝\")\n",
    "            raise\n",
    "\n",
    "    def _fallback_load(self):\n",
    "        \"\"\"降級載入 (Fallback loading)\"\"\"\n",
    "        print(\"🔄 嘗試CPU模式載入...\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_id, trust_remote_code=True\n",
    "            )\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_id,\n",
    "                torch_dtype=torch.float32,\n",
    "                device_map=None,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "            self.device = \"cpu\"\n",
    "            print(\"✅ CPU模式載入成功\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 降級載入也失敗: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        max_length: int = 1024,\n",
    "        temperature: float = 0.7,\n",
    "    ) -> str:\n",
    "        \"\"\"生成回應 (Generate response)\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            return \"模型未載入\"\n",
    "\n",
    "        try:\n",
    "            # Format messages for different model types\n",
    "            if \"qwen\" in self.model_id.lower():\n",
    "                # Qwen format\n",
    "                formatted_text = self._format_qwen_messages(messages)\n",
    "            elif \"chatglm\" in self.model_id.lower():\n",
    "                # ChatGLM format\n",
    "                formatted_text = self._format_chatglm_messages(messages)\n",
    "            elif \"deepseek\" in self.model_id.lower():\n",
    "                # DeepSeek format (similar to ChatML)\n",
    "                formatted_text = self._format_deepseek_messages(messages)\n",
    "            else:\n",
    "                # Generic format\n",
    "                formatted_text = self._format_generic_messages(messages)\n",
    "\n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                formatted_text, return_tensors=\"pt\", truncation=True, max_length=2048\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Generate\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=inputs.input_ids.shape[1] + max_length,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.8,\n",
    "                    top_k=50,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            # Decode response\n",
    "            response = self.tokenizer.decode(\n",
    "                outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
    "            ).strip()\n",
    "\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 生成失敗: {e}\")\n",
    "            return f\"生成錯誤: {str(e)}\"\n",
    "\n",
    "    def _format_qwen_messages(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"格式化Qwen消息 (Format Qwen messages)\"\"\"\n",
    "        formatted = \"<|im_start|>system\\n你是一個樂於助人的AI助手。<|im_end|>\\n\"\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            formatted += f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
    "        formatted += \"<|im_start|>assistant\\n\"\n",
    "        return formatted\n",
    "\n",
    "    def _format_chatglm_messages(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"格式化ChatGLM消息 (Format ChatGLM messages)\"\"\"\n",
    "        formatted = \"\"\n",
    "        for msg in messages:\n",
    "            if msg.get(\"role\") == \"user\":\n",
    "                formatted += f\"[Round 1]\\n\\n問：{msg.get('content', '')}\\n\\n答：\"\n",
    "        return formatted\n",
    "\n",
    "    def _format_deepseek_messages(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"格式化DeepSeek消息 (Format DeepSeek messages)\"\"\"\n",
    "        formatted = \"<｜begin▁of▁sentence｜>\"\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if role == \"user\":\n",
    "                formatted += f\"User: {content}\\n\\nAssistant: \"\n",
    "        return formatted\n",
    "\n",
    "    def _format_generic_messages(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"通用消息格式 (Generic message format)\"\"\"\n",
    "        formatted = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if role == \"user\":\n",
    "                formatted += f\"Human: {content}\\n\\nAssistant: \"\n",
    "        return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "print(f\"\\n[載入模型] 開始載入 {model_id}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    llm = LLMAdapter(\n",
    "        model_id=model_id,\n",
    "        backend=config.BACKEND,\n",
    "        load_in_4bit=selected_model_info[\"supports_4bit\"] and torch.cuda.is_available(),\n",
    "    )\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"✅ 模型載入完成，耗時: {load_time:.2f}秒\")\n",
    "\n",
    "    # Quick test\n",
    "    test_messages = [{\"role\": \"user\", \"content\": \"你好，請用中文回答：1+1等於多少？\"}]\n",
    "    test_response = llm.generate(test_messages, max_length=50)\n",
    "    print(f\"[測試回應] {test_response}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 模型初始化失敗: {e}\")\n",
    "    llm = None\n",
    "\n",
    "## Stage 2 Summary\n",
    "print(\n",
    "    f\"\"\"\n",
    "🎯 Stage 2 完成總結:\n",
    "✅ 完成項目:\n",
    "- 建立LLM適配器支援多種中文開源模型\n",
    "- 實現4-bit量化低顯存載入\n",
    "- 支援Qwen/DeepSeek/ChatGLM等主流中文模型\n",
    "\n",
    "🧠 核心觀念:\n",
    "- 開源優先策略：完全避免雲端API依賴\n",
    "- 中文模型適配：針對不同模型的提示詞格式\n",
    "- 資源優化：基於顯存容量自動選擇合適模型\n",
    "\n",
    "🚀 下一步: 建立中文文檔資料庫與嵌入系統\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ea18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 3 - Data Ingest (本地中文文檔)\n",
    "# 本地中文資料載入與處理\n",
    "\n",
    "\n",
    "def create_sample_chinese_docs():\n",
    "    \"\"\"建立範例中文文檔 (Create sample Chinese documents)\"\"\"\n",
    "    docs_dir = pathlib.Path(nb29_paths[\"docs\"])\n",
    "\n",
    "    sample_docs = {\n",
    "        \"AI技術發展趨勢.md\": \"\"\"# 人工智慧技術發展趨勢\n",
    "\n",
    "## 大型語言模型的突破\n",
    "\n",
    "近年來，大型語言模型（Large Language Models, LLMs）在自然語言處理領域取得了革命性突破。從GPT系列到國產的通義千問、文心一言等，這些模型展現出強大的文本理解和生成能力。\n",
    "\n",
    "### 技術特點\n",
    "\n",
    "1. **規模化訓練**：使用大量文本資料進行預訓練，參數量從數十億到數千億不等\n",
    "2. **多模態能力**：結合文本、圖像、語音等多種模態的處理能力\n",
    "3. **中文優化**：針對中文語言特性進行專門優化和訓練\n",
    "\n",
    "### 應用領域\n",
    "\n",
    "- 智能客服與對話系統\n",
    "- 內容創作與寫作輔助\n",
    "- 程式碼生成與除錯\n",
    "- 教育輔導與知識問答\n",
    "- 翻譯與多語言處理\n",
    "\n",
    "## 檢索增強生成技術\n",
    "\n",
    "檢索增強生成（Retrieval-Augmented Generation, RAG）是當前最重要的LLM應用架構之一。它結合了資訊檢索和文本生成的優勢，能夠基於外部知識庫提供準確且及時的答案。\n",
    "\n",
    "### 核心優勢\n",
    "\n",
    "1. **知識時效性**：可以整合最新的外部資訊\n",
    "2. **領域專業性**：針對特定領域的深度知識\n",
    "3. **可解釋性**：提供資訊來源和引用依據\n",
    "4. **成本效益**：避免重複訓練大模型的高昂成本\n",
    "\"\"\",\n",
    "        \"開源LLM生態系統.md\": \"\"\"# 開源大型語言模型生態系統\n",
    "\n",
    "## 主要開源模型家族\n",
    "\n",
    "### Llama系列（Meta）\n",
    "- Llama 2: 7B, 13B, 70B參數版本\n",
    "- Code Llama: 專門的程式碼生成模型\n",
    "- Llama 3: 更強的多語言和推理能力\n",
    "\n",
    "### 中文開源模型\n",
    "\n",
    "#### 通義千問（Qwen）系列\n",
    "- Qwen2.5: 阿里雲開源的強大中文模型\n",
    "- 支援32K上下文長度\n",
    "- 優秀的中文理解和生成能力\n",
    "- 多種規格：0.5B到72B參數\n",
    "\n",
    "#### DeepSeek系列\n",
    "- DeepSeek-R1: 基於強化學習的推理優化\n",
    "- DeepSeek-Coder: 專業的程式碼模型\n",
    "- 在數學和程式設計任務上表現優異\n",
    "\n",
    "#### ChatGLM系列（清華大學）\n",
    "- ChatGLM3-6B: 輕量級中文對話模型\n",
    "- 支援多輪對話和工具調用\n",
    "- 針對中文語境深度優化\n",
    "\n",
    "#### 其他重要模型\n",
    "- Baichuan: 百川智能開源模型\n",
    "- Yi: 零一萬物開源模型系列\n",
    "- InternLM: 上海AI實驗室書生模型\n",
    "\n",
    "## 技術生態與工具鏈\n",
    "\n",
    "### 推理引擎\n",
    "- vLLM: 高效能推理加速\n",
    "- llama.cpp: 跨平台CPU推理\n",
    "- Ollama: 本地模型管理平台\n",
    "- TensorRT-LLM: NVIDIA GPU優化\n",
    "\n",
    "### 量化技術\n",
    "- GPTQ: 權重量化技術\n",
    "- AWQ: 啟動感知量化\n",
    "- GGUF: llama.cpp的量化格式\n",
    "- BitsAndBytes: 動態量化載入\n",
    "\n",
    "### 微調框架\n",
    "- LoRA/QLoRA: 低秩適應微調\n",
    "- PEFT: 參數效率微調庫\n",
    "- DeepSpeed: 大規模分散式訓練\n",
    "- Unsloth: 高效微調加速\n",
    "\n",
    "## 部署與應用\n",
    "\n",
    "### 本地部署方案\n",
    "1. **個人電腦**: 使用量化模型和優化推理\n",
    "2. **企業私有雲**: 建立內部LLM服務\n",
    "3. **邊緣設備**: 輕量級模型部署\n",
    "\n",
    "### 成本考量\n",
    "- 開源模型免費使用，僅需承擔推理成本\n",
    "- 避免API調用費用和資料隱私風險\n",
    "- 可根據需求靈活調整模型規模\n",
    "\"\"\",\n",
    "        \"RAG系統實作指南.md\": \"\"\"# RAG檢索增強生成系統實作指南\n",
    "\n",
    "## 系統架構設計\n",
    "\n",
    "### 核心組件\n",
    "\n",
    "1. **文檔處理模組**\n",
    "   - 文檔載入：支援PDF、Word、Markdown等格式\n",
    "   - 文本切分：智能分段保持語義完整性\n",
    "   - 繁簡轉換：處理繁體與簡體中文差異\n",
    "\n",
    "2. **向量化模組**\n",
    "   - 嵌入模型：BGE-M3、text2vec等中文優化模型\n",
    "   - 向量資料庫：FAISS、Chroma、Pinecone等\n",
    "   - 索引建立：支援大規模文檔集合\n",
    "\n",
    "3. **檢索模組**\n",
    "   - 語義搜尋：基於向量相似度的檢索\n",
    "   - 重排序：使用專門的reranker模型提升精度\n",
    "   - 混合檢索：結合關鍵詞和語義檢索\n",
    "\n",
    "4. **生成模組**\n",
    "   - 提示工程：設計有效的RAG提示模板\n",
    "   - 上下文管理：控制檢索內容的長度和品質\n",
    "   - 回答生成：確保答案的準確性和相關性\n",
    "\n",
    "## 中文處理優化\n",
    "\n",
    "### 分詞與切分策略\n",
    "```python\n",
    "# 中文分隔符優先序列\n",
    "separators = [\"。\", \"！\", \"？\", \"；\", \"…\", \"\\\\n\\\\n\", \"\\\\n\", \" \"]\n",
    "\n",
    "# 適合中文的chunk size設定\n",
    "chunk_size = 512  # 中文字符數\n",
    "overlap = 64      # 重疊字符數\n",
    "```\n",
    "\n",
    "### 嵌入模型選擇\n",
    "- **BGE-M3**: 多語言模型，中英文效果佳\n",
    "- **BGE-small-zh**: 專門的中文嵌入模型\n",
    "- **text2vec-base-chinese**: 輕量級中文模型\n",
    "\n",
    "### 檢索評估指標\n",
    "- **Recall@K**: 前K個結果中正確答案的召回率\n",
    "- **MRR**: 平均倒數排名\n",
    "- **NDCG**: 標準化折扣累計增益\n",
    "\n",
    "## 系統評估與優化\n",
    "\n",
    "### 檢索品質評估\n",
    "1. **相關性評估**: 檢索結果與查詢的相關程度\n",
    "2. **覆蓋率分析**: 知識庫對查詢領域的覆蓋範圍\n",
    "3. **響應時間**: 檢索和生成的延遲指標\n",
    "\n",
    "### 生成品質評估\n",
    "1. **事實準確性**: 答案是否符合檢索到的事實\n",
    "2. **完整性**: 答案是否充分回應了問題\n",
    "3. **流暢性**: 生成文本的自然度和可讀性\n",
    "4. **引用準確性**: 是否正確引用了來源資訊\n",
    "\n",
    "### 系統優化策略\n",
    "- **快取機制**: 對常見查詢建立快取\n",
    "- **批次處理**: 提升向量化處理效率\n",
    "- **模型選擇**: 根據精度和效能需求選擇合適模型\n",
    "- **硬體優化**: GPU加速、記憶體管理\n",
    "\n",
    "## 實際部署考量\n",
    "\n",
    "### 效能最佳化\n",
    "- 使用量化模型減少記憶體佔用\n",
    "- 實施模型並行和流水線處理\n",
    "- 最佳化向量檢索演算法\n",
    "\n",
    "### 資料安全\n",
    "- 本地部署避免資料外洩\n",
    "- 存取控制和使用者認證\n",
    "- 敏感資訊過濾和匿名化\n",
    "\n",
    "### 擴展性設計\n",
    "- 微服務架構便於維護和擴展\n",
    "- 支援動態添加新文檔和知識源\n",
    "- 監控和日誌系統完善\n",
    "\n",
    "這個指南提供了建立高品質中文RAG系統的完整框架，從技術選型到實際部署都有詳細說明。\n",
    "\"\"\",\n",
    "    }\n",
    "\n",
    "    created_files = []\n",
    "    for filename, content in sample_docs.items():\n",
    "        file_path = docs_dir / filename\n",
    "        if not file_path.exists():\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(content)\n",
    "            created_files.append(filename)\n",
    "            print(f\"✅ 建立範例文檔: {filename}\")\n",
    "\n",
    "    return created_files\n",
    "\n",
    "\n",
    "def scan_local_documents() -> List[Dict[str, Any]]:\n",
    "    \"\"\"掃描本地文檔目錄 (Scan local document directory)\"\"\"\n",
    "    docs_dir = pathlib.Path(nb29_paths[\"docs\"])\n",
    "\n",
    "    if not any(docs_dir.iterdir()):\n",
    "        print(f\"📁 文檔目錄為空，建立範例文檔...\")\n",
    "        created_files = create_sample_chinese_docs()\n",
    "        print(f\"✅ 建立了 {len(created_files)} 個範例文檔\")\n",
    "\n",
    "    # Scan for documents\n",
    "    supported_extensions = [\".md\", \".txt\", \".mdx\"]\n",
    "    documents = []\n",
    "\n",
    "    for file_path in docs_dir.iterdir():\n",
    "        if file_path.is_file() and file_path.suffix.lower() in supported_extensions:\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                doc_info = {\n",
    "                    \"filename\": file_path.name,\n",
    "                    \"filepath\": str(file_path),\n",
    "                    \"content\": content,\n",
    "                    \"size\": len(content),\n",
    "                    \"extension\": file_path.suffix,\n",
    "                    \"modified\": datetime.fromtimestamp(\n",
    "                        file_path.stat().st_mtime\n",
    "                    ).isoformat(),\n",
    "                }\n",
    "                documents.append(doc_info)\n",
    "                print(f\"📄 載入文檔: {file_path.name} ({len(content)} 字符)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 載入失敗 {file_path.name}: {e}\")\n",
    "\n",
    "    print(f\"\\n📚 總共載入 {len(documents)} 個文檔\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Optional: Chinese text processing with OpenCC\n",
    "def setup_chinese_converter():\n",
    "    \"\"\"設置中文繁簡轉換 (Setup Chinese Traditional/Simplified converter)\"\"\"\n",
    "    try:\n",
    "        import opencc\n",
    "\n",
    "        converter = opencc.OpenCC(\"t2s\")  # Traditional to Simplified\n",
    "        print(\"✅ OpenCC 繁簡轉換器載入成功\")\n",
    "        return converter\n",
    "    except ImportError:\n",
    "        print(\"⚠️ OpenCC 未安裝，跳過繁簡轉換功能\")\n",
    "        return None\n",
    "\n",
    "\n",
    "chinese_converter = setup_chinese_converter()\n",
    "\n",
    "\n",
    "def preprocess_chinese_text(text: str, converter=None) -> str:\n",
    "    \"\"\"預處理中文文本 (Preprocess Chinese text)\"\"\"\n",
    "    # Basic cleaning\n",
    "    text = text.strip()\n",
    "\n",
    "    # Optional: Convert traditional to simplified\n",
    "    if converter:\n",
    "        try:\n",
    "            text = converter.convert(text)\n",
    "        except:\n",
    "            pass  # Ignore conversion errors\n",
    "\n",
    "    # Remove excessive whitespace\n",
    "    import re\n",
    "\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)  # Normalize line breaks\n",
    "    text = re.sub(r\" +\", \" \", text)  # Normalize spaces\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Text splitting with Chinese optimization\n",
    "def create_chinese_text_splitter():\n",
    "    \"\"\"建立中文文本分割器 (Create Chinese text splitter)\"\"\"\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=config.CHINESE_SEPARATORS,\n",
    "        chunk_size=config.CHUNK_SIZE,\n",
    "        chunk_overlap=config.CHUNK_OVERLAP,\n",
    "        length_function=len,  # Use character count for Chinese\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    print(f\"📝 中文文本分割器配置:\")\n",
    "    print(f\"- 塊大小: {config.CHUNK_SIZE} 字符\")\n",
    "    print(f\"- 重疊: {config.CHUNK_OVERLAP} 字符\")\n",
    "    print(f\"- 分隔符: {config.CHINESE_SEPARATORS[:5]}...\")\n",
    "\n",
    "    return text_splitter\n",
    "\n",
    "\n",
    "def process_documents_to_chunks(documents: List[Dict]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"將文檔處理為文本塊 (Process documents into text chunks)\"\"\"\n",
    "    text_splitter = create_chinese_text_splitter()\n",
    "    all_chunks = []\n",
    "\n",
    "    for doc in documents:\n",
    "        print(f\"\\n📄 處理文檔: {doc['filename']}\")\n",
    "\n",
    "        # Preprocess text\n",
    "        content = preprocess_chinese_text(doc[\"content\"], chinese_converter)\n",
    "\n",
    "        # Split into chunks\n",
    "        chunks = text_splitter.split_text(content)\n",
    "\n",
    "        # Create chunk metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_info = {\n",
    "                \"content\": chunk,\n",
    "                \"source_file\": doc[\"filename\"],\n",
    "                \"source_path\": doc[\"filepath\"],\n",
    "                \"chunk_index\": i,\n",
    "                \"chunk_id\": f\"{doc['filename']}_chunk_{i}\",\n",
    "                \"char_count\": len(chunk),\n",
    "                \"metadata\": {\n",
    "                    \"filename\": doc[\"filename\"],\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"file_size\": doc[\"size\"],\n",
    "                },\n",
    "            }\n",
    "            all_chunks.append(chunk_info)\n",
    "\n",
    "        print(f\"  ✅ 分割為 {len(chunks)} 個文本塊\")\n",
    "\n",
    "    print(f\"\\n📊 總計處理結果:\")\n",
    "    print(f\"- 文檔數量: {len(documents)}\")\n",
    "    print(f\"- 文本塊數量: {len(all_chunks)}\")\n",
    "    print(\n",
    "        f\"- 平均塊大小: {sum(c['char_count'] for c in all_chunks) / len(all_chunks):.1f} 字符\"\n",
    "    )\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# Load and process documents\n",
    "print(f\"\\n=== Stage 3: 載入本地中文文檔 ===\")\n",
    "documents = scan_local_documents()\n",
    "chunks = process_documents_to_chunks(documents)\n",
    "\n",
    "# Save chunks for debugging\n",
    "chunks_file = pathlib.Path(nb29_paths[\"vectorstore\"]) / \"chunks.json\"\n",
    "try:\n",
    "    with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"💾 文本塊已保存至: {chunks_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 保存失敗: {e}\")\n",
    "\n",
    "## Stage 3 Summary\n",
    "print(\n",
    "    f\"\"\"\n",
    "🎯 Stage 3 完成總結:\n",
    "✅ 完成項目:\n",
    "- 自動建立中文範例文檔（AI技術、開源LLM、RAG指南）\n",
    "- 實現中文文本預處理與繁簡轉換\n",
    "- 設計中文優化的文本分割策略\n",
    "- 生成帶有完整元資料的文本塊\n",
    "\n",
    "🧠 核心觀念:\n",
    "- 中文分詞特性：使用標點符號作為主要分隔符\n",
    "- 語義完整性：保持文本塊的語義連貫性\n",
    "- 元資料追蹤：為每個文本塊保留來源資訊\n",
    "\n",
    "🚀 下一步: 建立中文嵌入向量與FAISS索引\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a009c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 4 - Embedding & Index (中文嵌入與索引)\n",
    "# 中文嵌入模型與向量索引建立\n",
    "\n",
    "\n",
    "def load_chinese_embedding_model(model_name: str = \"bge-m3\"):\n",
    "    \"\"\"載入中文嵌入模型 (Load Chinese embedding model)\"\"\"\n",
    "    model_id = config.EMBEDDING_MODELS.get(\n",
    "        model_name, config.EMBEDDING_MODELS[\"bge-m3\"]\n",
    "    )\n",
    "\n",
    "    print(f\"\\n🔗 載入嵌入模型: {model_id}\")\n",
    "\n",
    "    try:\n",
    "        # Load with explicit cache directory\n",
    "        embedding_model = SentenceTransformer(\n",
    "            model_id, cache_folder=cache_paths[\"HF_HOME\"]\n",
    "        )\n",
    "\n",
    "        # Test embedding\n",
    "        test_text = \"這是一個中文測試句子。\"\n",
    "        test_embedding = embedding_model.encode([test_text])\n",
    "\n",
    "        print(f\"✅ 嵌入模型載入成功\")\n",
    "        print(f\"📏 向量維度: {test_embedding.shape[1]}\")\n",
    "        print(f\"🧪 測試嵌入: {test_embedding[0][:5]}...\")\n",
    "\n",
    "        return embedding_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 嵌入模型載入失敗: {e}\")\n",
    "        print(f\"🔄 嘗試使用備用模型...\")\n",
    "\n",
    "        # Fallback to smaller model\n",
    "        try:\n",
    "            fallback_model_id = config.EMBEDDING_MODELS[\"bge-small-zh\"]\n",
    "            embedding_model = SentenceTransformer(fallback_model_id)\n",
    "            print(f\"✅ 備用模型載入成功: {fallback_model_id}\")\n",
    "            return embedding_model\n",
    "        except Exception as e2:\n",
    "            print(f\"❌ 備用模型也失敗: {e2}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def create_embeddings_for_chunks(\n",
    "    chunks: List[Dict], embedding_model\n",
    ") -> Tuple[List[np.ndarray], List[Dict]]:\n",
    "    \"\"\"為文本塊建立嵌入向量 (Create embeddings for text chunks)\"\"\"\n",
    "    print(f\"\\n🧮 開始建立 {len(chunks)} 個文本塊的嵌入向量...\")\n",
    "\n",
    "    # Extract text content\n",
    "    texts = [chunk[\"content\"] for chunk in chunks]\n",
    "\n",
    "    # Create embeddings in batches to manage memory\n",
    "    batch_size = 32\n",
    "    all_embeddings = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_embeddings = embedding_model.encode(\n",
    "            batch_texts,\n",
    "            normalize_embeddings=True,  # Normalize for cosine similarity\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "        if (i // batch_size + 1) % 5 == 0:\n",
    "            print(\n",
    "                f\"  📊 進度: {min(i+batch_size, len(texts))}/{len(texts)} ({(i+batch_size)/len(texts)*100:.1f}%)\"\n",
    "            )\n",
    "\n",
    "    embedding_time = time.time() - start_time\n",
    "    print(f\"✅ 嵌入建立完成，耗時: {embedding_time:.2f}秒\")\n",
    "    print(f\"📈 平均速度: {len(chunks)/embedding_time:.1f} chunks/sec\")\n",
    "\n",
    "    # Convert to numpy array\n",
    "    embeddings_array = np.array(all_embeddings).astype(\"float32\")\n",
    "    print(f\"📏 嵌入矩陣形狀: {embeddings_array.shape}\")\n",
    "\n",
    "    return embeddings_array, chunks\n",
    "\n",
    "\n",
    "def build_faiss_index(\n",
    "    embeddings: np.ndarray, chunks: List[Dict]\n",
    ") -> Tuple[faiss.Index, List[Dict]]:\n",
    "    \"\"\"建立FAISS向量索引 (Build FAISS vector index)\"\"\"\n",
    "    print(f\"\\n🗃️ 建立FAISS索引...\")\n",
    "\n",
    "    dimension = embeddings.shape[1]\n",
    "    num_vectors = embeddings.shape[0]\n",
    "\n",
    "    print(f\"📏 向量維度: {dimension}\")\n",
    "    print(f\"📊 向量數量: {num_vectors}\")\n",
    "\n",
    "    # Choose index type based on dataset size\n",
    "    if num_vectors < 1000:\n",
    "        # For small datasets, use flat index (exact search)\n",
    "        index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity)\n",
    "        print(\"🔍 使用 Flat 索引 (精確搜索)\")\n",
    "    else:\n",
    "        # For larger datasets, use approximate search\n",
    "        nlist = min(100, num_vectors // 10)  # Number of clusters\n",
    "        index = faiss.IndexIVFFlat(faiss.IndexFlatIP(dimension), dimension, nlist)\n",
    "        print(f\"🔍 使用 IVF 索引 (近似搜索, nlist={nlist})\")\n",
    "\n",
    "        # Train the index\n",
    "        print(\"🏋️ 訓練索引...\")\n",
    "        index.train(embeddings)\n",
    "\n",
    "    # Add vectors to index\n",
    "    print(\"📥 添加向量到索引...\")\n",
    "    index.add(embeddings)\n",
    "\n",
    "    print(f\"✅ FAISS索引建立完成\")\n",
    "    print(f\"📊 索引統計: {index.ntotal} 個向量\")\n",
    "\n",
    "    return index, chunks\n",
    "\n",
    "\n",
    "def save_vector_store(\n",
    "    index: faiss.Index, chunks: List[Dict], embedding_model_name: str\n",
    "):\n",
    "    \"\"\"保存向量存儲 (Save vector store)\"\"\"\n",
    "    vectorstore_dir = pathlib.Path(nb29_paths[\"vectorstore\"])\n",
    "\n",
    "    # Save FAISS index\n",
    "    index_file = vectorstore_dir / \"index.faiss\"\n",
    "    faiss.write_index(index, str(index_file))\n",
    "\n",
    "    # Save chunks metadata\n",
    "    chunks_file = vectorstore_dir / \"chunks_metadata.json\"\n",
    "    with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Save configuration\n",
    "    config_file = vectorstore_dir / \"config.json\"\n",
    "    config_data = {\n",
    "        \"embedding_model\": embedding_model_name,\n",
    "        \"index_type\": \"flat\" if \"Flat\" in str(type(index)) else \"ivf\",\n",
    "        \"dimension\": index.d,\n",
    "        \"total_vectors\": index.ntotal,\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"chunk_size\": config.CHUNK_SIZE,\n",
    "        \"chunk_overlap\": config.CHUNK_OVERLAP,\n",
    "    }\n",
    "\n",
    "    with open(config_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config_data, f, indent=2)\n",
    "\n",
    "    print(f\"💾 向量存儲已保存至: {vectorstore_dir}\")\n",
    "    print(f\"  - 索引檔案: index.faiss\")\n",
    "    print(f\"  - 元資料: chunks_metadata.json\")\n",
    "    print(f\"  - 配置: config.json\")\n",
    "\n",
    "\n",
    "# Load embedding model and create index\n",
    "print(f\"\\n=== Stage 4: 建立中文嵌入與向量索引 ===\")\n",
    "\n",
    "embedding_model = load_chinese_embedding_model(\"bge-m3\")\n",
    "embeddings, chunk_metadata = create_embeddings_for_chunks(chunks, embedding_model)\n",
    "faiss_index, indexed_chunks = build_faiss_index(embeddings, chunk_metadata)\n",
    "\n",
    "# Save vector store\n",
    "save_vector_store(faiss_index, indexed_chunks, \"bge-m3\")\n",
    "\n",
    "# Memory cleanup\n",
    "del embeddings\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "## Stage 4 Summary\n",
    "print(\n",
    "    f\"\"\"\n",
    "🎯 Stage 4 完成總結:\n",
    "✅ 完成項目:\n",
    "- 載入中文優化的BGE-M3嵌入模型\n",
    "- 批次處理文本塊建立嵌入向量\n",
    "- 建立FAISS向量索引支援語義搜索\n",
    "- 保存完整的向量存儲與元資料\n",
    "\n",
    "🧠 核心觀念:\n",
    "- 中文嵌入：使用專門優化的中文嵌入模型\n",
    "- 向量正規化：確保餘弦相似度計算的準確性\n",
    "- 索引策略：根據資料規模選擇合適的索引類型\n",
    "- 記憶體管理：批次處理避免記憶體溢出\n",
    "\n",
    "🚀 下一步: 實現檢索、重排序與生成流程\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caeb826",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 5 - Retrieve → (Re-rank) → Generate\n",
    "# 檢索、重排序與生成流程\n",
    "\n",
    "\n",
    "class ChineseRAGRetriever:\n",
    "    \"\"\"中文RAG檢索器 (Chinese RAG Retriever)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        faiss_index: faiss.Index,\n",
    "        chunks: List[Dict],\n",
    "        embedding_model,\n",
    "        llm_adapter: LLMAdapter,\n",
    "    ):\n",
    "        self.index = faiss_index\n",
    "        self.chunks = chunks\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm = llm_adapter\n",
    "        self.reranker = None\n",
    "\n",
    "        print(f\"🔍 RAG檢索器初始化完成\")\n",
    "        print(f\"📊 索引向量數: {self.index.ntotal}\")\n",
    "        print(f\"📄 文本塊數: {len(self.chunks)}\")\n",
    "\n",
    "    def load_reranker(self, model_name: str = \"bge-reranker-base\"):\n",
    "        \"\"\"載入重排序模型 (Load reranker model)\"\"\"\n",
    "        try:\n",
    "            reranker_model_id = f\"BAAI/{model_name}\"\n",
    "            from sentence_transformers import CrossEncoder\n",
    "\n",
    "            self.reranker = CrossEncoder(\n",
    "                reranker_model_id, cache_folder=cache_paths[\"HF_HOME\"]\n",
    "            )\n",
    "            print(f\"✅ 重排序模型載入成功: {model_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 重排序模型載入失敗: {e}\")\n",
    "            print(f\"💡 將跳過重排序步驟\")\n",
    "            self.reranker = None\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"檢索相關文本塊 (Retrieve relevant text chunks)\"\"\"\n",
    "        # Encode query\n",
    "        query_embedding = self.embedding_model.encode(\n",
    "            [query], normalize_embeddings=True\n",
    "        )\n",
    "        query_vector = query_embedding.astype(\"float32\")\n",
    "\n",
    "        # Search in FAISS index\n",
    "        scores, indices = self.index.search(query_vector, top_k)\n",
    "\n",
    "        # Prepare results\n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if idx != -1:  # Valid index\n",
    "                chunk = self.chunks[idx].copy()\n",
    "                chunk[\"similarity_score\"] = float(score)\n",
    "                chunk[\"rank\"] = i + 1\n",
    "                results.append(chunk)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def rerank(self, query: str, retrieved_chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"重排序檢索結果 (Rerank retrieved results)\"\"\"\n",
    "        if not self.reranker or len(retrieved_chunks) <= 1:\n",
    "            return retrieved_chunks\n",
    "\n",
    "        try:\n",
    "            # Prepare query-document pairs\n",
    "            pairs = [(query, chunk[\"content\"]) for chunk in retrieved_chunks]\n",
    "\n",
    "            # Get reranking scores\n",
    "            rerank_scores = self.reranker.predict(pairs)\n",
    "\n",
    "            # Update chunks with rerank scores and sort\n",
    "            for chunk, score in zip(retrieved_chunks, rerank_scores):\n",
    "                chunk[\"rerank_score\"] = float(score)\n",
    "\n",
    "            # Sort by rerank score (descending)\n",
    "            reranked = sorted(\n",
    "                retrieved_chunks, key=lambda x: x[\"rerank_score\"], reverse=True\n",
    "            )\n",
    "\n",
    "            # Update ranks\n",
    "            for i, chunk in enumerate(reranked):\n",
    "                chunk[\"rerank_rank\"] = i + 1\n",
    "\n",
    "            print(f\"🔄 重排序完成，調整了 {len(reranked)} 個結果的順序\")\n",
    "            return reranked\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 重排序失敗: {e}\")\n",
    "            return retrieved_chunks\n",
    "\n",
    "    def generate_answer(\n",
    "        self, query: str, context_chunks: List[Dict], max_context_length: int = 2000\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"基於檢索上下文生成答案 (Generate answer based on retrieved context)\"\"\"\n",
    "\n",
    "        # Prepare context from chunks\n",
    "        context_parts = []\n",
    "        total_length = 0\n",
    "        used_chunks = []\n",
    "\n",
    "        for chunk in context_chunks:\n",
    "            content = chunk[\"content\"]\n",
    "            if total_length + len(content) <= max_context_length:\n",
    "                context_parts.append(f\"【來源：{chunk['source_file']}】\\n{content}\")\n",
    "                total_length += len(content)\n",
    "                used_chunks.append(chunk)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "        # Create RAG prompt\n",
    "        rag_prompt = f\"\"\"基於以下資料來源，請用中文回答問題。請確保答案準確且有根據，並在答案末尾列出參考的資料來源。\n",
    "\n",
    "問題：{query}\n",
    "\n",
    "參考資料：\n",
    "{context}\n",
    "\n",
    "請提供詳細且準確的回答：\"\"\"\n",
    "\n",
    "        # Generate answer\n",
    "        messages = [{\"role\": \"user\", \"content\": rag_prompt}]\n",
    "\n",
    "        start_time = time.time()\n",
    "        answer = self.llm.generate(messages, max_length=512, temperature=0.3)\n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "        # Prepare result\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"context_chunks\": used_chunks,\n",
    "            \"context_length\": total_length,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"sources\": list(set(chunk[\"source_file\"] for chunk in used_chunks)),\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def answer(\n",
    "        self, query: str, top_k: int = 5, use_reranker: bool = True\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"端到端問答 (End-to-end question answering)\"\"\"\n",
    "        print(f\"\\n❓ 問題: {query}\")\n",
    "\n",
    "        # Step 1: Retrieve\n",
    "        print(f\"🔍 檢索中...\")\n",
    "        retrieved = self.retrieve(query, top_k)\n",
    "        print(f\"📄 檢索到 {len(retrieved)} 個相關文本塊\")\n",
    "\n",
    "        # Step 2: Rerank (optional)\n",
    "        if use_reranker and self.reranker:\n",
    "            print(f\"🔄 重排序中...\")\n",
    "            retrieved = self.rerank(query, retrieved)\n",
    "\n",
    "        # Step 3: Generate\n",
    "        print(f\"✍️ 生成答案中...\")\n",
    "        result = self.generate_answer(query, retrieved)\n",
    "\n",
    "        print(f\"✅ 回答完成 (耗時: {result['generation_time']:.2f}秒)\")\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# Initialize retriever\n",
    "print(f\"\\n=== Stage 5: 建立RAG檢索生成系統 ===\")\n",
    "\n",
    "rag_retriever = ChineseRAGRetriever(\n",
    "    faiss_index=faiss_index,\n",
    "    chunks=indexed_chunks,\n",
    "    embedding_model=embedding_model,\n",
    "    llm_adapter=llm,\n",
    ")\n",
    "\n",
    "# Optional: Load reranker (comment out if VRAM is limited)\n",
    "# rag_retriever.load_reranker(\"bge-reranker-base\")\n",
    "\n",
    "# Demo queries\n",
    "demo_queries = [\n",
    "    \"什麼是大型語言模型？\",\n",
    "    \"RAG系統有什麼優勢？\",\n",
    "    \"如何選擇合適的開源中文模型？\",\n",
    "    \"中文文本處理有什麼特殊考量？\",\n",
    "]\n",
    "\n",
    "print(f\"\\n🎯 RAG系統演示\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "demo_results = []\n",
    "for query in demo_queries[:2]:  # Limit to 2 queries for demo\n",
    "    try:\n",
    "        result = rag_retriever.answer(query, top_k=3, use_reranker=False)\n",
    "        demo_results.append(result)\n",
    "\n",
    "        print(f\"\\n📝 問答結果:\")\n",
    "        print(f\"Q: {result['query']}\")\n",
    "        print(f\"A: {result['answer']}\")\n",
    "        print(f\"📚 參考來源: {', '.join(result['sources'])}\")\n",
    "        print(f\"⏱️ 生成時間: {result['generation_time']:.2f}秒\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 問答失敗: {e}\")\n",
    "\n",
    "## Stage 5 Summary\n",
    "print(\n",
    "    f\"\"\"\n",
    "🎯 Stage 5 完成總結:\n",
    "✅ 完成項目:\n",
    "- 實現完整的RAG檢索生成流程\n",
    "- 支援FAISS向量檢索與餘弦相似度計算\n",
    "- 設計中文優化的RAG提示模板\n",
    "- 提供端到端問答介面與來源追蹤\n",
    "\n",
    "🧠 核心觀念:\n",
    "- 檢索策略：基於語義相似度的向量檢索\n",
    "- 上下文管理：控制輸入長度避免超出模型限制\n",
    "- 提示工程：設計有效的中文RAG提示模板\n",
    "- 來源歸屬：確保答案可追溯到原始文檔\n",
    "\n",
    "🚀 下一步: 實現評估指標與效能分析\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5648c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 6 - Evaluation (輕量評估)\n",
    "# 檢索與生成品質評估\n",
    "\n",
    "\n",
    "def create_evaluation_dataset():\n",
    "    \"\"\"建立評估資料集 (Create evaluation dataset)\"\"\"\n",
    "    eval_queries = [\n",
    "        {\n",
    "            \"query\": \"什麼是大型語言模型？\",\n",
    "            \"expected_keywords\": [\"LLM\", \"語言模型\", \"GPT\", \"訓練\", \"參數\"],\n",
    "            \"expected_sources\": [\"AI技術發展趨勢.md\"],\n",
    "            \"category\": \"definition\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"RAG系統的核心優勢是什麼？\",\n",
    "            \"expected_keywords\": [\"RAG\", \"檢索增強\", \"知識庫\", \"時效性\", \"專業性\"],\n",
    "            \"expected_sources\": [\"AI技術發展趨勢.md\", \"RAG系統實作指南.md\"],\n",
    "            \"category\": \"concept\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"有哪些重要的開源中文模型？\",\n",
    "            \"expected_keywords\": [\"Qwen\", \"ChatGLM\", \"DeepSeek\", \"開源\", \"中文\"],\n",
    "            \"expected_sources\": [\"開源LLM生態系統.md\"],\n",
    "            \"category\": \"enumeration\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"如何評估RAG系統的檢索品質？\",\n",
    "            \"expected_keywords\": [\"Recall\", \"MRR\", \"NDCG\", \"相關性\", \"評估\"],\n",
    "            \"expected_sources\": [\"RAG系統實作指南.md\"],\n",
    "            \"category\": \"methodology\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"中文文本分割有什麼特殊考量？\",\n",
    "            \"expected_keywords\": [\"分詞\", \"標點符號\", \"語義\", \"chunk\", \"中文\"],\n",
    "            \"expected_sources\": [\"RAG系統實作指南.md\"],\n",
    "            \"category\": \"technical\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return eval_queries\n",
    "\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"RAG系統評估器 (RAG System Evaluator)\"\"\"\n",
    "\n",
    "    def __init__(self, retriever: ChineseRAGRetriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def evaluate_retrieval(\n",
    "        self, eval_queries: List[Dict], k_values: List[int] = [1, 3, 5]\n",
    "    ) -> Dict:\n",
    "        \"\"\"評估檢索品質 (Evaluate retrieval quality)\"\"\"\n",
    "        print(f\"\\n📊 評估檢索品質...\")\n",
    "\n",
    "        results = {\n",
    "            \"recall_at_k\": {k: [] for k in k_values},\n",
    "            \"mrr_scores\": [],\n",
    "            \"query_results\": [],\n",
    "        }\n",
    "\n",
    "        for query_data in eval_queries:\n",
    "            query = query_data[\"query\"]\n",
    "            expected_sources = set(query_data[\"expected_sources\"])\n",
    "\n",
    "            # Retrieve documents\n",
    "            retrieved = self.retriever.retrieve(query, top_k=max(k_values))\n",
    "            retrieved_sources = [chunk[\"source_file\"] for chunk in retrieved]\n",
    "\n",
    "            # Calculate Recall@K\n",
    "            query_recall = {}\n",
    "            for k in k_values:\n",
    "                retrieved_k = set(retrieved_sources[:k])\n",
    "                relevant_found = len(retrieved_k.intersection(expected_sources))\n",
    "                recall_k = (\n",
    "                    relevant_found / len(expected_sources) if expected_sources else 0\n",
    "                )\n",
    "                query_recall[k] = recall_k\n",
    "                results[\"recall_at_k\"][k].append(recall_k)\n",
    "\n",
    "            # Calculate MRR (Mean Reciprocal Rank)\n",
    "            reciprocal_rank = 0\n",
    "            for i, source in enumerate(retrieved_sources):\n",
    "                if source in expected_sources:\n",
    "                    reciprocal_rank = 1.0 / (i + 1)\n",
    "                    break\n",
    "            results[\"mrr_scores\"].append(reciprocal_rank)\n",
    "\n",
    "            # Store detailed results\n",
    "            query_result = {\n",
    "                \"query\": query,\n",
    "                \"expected_sources\": list(expected_sources),\n",
    "                \"retrieved_sources\": retrieved_sources,\n",
    "                \"recall_at_k\": query_recall,\n",
    "                \"reciprocal_rank\": reciprocal_rank,\n",
    "            }\n",
    "            results[\"query_results\"].append(query_result)\n",
    "\n",
    "            print(\n",
    "                f\"  📝 {query[:30]}... - Recall@3: {query_recall[3]:.2f}, RR: {reciprocal_rank:.2f}\"\n",
    "            )\n",
    "\n",
    "        # Calculate averages\n",
    "        avg_recall = {}\n",
    "        for k in k_values:\n",
    "            avg_recall[k] = sum(results[\"recall_at_k\"][k]) / len(\n",
    "                results[\"recall_at_k\"][k]\n",
    "            )\n",
    "\n",
    "        avg_mrr = sum(results[\"mrr_scores\"]) / len(results[\"mrr_scores\"])\n",
    "\n",
    "        results[\"average_recall_at_k\"] = avg_recall\n",
    "        results[\"average_mrr\"] = avg_mrr\n",
    "\n",
    "        return results\n",
    "\n",
    "    def evaluate_generation(self, eval_queries: List[Dict]) -> Dict:\n",
    "        \"\"\"評估生成品質 (Evaluate generation quality)\"\"\"\n",
    "        print(f\"\\n📝 評估生成品質...\")\n",
    "\n",
    "        results = {\n",
    "            \"groundedness_scores\": [],\n",
    "            \"keyword_coverage\": [],\n",
    "            \"response_lengths\": [],\n",
    "            \"generation_times\": [],\n",
    "            \"query_results\": [],\n",
    "        }\n",
    "\n",
    "        for query_data in eval_queries:\n",
    "            query = query_data[\"query\"]\n",
    "            expected_keywords = query_data[\"expected_keywords\"]\n",
    "\n",
    "            # Generate answer\n",
    "            answer_result = self.retriever.answer(query, top_k=3, use_reranker=False)\n",
    "            answer = answer_result[\"answer\"]\n",
    "\n",
    "            groundedness = self.calculate_groundedness(answer, context_text)\n",
    "            results[\"groundedness_scores\"].append(groundedness)\n",
    "\n",
    "            # Evaluate keyword coverage\n",
    "            answer_lower = answer.lower()\n",
    "            keyword_hits = sum(\n",
    "                1 for keyword in expected_keywords if keyword.lower() in answer_lower\n",
    "            )\n",
    "            coverage = keyword_hits / len(expected_keywords) if expected_keywords else 0\n",
    "            results[\"keyword_coverage\"].append(coverage)\n",
    "\n",
    "            # Record metrics\n",
    "            results[\"response_lengths\"].append(len(answer))\n",
    "            results[\"generation_times\"].append(answer_result[\"generation_time\"])\n",
    "\n",
    "            query_result = {\n",
    "                \"query\": query,\n",
    "                \"answer\": answer,\n",
    "                \"expected_keywords\": expected_keywords,\n",
    "                \"keyword_coverage\": coverage,\n",
    "                \"groundedness\": groundedness,\n",
    "                \"response_length\": len(answer),\n",
    "                \"generation_time\": answer_result[\"generation_time\"],\n",
    "                \"sources_used\": answer_result[\"sources\"],\n",
    "            }\n",
    "            results[\"query_results\"].append(query_result)\n",
    "\n",
    "            print(\n",
    "                f\"  📝 {query[:30]}... - 關鍵詞覆蓋: {coverage:.2f}, 事實性: {groundedness:.2f}\"\n",
    "            )\n",
    "\n",
    "        # Calculate averages\n",
    "        results[\"average_keyword_coverage\"] = sum(results[\"keyword_coverage\"]) / len(\n",
    "            results[\"keyword_coverage\"]\n",
    "        )\n",
    "        results[\"average_groundedness\"] = sum(results[\"groundedness_scores\"]) / len(\n",
    "            results[\"groundedness_scores\"]\n",
    "        )\n",
    "        results[\"average_response_length\"] = sum(results[\"response_lengths\"]) / len(\n",
    "            results[\"response_lengths\"]\n",
    "        )\n",
    "        results[\"average_generation_time\"] = sum(results[\"generation_times\"]) / len(\n",
    "            results[\"generation_times\"]\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def calculate_groundedness(self, answer: str, context: str) -> float:\n",
    "        \"\"\"計算答案的事實依據性 (Calculate answer groundedness)\"\"\"\n",
    "        if not context.strip():\n",
    "            return 0.0\n",
    "\n",
    "        # Simple approach: check if key phrases from answer appear in context\n",
    "        import re\n",
    "\n",
    "        # Split answer into sentences\n",
    "        answer_sentences = re.split(r\"[。！？]\", answer)\n",
    "        answer_sentences = [s.strip() for s in answer_sentences if s.strip()]\n",
    "\n",
    "        if not answer_sentences:\n",
    "            return 0.0\n",
    "\n",
    "        grounded_sentences = 0\n",
    "        for sentence in answer_sentences:\n",
    "            # Check if sentence has substantial overlap with context\n",
    "            sentence_words = set(sentence.split())\n",
    "            context_words = set(context.split())\n",
    "\n",
    "            if len(sentence_words) > 2:  # Only check substantial sentences\n",
    "                overlap = len(sentence_words.intersection(context_words))\n",
    "                overlap_ratio = overlap / len(sentence_words)\n",
    "\n",
    "                if overlap_ratio > 0.3:  # At least 30% word overlap\n",
    "                    grounded_sentences += 1\n",
    "\n",
    "        return grounded_sentences / len(answer_sentences)\n",
    "\n",
    "    def performance_benchmark(self, num_queries: int = 10) -> Dict:\n",
    "        \"\"\"效能基準測試 (Performance benchmark)\"\"\"\n",
    "        print(f\"\\n⚡ 執行效能基準測試...\")\n",
    "\n",
    "        test_query = \"什麼是大型語言模型的主要特點？\"\n",
    "\n",
    "        retrieval_times = []\n",
    "        generation_times = []\n",
    "        total_times = []\n",
    "\n",
    "        for i in range(num_queries):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Retrieval timing\n",
    "            retrieval_start = time.time()\n",
    "            retrieved = self.retriever.retrieve(test_query, top_k=5)\n",
    "            retrieval_time = time.time() - retrieval_start\n",
    "            retrieval_times.append(retrieval_time)\n",
    "\n",
    "            # Generation timing\n",
    "            generation_start = time.time()\n",
    "            result = self.retriever.generate_answer(test_query, retrieved)\n",
    "            generation_time = time.time() - generation_start\n",
    "            generation_times.append(generation_time)\n",
    "\n",
    "            total_time = time.time() - start_time\n",
    "            total_times.append(total_time)\n",
    "\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"  📊 完成 {i+1}/{num_queries} 次測試\")\n",
    "\n",
    "        # Calculate statistics\n",
    "        def calc_stats(times):\n",
    "            return {\n",
    "                \"mean\": sum(times) / len(times),\n",
    "                \"min\": min(times),\n",
    "                \"max\": max(times),\n",
    "                \"std\": (\n",
    "                    sum((t - sum(times) / len(times)) ** 2 for t in times) / len(times)\n",
    "                )\n",
    "                ** 0.5,\n",
    "            }\n",
    "\n",
    "        benchmark_results = {\n",
    "            \"retrieval\": calc_stats(retrieval_times),\n",
    "            \"generation\": calc_stats(generation_times),\n",
    "            \"total\": calc_stats(total_times),\n",
    "            \"num_queries\": num_queries,\n",
    "        }\n",
    "\n",
    "        return benchmark_results\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "print(f\"\\n=== Stage 6: RAG系統評估 ===\")\n",
    "\n",
    "eval_dataset = create_evaluation_dataset()\n",
    "evaluator = RAGEvaluator(rag_retriever)\n",
    "\n",
    "print(f\"📋 評估資料集: {len(eval_dataset)} 個查詢\")\n",
    "\n",
    "# Retrieval evaluation\n",
    "retrieval_results = evaluator.evaluate_retrieval(eval_dataset)\n",
    "\n",
    "print(f\"\\n📊 檢索評估結果:\")\n",
    "for k, recall in retrieval_results[\"average_recall_at_k\"].items():\n",
    "    print(f\"  - Recall@{k}: {recall:.3f}\")\n",
    "print(f\"  - MRR: {retrieval_results['average_mrr']:.3f}\")\n",
    "\n",
    "# Generation evaluation\n",
    "generation_results = evaluator.evaluate_generation(eval_dataset[:3])  # Limit for demo\n",
    "\n",
    "print(f\"\\n📝 生成評估結果:\")\n",
    "print(f\"  - 平均關鍵詞覆蓋率: {generation_results['average_keyword_coverage']:.3f}\")\n",
    "print(f\"  - 平均事實依據性: {generation_results['average_groundedness']:.3f}\")\n",
    "print(f\"  - 平均回應長度: {generation_results['average_response_length']:.1f} 字符\")\n",
    "print(f\"  - 平均生成時間: {generation_results['average_generation_time']:.2f} 秒\")\n",
    "\n",
    "# Performance benchmark\n",
    "performance_results = evaluator.performance_benchmark(num_queries=5)\n",
    "\n",
    "print(f\"\\n⚡ 效能基準測試:\")\n",
    "print(\n",
    "    f\"  - 檢索時間: {performance_results['retrieval']['mean']:.3f}±{performance_results['retrieval']['std']:.3f}s\"\n",
    ")\n",
    "print(\n",
    "    f\"  - 生成時間: {performance_results['generation']['mean']:.3f}±{performance_results['generation']['std']:.3f}s\"\n",
    ")\n",
    "print(\n",
    "    f\"  - 總時間: {performance_results['total']['mean']:.3f}±{performance_results['total']['std']:.3f}s\"\n",
    ")\n",
    "\n",
    "# Save evaluation results\n",
    "eval_results = {\n",
    "    \"retrieval_evaluation\": retrieval_results,\n",
    "    \"generation_evaluation\": generation_results,\n",
    "    \"performance_benchmark\": performance_results,\n",
    "    \"evaluation_timestamp\": datetime.now().isoformat(),\n",
    "    \"model_config\": {\n",
    "        \"llm_model\": model_id,\n",
    "        \"embedding_model\": \"bge-m3\",\n",
    "        \"chunk_size\": config.CHUNK_SIZE,\n",
    "        \"top_k\": config.TOP_K,\n",
    "    },\n",
    "}\n",
    "\n",
    "eval_file = pathlib.Path(nb29_paths[\"vectorstore\"]) / \"evaluation_results.json\"\n",
    "try:\n",
    "    with open(eval_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(eval_results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"💾 評估結果已保存至: {eval_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 保存評估結果失敗: {e}\")\n",
    "\n",
    "## Stage 6 Summary\n",
    "print(\n",
    "    f\"\"\"\n",
    "🎯 Stage 6 完成總結:\n",
    "✅ 完成項目:\n",
    "- 建立多維度RAG評估框架（檢索+生成+效能）\n",
    "- 實現Recall@K、MRR等檢索指標計算\n",
    "- 設計事實依據性與關鍵詞覆蓋率評估\n",
    "- 執行效能基準測試與統計分析\n",
    "\n",
    "🧠 核心觀念:\n",
    "- 檢索評估：使用標準資訊檢索指標\n",
    "- 生成評估：結合事實性與完整性指標\n",
    "- 效能分析：分離檢索與生成的時間成本\n",
    "- 持續改進：建立可重複的評估流程\n",
    "\n",
    "🚀 下一步: 建立Gradio互動介面\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e8e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 7 - (Optional) Gradio Quick Interface\n",
    "# 可選：Gradio快速互動介面\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "\n",
    "    gradio_available = True\n",
    "    print(\"✅ Gradio 可用，建立互動介面...\")\n",
    "except ImportError:\n",
    "    gradio_available = False\n",
    "    print(\"⚠️ Gradio 未安裝，跳過介面建立\")\n",
    "\n",
    "\n",
    "def create_gradio_interface():\n",
    "    \"\"\"建立Gradio聊天介面 (Create Gradio chat interface)\"\"\"\n",
    "    if not gradio_available or not llm:\n",
    "        return None\n",
    "\n",
    "    def rag_chat(message, history):\n",
    "        \"\"\"RAG聊天處理函數 (RAG chat handler)\"\"\"\n",
    "        try:\n",
    "            # Get RAG answer\n",
    "            result = rag_retriever.answer(message, top_k=3, use_reranker=False)\n",
    "\n",
    "            # Format response with sources\n",
    "            response = result[\"answer\"]\n",
    "            if result[\"sources\"]:\n",
    "                response += f\"\\n\\n📚 **參考來源**: {', '.join(result['sources'])}\"\n",
    "\n",
    "            # Add to history\n",
    "            history.append([message, response])\n",
    "            return history, \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            error_response = f\"❌ 處理失敗: {str(e)}\"\n",
    "            history.append([message, error_response])\n",
    "            return history, \"\"\n",
    "\n",
    "    # Create interface\n",
    "    with gr.Blocks(title=\"中文RAG系統\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"# 🇨🇳 中文開源RAG問答系統\")\n",
    "        gr.Markdown(\"基於DeepSeek/Qwen + BGE + FAISS的完全開源解決方案\")\n",
    "\n",
    "        chatbot = gr.Chatbot(label=\"AI助手\", height=400, show_label=True)\n",
    "\n",
    "        with gr.Row():\n",
    "            msg = gr.Textbox(label=\"輸入問題\", placeholder=\"請輸入您的問題...\", scale=4)\n",
    "            send_btn = gr.Button(\"發送\", scale=1, variant=\"primary\")\n",
    "\n",
    "        # Sample questions\n",
    "        gr.Markdown(\"### 💡 範例問題:\")\n",
    "        sample_questions = [\n",
    "            \"什麼是大型語言模型？\",\n",
    "            \"RAG系統有什麼優勢？\",\n",
    "            \"有哪些重要的開源中文模型？\",\n",
    "            \"如何評估RAG系統品質？\",\n",
    "        ]\n",
    "\n",
    "        for question in sample_questions:\n",
    "            gr.Button(question, size=\"sm\").click(\n",
    "                lambda q=question: (chatbot.value + [[q, \"處理中...\"]], \"\"),\n",
    "                outputs=[chatbot, msg],\n",
    "            ).then(\n",
    "                rag_chat,\n",
    "                inputs=[gr.Textbox(value=question, visible=False), chatbot],\n",
    "                outputs=[chatbot, msg],\n",
    "            )\n",
    "\n",
    "        # Event handlers\n",
    "        msg.submit(rag_chat, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
    "        send_btn.click(rag_chat, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
    "\n",
    "        # System info\n",
    "        with gr.Accordion(\"🔧 系統資訊\", open=False):\n",
    "            gr.Markdown(\n",
    "                f\"\"\"\n",
    "            - **LLM模型**: {model_id}\n",
    "            - **嵌入模型**: BGE-M3\n",
    "            - **向量數量**: {faiss_index.ntotal}\n",
    "            - **文檔數量**: {len(documents)}\n",
    "            - **後端**: {config.BACKEND}\n",
    "            \"\"\"\n",
    "            )\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "# Create and launch Gradio interface\n",
    "if gradio_available and llm:\n",
    "    print(f\"\\n🌐 建立Gradio介面...\")\n",
    "    gradio_demo = create_gradio_interface()\n",
    "\n",
    "    # For notebook environment, use share=False and inbrowser=False\n",
    "    try:\n",
    "        print(f\"🚀 啟動Gradio介面 (本地存取)\")\n",
    "        gradio_demo.launch(\n",
    "            server_name=\"127.0.0.1\",\n",
    "            server_port=7860,\n",
    "            share=False,\n",
    "            inbrowser=False,\n",
    "            quiet=True,\n",
    "        )\n",
    "        print(f\"✅ Gradio介面已啟動: http://127.0.0.1:7860\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Gradio啟動失敗: {e}\")\n",
    "        print(f\"💡 可能需要在終端機中運行此notebook\")\n",
    "\n",
    "## Stage 7 Summary\n",
    "print(\n",
    "    f\"\"\"\n",
    "🎯 Stage 7 完成總結:\n",
    "✅ 完成項目:\n",
    "- 建立Gradio互動式聊天介面\n",
    "- 整合RAG問答與來源顯示功能\n",
    "- 提供範例問題快速測試\n",
    "- 顯示系統配置資訊\n",
    "\n",
    "🧠 核心觀念:\n",
    "- 使用者體驗：提供直觀的聊天介面\n",
    "- 資訊透明：顯示答案來源與系統狀態\n",
    "- 快速測試：預設範例問題便於驗證\n",
    "- 本地部署：完全本地化的互動環境\n",
    "\n",
    "🚀 下一步: 完整系統總結與優化建議\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba7864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 8 - Final Summary & Optimizations\n",
    "# 最終總結與優化建議\n",
    "\n",
    "\n",
    "def generate_system_report():\n",
    "    \"\"\"生成系統完整報告 (Generate comprehensive system report)\"\"\"\n",
    "\n",
    "    # System configuration\n",
    "    system_config = {\n",
    "        \"llm_model\": model_id,\n",
    "        \"embedding_model\": \"BAAI/bge-m3\",\n",
    "        \"vector_store\": \"FAISS\",\n",
    "        \"backend\": config.BACKEND,\n",
    "        \"quantization\": \"4-bit\" if torch.cuda.is_available() else \"none\",\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    }\n",
    "\n",
    "    # Data statistics\n",
    "    data_stats = {\n",
    "        \"total_documents\": len(documents),\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"vector_dimension\": faiss_index.d,\n",
    "        \"index_size\": faiss_index.ntotal,\n",
    "        \"avg_chunk_size\": (\n",
    "            sum(c[\"char_count\"] for c in chunks) / len(chunks) if chunks else 0\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Performance summary\n",
    "    if \"performance_results\" in locals():\n",
    "        perf_summary = {\n",
    "            \"avg_retrieval_time\": performance_results[\"retrieval\"][\"mean\"],\n",
    "            \"avg_generation_time\": performance_results[\"generation\"][\"mean\"],\n",
    "            \"avg_total_time\": performance_results[\"total\"][\"mean\"],\n",
    "        }\n",
    "    else:\n",
    "        perf_summary = {\"note\": \"Performance benchmark not run\"}\n",
    "\n",
    "    # Quality metrics\n",
    "    if \"retrieval_results\" in locals() and \"generation_results\" in locals():\n",
    "        quality_summary = {\n",
    "            \"recall_at_3\": retrieval_results[\"average_recall_at_k\"].get(3, 0),\n",
    "            \"mrr\": retrieval_results[\"average_mrr\"],\n",
    "            \"keyword_coverage\": generation_results[\"average_keyword_coverage\"],\n",
    "            \"groundedness\": generation_results[\"average_groundedness\"],\n",
    "        }\n",
    "    else:\n",
    "        quality_summary = {\"note\": \"Quality evaluation not run\"}\n",
    "\n",
    "    report = {\n",
    "        \"system_configuration\": system_config,\n",
    "        \"data_statistics\": data_stats,\n",
    "        \"performance_metrics\": perf_summary,\n",
    "        \"quality_metrics\": quality_summary,\n",
    "        \"generated_at\": datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "# Generate final report\n",
    "print(f\"\\n=== 🎯 中文開源RAG系統完整報告 ===\")\n",
    "\n",
    "final_report = generate_system_report()\n",
    "\n",
    "print(f\"\\n📋 系統配置:\")\n",
    "for key, value in final_report[\"system_configuration\"].items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "print(f\"\\n📊 資料統計:\")\n",
    "for key, value in final_report[\"data_statistics\"].items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "if \"note\" not in final_report[\"performance_metrics\"]:\n",
    "    print(f\"\\n⚡ 效能指標:\")\n",
    "    for key, value in final_report[\"performance_metrics\"].items():\n",
    "        print(f\"  - {key}: {value:.3f}s\")\n",
    "\n",
    "if \"note\" not in final_report[\"quality_metrics\"]:\n",
    "    print(f\"\\n🎯 品質指標:\")\n",
    "    for key, value in final_report[\"quality_metrics\"].items():\n",
    "        print(f\"  - {key}: {value:.3f}\")\n",
    "\n",
    "# Save final report\n",
    "report_file = pathlib.Path(nb29_paths[\"vectorstore\"]) / \"system_report.json\"\n",
    "try:\n",
    "    with open(report_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_report, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\n💾 完整報告已保存至: {report_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 保存報告失敗: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3111b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Smoke Test & Validation\n",
    "# 煙霧測試與驗證\n",
    "\n",
    "\n",
    "def run_comprehensive_smoke_test():\n",
    "    \"\"\"執行完整的煙霧測試 (Run comprehensive smoke test)\"\"\"\n",
    "    print(f\"\\n🧪 執行系統煙霧測試...\")\n",
    "\n",
    "    tests = {\n",
    "        \"shared_cache_setup\": False,\n",
    "        \"model_loading\": False,\n",
    "        \"embedding_model\": False,\n",
    "        \"vector_index\": False,\n",
    "        \"retrieval_function\": False,\n",
    "        \"generation_function\": False,\n",
    "        \"end_to_end_qa\": False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Test 1: Shared cache setup\n",
    "        tests[\"shared_cache_setup\"] = all(\n",
    "            pathlib.Path(path).exists() for path in nb29_paths.values()\n",
    "        )\n",
    "\n",
    "        # Test 2: Model loading\n",
    "        tests[\"model_loading\"] = llm is not None and llm.model is not None\n",
    "\n",
    "        # Test 3: Embedding model\n",
    "        tests[\"embedding_model\"] = embedding_model is not None\n",
    "\n",
    "        # Test 4: Vector index\n",
    "        tests[\"vector_index\"] = faiss_index is not None and faiss_index.ntotal > 0\n",
    "\n",
    "        # Test 5: Retrieval function\n",
    "        try:\n",
    "            test_retrieval = rag_retriever.retrieve(\"測試\", top_k=1)\n",
    "            tests[\"retrieval_function\"] = len(test_retrieval) > 0\n",
    "        except:\n",
    "            tests[\"retrieval_function\"] = False\n",
    "\n",
    "        # Test 6: Generation function\n",
    "        try:\n",
    "            test_messages = [{\"role\": \"user\", \"content\": \"你好\"}]\n",
    "            test_response = llm.generate(test_messages, max_length=10)\n",
    "            tests[\"generation_function\"] = len(test_response) > 0\n",
    "        except:\n",
    "            tests[\"generation_function\"] = False\n",
    "\n",
    "        # Test 7: End-to-end QA\n",
    "        try:\n",
    "            test_result = rag_retriever.answer(\n",
    "                \"什麼是AI？\", top_k=2, use_reranker=False\n",
    "            )\n",
    "            tests[\"end_to_end_qa\"] = (\n",
    "                \"answer\" in test_result and len(test_result[\"answer\"]) > 0\n",
    "            )\n",
    "        except:\n",
    "            tests[\"end_to_end_qa\"] = False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 測試執行失敗: {e}\")\n",
    "\n",
    "    # Report results\n",
    "    print(f\"\\n📊 煙霧測試結果:\")\n",
    "    for test_name, passed in tests.items():\n",
    "        status = \"✅ PASS\" if passed else \"❌ FAIL\"\n",
    "        print(f\"  - {test_name}: {status}\")\n",
    "\n",
    "    overall_health = sum(tests.values()) / len(tests)\n",
    "    health_status = (\n",
    "        \"🟢 優秀\"\n",
    "        if overall_health >= 0.9\n",
    "        else \"🟡 良好\" if overall_health >= 0.7 else \"🔴 需要改進\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n🏥 系統整體健康度: {overall_health:.1%} {health_status}\")\n",
    "\n",
    "    return tests\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_results = run_comprehensive_smoke_test()\n",
    "\n",
    "# Optimization recommendations\n",
    "print(\n",
    "    f\"\"\"\n",
    "🚀 系統優化建議:\n",
    "===============\n",
    "\n",
    "💡 效能優化:\n",
    "- 使用GGUF量化模型進一步減少記憶體使用\n",
    "- 實現向量檢索結果快取機制\n",
    "- 考慮使用vLLM或TensorRT-LLM加速推理\n",
    "- 批次處理多個查詢提升吞吐量\n",
    "\n",
    "📚 資料品質:\n",
    "- 增加更多領域文檔豐富知識庫\n",
    "- 實現增量文檔更新機制\n",
    "- 優化中文文本切分策略\n",
    "- 建立文檔品質評估流程\n",
    "\n",
    "🔧 功能擴展:\n",
    "- 增加多模態支援（圖片、表格）\n",
    "- 實現對話歷史記憶功能\n",
    "- 添加即時網路搜尋補充\n",
    "- 建立使用者反饋收集機制\n",
    "\n",
    "🛡️ 穩定性提升:\n",
    "- 添加更完善的錯誤處理與重試機制\n",
    "- 實現模型熱切換功能\n",
    "- 建立系統監控與告警\n",
    "- 優化記憶體管理避免洩漏\n",
    "\n",
    "📈 擴展部署:\n",
    "- 容器化部署（Docker）\n",
    "- 微服務架構拆分\n",
    "- 負載均衡與高可用性\n",
    "- API介面標準化\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "✅ 中文開源RAG系統建立完成！\n",
    "\n",
    "🎯 關鍵成就:\n",
    "- 完全基於開源模型的中文RAG系統\n",
    "- 支援DeepSeek/Qwen等主流中文模型\n",
    "- 實現從文檔載入到問答的完整流程\n",
    "- 提供評估指標與效能分析\n",
    "- 建立互動式聊天介面\n",
    "\n",
    "📚 系統能力:\n",
    "- 🤖 中文LLM: {model_id}\n",
    "- 🔍 向量檢索: FAISS + BGE-M3\n",
    "- 📄 文檔處理: {len(documents)} 個文檔，{len(chunks)} 個文本塊\n",
    "- ⚡ 即時問答: 平均 {final_report.get('performance_metrics', {}).get('avg_total_time', 'N/A')} 秒響應\n",
    "- 🌐 Web介面: Gradio互動介面\n",
    "\n",
    "🚀 可立即使用於:\n",
    "- 企業內部知識問答\n",
    "- 技術文檔查詢系統\n",
    "- 教育輔助工具\n",
    "- 研究資料分析\n",
    "\n",
    "💡 完全離線運行，保護資料隱私！\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAdapter:\n",
    "    def __init__(\n",
    "        self, model_id: str, backend: str = \"transformers\", load_in_4bit: bool = True\n",
    "    ):\n",
    "        # 支援多種中文模型格式的統一介面\n",
    "        self._load_model()\n",
    "\n",
    "    def _format_qwen_messages(self, messages: List[Dict]) -> str:\n",
    "        # Qwen專用的ChatML格式\n",
    "        formatted = \"<|im_start|>system\\n你是一個樂於助人的AI助手。<|im_end|>\\n\"\n",
    "        # ... 處理訊息格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文分隔符優先序列\n",
    "CHINESE_SEPARATORS = [\"。\", \"！\", \"？\", \"；\", \"…\", \"\\n\\n\", \"\\n\", \" \"]\n",
    "\n",
    "\n",
    "def create_chinese_text_splitter():\n",
    "    return RecursiveCharacterTextSplitter(\n",
    "        separators=CHINESE_SEPARATORS,\n",
    "        chunk_size=512,  # 中文字符數\n",
    "        chunk_overlap=64,\n",
    "        length_function=len,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce32a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChineseRAGRetriever:\n",
    "    def answer(self, query: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "        # 1. 向量檢索\n",
    "        retrieved = self.retrieve(query, top_k)\n",
    "\n",
    "        # 2. 可選重排序\n",
    "        if self.reranker:\n",
    "            retrieved = self.rerank(query, retrieved)\n",
    "\n",
    "        # 3. 生成答案\n",
    "        return self.generate_answer(query, retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 中文開源RAG系統煙霧測試 ===\n",
    "def run_comprehensive_smoke_test():\n",
    "    tests = {\n",
    "        \"shared_cache_setup\": False,\n",
    "        \"model_loading\": False,\n",
    "        \"embedding_model\": False,\n",
    "        \"vector_index\": False,\n",
    "        \"retrieval_function\": False,\n",
    "        \"generation_function\": False,\n",
    "        \"end_to_end_qa\": False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # 測試共享快取\n",
    "        tests[\"shared_cache_setup\"] = all(\n",
    "            pathlib.Path(path).exists() for path in nb29_paths.values()\n",
    "        )\n",
    "\n",
    "        # 測試模型載入\n",
    "        tests[\"model_loading\"] = llm is not None and llm.model is not None\n",
    "\n",
    "        # 測試端到端問答\n",
    "        test_result = rag_retriever.answer(\"什麼是AI？\", top_k=2)\n",
    "        tests[\"end_to_end_qa\"] = len(test_result[\"answer\"]) > 0\n",
    "\n",
    "        # ... 其他測試\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 測試失敗: {e}\")\n",
    "\n",
    "    overall_health = sum(tests.values()) / len(tests)\n",
    "    print(f\"🏥 系統健康度: {overall_health:.1%}\")\n",
    "    return tests\n",
    "\n",
    "\n",
    "smoke_test_results = run_comprehensive_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d8dcd",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 本章小結\n",
    "\n",
    "### ✅ 完成項目\n",
    "* **完全開源的中文RAG系統** (Fully Open-Source Chinese RAG)：無需任何雲端API，支援DeepSeek/Qwen/ChatGLM等主流中文模型\n",
    "* **中文優化的文本處理** (Chinese-Optimized Text Processing)：專門的中文分詞、繁簡轉換與語義切分策略\n",
    "* **高效能向量檢索** (High-Performance Vector Retrieval)：BGE-M3嵌入 + FAISS索引，支援大規模中文文檔檢索\n",
    "* **多維度評估體系** (Multi-Dimensional Evaluation)：包含檢索品質(Recall@K, MRR)、生成品質(事實性、完整性)與效能指標\n",
    "* **互動式Web介面** (Interactive Web Interface)：Gradio聊天介面支援即時問答與來源追蹤\n",
    "\n",
    "### 🧠 核心概念與原理要點\n",
    "* **開源優先策略** (Open-Source First Strategy)：完全避免對商業API的依賴，確保資料隱私與成本可控\n",
    "* **中文語言特性** (Chinese Language Characteristics)：針對中文標點符號、語義結構與繁簡差異的專門處理\n",
    "* **模型量化技術** (Model Quantization)：使用4-bit量化大幅降低顯存需求，讓7B模型在8GB顯卡上運行\n",
    "* **向量語義檢索** (Vector Semantic Retrieval)：基於BGE-M3的中文語義理解，超越關鍵詞匹配的檢索精度\n",
    "* **端到端評估** (End-to-End Evaluation)：從檢索到生成的完整品質評估，確保系統可靠性\n",
    "\n",
    "### ⚠️ 常見問題與注意事項\n",
    "* **模型相容性**：不同中文模型的提示詞格式差異需要專門適配\n",
    "* **顯存管理**：大型模型載入需要合理的量化策略與記憶體清理\n",
    "* **中文編碼**：確保UTF-8編碼一致性，避免亂碼問題\n",
    "* **檢索精度**：向量檢索可能遺漏關鍵詞精確匹配，需要混合檢索策略\n",
    "* **回答品質**：開源模型的生成品質可能不如商業模型，需要更精細的提示工程\n",
    "\n",
    "### 🚀 下一步優化建議\n",
    "\n",
    "1. **混合檢索架構** (Hybrid Retrieval Architecture)：結合BM25關鍵詞檢索與向量語義檢索，提升檢索召回率\n",
    "2. **模型蒸餾優化** (Model Distillation)：使用教師-學生架構進一步壓縮模型大小\n",
    "3. **增量學習機制** (Incremental Learning)：支援動態添加新文檔而無需重建整個索引\n",
    "4. **多輪對話記憶** (Multi-turn Conversation Memory)：整合對話歷史提升上下文理解\n",
    "5. **領域適應微調** (Domain Adaptation Fine-tuning)：針對特定領域使用LoRA微調提升專業性\n",
    "\n",
    "---\n",
    "\n",
    "**🎯 關鍵成就**: 建立了完全基於開源技術的中文RAG系統，實現從文檔處理到問答生成的端到端流程，無需任何商業API依賴\n",
    "\n",
    "**📚 準備就緒**: 具備了部署生產級中文知識問答系統的完整技術棧，可應用於企業內部知識管理、教育輔助、技術支援等場景\n",
    "\n",
    "**💡 核心價值**: \n",
    "- **隱私保護**: 完全本地運行，企業敏感資料不會外洩\n",
    "- **成本可控**: 僅需一次性硬體投資，無持續API費用\n",
    "- **自主可控**: 基於開源技術，避免供應商鎖定風險\n",
    "- **中文優化**: 專門針對中文語言特性設計，效果優於通用方案\n",
    "\n",
    "這個替代版本完全滿足了您的要求：中文優先、開源LLM優先、嚴格遵守共享模型快取、提供完整的MVP與評估體系。可以與原nb29並存，為不同需求場景提供選擇。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
