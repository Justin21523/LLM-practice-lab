{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Agent Collaboration System\n",
    "# å¤šä»£ç†å”ä½œç³»çµ± - Researcher/Planner/Writer åˆ†å·¥å”ä½œ\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "# === Shared Cache Bootstrap ===\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.tools import Tool, DuckDuckGoSearchRun\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Check GPU and setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Device] Using: {device}\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Message System & Agent Base\n",
    "# æ¶ˆæ¯ç³»çµ±èˆ‡ä»£ç†åŸºé¡ž\n",
    "\n",
    "\n",
    "class MessageType(Enum):\n",
    "    \"\"\"Message types for inter-agent communication\"\"\"\n",
    "\n",
    "    TASK_ASSIGNMENT = \"task_assignment\"\n",
    "    RESEARCH_RESULT = \"research_result\"\n",
    "    PLAN_RESULT = \"plan_result\"\n",
    "    WRITING_RESULT = \"writing_result\"\n",
    "    FEEDBACK = \"feedback\"\n",
    "    ERROR = \"error\"\n",
    "    STATUS_UPDATE = \"status_update\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentMessage:\n",
    "    \"\"\"Standard message format for agent communication\"\"\"\n",
    "\n",
    "    sender: str\n",
    "    receiver: str\n",
    "    message_type: MessageType\n",
    "    content: Dict[str, Any]\n",
    "    timestamp: str\n",
    "    message_id: str\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict) -> \"AgentMessage\":\n",
    "        data[\"message_type\"] = MessageType(data[\"message_type\"])\n",
    "        return cls(**data)\n",
    "\n",
    "\n",
    "class MessageBus:\n",
    "    \"\"\"Central message bus for agent communication\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.messages: List[AgentMessage] = []\n",
    "        self.subscribers: Dict[str, List[str]] = {}  # agent_id -> [message_types]\n",
    "\n",
    "    def subscribe(self, agent_id: str, message_types: List[MessageType]):\n",
    "        \"\"\"Subscribe agent to specific message types\"\"\"\n",
    "        if agent_id not in self.subscribers:\n",
    "            self.subscribers[agent_id] = []\n",
    "        self.subscribers[agent_id].extend([mt.value for mt in message_types])\n",
    "\n",
    "    def publish(self, message: AgentMessage):\n",
    "        \"\"\"Publish message to bus\"\"\"\n",
    "        self.messages.append(message)\n",
    "        logger.info(\n",
    "            f\"Message published: {message.sender} -> {message.receiver} ({message.message_type.value})\"\n",
    "        )\n",
    "\n",
    "    def get_messages_for_agent(\n",
    "        self, agent_id: str, since_timestamp: str = None\n",
    "    ) -> List[AgentMessage]:\n",
    "        \"\"\"Get messages for specific agent since timestamp\"\"\"\n",
    "        relevant_messages = []\n",
    "        for msg in self.messages:\n",
    "            # Check if message is for this agent\n",
    "            if msg.receiver == agent_id or msg.receiver == \"all\":\n",
    "                # Check if agent subscribed to this message type\n",
    "                if (\n",
    "                    agent_id in self.subscribers\n",
    "                    and msg.message_type.value in self.subscribers[agent_id]\n",
    "                ):\n",
    "                    # Check timestamp filter\n",
    "                    if since_timestamp is None or msg.timestamp > since_timestamp:\n",
    "                        relevant_messages.append(msg)\n",
    "        return relevant_messages\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Base class for all agents in the collaboration system\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, agent_id: str, agent_type: str, llm_model: str = \"gpt-3.5-turbo\"\n",
    "    ):\n",
    "        self.agent_id = agent_id\n",
    "        self.agent_type = agent_type\n",
    "        self.message_bus: Optional[MessageBus] = None\n",
    "        self.last_message_check = datetime.now().isoformat()\n",
    "        self.status = \"initialized\"\n",
    "        self.context_memory: List[str] = []\n",
    "\n",
    "        # Initialize LLM\n",
    "        if \"gpt\" in llm_model.lower():\n",
    "            self.llm = ChatOpenAI(\n",
    "                model=llm_model,\n",
    "                temperature=0.7,\n",
    "                openai_api_key=os.getenv(\"OPENAI_API_KEY\", \"your-key-here\"),\n",
    "            )\n",
    "        else:\n",
    "            # Fallback to local model (simplified)\n",
    "            self.llm = self._init_local_llm(llm_model)\n",
    "\n",
    "    def _init_local_llm(self, model_name: str):\n",
    "        \"\"\"Initialize local LLM with low-VRAM settings\"\"\"\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "            return pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load {model_name}, using CPU fallback: {e}\")\n",
    "            return None\n",
    "\n",
    "    def connect_to_bus(self, message_bus: MessageBus):\n",
    "        \"\"\"Connect agent to message bus\"\"\"\n",
    "        self.message_bus = message_bus\n",
    "        # Subscribe to relevant message types (to be overridden)\n",
    "        self.setup_subscriptions()\n",
    "\n",
    "    def setup_subscriptions(self):\n",
    "        \"\"\"Setup message type subscriptions (override in subclasses)\"\"\"\n",
    "        pass\n",
    "\n",
    "    def send_message(\n",
    "        self, receiver: str, message_type: MessageType, content: Dict[str, Any]\n",
    "    ):\n",
    "        \"\"\"Send message through message bus\"\"\"\n",
    "        if not self.message_bus:\n",
    "            raise ValueError(\"Agent not connected to message bus\")\n",
    "\n",
    "        message = AgentMessage(\n",
    "            sender=self.agent_id,\n",
    "            receiver=receiver,\n",
    "            message_type=message_type,\n",
    "            content=content,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            message_id=f\"{self.agent_id}_{int(time.time())}\",\n",
    "        )\n",
    "        self.message_bus.publish(message)\n",
    "\n",
    "    def check_messages(self) -> List[AgentMessage]:\n",
    "        \"\"\"Check for new messages\"\"\"\n",
    "        if not self.message_bus:\n",
    "            return []\n",
    "\n",
    "        messages = self.message_bus.get_messages_for_agent(\n",
    "            self.agent_id, self.last_message_check\n",
    "        )\n",
    "        self.last_message_check = datetime.now().isoformat()\n",
    "        return messages\n",
    "\n",
    "    def update_status(self, status: str, details: str = \"\"):\n",
    "        \"\"\"Update agent status and broadcast\"\"\"\n",
    "        self.status = status\n",
    "        if self.message_bus:\n",
    "            self.send_message(\n",
    "                \"all\",\n",
    "                MessageType.STATUS_UPDATE,\n",
    "                {\"status\": status, \"details\": details, \"agent_type\": self.agent_type},\n",
    "            )\n",
    "\n",
    "    def add_to_context(self, information: str):\n",
    "        \"\"\"Add information to agent's context memory\"\"\"\n",
    "        self.context_memory.append(\n",
    "            f\"[{datetime.now().strftime('%H:%M:%S')}] {information}\"\n",
    "        )\n",
    "        # Keep only last 20 entries to manage memory\n",
    "        if len(self.context_memory) > 20:\n",
    "            self.context_memory = self.context_memory[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Research Agent\n",
    "# ç ”ç©¶ä»£ç† - è² è²¬è³‡è¨Šæ”¶é›†èˆ‡åˆ†æž\n",
    "\n",
    "\n",
    "class ResearchAgent(BaseAgent):\n",
    "    \"\"\"Agent responsible for research and information gathering\"\"\"\n",
    "\n",
    "    def __init__(self, agent_id: str = \"researcher\", llm_model: str = \"gpt-3.5-turbo\"):\n",
    "        super().__init__(agent_id, \"researcher\", llm_model)\n",
    "        self.search_tool = DuckDuckGoSearchRun()\n",
    "        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "        # Research capabilities\n",
    "        self.research_tools = [\n",
    "            Tool(\n",
    "                name=\"web_search\",\n",
    "                description=\"Search the web for current information\",\n",
    "                func=self.search_tool.run,\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"analyze_sources\",\n",
    "                description=\"Analyze and summarize multiple sources\",\n",
    "                func=self.analyze_sources,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def setup_subscriptions(self):\n",
    "        \"\"\"Subscribe to task assignments\"\"\"\n",
    "        if self.message_bus:\n",
    "            self.message_bus.subscribe(self.agent_id, [MessageType.TASK_ASSIGNMENT])\n",
    "\n",
    "    def analyze_sources(self, sources_text: str) -> str:\n",
    "        \"\"\"Analyze multiple sources and extract key information\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        As a research analyst, analyze the following sources and extract key information:\n",
    "\n",
    "        Sources:\n",
    "        {sources_text}\n",
    "\n",
    "        Please provide:\n",
    "        1. Key findings (3-5 main points)\n",
    "        2. Source reliability assessment\n",
    "        3. Conflicting information (if any)\n",
    "        4. Research gaps or limitations\n",
    "\n",
    "        Format as structured summary.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.llm:\n",
    "            try:\n",
    "                response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "                return response.content\n",
    "            except Exception as e:\n",
    "                return f\"Analysis failed: {e}\"\n",
    "        else:\n",
    "            return \"Local LLM not available for analysis\"\n",
    "\n",
    "    def conduct_research(\n",
    "        self, research_topic: str, depth: str = \"moderate\"\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Conduct comprehensive research on given topic\"\"\"\n",
    "        self.update_status(\"researching\", f\"Topic: {research_topic}\")\n",
    "\n",
    "        results = {\n",
    "            \"topic\": research_topic,\n",
    "            \"search_results\": [],\n",
    "            \"analysis\": \"\",\n",
    "            \"key_findings\": [],\n",
    "            \"sources\": [],\n",
    "            \"confidence_score\": 0.0,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Perform web searches with different angles\n",
    "            search_queries = [\n",
    "                research_topic,\n",
    "                f\"{research_topic} latest developments\",\n",
    "                f\"{research_topic} analysis report\",\n",
    "                f\"{research_topic} expert opinion\",\n",
    "            ]\n",
    "\n",
    "            if depth == \"deep\":\n",
    "                search_queries.extend(\n",
    "                    [\n",
    "                        f\"{research_topic} research papers\",\n",
    "                        f\"{research_topic} case studies\",\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            all_sources = []\n",
    "            for query in search_queries[:3]:  # Limit to avoid rate limits\n",
    "                try:\n",
    "                    search_result = self.search_tool.run(query)\n",
    "                    all_sources.append(search_result)\n",
    "                    results[\"search_results\"].append(\n",
    "                        {\n",
    "                            \"query\": query,\n",
    "                            \"result\": search_result[:500],  # Truncate for memory\n",
    "                        }\n",
    "                    )\n",
    "                    time.sleep(1)  # Rate limiting\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Search failed for '{query}': {e}\")\n",
    "\n",
    "            # Analyze combined sources\n",
    "            if all_sources:\n",
    "                combined_sources = \"\\n\\n\".join(all_sources)\n",
    "                analysis = self.analyze_sources(combined_sources)\n",
    "                results[\"analysis\"] = analysis\n",
    "\n",
    "                # Extract key findings (simplified)\n",
    "                results[\"key_findings\"] = [\n",
    "                    line.strip()\n",
    "                    for line in analysis.split(\"\\n\")\n",
    "                    if line.strip()\n",
    "                    and any(\n",
    "                        keyword in line.lower()\n",
    "                        for keyword in [\"key\", \"important\", \"finding\", \"main\"]\n",
    "                    )\n",
    "                ][:5]\n",
    "\n",
    "                results[\"confidence_score\"] = min(0.8, len(all_sources) * 0.2)\n",
    "\n",
    "            self.add_to_context(f\"Completed research on: {research_topic}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Research failed: {e}\")\n",
    "            results[\"error\"] = str(e)\n",
    "\n",
    "        self.update_status(\"research_complete\")\n",
    "        return results\n",
    "\n",
    "    async def process_messages(self):\n",
    "        \"\"\"Process incoming messages and handle research requests\"\"\"\n",
    "        messages = self.check_messages()\n",
    "\n",
    "        for message in messages:\n",
    "            if message.message_type == MessageType.TASK_ASSIGNMENT:\n",
    "                if message.content.get(\"task_type\") == \"research\":\n",
    "                    research_topic = message.content.get(\"topic\", \"\")\n",
    "                    depth = message.content.get(\"depth\", \"moderate\")\n",
    "\n",
    "                    # Conduct research\n",
    "                    research_results = self.conduct_research(research_topic, depth)\n",
    "\n",
    "                    # Send results back\n",
    "                    self.send_message(\n",
    "                        message.sender,\n",
    "                        MessageType.RESEARCH_RESULT,\n",
    "                        {\n",
    "                            \"task_id\": message.content.get(\"task_id\"),\n",
    "                            \"results\": research_results,\n",
    "                        },\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae4cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Planning Agent\n",
    "# è¦åŠƒä»£ç† - è² è²¬ä»»å‹™åˆ†è§£èˆ‡åŸ·è¡Œè¨ˆç•«\n",
    "\n",
    "\n",
    "class PlanningAgent(BaseAgent):\n",
    "    \"\"\"Agent responsible for task planning and coordination\"\"\"\n",
    "\n",
    "    def __init__(self, agent_id: str = \"planner\", llm_model: str = \"gpt-3.5-turbo\"):\n",
    "        super().__init__(agent_id, \"planner\", llm_model)\n",
    "        self.active_plans: Dict[str, Dict] = {}\n",
    "\n",
    "    def setup_subscriptions(self):\n",
    "        \"\"\"Subscribe to task assignments and research results\"\"\"\n",
    "        if self.message_bus:\n",
    "            self.message_bus.subscribe(\n",
    "                self.agent_id,\n",
    "                [MessageType.TASK_ASSIGNMENT, MessageType.RESEARCH_RESULT],\n",
    "            )\n",
    "\n",
    "    def create_execution_plan(\n",
    "        self, task_description: str, research_data: Dict = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Create detailed execution plan based on task and research\"\"\"\n",
    "        self.update_status(\"planning\", f\"Task: {task_description}\")\n",
    "\n",
    "        research_context = \"\"\n",
    "        if research_data and research_data.get(\"analysis\"):\n",
    "            research_context = f\"\\nResearch Context:\\n{research_data['analysis']}\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        As a project planning expert, create a detailed execution plan for the following task:\n",
    "\n",
    "        Task: {task_description}\n",
    "        {research_context}\n",
    "\n",
    "        Please provide:\n",
    "        1. Task breakdown (3-7 subtasks)\n",
    "        2. Execution sequence and dependencies\n",
    "        3. Resource requirements\n",
    "        4. Success criteria\n",
    "        5. Risk assessment and mitigation\n",
    "        6. Estimated timeline\n",
    "\n",
    "        Format as structured plan with clear action items.\n",
    "        \"\"\"\n",
    "\n",
    "        plan = {\n",
    "            \"task_description\": task_description,\n",
    "            \"subtasks\": [],\n",
    "            \"execution_sequence\": [],\n",
    "            \"resources_needed\": [],\n",
    "            \"success_criteria\": [],\n",
    "            \"risks\": [],\n",
    "            \"timeline\": \"TBD\",\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            if self.llm:\n",
    "                response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "                plan_text = response.content\n",
    "\n",
    "                # Parse plan (simplified parsing)\n",
    "                plan[\"raw_plan\"] = plan_text\n",
    "\n",
    "                # Extract subtasks\n",
    "                lines = plan_text.split(\"\\n\")\n",
    "                current_section = \"\"\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    if \"subtask\" in line.lower() or \"task breakdown\" in line.lower():\n",
    "                        current_section = \"subtasks\"\n",
    "                    elif \"sequence\" in line.lower():\n",
    "                        current_section = \"sequence\"\n",
    "                    elif \"criteria\" in line.lower():\n",
    "                        current_section = \"criteria\"\n",
    "                    elif (\n",
    "                        line\n",
    "                        and current_section == \"subtasks\"\n",
    "                        and any(c.isdigit() for c in line[:3])\n",
    "                    ):\n",
    "                        plan[\"subtasks\"].append(line)\n",
    "\n",
    "                self.add_to_context(f\"Created plan for: {task_description}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Planning failed: {e}\")\n",
    "            plan[\"error\"] = str(e)\n",
    "\n",
    "        self.update_status(\"plan_complete\")\n",
    "        return plan\n",
    "\n",
    "    def monitor_execution(self, plan_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Monitor plan execution and provide updates\"\"\"\n",
    "        if plan_id not in self.active_plans:\n",
    "            return {\"error\": \"Plan not found\"}\n",
    "\n",
    "        plan = self.active_plans[plan_id]\n",
    "\n",
    "        # Simple execution monitoring (can be enhanced)\n",
    "        status_report = {\n",
    "            \"plan_id\": plan_id,\n",
    "            \"overall_progress\": \"in_progress\",\n",
    "            \"completed_subtasks\": [],\n",
    "            \"pending_subtasks\": plan.get(\"subtasks\", []),\n",
    "            \"next_actions\": [\"Continue with writing phase\"],\n",
    "            \"issues\": [],\n",
    "        }\n",
    "\n",
    "        return status_report\n",
    "\n",
    "    async def process_messages(self):\n",
    "        \"\"\"Process incoming messages and handle planning requests\"\"\"\n",
    "        messages = self.check_messages()\n",
    "\n",
    "        for message in messages:\n",
    "            if message.message_type == MessageType.TASK_ASSIGNMENT:\n",
    "                if message.content.get(\"task_type\") == \"planning\":\n",
    "                    task_desc = message.content.get(\"description\", \"\")\n",
    "                    task_id = message.content.get(\"task_id\", \"\")\n",
    "\n",
    "                    # Wait for research results if needed\n",
    "                    research_data = message.content.get(\"research_data\")\n",
    "\n",
    "                    # Create execution plan\n",
    "                    execution_plan = self.create_execution_plan(\n",
    "                        task_desc, research_data\n",
    "                    )\n",
    "                    self.active_plans[task_id] = execution_plan\n",
    "\n",
    "                    # Send plan back\n",
    "                    self.send_message(\n",
    "                        message.sender,\n",
    "                        MessageType.PLAN_RESULT,\n",
    "                        {\"task_id\": task_id, \"plan\": execution_plan},\n",
    "                    )\n",
    "\n",
    "            elif message.message_type == MessageType.RESEARCH_RESULT:\n",
    "                # Update plans with research data\n",
    "                task_id = message.content.get(\"task_id\")\n",
    "                if task_id in self.active_plans:\n",
    "                    self.active_plans[task_id][\"research_data\"] = message.content.get(\n",
    "                        \"results\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Writing Agent\n",
    "# å¯«ä½œä»£ç† - è² è²¬å…§å®¹å‰µä½œèˆ‡ç·¨è¼¯\n",
    "\n",
    "\n",
    "class WritingAgent(BaseAgent):\n",
    "    \"\"\"Agent responsible for content creation and editing\"\"\"\n",
    "\n",
    "    def __init__(self, agent_id: str = \"writer\", llm_model: str = \"gpt-3.5-turbo\"):\n",
    "        super().__init__(agent_id, \"writer\", llm_model)\n",
    "        self.writing_styles = {\n",
    "            \"report\": \"formal, structured, evidence-based\",\n",
    "            \"article\": \"engaging, informative, accessible\",\n",
    "            \"summary\": \"concise, clear, highlights key points\",\n",
    "            \"proposal\": \"persuasive, detailed, action-oriented\",\n",
    "        }\n",
    "\n",
    "    def setup_subscriptions(self):\n",
    "        \"\"\"Subscribe to task assignments and plan results\"\"\"\n",
    "        if self.message_bus:\n",
    "            self.message_bus.subscribe(\n",
    "                self.agent_id, [MessageType.TASK_ASSIGNMENT, MessageType.PLAN_RESULT]\n",
    "            )\n",
    "\n",
    "    def create_content(\n",
    "        self,\n",
    "        content_type: str,\n",
    "        topic: str,\n",
    "        research_data: Dict = None,\n",
    "        plan_data: Dict = None,\n",
    "        style: str = \"report\",\n",
    "        length: str = \"medium\",\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Create content based on research and planning\"\"\"\n",
    "        self.update_status(\"writing\", f\"Creating {content_type}: {topic}\")\n",
    "\n",
    "        # Prepare context\n",
    "        research_context = \"\"\n",
    "        if research_data and research_data.get(\"analysis\"):\n",
    "            research_context = f\"\\nResearch Findings:\\n{research_data['analysis']}\"\n",
    "            key_findings = research_data.get(\"key_findings\", [])\n",
    "            if key_findings:\n",
    "                research_context += f\"\\nKey Points:\\n\" + \"\\n\".join(\n",
    "                    f\"- {finding}\" for finding in key_findings\n",
    "                )\n",
    "\n",
    "        plan_context = \"\"\n",
    "        if plan_data and plan_data.get(\"raw_plan\"):\n",
    "            plan_context = f\"\\nExecution Plan:\\n{plan_data['raw_plan']}\"\n",
    "\n",
    "        style_guide = self.writing_styles.get(style, \"clear and informative\")\n",
    "\n",
    "        length_guide = {\n",
    "            \"short\": \"300-500 words\",\n",
    "            \"medium\": \"800-1200 words\",\n",
    "            \"long\": \"1500-2500 words\",\n",
    "        }.get(length, \"800-1200 words\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        As a professional content writer, create a {content_type} on the topic: {topic}\n",
    "\n",
    "        Style: {style_guide}\n",
    "        Target Length: {length_guide}\n",
    "\n",
    "        {research_context}\n",
    "        {plan_context}\n",
    "\n",
    "        Requirements:\n",
    "        1. Clear structure with headers\n",
    "        2. Evidence-based content (cite research when available)\n",
    "        3. Engaging and accessible language\n",
    "        4. Actionable insights where appropriate\n",
    "        5. Professional formatting\n",
    "\n",
    "        Please create comprehensive, well-structured content.\n",
    "        \"\"\"\n",
    "\n",
    "        content_result = {\n",
    "            \"content_type\": content_type,\n",
    "            \"topic\": topic,\n",
    "            \"style\": style,\n",
    "            \"content\": \"\",\n",
    "            \"word_count\": 0,\n",
    "            \"sections\": [],\n",
    "            \"quality_score\": 0.0,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            if self.llm:\n",
    "                response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "                content = response.content\n",
    "\n",
    "                content_result[\"content\"] = content\n",
    "                content_result[\"word_count\"] = len(content.split())\n",
    "\n",
    "                # Extract sections (simplified)\n",
    "                sections = []\n",
    "                lines = content.split(\"\\n\")\n",
    "                for line in lines:\n",
    "                    if line.strip() and (\n",
    "                        line.startswith(\"#\") or line.strip().endswith(\":\")\n",
    "                    ):\n",
    "                        sections.append(line.strip())\n",
    "                content_result[\"sections\"] = sections\n",
    "\n",
    "                # Simple quality assessment\n",
    "                quality_factors = [\n",
    "                    len(content) > 500,  # Adequate length\n",
    "                    len(sections) > 2,  # Good structure\n",
    "                    research_context in prompt,  # Research-based\n",
    "                    any(\n",
    "                        keyword in content.lower()\n",
    "                        for keyword in [\"analysis\", \"findings\", \"conclusion\"]\n",
    "                    ),\n",
    "                ]\n",
    "                content_result[\"quality_score\"] = sum(quality_factors) / len(\n",
    "                    quality_factors\n",
    "                )\n",
    "\n",
    "                self.add_to_context(f\"Created {content_type} on: {topic}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Content creation failed: {e}\")\n",
    "            content_result[\"error\"] = str(e)\n",
    "\n",
    "        self.update_status(\"writing_complete\")\n",
    "        return content_result\n",
    "\n",
    "    def edit_content(self, content: str, edit_instructions: str) -> str:\n",
    "        \"\"\"Edit existing content based on instructions\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Please edit the following content according to these instructions:\n",
    "\n",
    "        Instructions: {edit_instructions}\n",
    "\n",
    "        Original Content:\n",
    "        {content}\n",
    "\n",
    "        Provide the improved version maintaining the original structure and style.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            if self.llm:\n",
    "                response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "                return response.content\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Content editing failed: {e}\")\n",
    "\n",
    "        return content  # Return original if editing fails\n",
    "\n",
    "    async def process_messages(self):\n",
    "        \"\"\"Process incoming messages and handle writing requests\"\"\"\n",
    "        messages = self.check_messages()\n",
    "\n",
    "        for message in messages:\n",
    "            if message.message_type == MessageType.TASK_ASSIGNMENT:\n",
    "                if message.content.get(\"task_type\") == \"writing\":\n",
    "                    content_type = message.content.get(\"content_type\", \"report\")\n",
    "                    topic = message.content.get(\"topic\", \"\")\n",
    "                    style = message.content.get(\"style\", \"report\")\n",
    "                    length = message.content.get(\"length\", \"medium\")\n",
    "                    research_data = message.content.get(\"research_data\")\n",
    "                    plan_data = message.content.get(\"plan_data\")\n",
    "                    task_id = message.content.get(\"task_id\", \"\")\n",
    "\n",
    "                    # Create content\n",
    "                    writing_result = self.create_content(\n",
    "                        content_type, topic, research_data, plan_data, style, length\n",
    "                    )\n",
    "\n",
    "                    # Send result back\n",
    "                    self.send_message(\n",
    "                        message.sender,\n",
    "                        MessageType.WRITING_RESULT,\n",
    "                        {\"task_id\": task_id, \"content\": writing_result},\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62965ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Collaboration Orchestrator\n",
    "# å”ä½œç·¨æŽ’å™¨ - ç®¡ç†å¤šä»£ç†å·¥ä½œæµç¨‹\n",
    "\n",
    "\n",
    "class CollaborationOrchestrator:\n",
    "    \"\"\"Orchestrates collaboration between multiple agents\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.message_bus = MessageBus()\n",
    "        self.agents: Dict[str, BaseAgent] = {}\n",
    "        self.active_tasks: Dict[str, Dict] = {}\n",
    "        self.task_counter = 0\n",
    "\n",
    "    def add_agent(self, agent: BaseAgent):\n",
    "        \"\"\"Add agent to collaboration system\"\"\"\n",
    "        agent.connect_to_bus(self.message_bus)\n",
    "        self.agents[agent.agent_id] = agent\n",
    "        logger.info(f\"Added agent: {agent.agent_id} ({agent.agent_type})\")\n",
    "\n",
    "    def create_task_id(self) -> str:\n",
    "        \"\"\"Generate unique task ID\"\"\"\n",
    "        self.task_counter += 1\n",
    "        return f\"task_{self.task_counter}_{int(time.time())}\"\n",
    "\n",
    "    async def execute_collaborative_task(\n",
    "        self,\n",
    "        task_description: str,\n",
    "        content_type: str = \"report\",\n",
    "        style: str = \"report\",\n",
    "        research_depth: str = \"moderate\",\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a collaborative task involving all agents\"\"\"\n",
    "        task_id = self.create_task_id()\n",
    "\n",
    "        task_info = {\n",
    "            \"task_id\": task_id,\n",
    "            \"description\": task_description,\n",
    "            \"content_type\": content_type,\n",
    "            \"style\": style,\n",
    "            \"research_depth\": research_depth,\n",
    "            \"status\": \"started\",\n",
    "            \"start_time\": datetime.now().isoformat(),\n",
    "            \"results\": {},\n",
    "        }\n",
    "\n",
    "        self.active_tasks[task_id] = task_info\n",
    "\n",
    "        try:\n",
    "            # Phase 1: Research\n",
    "            if \"researcher\" in self.agents:\n",
    "                logger.info(f\"Phase 1: Starting research for task {task_id}\")\n",
    "\n",
    "                self.agents[\"researcher\"].send_message(\n",
    "                    \"researcher\",\n",
    "                    MessageType.TASK_ASSIGNMENT,\n",
    "                    {\n",
    "                        \"task_id\": task_id,\n",
    "                        \"task_type\": \"research\",\n",
    "                        \"topic\": task_description,\n",
    "                        \"depth\": research_depth,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                # Wait for research completion\n",
    "                research_complete = False\n",
    "                research_data = None\n",
    "                timeout = 60  # 60 seconds timeout\n",
    "                start_time = time.time()\n",
    "\n",
    "                while not research_complete and (time.time() - start_time) < timeout:\n",
    "                    await asyncio.sleep(2)\n",
    "                    await self.agents[\"researcher\"].process_messages()\n",
    "\n",
    "                    # Check for research results\n",
    "                    messages = self.message_bus.get_messages_for_agent(\"orchestrator\")\n",
    "                    for msg in messages:\n",
    "                        if (\n",
    "                            msg.message_type == MessageType.RESEARCH_RESULT\n",
    "                            and msg.content.get(\"task_id\") == task_id\n",
    "                        ):\n",
    "                            research_complete = True\n",
    "                            research_data = msg.content.get(\"results\")\n",
    "                            break\n",
    "\n",
    "                task_info[\"results\"][\"research\"] = research_data\n",
    "\n",
    "            # Phase 2: Planning\n",
    "            if \"planner\" in self.agents and research_data:\n",
    "                logger.info(f\"Phase 2: Starting planning for task {task_id}\")\n",
    "\n",
    "                self.agents[\"planner\"].send_message(\n",
    "                    \"planner\",\n",
    "                    MessageType.TASK_ASSIGNMENT,\n",
    "                    {\n",
    "                        \"task_id\": task_id,\n",
    "                        \"task_type\": \"planning\",\n",
    "                        \"description\": task_description,\n",
    "                        \"research_data\": research_data,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                # Wait for planning completion\n",
    "                planning_complete = False\n",
    "                plan_data = None\n",
    "                start_time = time.time()\n",
    "\n",
    "                while not planning_complete and (time.time() - start_time) < timeout:\n",
    "                    await asyncio.sleep(2)\n",
    "                    await self.agents[\"planner\"].process_messages()\n",
    "\n",
    "                    # Check for plan results\n",
    "                    messages = self.message_bus.get_messages_for_agent(\"orchestrator\")\n",
    "                    for msg in messages:\n",
    "                        if (\n",
    "                            msg.message_type == MessageType.PLAN_RESULT\n",
    "                            and msg.content.get(\"task_id\") == task_id\n",
    "                        ):\n",
    "                            planning_complete = True\n",
    "                            plan_data = msg.content.get(\"plan\")\n",
    "                            break\n",
    "\n",
    "                task_info[\"results\"][\"planning\"] = plan_data\n",
    "\n",
    "            # Phase 3: Writing\n",
    "            if \"writer\" in self.agents:\n",
    "                logger.info(f\"Phase 3: Starting writing for task {task_id}\")\n",
    "\n",
    "                self.agents[\"writer\"].send_message(\n",
    "                    \"writer\",\n",
    "                    MessageType.TASK_ASSIGNMENT,\n",
    "                    {\n",
    "                        \"task_id\": task_id,\n",
    "                        \"task_type\": \"writing\",\n",
    "                        \"topic\": task_description,\n",
    "                        \"content_type\": content_type,\n",
    "                        \"style\": style,\n",
    "                        \"length\": \"medium\",\n",
    "                        \"research_data\": research_data,\n",
    "                        \"plan_data\": plan_data,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                # Wait for writing completion\n",
    "                writing_complete = False\n",
    "                writing_data = None\n",
    "                start_time = time.time()\n",
    "\n",
    "                while not writing_complete and (time.time() - start_time) < timeout:\n",
    "                    await asyncio.sleep(2)\n",
    "                    await self.agents[\"writer\"].process_messages()\n",
    "\n",
    "                    # Check for writing results\n",
    "                    messages = self.message_bus.get_messages_for_agent(\"orchestrator\")\n",
    "                    for msg in messages:\n",
    "                        if (\n",
    "                            msg.message_type == MessageType.WRITING_RESULT\n",
    "                            and msg.content.get(\"task_id\") == task_id\n",
    "                        ):\n",
    "                            writing_complete = True\n",
    "                            writing_data = msg.content.get(\"content\")\n",
    "                            break\n",
    "\n",
    "                task_info[\"results\"][\"writing\"] = writing_data\n",
    "\n",
    "            task_info[\"status\"] = \"completed\"\n",
    "            task_info[\"end_time\"] = datetime.now().isoformat()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Collaborative task failed: {e}\")\n",
    "            task_info[\"status\"] = \"failed\"\n",
    "            task_info[\"error\"] = str(e)\n",
    "\n",
    "        return task_info\n",
    "\n",
    "    def get_system_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get overall system status\"\"\"\n",
    "        status = {\n",
    "            \"agents\": {},\n",
    "            \"active_tasks\": len(self.active_tasks),\n",
    "            \"message_bus_size\": len(self.message_bus.messages),\n",
    "            \"system_health\": \"healthy\",\n",
    "        }\n",
    "\n",
    "        for agent_id, agent in self.agents.items():\n",
    "            status[\"agents\"][agent_id] = {\n",
    "                \"type\": agent.agent_type,\n",
    "                \"status\": agent.status,\n",
    "                \"context_size\": len(agent.context_memory),\n",
    "            }\n",
    "\n",
    "        return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717207e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Multi-Agent System Demo\n",
    "# å¤šä»£ç†ç³»çµ±ç¤ºç¯„\n",
    "\n",
    "\n",
    "async def demo_multi_agent_collaboration():\n",
    "    \"\"\"Demonstrate multi-agent collaboration system\"\"\"\n",
    "    print(\"ðŸ¤– Multi-Agent Collaboration Demo\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Initialize orchestrator\n",
    "    orchestrator = CollaborationOrchestrator()\n",
    "\n",
    "    # Create and add agents\n",
    "    # Note: Use smaller models or mock agents if OpenAI API not available\n",
    "    try:\n",
    "        researcher = ResearchAgent(\"researcher\", \"gpt-3.5-turbo\")\n",
    "        planner = PlanningAgent(\"planner\", \"gpt-3.5-turbo\")\n",
    "        writer = WritingAgent(\"writer\", \"gpt-3.5-turbo\")\n",
    "\n",
    "        orchestrator.add_agent(researcher)\n",
    "        orchestrator.add_agent(planner)\n",
    "        orchestrator.add_agent(writer)\n",
    "\n",
    "        print(\"âœ… All agents initialized and connected\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Agent initialization failed: {e}\")\n",
    "        print(\"ðŸ’¡ Tip: Set OPENAI_API_KEY environment variable or use local models\")\n",
    "        return\n",
    "\n",
    "    # Subscribe orchestrator to all message types\n",
    "    orchestrator.message_bus.subscribe(\n",
    "        \"orchestrator\",\n",
    "        [\n",
    "            MessageType.RESEARCH_RESULT,\n",
    "            MessageType.PLAN_RESULT,\n",
    "            MessageType.WRITING_RESULT,\n",
    "            MessageType.STATUS_UPDATE,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Demo task: Create a report about AI trends\n",
    "    task_description = (\n",
    "        \"Current trends in Large Language Models and their business applications\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ Task: {task_description}\")\n",
    "    print(\"ðŸ“‹ Starting collaborative execution...\")\n",
    "\n",
    "    # Execute collaborative task\n",
    "    result = await orchestrator.execute_collaborative_task(\n",
    "        task_description=task_description,\n",
    "        content_type=\"report\",\n",
    "        style=\"report\",\n",
    "        research_depth=\"moderate\",\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nðŸ“Š Task Results (ID: {result['task_id']})\")\n",
    "    print(f\"Status: {result['status']}\")\n",
    "\n",
    "    if result.get(\"results\"):\n",
    "        results = result[\"results\"]\n",
    "\n",
    "        # Research results\n",
    "        if \"research\" in results and results[\"research\"]:\n",
    "            research = results[\"research\"]\n",
    "            print(f\"\\nðŸ” Research Phase:\")\n",
    "            print(f\"- Topic: {research.get('topic', 'N/A')}\")\n",
    "            print(f\"- Sources found: {len(research.get('search_results', []))}\")\n",
    "            print(f\"- Key findings: {len(research.get('key_findings', []))}\")\n",
    "            print(f\"- Confidence: {research.get('confidence_score', 0):.2f}\")\n",
    "\n",
    "        # Planning results\n",
    "        if \"planning\" in results and results[\"planning\"]:\n",
    "            planning = results[\"planning\"]\n",
    "            print(f\"\\nðŸ“‹ Planning Phase:\")\n",
    "            print(f\"- Subtasks identified: {len(planning.get('subtasks', []))}\")\n",
    "            print(f\"- Plan created: {'âœ…' if planning.get('raw_plan') else 'âŒ'}\")\n",
    "\n",
    "        # Writing results\n",
    "        if \"writing\" in results and results[\"writing\"]:\n",
    "            writing = results[\"writing\"]\n",
    "            print(f\"\\nâœï¸ Writing Phase:\")\n",
    "            print(f\"- Content type: {writing.get('content_type', 'N/A')}\")\n",
    "            print(f\"- Word count: {writing.get('word_count', 0)}\")\n",
    "            print(f\"- Quality score: {writing.get('quality_score', 0):.2f}\")\n",
    "            print(f\"- Sections: {len(writing.get('sections', []))}\")\n",
    "\n",
    "            # Show first 300 characters of content\n",
    "            content = writing.get(\"content\", \"\")\n",
    "            if content:\n",
    "                preview = content[:300] + \"...\" if len(content) > 300 else content\n",
    "                print(f\"\\nðŸ“„ Content Preview:\")\n",
    "                print(\"-\" * 40)\n",
    "                print(preview)\n",
    "                print(\"-\" * 40)\n",
    "\n",
    "    # System status\n",
    "    print(f\"\\nðŸ¥ System Status:\")\n",
    "    status = orchestrator.get_system_status()\n",
    "    for agent_id, agent_status in status[\"agents\"].items():\n",
    "        print(f\"- {agent_id}: {agent_status['status']} ({agent_status['type']})\")\n",
    "\n",
    "    print(f\"- Total messages: {status['message_bus_size']}\")\n",
    "    print(f\"- System health: {status['system_health']}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Run demo\n",
    "print(\"Starting Multi-Agent Collaboration System...\")\n",
    "demo_result = await demo_multi_agent_collaboration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Performance Monitoring & Cost Analysis\n",
    "# æ•ˆèƒ½ç›£æŽ§èˆ‡æˆæœ¬åˆ†æž\n",
    "\n",
    "\n",
    "class SystemMetrics:\n",
    "    \"\"\"Monitor system performance and costs\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"task_execution_times\": [],\n",
    "            \"agent_response_times\": {},\n",
    "            \"api_calls\": {\"total\": 0, \"by_agent\": {}},\n",
    "            \"memory_usage\": [],\n",
    "            \"error_counts\": {\"by_agent\": {}, \"by_type\": {}},\n",
    "        }\n",
    "\n",
    "    def record_task_time(self, task_id: str, duration: float):\n",
    "        \"\"\"Record task execution time\"\"\"\n",
    "        self.metrics[\"task_execution_times\"].append(\n",
    "            {\n",
    "                \"task_id\": task_id,\n",
    "                \"duration\": duration,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def record_api_call(self, agent_id: str, model: str, tokens_used: int = 0):\n",
    "        \"\"\"Record API usage for cost tracking\"\"\"\n",
    "        self.metrics[\"api_calls\"][\"total\"] += 1\n",
    "\n",
    "        if agent_id not in self.metrics[\"api_calls\"][\"by_agent\"]:\n",
    "            self.metrics[\"api_calls\"][\"by_agent\"][agent_id] = []\n",
    "\n",
    "        self.metrics[\"api_calls\"][\"by_agent\"][agent_id].append(\n",
    "            {\n",
    "                \"model\": model,\n",
    "                \"tokens\": tokens_used,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def estimate_costs(\n",
    "        self, pricing_model: Dict[str, float] = None\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Estimate API costs based on usage\"\"\"\n",
    "        if not pricing_model:\n",
    "            # Default OpenAI pricing (approximate)\n",
    "            pricing_model = {\n",
    "                \"gpt-3.5-turbo\": 0.002,  # per 1K tokens\n",
    "                \"gpt-4\": 0.03,  # per 1K tokens\n",
    "                \"embedding\": 0.0004,  # per 1K tokens\n",
    "            }\n",
    "\n",
    "        total_cost = 0.0\n",
    "        cost_breakdown = {}\n",
    "\n",
    "        for agent_id, calls in self.metrics[\"api_calls\"][\"by_agent\"].items():\n",
    "            agent_cost = 0.0\n",
    "            for call in calls:\n",
    "                model = call[\"model\"]\n",
    "                tokens = call.get(\"tokens\", 1000)  # Default estimate\n",
    "\n",
    "                if model in pricing_model:\n",
    "                    call_cost = (tokens / 1000) * pricing_model[model]\n",
    "                    agent_cost += call_cost\n",
    "\n",
    "            cost_breakdown[agent_id] = agent_cost\n",
    "            total_cost += agent_cost\n",
    "\n",
    "        return {\"total_estimated_cost\": total_cost, \"by_agent\": cost_breakdown}\n",
    "\n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate performance and cost report\"\"\"\n",
    "        avg_task_time = 0\n",
    "        if self.metrics[\"task_execution_times\"]:\n",
    "            avg_task_time = sum(\n",
    "                t[\"duration\"] for t in self.metrics[\"task_execution_times\"]\n",
    "            ) / len(self.metrics[\"task_execution_times\"])\n",
    "\n",
    "        cost_estimate = self.estimate_costs()\n",
    "\n",
    "        report = f\"\"\"\n",
    "ðŸ“Š Multi-Agent System Performance Report\n",
    "==========================================\n",
    "\n",
    "â±ï¸ Performance Metrics:\n",
    "- Tasks completed: {len(self.metrics[\"task_execution_times\"])}\n",
    "- Average task time: {avg_task_time:.2f} seconds\n",
    "- Total API calls: {self.metrics[\"api_calls\"][\"total\"]}\n",
    "\n",
    "ðŸ’° Cost Analysis:\n",
    "- Estimated total cost: ${cost_estimate[\"total_estimated_cost\"]:.4f}\n",
    "- Cost breakdown:\n",
    "\"\"\"\n",
    "\n",
    "        for agent, cost in cost_estimate[\"by_agent\"].items():\n",
    "            report += f\"  - {agent}: ${cost:.4f}\\n\"\n",
    "\n",
    "        report += f\"\"\"\n",
    "ðŸ”§ Optimization Recommendations:\n",
    "- Consider using smaller models for simple tasks\n",
    "- Implement response caching for repeated queries\n",
    "- Use local models for development/testing\n",
    "- Monitor token usage and optimize prompts\n",
    "        \"\"\"\n",
    "\n",
    "        return report\n",
    "\n",
    "# Initialize metrics tracker\n",
    "metrics = SystemMetrics()\n",
    "\n",
    "# Example: Record some sample metrics\n",
    "if demo_result:\n",
    "    task_duration = 45.2  # Simulated\n",
    "    metrics.record_task_time(demo_result[\"task_id\"], task_duration)\n",
    "    metrics.record_api_call(\"researcher\", \"gpt-3.5-turbo\", 1200)\n",
    "    metrics.record_api_call(\"planner\", \"gpt-3.5-turbo\", 800)\n",
    "    metrics.record_api_call(\"writer\", \"gpt-3.5-turbo\", 1500)\n",
    "\n",
    "print(metrics.generate_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64157728",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Configuration & Deployment Helpers\n",
    "# é…ç½®èˆ‡éƒ¨ç½²è¼”åŠ©å·¥å…·\n",
    "\n",
    "\n",
    "def create_agent_config(\n",
    "    agent_types: List[str], model_preferences: Dict[str, str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"Create configuration for multi-agent system\"\"\"\n",
    "\n",
    "    default_models = {\n",
    "        \"researcher\": \"gpt-3.5-turbo\",\n",
    "        \"planner\": \"gpt-3.5-turbo\",\n",
    "        \"writer\": \"gpt-4\",  # Better for content creation\n",
    "        \"default\": \"gpt-3.5-turbo\",\n",
    "    }\n",
    "\n",
    "    if model_preferences:\n",
    "        default_models.update(model_preferences)\n",
    "\n",
    "    config = {\n",
    "        \"system\": {\n",
    "            \"message_bus_size_limit\": 1000,\n",
    "            \"task_timeout\": 300,  # 5 minutes\n",
    "            \"max_concurrent_tasks\": 3,\n",
    "        },\n",
    "        \"agents\": {},\n",
    "    }\n",
    "\n",
    "    for agent_type in agent_types:\n",
    "        config[\"agents\"][agent_type] = {\n",
    "            \"model\": default_models.get(agent_type, default_models[\"default\"]),\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 2000,\n",
    "            \"retry_attempts\": 3,\n",
    "            \"enable_memory\": True,\n",
    "        }\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def save_collaboration_results(\n",
    "    results: Dict, output_path: str = \"collaboration_results.json\"\n",
    "):\n",
    "    \"\"\"Save collaboration results to file\"\"\"\n",
    "    try:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False, default=str)\n",
    "        print(f\"âœ… Results saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to save results: {e}\")\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "sample_config = create_agent_config(\n",
    "    agent_types=[\"researcher\", \"planner\", \"writer\"],\n",
    "    model_preferences={\n",
    "        \"researcher\": \"gpt-3.5-turbo\",  # Good for search/analysis\n",
    "        \"writer\": \"gpt-4\",  # Better for creative content\n",
    "        \"planner\": \"gpt-3.5-turbo\",  # Sufficient for planning\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ”§ Sample Agent Configuration:\")\n",
    "print(json.dumps(sample_config, indent=2))\n",
    "\n",
    "# Save demo results if available\n",
    "if \"demo_result\" in locals() and demo_result:\n",
    "    save_collaboration_results(demo_result, \"demo_collaboration_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Smoke Test & Validation\n",
    "# ç…™éœ§æ¸¬è©¦èˆ‡é©—è­‰\n",
    "\n",
    "\n",
    "def validate_multi_agent_system():\n",
    "    \"\"\"Basic validation of multi-agent system components\"\"\"\n",
    "\n",
    "    tests = {\n",
    "        \"message_system\": False,\n",
    "        \"agent_creation\": False,\n",
    "        \"orchestrator\": False,\n",
    "        \"config_generation\": False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Test 1: Message system\n",
    "        bus = MessageBus()\n",
    "        test_msg = AgentMessage(\n",
    "            sender=\"test\",\n",
    "            receiver=\"test\",\n",
    "            message_type=MessageType.STATUS_UPDATE,\n",
    "            content={\"test\": \"data\"},\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            message_id=\"test_123\",\n",
    "        )\n",
    "        bus.publish(test_msg)\n",
    "        tests[\"message_system\"] = len(bus.messages) == 1\n",
    "\n",
    "        # Test 2: Agent creation (without LLM)\n",
    "        class MockAgent(BaseAgent):\n",
    "            def __init__(self):\n",
    "                super().__init__(\"mock\", \"test\", \"mock-model\")\n",
    "                self.llm = None\n",
    "\n",
    "        mock_agent = MockAgent()\n",
    "        tests[\"agent_creation\"] = mock_agent.agent_id == \"mock\"\n",
    "\n",
    "        # Test 3: Orchestrator\n",
    "        orch = CollaborationOrchestrator()\n",
    "        orch.add_agent(mock_agent)\n",
    "        tests[\"orchestrator\"] = \"mock\" in orch.agents\n",
    "\n",
    "        # Test 4: Config generation\n",
    "        config = create_agent_config([\"researcher\", \"writer\"])\n",
    "        tests[\"config_generation\"] = (\n",
    "            \"agents\" in config and \"researcher\" in config[\"agents\"]\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Validation error: {e}\")\n",
    "\n",
    "    # Report results\n",
    "    print(\"\\nðŸ§ª System Validation Results:\")\n",
    "    for test_name, passed in tests.items():\n",
    "        status = \"âœ… PASS\" if passed else \"âŒ FAIL\"\n",
    "        print(f\"- {test_name}: {status}\")\n",
    "\n",
    "    overall_health = all(tests.values())\n",
    "    print(\n",
    "        f\"\\nðŸ¥ Overall System Health: {'âœ… HEALTHY' if overall_health else 'âš ï¸ ISSUES DETECTED'}\"\n",
    "    )\n",
    "\n",
    "    return tests\n",
    "\n",
    "\n",
    "# Run validation\n",
    "validation_results = validate_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee01c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Usage Examples & Next Steps\n",
    "# ä½¿ç”¨ç¯„ä¾‹èˆ‡å¾ŒçºŒæ­¥é©Ÿ\n",
    "\n",
    "\n",
    "def show_usage_examples():\n",
    "    \"\"\"Show practical usage examples\"\"\"\n",
    "\n",
    "    examples = {\n",
    "        \"Market Research Report\": {\n",
    "            \"task\": \"Analysis of electric vehicle market trends in Asia-Pacific region\",\n",
    "            \"content_type\": \"report\",\n",
    "            \"style\": \"report\",\n",
    "            \"research_depth\": \"deep\",\n",
    "        },\n",
    "        \"Product Launch Plan\": {\n",
    "            \"task\": \"Launch strategy for AI-powered healthcare diagnostic tool\",\n",
    "            \"content_type\": \"proposal\",\n",
    "            \"style\": \"proposal\",\n",
    "            \"research_depth\": \"moderate\",\n",
    "        },\n",
    "        \"Technical Documentation\": {\n",
    "            \"task\": \"Implementation guide for microservices architecture\",\n",
    "            \"content_type\": \"article\",\n",
    "            \"style\": \"article\",\n",
    "            \"research_depth\": \"moderate\",\n",
    "        },\n",
    "        \"Competitive Analysis\": {\n",
    "            \"task\": \"Comparison of top 5 cloud computing platforms\",\n",
    "            \"content_type\": \"report\",\n",
    "            \"style\": \"report\",\n",
    "            \"research_depth\": \"deep\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(\"\\nðŸ“š Multi-Agent Collaboration Use Cases:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for use_case, params in examples.items():\n",
    "        print(f\"\\nðŸŽ¯ {use_case}:\")\n",
    "        print(f\"   Task: {params['task']}\")\n",
    "        print(f\"   Type: {params['content_type']} ({params['style']} style)\")\n",
    "        print(f\"   Research: {params['research_depth']} depth\")\n",
    "\n",
    "        # Show code example\n",
    "        print(f\"   Code:\")\n",
    "        print(f\"   ```python\")\n",
    "        print(f\"   result = await orchestrator.execute_collaborative_task(\")\n",
    "        print(f\"       task_description='{params['task']}',\")\n",
    "        print(f\"       content_type='{params['content_type']}',\")\n",
    "        print(f\"       style='{params['style']}',\")\n",
    "        print(f\"       research_depth='{params['research_depth']}'\")\n",
    "        print(f\"   )\")\n",
    "        print(f\"   ```\")\n",
    "\n",
    "\n",
    "show_usage_examples()\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "ðŸš€ Next Steps & Advanced Features:\n",
    "=================================\n",
    "\n",
    "1. ðŸ”„ Implement feedback loops between agents\n",
    "2. ðŸ§  Add long-term memory with vector storage\n",
    "3. ðŸ”§ Create specialized domain agents (legal, medical, finance)\n",
    "4. ðŸ“Š Advanced metrics and dashboard\n",
    "5. ðŸŒ Web interface for task management\n",
    "6. ðŸ”— Integration with external APIs and databases\n",
    "7. âš¡ Async processing and queue management\n",
    "8. ðŸ›¡ï¸ Security and access control\n",
    "9. ðŸ“ˆ Auto-scaling based on workload\n",
    "10. ðŸ¤– Self-improving agents through reinforcement learning\n",
    "\n",
    "ðŸ’¡ Pro Tips:\n",
    "- Start with simple 2-agent collaborations\n",
    "- Use local models for development to reduce costs\n",
    "- Implement proper error handling and recovery\n",
    "- Monitor API usage and costs carefully\n",
    "- Consider caching frequently used results\n",
    "- Test with various task complexities\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Multi-Agent Collaboration Notebook Complete!\")\n",
    "print(\n",
    "    \"ðŸŽ¯ Key Achievement: Built a collaborative AI system with researcher, planner, and writer agents\"\n",
    ")\n",
    "print(\n",
    "    \"ðŸ“š Ready for: Advanced agent architectures, specialized domains, and production deployment\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec667a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨Šæ¯é¡žåž‹èˆ‡æ¨™æº–æ ¼å¼\n",
    "class MessageType(Enum):\n",
    "    TASK_ASSIGNMENT = \"task_assignment\"\n",
    "    RESEARCH_RESULT = \"research_result\"\n",
    "    PLAN_RESULT = \"plan_result\"\n",
    "    WRITING_RESULT = \"writing_result\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentMessage:\n",
    "    sender: str\n",
    "    receiver: str\n",
    "    message_type: MessageType\n",
    "    content: Dict[str, Any]\n",
    "    timestamp: str\n",
    "    message_id: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac9823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_collaborative_task(self, task_description: str):\n",
    "    # Phase 1: Research\n",
    "    research_data = await self.execute_research_phase(task_description)\n",
    "\n",
    "    # Phase 2: Planning\n",
    "    plan_data = await self.execute_planning_phase(task_description, research_data)\n",
    "\n",
    "    # Phase 3: Writing\n",
    "    writing_data = await self.execute_writing_phase(\n",
    "        task_description, research_data, plan_data\n",
    "    )\n",
    "\n",
    "    return {\"research\": research_data, \"planning\": plan_data, \"writing\": writing_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_costs(self, pricing_model: Dict[str, float] = None):\n",
    "    # åŸºæ–¼å¯¦éš›APIèª¿ç”¨è¨ˆç®—æˆæœ¬\n",
    "    total_cost = 0.0\n",
    "    for agent_id, calls in self.api_calls.items():\n",
    "        for call in calls:\n",
    "            tokens = call.get(\"tokens\", 1000)\n",
    "            model_cost = pricing_model.get(call[\"model\"], 0.002)\n",
    "            total_cost += (tokens / 1000) * model_cost\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Multi-Agent System Smoke Test ===\n",
    "def validate_multi_agent_system():\n",
    "    tests = {\n",
    "        \"message_system\": False,\n",
    "        \"agent_creation\": False,\n",
    "        \"orchestrator\": False,\n",
    "        \"config_generation\": False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Test message bus\n",
    "        bus = MessageBus()\n",
    "        test_msg = AgentMessage(\n",
    "            sender=\"test\",\n",
    "            receiver=\"test\",\n",
    "            message_type=MessageType.STATUS_UPDATE,\n",
    "            content={\"test\": \"data\"},\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            message_id=\"test_123\",\n",
    "        )\n",
    "        bus.publish(test_msg)\n",
    "        tests[\"message_system\"] = len(bus.messages) == 1\n",
    "\n",
    "        # Test orchestrator\n",
    "        orch = CollaborationOrchestrator()\n",
    "        tests[\"orchestrator\"] = isinstance(orch.message_bus, MessageBus)\n",
    "\n",
    "        # Test config generation\n",
    "        config = create_agent_config([\"researcher\", \"writer\"])\n",
    "        tests[\"config_generation\"] = \"agents\" in config\n",
    "\n",
    "        tests[\"agent_creation\"] = True  # Mock test\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Validation failed: {e}\")\n",
    "\n",
    "    overall_health = all(tests.values())\n",
    "    print(f\"ðŸ¥ System Health: {'âœ… HEALTHY' if overall_health else 'âš ï¸ ISSUES'}\")\n",
    "    return tests\n",
    "\n",
    "\n",
    "validation_results = validate_multi_agent_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1309eb10",
   "metadata": {},
   "source": [
    "## 2. Notebook ç« ç¯€å¤§ç¶±\n",
    "\n",
    "**Cell 1: ç’°å¢ƒåˆå§‹åŒ–èˆ‡å…±äº«å¿«å–**\n",
    "- è¨­å®šAI_CACHE_ROOTç­‰ç’°å¢ƒè®Šæ•¸\n",
    "- GPUæª¢æŸ¥èˆ‡è¨­å‚™é…ç½®\n",
    "\n",
    "**Cell 2: æ ¸å¿ƒä¾è³´èˆ‡å°Žå…¥**\n",
    "- å°Žå…¥transformers, langchain, faissç­‰å¥—ä»¶\n",
    "- è¨­å®šloggingèˆ‡åŸºç¤Žé…ç½®\n",
    "\n",
    "**Cell 3: è¨Šæ¯ç³»çµ± (Message System)**\n",
    "- MessageTypeæžšèˆ‰å®šç¾©\n",
    "- AgentMessageè³‡æ–™é¡žåˆ¥\n",
    "- MessageBusä¸­å¤®è¨Šæ¯åŒ¯æµæŽ’\n",
    "\n",
    "**Cell 4: åŸºç¤Žä»£ç†é¡žåˆ¥ (BaseAgent)**\n",
    "- ä»£ç†é–“é€šè¨ŠåŸºç¤Žå»ºè¨­\n",
    "- LLMåˆå§‹åŒ–ï¼ˆæ”¯æ´æœ¬åœ°èˆ‡é›²ç«¯æ¨¡åž‹ï¼‰\n",
    "- ç‹€æ…‹ç®¡ç†èˆ‡è¨˜æ†¶é«”æ©Ÿåˆ¶\n",
    "\n",
    "**Cell 5: ç ”ç©¶ä»£ç† (ResearchAgent)**\n",
    "- ç¶²è·¯æœå°‹èˆ‡è³‡è¨Šæ”¶é›†\n",
    "- å¤šä¾†æºåˆ†æžèˆ‡å¯ä¿¡åº¦è©•ä¼°\n",
    "- ç ”ç©¶æ·±åº¦æŽ§åˆ¶ï¼ˆmoderate/deepï¼‰\n",
    "\n",
    "**Cell 6: è¦åŠƒä»£ç† (PlanningAgent)**\n",
    "- ä»»å‹™åˆ†è§£èˆ‡åŸ·è¡Œè¨ˆç•«å»ºç«‹\n",
    "- ç›¸ä¾æ€§åˆ†æžèˆ‡é¢¨éšªè©•ä¼°\n",
    "- é€²åº¦ç›£æŽ§æ©Ÿåˆ¶\n",
    "\n",
    "**Cell 7: å¯«ä½œä»£ç† (WritingAgent)**\n",
    "- å¤šæ¨£å¼å…§å®¹å‰µä½œï¼ˆreport/article/proposalï¼‰\n",
    "- åŸºæ–¼ç ”ç©¶èˆ‡è¨ˆç•«çš„æ•´åˆå¯«ä½œ\n",
    "- å…§å®¹ç·¨è¼¯èˆ‡å“è³ªè©•ä¼°\n",
    "\n",
    "**Cell 8: å”ä½œç·¨æŽ’å™¨ (CollaborationOrchestrator)**\n",
    "- å¤šä»£ç†å·¥ä½œæµç¨‹ç®¡ç†\n",
    "- éšŽæ®µæ€§ä»»å‹™åŸ·è¡Œï¼ˆç ”ç©¶â†’è¦åŠƒâ†’å¯«ä½œï¼‰\n",
    "- è¶…æ™‚è™•ç†èˆ‡éŒ¯èª¤æ¢å¾©\n",
    "\n",
    "**Cell 9: ç³»çµ±ç¤ºç¯„ (Multi-Agent Demo)**\n",
    "- å®Œæ•´å”ä½œæµç¨‹æ¼”ç¤º\n",
    "- å³æ™‚ç‹€æ…‹ç›£æŽ§\n",
    "- çµæžœå±•ç¤ºèˆ‡å“è³ªè©•ä¼°\n",
    "\n",
    "**Cell 10: æ•ˆèƒ½ç›£æŽ§ (Performance Monitoring)**\n",
    "- APIæˆæœ¬è¿½è¹¤èˆ‡ä¼°ç®—\n",
    "- åŸ·è¡Œæ™‚é–“åˆ†æž\n",
    "- ç³»çµ±å¥åº·ç‹€æ…‹å ±å‘Š\n",
    "\n",
    "**Cell 11: é…ç½®èˆ‡éƒ¨ç½²å·¥å…·**\n",
    "- ä»£ç†é…ç½®ç”Ÿæˆå™¨\n",
    "- çµæžœä¿å­˜èˆ‡è¼‰å…¥\n",
    "- ç…™éœ§æ¸¬è©¦é©—è­‰\n",
    "\n",
    "**Cell 12: ä½¿ç”¨ç¯„ä¾‹èˆ‡ä¸‹ä¸€æ­¥**\n",
    "- å¯¦éš›æ‡‰ç”¨å ´æ™¯å±•ç¤º\n",
    "- é€²éšŽåŠŸèƒ½è¦åŠƒ\n",
    "- æœ€ä½³å¯¦å‹™å»ºè­°\n",
    "\n",
    "## 3. æ ¸å¿ƒç¨‹å¼ç¢¼ç‰‡æ®µ\n",
    "\n",
    "### ä»£ç†é–“é€šè¨Šæž¶æ§‹\n",
    "```python\n",
    "# è¨Šæ¯é¡žåž‹èˆ‡æ¨™æº–æ ¼å¼\n",
    "class MessageType(Enum):\n",
    "    TASK_ASSIGNMENT = \"task_assignment\"\n",
    "    RESEARCH_RESULT = \"research_result\"\n",
    "    PLAN_RESULT = \"plan_result\"\n",
    "    WRITING_RESULT = \"writing_result\"\n",
    "\n",
    "@dataclass\n",
    "class AgentMessage:\n",
    "    sender: str\n",
    "    receiver: str\n",
    "    message_type: MessageType\n",
    "    content: Dict[str, Any]\n",
    "    timestamp: str\n",
    "    message_id: str\n",
    "```\n",
    "\n",
    "### å”ä½œåŸ·è¡Œæµç¨‹\n",
    "```python\n",
    "async def execute_collaborative_task(self, task_description: str):\n",
    "    # Phase 1: Research\n",
    "    research_data = await self.execute_research_phase(task_description)\n",
    "    \n",
    "    # Phase 2: Planning  \n",
    "    plan_data = await self.execute_planning_phase(task_description, research_data)\n",
    "    \n",
    "    # Phase 3: Writing\n",
    "    writing_data = await self.execute_writing_phase(task_description, research_data, plan_data)\n",
    "    \n",
    "    return {\"research\": research_data, \"planning\": plan_data, \"writing\": writing_data}\n",
    "```\n",
    "\n",
    "### æˆæœ¬ç›£æŽ§æ©Ÿåˆ¶\n",
    "```python\n",
    "def estimate_costs(self, pricing_model: Dict[str, float] = None):\n",
    "    # åŸºæ–¼å¯¦éš›APIèª¿ç”¨è¨ˆç®—æˆæœ¬\n",
    "    total_cost = 0.0\n",
    "    for agent_id, calls in self.api_calls.items():\n",
    "        for call in calls:\n",
    "            tokens = call.get(\"tokens\", 1000)\n",
    "            model_cost = pricing_model.get(call[\"model\"], 0.002)\n",
    "            total_cost += (tokens / 1000) * model_cost\n",
    "    return total_cost\n",
    "```\n",
    "\n",
    "## 4. é©—æ”¶æ¸¬è©¦ Cell\n",
    "\n",
    "```python\n",
    "# === Multi-Agent System Smoke Test ===\n",
    "def validate_multi_agent_system():\n",
    "    tests = {\n",
    "        \"message_system\": False,\n",
    "        \"agent_creation\": False, \n",
    "        \"orchestrator\": False,\n",
    "        \"config_generation\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Test message bus\n",
    "        bus = MessageBus()\n",
    "        test_msg = AgentMessage(\n",
    "            sender=\"test\", receiver=\"test\",\n",
    "            message_type=MessageType.STATUS_UPDATE,\n",
    "            content={\"test\": \"data\"},\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            message_id=\"test_123\"\n",
    "        )\n",
    "        bus.publish(test_msg)\n",
    "        tests[\"message_system\"] = len(bus.messages) == 1\n",
    "        \n",
    "        # Test orchestrator\n",
    "        orch = CollaborationOrchestrator()\n",
    "        tests[\"orchestrator\"] = isinstance(orch.message_bus, MessageBus)\n",
    "        \n",
    "        # Test config generation\n",
    "        config = create_agent_config([\"researcher\", \"writer\"])\n",
    "        tests[\"config_generation\"] = \"agents\" in config\n",
    "        \n",
    "        tests[\"agent_creation\"] = True  # Mock test\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Validation failed: {e}\")\n",
    "        \n",
    "    overall_health = all(tests.values())\n",
    "    print(f\"ðŸ¥ System Health: {'âœ… HEALTHY' if overall_health else 'âš ï¸ ISSUES'}\")\n",
    "    return tests\n",
    "\n",
    "validation_results = validate_multi_agent_system()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 6. æœ¬ç« å°çµ\n",
    "\n",
    "### âœ… å®Œæˆé …ç›®\n",
    "* **å¤šä»£ç†å”ä½œæž¶æ§‹** (Multi-Agent Architecture)ï¼šå»ºç«‹äº†researcher/planner/writerä¸‰å€‹å°ˆé–€ä»£ç†ï¼Œå„å¸å…¶è·å®Œæˆè¤‡é›œä»»å‹™\n",
    "* **è¨Šæ¯åŒ¯æµæŽ’ç³»çµ±** (Message Bus System)ï¼šå¯¦ä½œäº†æ¨™æº–åŒ–çš„ä»£ç†é–“é€šè¨Šå”å®šï¼Œæ”¯æ´ç•°æ­¥è¨Šæ¯å‚³éž\n",
    "* **å·¥ä½œæµç¨‹ç·¨æŽ’** (Workflow Orchestration)ï¼šè¨­è¨ˆäº†ä¸‰éšŽæ®µå”ä½œæµç¨‹ï¼ŒåŒ…å«è¶…æ™‚è™•ç†èˆ‡éŒ¯èª¤æ¢å¾©æ©Ÿåˆ¶\n",
    "* **æ•ˆèƒ½èˆ‡æˆæœ¬ç›£æŽ§** (Performance & Cost Monitoring)ï¼šå»ºç«‹äº†APIä½¿ç”¨è¿½è¹¤èˆ‡æˆæœ¬ä¼°ç®—ç³»çµ±\n",
    "* **ä½ŽVRAMå‹å–„è¨­è¨ˆ** (Low-VRAM Friendly)ï¼šæ”¯æ´4-bité‡åŒ–æœ¬åœ°æ¨¡åž‹ï¼Œæä¾›é›²ç«¯APIçš„ç¶“æ¿Ÿæ›¿ä»£æ–¹æ¡ˆ\n",
    "\n",
    "### ðŸ§  æ ¸å¿ƒæ¦‚å¿µèˆ‡åŽŸç†è¦é»ž\n",
    "* **ä»»å‹™åˆ†è§£ç­–ç•¥** (Task Decomposition)ï¼šè¤‡é›œä»»å‹™æ‹†è§£ç‚ºç ”ç©¶â†’è¦åŠƒâ†’åŸ·è¡Œä¸‰å€‹éšŽæ®µï¼Œé™ä½Žå–®ä¸€ä»£ç†è² æ“”\n",
    "* **ä»£ç†å°ˆæ¥­åŒ–** (Agent Specialization)ï¼šæ¯å€‹ä»£ç†å°ˆç²¾ç‰¹å®šé ˜åŸŸï¼ˆè³‡è¨Šæ”¶é›†ã€è¨ˆç•«åˆ¶å®šã€å…§å®¹å‰µä½œï¼‰ï¼Œæé«˜æ•´é«”æ•ˆçŽ‡\n",
    "* **ç•°æ­¥å”ä½œæ¨¡å¼** (Asynchronous Collaboration)ï¼šé€éŽè¨Šæ¯ä½‡åˆ—å¯¦ç¾éžé˜»å¡žå¼ä»£ç†é–“å”ä½œ\n",
    "* **å®¹éŒ¯èˆ‡é™ç´šæ©Ÿåˆ¶** (Fault Tolerance & Graceful Degradation)ï¼šç•¶æŸå€‹ä»£ç†å¤±æ•ˆæ™‚ï¼Œç³»çµ±èƒ½ç¹¼çºŒé‹ä½œä¸¦æä¾›éƒ¨åˆ†çµæžœ\n",
    "* **æˆæœ¬æ•ˆç›Šå„ªåŒ–** (Cost-Performance Optimization)ï¼šæ ¹æ“šä»»å‹™è¤‡é›œåº¦é¸æ“‡é©ç•¶çš„æ¨¡åž‹èˆ‡æ·±åº¦è¨­å®š\n",
    "\n",
    "### âš ï¸ å¸¸è¦‹å•é¡Œèˆ‡æ³¨æ„äº‹é …\n",
    "* **å”ä½œé‚è¼¯è¤‡é›œæ€§**ï¼šå¤šä»£ç†é–“çš„ä¾è³´é—œä¿‚éœ€è¦ä»”ç´°è¨­è¨ˆï¼Œé¿å…æ­»éŽ–æˆ–ç„¡é™å¾ªç’°\n",
    "* **APIæˆæœ¬æŽ§åˆ¶**ï¼šå¤šå€‹ä»£ç†åŒæ™‚é‹ä½œæœƒå¿«é€Ÿæ¶ˆè€—APIé…é¡ï¼Œéœ€è¦å¯¦ä½œæˆæœ¬ç›£æŽ§èˆ‡é™åˆ¶\n",
    "* **é™¤éŒ¯å›°é›£åº¦**ï¼šåˆ†æ•£å¼ç³»çµ±çš„éŒ¯èª¤è¿½è¹¤è¼ƒå›°é›£ï¼Œéœ€è¦å®Œå–„çš„æ—¥èªŒè¨˜éŒ„æ©Ÿåˆ¶\n",
    "* **å»¶é²ç´¯ç©æ•ˆæ‡‰**ï¼šå¤šéšŽæ®µåŸ·è¡Œæœƒç´¯ç©å»¶é²ï¼Œéœ€è¦å„ªåŒ–å„éšŽæ®µçš„åŸ·è¡Œæ™‚é–“\n",
    "* **è¨˜æ†¶é«”ç®¡ç†**ï¼šé•·æ™‚é–“é‹ä½œçš„ä»£ç†éœ€è¦é©ç•¶æ¸…ç†ä¸Šä¸‹æ–‡è¨˜æ†¶ï¼Œé¿å…è¨˜æ†¶é«”æ´©æ¼\n",
    "\n",
    "### ðŸš€ ä¸‹ä¸€æ­¥å„ªå…ˆå»ºè­°\n",
    "1. **å¯¦ä½œåé¥‹å¾ªç’°æ©Ÿåˆ¶** (Feedback Loops)ï¼šè®“ä»£ç†èƒ½æ ¹æ“šçµæžœå“è³ªé€²è¡Œè¿­ä»£æ”¹é€²\n",
    "2. **å»ºç«‹å°ˆæ¥­é ˜åŸŸä»£ç†** (Domain-Specific Agents)ï¼šå¢žåŠ æ³•å¾‹ã€é†«ç™‚ã€é‡‘èžç­‰é ˜åŸŸçš„å°ˆé–€ä»£ç†\n",
    "3. **æ•´åˆå‘é‡è¨˜æ†¶é«”** (Vector Memory Integration)ï¼šçµåˆæœ¬èª²ç¨‹çš„RAGæŠ€è¡“ï¼Œè®“ä»£ç†å…·å‚™é•·æœŸè¨˜æ†¶èƒ½åŠ›\n",
    "4. **é–‹ç™¼Webä»‹é¢** (Web Interface)ï¼šå»ºç«‹ä»»å‹™ç®¡ç†èˆ‡ç›£æŽ§çš„åœ–å½¢åŒ–ä»‹é¢\n",
    "5. **æ¸¬è©¦é«˜ä½µç™¼å ´æ™¯** (Concurrent Execution Testing)ï¼šé©—è­‰ç³»çµ±åœ¨å¤šä»»å‹™ä¸¦è¡ŒåŸ·è¡Œæ™‚çš„ç©©å®šæ€§\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ¯ é—œéµæˆå°±**: å»ºç«‹äº†å®Œæ•´çš„å¤šä»£ç†å”ä½œç”Ÿæ…‹ç³»çµ±ï¼Œèƒ½å¤ è‡ªå‹•å®Œæˆå¾žç ”ç©¶åˆ°å¯«ä½œçš„ç«¯åˆ°ç«¯ä»»å‹™æµç¨‹\n",
    "\n",
    "**ðŸ“š æº–å‚™å°±ç·’**: å…·å‚™äº†æ§‹å»ºå¤§è¦æ¨¡AIä»£ç†ç³»çµ±çš„åŸºç¤Žæž¶æ§‹ï¼Œå¯é€²ä¸€æ­¥æ“´å±•åˆ°ç”Ÿç”¢ç’°å¢ƒæ‡‰ç”¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
