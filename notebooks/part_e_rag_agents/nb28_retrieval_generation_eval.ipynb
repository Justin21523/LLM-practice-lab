{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === E3: Retrieval-Generation Evaluation System ===\n",
    "# 檢索生成分離評估系統 (Decoupled RAG Evaluation)\n",
    "\n",
    "# Cell 1: Environment Setup and Shared Cache\n",
    "import os, pathlib, torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Shared cache bootstrap\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "try:\n",
    "    import evaluate\n",
    "    import bert_score\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import faiss\n",
    "\n",
    "    print(\"[Packages] All evaluation packages loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"[Error] Missing package: {e}\")\n",
    "    print(\"Run: pip install evaluate bert-score sentence-transformers faiss-cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31992f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 2: Evaluation Dataset Preparation\n",
    "# 評估資料集準備\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "\n",
    "class EvaluationDatasetManager:\n",
    "    \"\"\"Manages evaluation datasets for RAG systems\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.eval_data = None\n",
    "        self.documents = None\n",
    "\n",
    "    def create_synthetic_qa_dataset(self, size: int = 50) -> Dataset:\n",
    "        \"\"\"Create synthetic QA dataset for evaluation\"\"\"\n",
    "\n",
    "        # Sample documents (technology domain)\n",
    "        documents = [\n",
    "            \"Python is a high-level programming language known for its simplicity and readability. It was created by Guido van Rossum in 1991.\",\n",
    "            \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed.\",\n",
    "            \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\",\n",
    "            \"Natural language processing (NLP) is a field of AI that focuses on the interaction between computers and human language.\",\n",
    "            \"Computer vision is a field of AI that trains computers to interpret and understand visual information from the world.\",\n",
    "            \"Transformers are a type of neural network architecture that has revolutionized natural language processing tasks.\",\n",
    "            \"BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google.\",\n",
    "            \"GPT (Generative Pre-trained Transformer) is an autoregressive language model that generates human-like text.\",\n",
    "            \"Retrieval-Augmented Generation (RAG) combines information retrieval with text generation for better factual accuracy.\",\n",
    "            \"Vector databases store high-dimensional vectors and enable efficient similarity search for AI applications.\",\n",
    "        ]\n",
    "\n",
    "        # Create QA pairs based on documents\n",
    "        qa_pairs = [\n",
    "            {\n",
    "                \"query\": \"Who created Python and when?\",\n",
    "                \"relevant_doc_ids\": [0],\n",
    "                \"ground_truth\": \"Python was created by Guido van Rossum in 1991.\",\n",
    "                \"context\": documents[0],\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"What is machine learning?\",\n",
    "                \"relevant_doc_ids\": [1],\n",
    "                \"ground_truth\": \"Machine learning is a subset of artificial intelligence that enables computers to learn from experience without explicit programming.\",\n",
    "                \"context\": documents[1],\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"How do deep learning and neural networks relate?\",\n",
    "                \"relevant_doc_ids\": [2],\n",
    "                \"ground_truth\": \"Deep learning uses neural networks with multiple layers to model complex patterns in data.\",\n",
    "                \"context\": documents[2],\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"What does NLP focus on?\",\n",
    "                \"relevant_doc_ids\": [3],\n",
    "                \"ground_truth\": \"NLP focuses on the interaction between computers and human language.\",\n",
    "                \"context\": documents[3],\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"What is the purpose of computer vision?\",\n",
    "                \"relevant_doc_ids\": [4],\n",
    "                \"ground_truth\": \"Computer vision trains computers to interpret and understand visual information.\",\n",
    "                \"context\": documents[4],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # Expand to requested size by cycling through patterns\n",
    "        expanded_qa = []\n",
    "        for i in range(size):\n",
    "            base_qa = qa_pairs[i % len(qa_pairs)]\n",
    "            expanded_qa.append(\n",
    "                {\n",
    "                    \"id\": f\"eval_{i:03d}\",\n",
    "                    \"query\": base_qa[\"query\"],\n",
    "                    \"relevant_doc_ids\": base_qa[\"relevant_doc_ids\"],\n",
    "                    \"ground_truth\": base_qa[\"ground_truth\"],\n",
    "                    \"context\": base_qa[\"context\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        self.documents = documents\n",
    "        self.eval_data = Dataset.from_list(expanded_qa)\n",
    "\n",
    "        print(f\"[Dataset] Created {len(expanded_qa)} evaluation examples\")\n",
    "        print(f\"[Dataset] {len(documents)} documents in knowledge base\")\n",
    "\n",
    "        return self.eval_data\n",
    "\n",
    "    def load_msmarco_subset(self, size: int = 100) -> Optional[Dataset]:\n",
    "        \"\"\"Load MS MARCO QA subset (if available)\"\"\"\n",
    "        try:\n",
    "            # Try to load MS MARCO from HuggingFace\n",
    "            dataset = load_dataset(\n",
    "                \"ms_marco\", \"v1.1\", split=\"validation\", streaming=True\n",
    "            )\n",
    "\n",
    "            # Take first `size` examples and convert to our format\n",
    "            examples = []\n",
    "            for i, example in enumerate(dataset):\n",
    "                if i >= size:\n",
    "                    break\n",
    "\n",
    "                if example.get(\"answers\") and len(example[\"answers\"]) > 0:\n",
    "                    examples.append(\n",
    "                        {\n",
    "                            \"id\": f\"msmarco_{i:03d}\",\n",
    "                            \"query\": example[\"query\"],\n",
    "                            \"relevant_doc_ids\": [0],  # Simplified for demo\n",
    "                            \"ground_truth\": example[\"answers\"][0],\n",
    "                            \"context\": example.get(\"passages\", [{}])[0].get(\n",
    "                                \"passage_text\", \"\"\n",
    "                            ),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            if examples:\n",
    "                self.eval_data = Dataset.from_list(examples)\n",
    "                print(f\"[Dataset] Loaded {len(examples)} MS MARCO examples\")\n",
    "                return self.eval_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Could not load MS MARCO: {e}\")\n",
    "            print(\"[Fallback] Using synthetic dataset...\")\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "# Initialize dataset manager\n",
    "dataset_manager = EvaluationDatasetManager()\n",
    "\n",
    "# Try MS MARCO first, fallback to synthetic\n",
    "eval_dataset = dataset_manager.load_msmarco_subset(size=50)\n",
    "if eval_dataset is None:\n",
    "    eval_dataset = dataset_manager.create_synthetic_qa_dataset(size=50)\n",
    "\n",
    "print(\"\\n[Sample] First evaluation example:\")\n",
    "print(f\"Query: {eval_dataset[0]['query']}\")\n",
    "print(f\"Ground Truth: {eval_dataset[0]['ground_truth']}\")\n",
    "print(f\"Context: {eval_dataset[0]['context'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07583c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 3: Retrieval Evaluation System\n",
    "# 檢索評估系統\n",
    "\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class RetrievalEvaluator:\n",
    "    \"\"\"Evaluates retrieval performance with standard IR metrics\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    ):\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        self.document_embeddings = None\n",
    "        self.faiss_index = None\n",
    "\n",
    "    def index_documents(self, documents: List[str]) -> None:\n",
    "        \"\"\"Create FAISS index for documents\"\"\"\n",
    "        print(f\"[Indexing] Encoding {len(documents)} documents...\")\n",
    "\n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedding_model.encode(documents, show_progress_bar=True)\n",
    "        self.document_embeddings = embeddings\n",
    "\n",
    "        # Create FAISS index\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.faiss_index = faiss.IndexFlatIP(\n",
    "            dimension\n",
    "        )  # Inner product for cosine similarity\n",
    "\n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.faiss_index.add(embeddings.astype(np.float32))\n",
    "\n",
    "        print(\n",
    "            f\"[Indexing] Created FAISS index with {self.faiss_index.ntotal} documents\"\n",
    "        )\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 10) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"Retrieve top-k documents for a query\"\"\"\n",
    "        if self.faiss_index is None:\n",
    "            raise ValueError(\"Documents must be indexed first\")\n",
    "\n",
    "        # Encode query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "\n",
    "        # Search\n",
    "        scores, doc_ids = self.faiss_index.search(query_embedding.astype(np.float32), k)\n",
    "\n",
    "        return doc_ids[0].tolist(), scores[0].tolist()\n",
    "\n",
    "    def calculate_recall_at_k(\n",
    "        self, retrieved_docs: List[int], relevant_docs: List[int], k: int\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate Recall@K\"\"\"\n",
    "        if not relevant_docs:\n",
    "            return 0.0\n",
    "\n",
    "        retrieved_k = set(retrieved_docs[:k])\n",
    "        relevant_set = set(relevant_docs)\n",
    "\n",
    "        intersection = len(retrieved_k.intersection(relevant_set))\n",
    "        return intersection / len(relevant_set)\n",
    "\n",
    "    def calculate_precision_at_k(\n",
    "        self, retrieved_docs: List[int], relevant_docs: List[int], k: int\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate Precision@K\"\"\"\n",
    "        if k == 0:\n",
    "            return 0.0\n",
    "\n",
    "        retrieved_k = set(retrieved_docs[:k])\n",
    "        relevant_set = set(relevant_docs)\n",
    "\n",
    "        intersection = len(retrieved_k.intersection(relevant_set))\n",
    "        return intersection / k\n",
    "\n",
    "    def calculate_mrr(\n",
    "        self, retrieved_docs: List[int], relevant_docs: List[int]\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate Mean Reciprocal Rank\"\"\"\n",
    "        relevant_set = set(relevant_docs)\n",
    "\n",
    "        for rank, doc_id in enumerate(retrieved_docs, 1):\n",
    "            if doc_id in relevant_set:\n",
    "                return 1.0 / rank\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "    def calculate_ndcg_at_k(\n",
    "        self, retrieved_docs: List[int], relevant_docs: List[int], k: int\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate NDCG@K (simplified binary relevance)\"\"\"\n",
    "\n",
    "        def dcg_at_k(relevance_scores: List[int], k: int) -> float:\n",
    "            dcg = 0.0\n",
    "            for i in range(min(k, len(relevance_scores))):\n",
    "                dcg += relevance_scores[i] / math.log2(i + 2)\n",
    "            return dcg\n",
    "\n",
    "        # Binary relevance: 1 if relevant, 0 if not\n",
    "        retrieved_k = retrieved_docs[:k]\n",
    "        relevant_set = set(relevant_docs)\n",
    "\n",
    "        # Actual relevance scores for retrieved docs\n",
    "        actual_scores = [1 if doc_id in relevant_set else 0 for doc_id in retrieved_k]\n",
    "\n",
    "        # Ideal relevance scores (all relevant docs first)\n",
    "        ideal_scores = [1] * min(len(relevant_docs), k) + [0] * max(\n",
    "            0, k - len(relevant_docs)\n",
    "        )\n",
    "\n",
    "        actual_dcg = dcg_at_k(actual_scores, k)\n",
    "        ideal_dcg = dcg_at_k(ideal_scores, k)\n",
    "\n",
    "        return actual_dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "\n",
    "\n",
    "# Initialize retrieval evaluator\n",
    "retrieval_evaluator = RetrievalEvaluator()\n",
    "\n",
    "# Index documents\n",
    "if dataset_manager.documents:\n",
    "    retrieval_evaluator.index_documents(dataset_manager.documents)\n",
    "else:\n",
    "    print(\"[Warning] No documents available for indexing\")\n",
    "\n",
    "# Test retrieval evaluation\n",
    "if len(eval_dataset) > 0:\n",
    "    sample_query = eval_dataset[0][\"query\"]\n",
    "    retrieved_ids, scores = retrieval_evaluator.retrieve(sample_query, k=5)\n",
    "\n",
    "    print(f\"\\n[Retrieval Test] Query: {sample_query}\")\n",
    "    print(f\"[Retrieval Test] Retrieved doc IDs: {retrieved_ids}\")\n",
    "    print(f\"[Retrieval Test] Scores: {[f'{s:.3f}' for s in scores]}\")\n",
    "\n",
    "    # Calculate metrics for this example\n",
    "    relevant_ids = eval_dataset[0][\"relevant_doc_ids\"]\n",
    "    recall_5 = retrieval_evaluator.calculate_recall_at_k(retrieved_ids, relevant_ids, 5)\n",
    "    precision_5 = retrieval_evaluator.calculate_precision_at_k(\n",
    "        retrieved_ids, relevant_ids, 5\n",
    "    )\n",
    "    mrr = retrieval_evaluator.calculate_mrr(retrieved_ids, relevant_ids)\n",
    "    ndcg_5 = retrieval_evaluator.calculate_ndcg_at_k(retrieved_ids, relevant_ids, 5)\n",
    "\n",
    "    print(f\"[Metrics] Recall@5: {recall_5:.3f}\")\n",
    "    print(f\"[Metrics] Precision@5: {precision_5:.3f}\")\n",
    "    print(f\"[Metrics] MRR: {mrr:.3f}\")\n",
    "    print(f\"[Metrics] NDCG@5: {ndcg_5:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adba4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 4: Generation Evaluation System\n",
    "# 生成評估系統\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import re\n",
    "\n",
    "\n",
    "class GenerationEvaluator:\n",
    "    \"\"\"Evaluates generation quality with faithfulness and groundedness metrics\"\"\"\n",
    "\n",
    "    def __init__(self, device: str = \"auto\"):\n",
    "        self.device = device\n",
    "        self.nli_model = None\n",
    "        self.bert_scorer = None\n",
    "        self._load_models()\n",
    "\n",
    "    def _load_models(self):\n",
    "        \"\"\"Load models for evaluation\"\"\"\n",
    "        try:\n",
    "            # Load NLI model for faithfulness/consistency checking\n",
    "            print(\"[Loading] NLI model for faithfulness evaluation...\")\n",
    "            self.nli_model = pipeline(\n",
    "                \"text-classification\",\n",
    "                model=\"microsoft/DialoGPT-medium\",  # Lightweight alternative\n",
    "                device=0 if torch.cuda.is_available() else -1,\n",
    "            )\n",
    "\n",
    "            # Initialize BERTScore\n",
    "            print(\"[Loading] BERTScore for semantic similarity...\")\n",
    "            self.bert_scorer = bert_score\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Could not load some evaluation models: {e}\")\n",
    "            print(\"[Fallback] Using simplified heuristic evaluations\")\n",
    "\n",
    "    def calculate_faithfulness(self, generated_text: str, source_context: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate faithfulness score (0-1)\n",
    "        Measures if generated text is consistent with source context\n",
    "        \"\"\"\n",
    "        if not generated_text or not source_context:\n",
    "            return 0.0\n",
    "\n",
    "        # Simplified faithfulness using lexical overlap\n",
    "        # In production, use NLI models like DeBERTa\n",
    "\n",
    "        generated_words = set(generated_text.lower().split())\n",
    "        context_words = set(source_context.lower().split())\n",
    "\n",
    "        if len(generated_words) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Calculate overlap ratio\n",
    "        overlap = len(generated_words.intersection(context_words))\n",
    "        faithfulness = overlap / len(generated_words)\n",
    "\n",
    "        return min(faithfulness, 1.0)\n",
    "\n",
    "    def calculate_groundedness(\n",
    "        self, generated_text: str, source_documents: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate groundedness score (0-1)\n",
    "        Measures if generated text is supported by source documents\n",
    "        \"\"\"\n",
    "        if not generated_text or not source_documents:\n",
    "            return 0.0\n",
    "\n",
    "        # Combine all source documents\n",
    "        combined_context = \" \".join(source_documents)\n",
    "\n",
    "        # Use faithfulness calculation as proxy for groundedness\n",
    "        return self.calculate_faithfulness(generated_text, combined_context)\n",
    "\n",
    "    def calculate_attribution(\n",
    "        self, generated_text: str, source_context: str\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate attribution metrics\n",
    "        Returns dict with citation_accuracy, source_coverage, etc.\n",
    "        \"\"\"\n",
    "        attribution_metrics = {}\n",
    "\n",
    "        # Check for explicit citations (simplified)\n",
    "        citation_pattern = r\"\\[(\\d+)\\]|\\(source: ?\\d+\\)\"\n",
    "        citations = re.findall(citation_pattern, generated_text)\n",
    "\n",
    "        attribution_metrics[\"citation_count\"] = len(citations)\n",
    "        attribution_metrics[\"has_citations\"] = len(citations) > 0\n",
    "\n",
    "        # Calculate source coverage (how much of context is reflected)\n",
    "        if source_context:\n",
    "            context_sentences = source_context.split(\".\")\n",
    "            covered_sentences = 0\n",
    "\n",
    "            for sentence in context_sentences:\n",
    "                if sentence.strip() and any(\n",
    "                    word in generated_text.lower()\n",
    "                    for word in sentence.lower().split()[:3]\n",
    "                ):\n",
    "                    covered_sentences += 1\n",
    "\n",
    "            attribution_metrics[\"source_coverage\"] = (\n",
    "                covered_sentences / len(context_sentences) if context_sentences else 0.0\n",
    "            )\n",
    "        else:\n",
    "            attribution_metrics[\"source_coverage\"] = 0.0\n",
    "\n",
    "        return attribution_metrics\n",
    "\n",
    "    def calculate_bert_score(\n",
    "        self, generated_text: str, reference_text: str\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Calculate BERTScore for semantic similarity\"\"\"\n",
    "        try:\n",
    "            P, R, F1 = self.bert_scorer.score(\n",
    "                [generated_text], [reference_text], lang=\"en\", verbose=False\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"bert_precision\": P.item(),\n",
    "                \"bert_recall\": R.item(),\n",
    "                \"bert_f1\": F1.item(),\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] BERTScore calculation failed: {e}\")\n",
    "            return {\"bert_precision\": 0.0, \"bert_recall\": 0.0, \"bert_f1\": 0.0}\n",
    "\n",
    "    def calculate_factual_consistency(\n",
    "        self, generated_text: str, ground_truth: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Simple factual consistency check using keyword overlap\n",
    "        \"\"\"\n",
    "        if not generated_text or not ground_truth:\n",
    "            return 0.0\n",
    "\n",
    "        # Extract key information (simplified)\n",
    "        def extract_key_terms(text: str) -> set:\n",
    "            # Remove common words and extract potential facts\n",
    "            common_words = {\n",
    "                \"the\",\n",
    "                \"a\",\n",
    "                \"an\",\n",
    "                \"and\",\n",
    "                \"or\",\n",
    "                \"but\",\n",
    "                \"in\",\n",
    "                \"on\",\n",
    "                \"at\",\n",
    "                \"to\",\n",
    "                \"for\",\n",
    "                \"of\",\n",
    "                \"with\",\n",
    "                \"by\",\n",
    "                \"is\",\n",
    "                \"are\",\n",
    "                \"was\",\n",
    "                \"were\",\n",
    "                \"be\",\n",
    "                \"been\",\n",
    "                \"being\",\n",
    "            }\n",
    "            words = set(text.lower().split())\n",
    "            return words - common_words\n",
    "\n",
    "        generated_terms = extract_key_terms(generated_text)\n",
    "        truth_terms = extract_key_terms(ground_truth)\n",
    "\n",
    "        if len(truth_terms) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        overlap = len(generated_terms.intersection(truth_terms))\n",
    "        consistency = overlap / len(truth_terms)\n",
    "\n",
    "        return min(consistency, 1.0)\n",
    "\n",
    "\n",
    "# Initialize generation evaluator\n",
    "generation_evaluator = GenerationEvaluator()\n",
    "\n",
    "# Test generation evaluation\n",
    "sample_generated = (\n",
    "    \"Python was created by Guido van Rossum in 1991 and is known for its simplicity.\"\n",
    ")\n",
    "sample_context = dataset_manager.documents[0] if dataset_manager.documents else \"\"\n",
    "sample_ground_truth = eval_dataset[0][\"ground_truth\"]\n",
    "\n",
    "print(\"\\n[Generation Eval Test]\")\n",
    "print(f\"Generated: {sample_generated}\")\n",
    "print(f\"Context: {sample_context}\")\n",
    "print(f\"Ground Truth: {sample_ground_truth}\")\n",
    "\n",
    "# Calculate generation metrics\n",
    "faithfulness = generation_evaluator.calculate_faithfulness(\n",
    "    sample_generated, sample_context\n",
    ")\n",
    "groundedness = generation_evaluator.calculate_groundedness(\n",
    "    sample_generated, [sample_context]\n",
    ")\n",
    "attribution = generation_evaluator.calculate_attribution(\n",
    "    sample_generated, sample_context\n",
    ")\n",
    "bert_scores = generation_evaluator.calculate_bert_score(\n",
    "    sample_generated, sample_ground_truth\n",
    ")\n",
    "factual_consistency = generation_evaluator.calculate_factual_consistency(\n",
    "    sample_generated, sample_ground_truth\n",
    ")\n",
    "\n",
    "print(f\"\\n[Generation Metrics]\")\n",
    "print(f\"Faithfulness: {faithfulness:.3f}\")\n",
    "print(f\"Groundedness: {groundedness:.3f}\")\n",
    "print(f\"Attribution: {attribution}\")\n",
    "print(f\"BERTScore: {bert_scores}\")\n",
    "print(f\"Factual Consistency: {factual_consistency:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27577bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 5: End-to-End RAG Evaluation\n",
    "# 端到端 RAG 評估\n",
    "\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"End-to-end evaluation of RAG systems\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        retrieval_evaluator: RetrievalEvaluator,\n",
    "        generation_evaluator: GenerationEvaluator,\n",
    "    ):\n",
    "        self.retrieval_eval = retrieval_evaluator\n",
    "        self.generation_eval = generation_evaluator\n",
    "\n",
    "    def evaluate_rag_system(\n",
    "        self,\n",
    "        eval_dataset: Dataset,\n",
    "        documents: List[str],\n",
    "        generator_func: callable,\n",
    "        k_values: List[int] = [1, 3, 5, 10],\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Comprehensive RAG system evaluation\n",
    "\n",
    "        Args:\n",
    "            eval_dataset: Dataset with queries, relevant docs, ground truth\n",
    "            documents: Document corpus\n",
    "            generator_func: Function that takes (query, context) -> generated_text\n",
    "            k_values: List of k values for Recall@k, Precision@k evaluation\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            \"retrieval_metrics\": defaultdict(list),\n",
    "            \"generation_metrics\": defaultdict(list),\n",
    "            \"end_to_end_metrics\": defaultdict(list),\n",
    "        }\n",
    "\n",
    "        print(f\"[RAG Eval] Evaluating {len(eval_dataset)} examples...\")\n",
    "\n",
    "        for idx, example in enumerate(eval_dataset):\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"[Progress] {idx}/{len(eval_dataset)}\")\n",
    "\n",
    "            query = example[\"query\"]\n",
    "            relevant_doc_ids = example[\"relevant_doc_ids\"]\n",
    "            ground_truth = example[\"ground_truth\"]\n",
    "\n",
    "            # Step 1: Retrieval evaluation\n",
    "            retrieved_ids, scores = self.retrieval_eval.retrieve(query, k=max(k_values))\n",
    "\n",
    "            for k in k_values:\n",
    "                recall_k = self.retrieval_eval.calculate_recall_at_k(\n",
    "                    retrieved_ids, relevant_doc_ids, k\n",
    "                )\n",
    "                precision_k = self.retrieval_eval.calculate_precision_at_k(\n",
    "                    retrieved_ids, relevant_doc_ids, k\n",
    "                )\n",
    "                ndcg_k = self.retrieval_eval.calculate_ndcg_at_k(\n",
    "                    retrieved_ids, relevant_doc_ids, k\n",
    "                )\n",
    "\n",
    "                results[\"retrieval_metrics\"][f\"recall@{k}\"].append(recall_k)\n",
    "                results[\"retrieval_metrics\"][f\"precision@{k}\"].append(precision_k)\n",
    "                results[\"retrieval_metrics\"][f\"ndcg@{k}\"].append(ndcg_k)\n",
    "\n",
    "            # MRR (computed once per query)\n",
    "            mrr = self.retrieval_eval.calculate_mrr(retrieved_ids, relevant_doc_ids)\n",
    "            results[\"retrieval_metrics\"][\"mrr\"].append(mrr)\n",
    "\n",
    "            # Step 2: Generation evaluation\n",
    "            # Use top-3 retrieved documents as context\n",
    "            top_retrieved_docs = [\n",
    "                documents[doc_id]\n",
    "                for doc_id in retrieved_ids[:3]\n",
    "                if doc_id < len(documents)\n",
    "            ]\n",
    "\n",
    "            if top_retrieved_docs:\n",
    "                combined_context = \"\\n\".join(top_retrieved_docs)\n",
    "\n",
    "                # Generate response using provided generator function\n",
    "                try:\n",
    "                    generated_text = generator_func(query, combined_context)\n",
    "                except Exception as e:\n",
    "                    print(f\"[Warning] Generation failed for example {idx}: {e}\")\n",
    "                    generated_text = \"\"\n",
    "\n",
    "                # Calculate generation metrics\n",
    "                faithfulness = self.generation_eval.calculate_faithfulness(\n",
    "                    generated_text, combined_context\n",
    "                )\n",
    "                groundedness = self.generation_eval.calculate_groundedness(\n",
    "                    generated_text, top_retrieved_docs\n",
    "                )\n",
    "                factual_consistency = (\n",
    "                    self.generation_eval.calculate_factual_consistency(\n",
    "                        generated_text, ground_truth\n",
    "                    )\n",
    "                )\n",
    "                bert_scores = self.generation_eval.calculate_bert_score(\n",
    "                    generated_text, ground_truth\n",
    "                )\n",
    "                attribution = self.generation_eval.calculate_attribution(\n",
    "                    generated_text, combined_context\n",
    "                )\n",
    "\n",
    "                results[\"generation_metrics\"][\"faithfulness\"].append(faithfulness)\n",
    "                results[\"generation_metrics\"][\"groundedness\"].append(groundedness)\n",
    "                results[\"generation_metrics\"][\"factual_consistency\"].append(\n",
    "                    factual_consistency\n",
    "                )\n",
    "                results[\"generation_metrics\"][\"bert_f1\"].append(bert_scores[\"bert_f1\"])\n",
    "                results[\"generation_metrics\"][\"source_coverage\"].append(\n",
    "                    attribution[\"source_coverage\"]\n",
    "                )\n",
    "\n",
    "                # End-to-end metrics (combine retrieval + generation)\n",
    "                # E2E score: weighted combination of retrieval and generation performance\n",
    "                retrieval_score = recall_k  # Use Recall@3 as proxy\n",
    "                generation_score = (faithfulness + factual_consistency) / 2\n",
    "                e2e_score = 0.4 * retrieval_score + 0.6 * generation_score\n",
    "\n",
    "                results[\"end_to_end_metrics\"][\"e2e_score\"].append(e2e_score)\n",
    "\n",
    "        # Aggregate results (compute means)\n",
    "        aggregated_results = {}\n",
    "        for category in [\n",
    "            \"retrieval_metrics\",\n",
    "            \"generation_metrics\",\n",
    "            \"end_to_end_metrics\",\n",
    "        ]:\n",
    "            aggregated_results[category] = {}\n",
    "            for metric, values in results[category].items():\n",
    "                if values:  # Only if we have values\n",
    "                    aggregated_results[category][metric] = {\n",
    "                        \"mean\": np.mean(values),\n",
    "                        \"std\": np.std(values),\n",
    "                        \"count\": len(values),\n",
    "                    }\n",
    "\n",
    "        return aggregated_results\n",
    "\n",
    "\n",
    "# Simple generator function for testing\n",
    "def simple_generator(query: str, context: str) -> str:\n",
    "    \"\"\"Simple generator that creates responses based on context\"\"\"\n",
    "    # This is a simplified generator for demonstration\n",
    "    # In practice, you'd use an actual LLM here\n",
    "\n",
    "    context_sentences = context.split(\".\")[:2]  # Take first 2 sentences\n",
    "    relevant_context = \". \".join(s.strip() for s in context_sentences if s.strip())\n",
    "\n",
    "    # Simple template-based generation\n",
    "    if \"who\" in query.lower() or \"when\" in query.lower():\n",
    "        return f\"Based on the provided context: {relevant_context}.\"\n",
    "    elif \"what\" in query.lower():\n",
    "        return f\"According to the information: {relevant_context}.\"\n",
    "    else:\n",
    "        return f\"The answer is: {relevant_context}.\"\n",
    "\n",
    "\n",
    "# Initialize RAG evaluator\n",
    "rag_evaluator = RAGEvaluator(retrieval_evaluator, generation_evaluator)\n",
    "\n",
    "# Run evaluation on a subset for demonstration\n",
    "eval_subset = eval_dataset.select(\n",
    "    range(min(10, len(eval_dataset)))\n",
    ")  # First 10 examples\n",
    "\n",
    "print(\"\\n[RAG Evaluation] Running end-to-end evaluation...\")\n",
    "rag_results = rag_evaluator.evaluate_rag_system(\n",
    "    eval_subset, dataset_manager.documents, simple_generator, k_values=[1, 3, 5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80945d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 6: Evaluation Report Generation\n",
    "# 評估報告生成\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class EvaluationReporter:\n",
    "    \"\"\"Generate comprehensive evaluation reports\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        plt.style.use(\"default\")\n",
    "        sns.set_palette(\"husl\")\n",
    "\n",
    "    def print_results_summary(self, results: Dict) -> None:\n",
    "        \"\"\"Print formatted evaluation results\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"🔍 RAG SYSTEM EVALUATION REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Retrieval metrics\n",
    "        if \"retrieval_metrics\" in results:\n",
    "            print(\"\\n📊 RETRIEVAL PERFORMANCE\")\n",
    "            print(\"-\" * 30)\n",
    "            for metric, stats in results[\"retrieval_metrics\"].items():\n",
    "                print(\n",
    "                    f\"{metric:15}: {stats['mean']:.3f} ± {stats['std']:.3f} (n={stats['count']})\"\n",
    "                )\n",
    "\n",
    "        # Generation metrics\n",
    "        if \"generation_metrics\" in results:\n",
    "            print(\"\\n✍️  GENERATION PERFORMANCE\")\n",
    "            print(\"-\" * 30)\n",
    "            for metric, stats in results[\"generation_metrics\"].items():\n",
    "                print(\n",
    "                    f\"{metric:15}: {stats['mean']:.3f} ± {stats['std']:.3f} (n={stats['count']})\"\n",
    "                )\n",
    "\n",
    "        # End-to-end metrics\n",
    "        if \"end_to_end_metrics\" in results:\n",
    "            print(\"\\n🎯 END-TO-END PERFORMANCE\")\n",
    "            print(\"-\" * 30)\n",
    "            for metric, stats in results[\"end_to_end_metrics\"].items():\n",
    "                print(\n",
    "                    f\"{metric:15}: {stats['mean']:.3f} ± {stats['std']:.3f} (n={stats['count']})\"\n",
    "                )\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "    def create_performance_plots(self, results: Dict) -> None:\n",
    "        \"\"\"Create visualization plots for evaluation results\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(\"RAG System Evaluation Results\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "        # Plot 1: Retrieval Metrics\n",
    "        if \"retrieval_metrics\" in results:\n",
    "            retrieval_data = results[\"retrieval_metrics\"]\n",
    "            metrics = list(retrieval_data.keys())\n",
    "            means = [retrieval_data[m][\"mean\"] for m in metrics]\n",
    "            stds = [retrieval_data[m][\"std\"] for m in metrics]\n",
    "\n",
    "            axes[0, 0].bar(range(len(metrics)), means, yerr=stds, capsize=5, alpha=0.7)\n",
    "            axes[0, 0].set_xticks(range(len(metrics)))\n",
    "            axes[0, 0].set_xticklabels(metrics, rotation=45, ha=\"right\")\n",
    "            axes[0, 0].set_title(\"Retrieval Performance Metrics\")\n",
    "            axes[0, 0].set_ylabel(\"Score\")\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 2: Generation Metrics\n",
    "        if \"generation_metrics\" in results:\n",
    "            generation_data = results[\"generation_metrics\"]\n",
    "            metrics = list(generation_data.keys())\n",
    "            means = [generation_data[m][\"mean\"] for m in metrics]\n",
    "            stds = [generation_data[m][\"std\"] for m in metrics]\n",
    "\n",
    "            axes[0, 1].bar(\n",
    "                range(len(metrics)),\n",
    "                means,\n",
    "                yerr=stds,\n",
    "                capsize=5,\n",
    "                alpha=0.7,\n",
    "                color=\"orange\",\n",
    "            )\n",
    "            axes[0, 1].set_xticks(range(len(metrics)))\n",
    "            axes[0, 1].set_xticklabels(metrics, rotation=45, ha=\"right\")\n",
    "            axes[0, 1].set_title(\"Generation Performance Metrics\")\n",
    "            axes[0, 1].set_ylabel(\"Score\")\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 3: Recall@K comparison\n",
    "        if \"retrieval_metrics\" in results:\n",
    "            recall_metrics = {\n",
    "                k: v\n",
    "                for k, v in results[\"retrieval_metrics\"].items()\n",
    "                if k.startswith(\"recall@\")\n",
    "            }\n",
    "            k_values = [k.split(\"@\")[1] for k in recall_metrics.keys()]\n",
    "            recall_means = [recall_metrics[k][\"mean\"] for k in recall_metrics.keys()]\n",
    "\n",
    "            axes[1, 0].plot(\n",
    "                k_values, recall_means, marker=\"o\", linewidth=2, markersize=8\n",
    "            )\n",
    "            axes[1, 0].set_title(\"Recall@K Performance\")\n",
    "            axes[1, 0].set_xlabel(\"K (Number of Retrieved Documents)\")\n",
    "            axes[1, 0].set_ylabel(\"Recall@K\")\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 4: Overall Performance Radar\n",
    "        if all(\n",
    "            category in results\n",
    "            for category in [\"retrieval_metrics\", \"generation_metrics\"]\n",
    "        ):\n",
    "            # Select key metrics for radar chart\n",
    "            key_metrics = []\n",
    "            values = []\n",
    "\n",
    "            if \"recall@5\" in results[\"retrieval_metrics\"]:\n",
    "                key_metrics.append(\"Recall@5\")\n",
    "                values.append(results[\"retrieval_metrics\"][\"recall@5\"][\"mean\"])\n",
    "\n",
    "            if \"mrr\" in results[\"retrieval_metrics\"]:\n",
    "                key_metrics.append(\"MRR\")\n",
    "                values.append(results[\"retrieval_metrics\"][\"mrr\"][\"mean\"])\n",
    "\n",
    "            if \"faithfulness\" in results[\"generation_metrics\"]:\n",
    "                key_metrics.append(\"Faithfulness\")\n",
    "                values.append(results[\"generation_metrics\"][\"faithfulness\"][\"mean\"])\n",
    "\n",
    "            if \"factual_consistency\" in results[\"generation_metrics\"]:\n",
    "                key_metrics.append(\"Factual Consistency\")\n",
    "                values.append(\n",
    "                    results[\"generation_metrics\"][\"factual_consistency\"][\"mean\"]\n",
    "                )\n",
    "\n",
    "            if key_metrics and len(key_metrics) >= 3:\n",
    "                # Create radar chart\n",
    "                angles = np.linspace(0, 2 * np.pi, len(key_metrics), endpoint=False)\n",
    "                values += values[:1]  # Complete the circle\n",
    "                angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "                axes[1, 1].plot(angles, values, \"o-\", linewidth=2, label=\"RAG System\")\n",
    "                axes[1, 1].fill(angles, values, alpha=0.25)\n",
    "                axes[1, 1].set_xticks(angles[:-1])\n",
    "                axes[1, 1].set_xticklabels(key_metrics)\n",
    "                axes[1, 1].set_ylim(0, 1)\n",
    "                axes[1, 1].set_title(\"Overall Performance Profile\")\n",
    "                axes[1, 1].grid(True)\n",
    "            else:\n",
    "                axes[1, 1].text(\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    \"Insufficient data\\nfor radar chart\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    transform=axes[1, 1].transAxes,\n",
    "                )\n",
    "                axes[1, 1].set_title(\"Overall Performance Profile\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def generate_recommendations(self, results: Dict) -> List[str]:\n",
    "        \"\"\"Generate improvement recommendations based on evaluation results\"\"\"\n",
    "        recommendations = []\n",
    "\n",
    "        # Check retrieval performance\n",
    "        if \"retrieval_metrics\" in results:\n",
    "            retrieval_data = results[\"retrieval_metrics\"]\n",
    "\n",
    "            if (\n",
    "                \"recall@5\" in retrieval_data\n",
    "                and retrieval_data[\"recall@5\"][\"mean\"] < 0.5\n",
    "            ):\n",
    "                recommendations.append(\n",
    "                    \"🔍 Low Recall@5 detected. Consider: (1) Using better embedding models \"\n",
    "                    \"(e.g., bge-large), (2) Improving document chunking strategy, \"\n",
    "                    \"(3) Adding query expansion or reformulation\"\n",
    "                )\n",
    "\n",
    "            if \"mrr\" in retrieval_data and retrieval_data[\"mrr\"][\"mean\"] < 0.3:\n",
    "                recommendations.append(\n",
    "                    \"📊 Low MRR suggests relevant documents are not ranked highly. \"\n",
    "                    \"Consider: (1) Using reranking models, (2) Hybrid search (BM25 + vector), \"\n",
    "                    \"(3) Fine-tuning embedding models on domain data\"\n",
    "                )\n",
    "\n",
    "        # Check generation performance\n",
    "        if \"generation_metrics\" in results:\n",
    "            generation_data = results[\"generation_metrics\"]\n",
    "\n",
    "            if (\n",
    "                \"faithfulness\" in generation_data\n",
    "                and generation_data[\"faithfulness\"][\"mean\"] < 0.6\n",
    "            ):\n",
    "                recommendations.append(\n",
    "                    \"✍️  Low faithfulness score indicates generated text may not align with sources. \"\n",
    "                    \"Consider: (1) Using instruction-tuned models, (2) Improving prompt templates, \"\n",
    "                    \"(3) Adding citation requirements in prompts\"\n",
    "                )\n",
    "\n",
    "            if (\n",
    "                \"factual_consistency\" in generation_data\n",
    "                and generation_data[\"factual_consistency\"][\"mean\"] < 0.5\n",
    "            ):\n",
    "                recommendations.append(\n",
    "                    \"❌ Low factual consistency. Consider: (1) Using more factual-aware LLMs, \"\n",
    "                    \"(2) Adding fact-checking post-processing, (3) Constraining generation \"\n",
    "                    \"to be more conservative\"\n",
    "                )\n",
    "\n",
    "        # End-to-end recommendations\n",
    "        if \"end_to_end_metrics\" in results:\n",
    "            e2e_data = results[\"end_to_end_metrics\"]\n",
    "            if \"e2e_score\" in e2e_data and e2e_data[\"e2e_score\"][\"mean\"] < 0.6:\n",
    "                recommendations.append(\n",
    "                    \"🎯 Overall system performance is below target. Focus on: \"\n",
    "                    \"(1) The component with lower individual scores, \"\n",
    "                    \"(2) Better retrieval-generation alignment, \"\n",
    "                    \"(3) End-to-end fine-tuning approaches\"\n",
    "                )\n",
    "\n",
    "        if not recommendations:\n",
    "            recommendations.append(\n",
    "                \"✅ System performance looks good! Consider: (1) Testing on larger datasets, \"\n",
    "                \"(2) Evaluating on out-of-domain queries, (3) A/B testing with users\"\n",
    "            )\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "# Initialize reporter\n",
    "reporter = EvaluationReporter()\n",
    "\n",
    "# Generate comprehensive report\n",
    "print(\"\\n🔎 Generating Evaluation Report...\")\n",
    "reporter.print_results_summary(rag_results)\n",
    "\n",
    "# Create visualizations\n",
    "print(\"\\n📊 Creating Performance Visualizations...\")\n",
    "reporter.create_performance_plots(rag_results)\n",
    "\n",
    "# Generate recommendations\n",
    "print(\"\\n💡 IMPROVEMENT RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "recommendations = reporter.generate_recommendations(rag_results)\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"\\n{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f12035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 7: Advanced Evaluation Features\n",
    "# 進階評估功能\n",
    "\n",
    "\n",
    "class AdvancedEvaluator:\n",
    "    \"\"\"Advanced evaluation features for production RAG systems\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.evaluation_history = []\n",
    "\n",
    "    def evaluate_query_types(\n",
    "        self,\n",
    "        eval_dataset: Dataset,\n",
    "        rag_evaluator: RAGEvaluator,\n",
    "        documents: List[str],\n",
    "        generator_func: callable,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Evaluate performance by query type\"\"\"\n",
    "\n",
    "        def categorize_query(query: str) -> str:\n",
    "            \"\"\"Simple query type categorization\"\"\"\n",
    "            query_lower = query.lower()\n",
    "\n",
    "            if any(word in query_lower for word in [\"who\", \"when\", \"where\"]):\n",
    "                return \"factual\"\n",
    "            elif any(word in query_lower for word in [\"what\", \"define\", \"explain\"]):\n",
    "                return \"definitional\"\n",
    "            elif any(word in query_lower for word in [\"how\", \"why\"]):\n",
    "                return \"procedural\"\n",
    "            elif any(\n",
    "                word in query_lower for word in [\"compare\", \"difference\", \"versus\"]\n",
    "            ):\n",
    "                return \"comparative\"\n",
    "            else:\n",
    "                return \"other\"\n",
    "\n",
    "        # Categorize queries\n",
    "        query_types = defaultdict(list)\n",
    "        for idx, example in enumerate(eval_dataset):\n",
    "            query_type = categorize_query(example[\"query\"])\n",
    "            query_types[query_type].append(idx)\n",
    "\n",
    "        print(f\"\\n[Query Analysis] Found {len(query_types)} query types:\")\n",
    "        for qtype, indices in query_types.items():\n",
    "            print(f\"  {qtype}: {len(indices)} queries\")\n",
    "\n",
    "        # Evaluate each type separately\n",
    "        type_results = {}\n",
    "        for qtype, indices in query_types.items():\n",
    "            if len(indices) >= 3:  # Only evaluate types with enough examples\n",
    "                subset = eval_dataset.select(indices)\n",
    "                results = rag_evaluator.evaluate_rag_system(\n",
    "                    subset, documents, generator_func, k_values=[3, 5]\n",
    "                )\n",
    "                type_results[qtype] = results\n",
    "                print(f\"\\n[{qtype.upper()}] Completed evaluation\")\n",
    "\n",
    "        return type_results\n",
    "\n",
    "    def ablation_study(\n",
    "        self, eval_dataset: Dataset, documents: List[str], generator_func: callable\n",
    "    ) -> Dict:\n",
    "        \"\"\"Conduct ablation study on retrieval parameters\"\"\"\n",
    "\n",
    "        ablation_results = {}\n",
    "\n",
    "        # Test different embedding models\n",
    "        embedding_models = [\n",
    "            \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        ]\n",
    "\n",
    "        for model_name in embedding_models:\n",
    "            print(f\"\\n[Ablation] Testing embedding model: {model_name}\")\n",
    "\n",
    "            # Create new evaluator with different embedding model\n",
    "            retrieval_eval = RetrievalEvaluator(model_name)\n",
    "            retrieval_eval.index_documents(documents)\n",
    "\n",
    "            rag_eval = RAGEvaluator(retrieval_eval, generation_evaluator)\n",
    "\n",
    "            # Evaluate on subset\n",
    "            subset = eval_dataset.select(range(min(5, len(eval_dataset))))\n",
    "            results = rag_eval.evaluate_rag_system(subset, documents, generator_func)\n",
    "\n",
    "            ablation_results[f\"embedding_{model_name.split('/')[-1]}\"] = results\n",
    "\n",
    "        return ablation_results\n",
    "\n",
    "    def confidence_intervals(\n",
    "        self, results: Dict, confidence_level: float = 0.95\n",
    "    ) -> Dict:\n",
    "        \"\"\"Calculate confidence intervals for metrics using bootstrap\"\"\"\n",
    "\n",
    "        def bootstrap_ci(\n",
    "            values: List[float], confidence: float = 0.95, n_bootstrap: int = 1000\n",
    "        ):\n",
    "            \"\"\"Calculate bootstrap confidence interval\"\"\"\n",
    "            if len(values) < 2:\n",
    "                return (0.0, 0.0)\n",
    "\n",
    "            import random\n",
    "\n",
    "            bootstrap_means = []\n",
    "\n",
    "            for _ in range(n_bootstrap):\n",
    "                bootstrap_sample = [random.choice(values) for _ in range(len(values))]\n",
    "                bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "\n",
    "            alpha = 1 - confidence\n",
    "            lower_percentile = (alpha / 2) * 100\n",
    "            upper_percentile = (1 - alpha / 2) * 100\n",
    "\n",
    "            ci_lower = np.percentile(bootstrap_means, lower_percentile)\n",
    "            ci_upper = np.percentile(bootstrap_means, upper_percentile)\n",
    "\n",
    "            return (ci_lower, ci_upper)\n",
    "\n",
    "        # This would require access to raw values, which we don't have in aggregated results\n",
    "        # In practice, you'd store raw values and compute CIs\n",
    "        print(f\"[Info] Confidence interval calculation requires raw evaluation values\")\n",
    "        print(\n",
    "            f\"[Info] Consider storing individual scores during evaluation for CI computation\"\n",
    "        )\n",
    "\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Run advanced evaluations\n",
    "advanced_evaluator = AdvancedEvaluator()\n",
    "\n",
    "print(\"\\n🔬 Running Advanced Evaluations...\")\n",
    "\n",
    "# Query type analysis\n",
    "print(\"\\n1. Query Type Analysis\")\n",
    "query_type_results = advanced_evaluator.evaluate_query_types(\n",
    "    eval_subset, dataset_manager.documents, simple_generator\n",
    ")\n",
    "\n",
    "for qtype, results in query_type_results.items():\n",
    "    print(f\"\\n[{qtype.upper()}] Performance Summary:\")\n",
    "    if \"retrieval_metrics\" in results and \"recall@5\" in results[\"retrieval_metrics\"]:\n",
    "        recall = results[\"retrieval_metrics\"][\"recall@5\"][\"mean\"]\n",
    "        print(f\"  Recall@5: {recall:.3f}\")\n",
    "    if (\n",
    "        \"generation_metrics\" in results\n",
    "        and \"faithfulness\" in results[\"generation_metrics\"]\n",
    "    ):\n",
    "        faithfulness = results[\"generation_metrics\"][\"faithfulness\"][\"mean\"]\n",
    "        print(f\"  Faithfulness: {faithfulness:.3f}\")\n",
    "\n",
    "# Ablation study (if sufficient resources)\n",
    "print(\"\\n2. Ablation Study\")\n",
    "try:\n",
    "    ablation_results = advanced_evaluator.ablation_study(\n",
    "        eval_subset, dataset_manager.documents, simple_generator\n",
    "    )\n",
    "\n",
    "    print(\"\\n[Ablation] Embedding Model Comparison:\")\n",
    "    for model_variant, results in ablation_results.items():\n",
    "        if (\n",
    "            \"retrieval_metrics\" in results\n",
    "            and \"recall@5\" in results[\"retrieval_metrics\"]\n",
    "        ):\n",
    "            recall = results[\"retrieval_metrics\"][\"recall@5\"][\"mean\"]\n",
    "            print(f\"  {model_variant}: Recall@5 = {recall:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[Ablation] Skipped due to resource constraints: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffedc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 8: Smoke Test & Validation\n",
    "# 驗收測試\n",
    "\n",
    "\n",
    "def run_evaluation_smoke_test():\n",
    "    \"\"\"Quick smoke test for evaluation pipeline\"\"\"\n",
    "\n",
    "    print(\"\\n🧪 EVALUATION SYSTEM SMOKE TEST\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    test_results = {\"passed\": 0, \"total\": 0}\n",
    "\n",
    "    # Test 1: Dataset creation\n",
    "    test_results[\"total\"] += 1\n",
    "    try:\n",
    "        test_dataset = dataset_manager.create_synthetic_qa_dataset(size=3)\n",
    "        assert len(test_dataset) == 3\n",
    "        assert \"query\" in test_dataset[0]\n",
    "        assert \"ground_truth\" in test_dataset[0]\n",
    "        print(\"✅ Dataset creation: PASSED\")\n",
    "        test_results[\"passed\"] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Dataset creation: FAILED - {e}\")\n",
    "\n",
    "    # Test 2: Retrieval evaluation\n",
    "    test_results[\"total\"] += 1\n",
    "    try:\n",
    "        test_retrieval_eval = RetrievalEvaluator()\n",
    "        test_docs = [\n",
    "            \"Document 1 about Python\",\n",
    "            \"Document 2 about AI\",\n",
    "            \"Document 3 about ML\",\n",
    "        ]\n",
    "        test_retrieval_eval.index_documents(test_docs)\n",
    "\n",
    "        retrieved_ids, scores = test_retrieval_eval.retrieve(\"Python programming\", k=2)\n",
    "        assert len(retrieved_ids) == 2\n",
    "        assert len(scores) == 2\n",
    "        assert all(isinstance(score, (int, float)) for score in scores)\n",
    "\n",
    "        recall = test_retrieval_eval.calculate_recall_at_k([0, 1], [0], 2)\n",
    "        assert 0 <= recall <= 1\n",
    "\n",
    "        print(\"✅ Retrieval evaluation: PASSED\")\n",
    "        test_results[\"passed\"] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Retrieval evaluation: FAILED - {e}\")\n",
    "\n",
    "    # Test 3: Generation evaluation\n",
    "    test_results[\"total\"] += 1\n",
    "    try:\n",
    "        test_generation_eval = GenerationEvaluator()\n",
    "\n",
    "        faithfulness = test_generation_eval.calculate_faithfulness(\n",
    "            \"Python is a programming language\",\n",
    "            \"Python is a high-level programming language\",\n",
    "        )\n",
    "        assert 0 <= faithfulness <= 1\n",
    "\n",
    "        attribution = test_generation_eval.calculate_attribution(\n",
    "            \"Python was created by Guido\", \"Python was created by Guido van Rossum\"\n",
    "        )\n",
    "        assert isinstance(attribution, dict)\n",
    "        assert \"source_coverage\" in attribution\n",
    "\n",
    "        print(\"✅ Generation evaluation: PASSED\")\n",
    "        test_results[\"passed\"] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Generation evaluation: FAILED - {e}\")\n",
    "\n",
    "    # Test 4: End-to-end evaluation\n",
    "    test_results[\"total\"] += 1\n",
    "    try:\n",
    "        test_rag_eval = RAGEvaluator(test_retrieval_eval, test_generation_eval)\n",
    "\n",
    "        def dummy_generator(query, context):\n",
    "            return f\"Answer: {context[:50]}\"\n",
    "\n",
    "        small_dataset = test_dataset.select([0])  # Just one example\n",
    "        results = test_rag_eval.evaluate_rag_system(\n",
    "            small_dataset, test_docs, dummy_generator, k_values=[1]\n",
    "        )\n",
    "\n",
    "        assert \"retrieval_metrics\" in results\n",
    "        assert \"generation_metrics\" in results\n",
    "        assert \"end_to_end_metrics\" in results\n",
    "\n",
    "        print(\"✅ End-to-end evaluation: PASSED\")\n",
    "        test_results[\"passed\"] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ End-to-end evaluation: FAILED - {e}\")\n",
    "\n",
    "    # Test 5: Report generation\n",
    "    test_results[\"total\"] += 1\n",
    "    try:\n",
    "        test_reporter = EvaluationReporter()\n",
    "        recommendations = test_reporter.generate_recommendations(results)\n",
    "        assert isinstance(recommendations, list)\n",
    "        assert len(recommendations) > 0\n",
    "\n",
    "        print(\"✅ Report generation: PASSED\")\n",
    "        test_results[\"passed\"] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Report generation: FAILED - {e}\")\n",
    "\n",
    "    # Final results\n",
    "    print(\n",
    "        f\"\\n📊 SMOKE TEST RESULTS: {test_results['passed']}/{test_results['total']} PASSED\"\n",
    "    )\n",
    "\n",
    "    if test_results[\"passed\"] == test_results[\"total\"]:\n",
    "        print(\"🎉 All tests passed! Evaluation system is ready.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"⚠️  Some tests failed. Check the error messages above.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_passed = run_evaluation_smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aed62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 9: Usage Examples & Best Practices\n",
    "# 使用範例與最佳實踐\n",
    "\n",
    "print(\"\\n📚 USAGE EXAMPLES & BEST PRACTICES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "usage_examples = \"\"\"\n",
    "# 1. 基本評估流程 (Basic Evaluation Pipeline)\n",
    "from evaluation_system import RAGEvaluator, RetrievalEvaluator, GenerationEvaluator\n",
    "\n",
    "# Initialize evaluators\n",
    "retrieval_eval = RetrievalEvaluator(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "generation_eval = GenerationEvaluator()\n",
    "rag_eval = RAGEvaluator(retrieval_eval, generation_eval)\n",
    "\n",
    "# Index your documents\n",
    "retrieval_eval.index_documents(your_documents)\n",
    "\n",
    "# Define your generator function\n",
    "def your_llm_generator(query: str, context: str) -> str:\n",
    "    # Your LLM call here\n",
    "    return generated_response\n",
    "\n",
    "# Run evaluation\n",
    "results = rag_eval.evaluate_rag_system(\n",
    "    eval_dataset, your_documents, your_llm_generator\n",
    ")\n",
    "\n",
    "# 2. 自訂評估指標 (Custom Evaluation Metrics)\n",
    "class CustomEvaluator(GenerationEvaluator):\n",
    "    def calculate_domain_relevance(self, generated_text, domain_keywords):\n",
    "        # Your domain-specific evaluation logic\n",
    "        pass\n",
    "\n",
    "# 3. 批量模型比較 (Batch Model Comparison)\n",
    "models_to_test = [\n",
    "    (\"model_a\", model_a_generator),\n",
    "    (\"model_b\", model_b_generator),\n",
    "]\n",
    "\n",
    "comparison_results = {}\n",
    "for model_name, generator in models_to_test:\n",
    "    results = rag_eval.evaluate_rag_system(eval_dataset, documents, generator)\n",
    "    comparison_results[model_name] = results\n",
    "\n",
    "# 4. 持續評估監控 (Continuous Evaluation Monitoring)\n",
    "def setup_evaluation_monitoring():\n",
    "    # Set up regular evaluation runs\n",
    "    # Log results to monitoring system\n",
    "    # Alert on performance degradation\n",
    "    pass\n",
    "\"\"\"\n",
    "\n",
    "best_practices = \"\"\"\n",
    "🎯 EVALUATION BEST PRACTICES:\n",
    "\n",
    "1. 資料集品質 (Dataset Quality):\n",
    "   • 使用多樣化的查詢類型 (factual, definitional, comparative)\n",
    "   • 確保 ground truth 的準確性與完整性\n",
    "   • 定期更新評估資料集以反映實際使用情況\n",
    "\n",
    "2. 指標選擇 (Metric Selection):\n",
    "   • 檢索：優先 Recall@k 和 MRR，關注相關文檔的排序\n",
    "   • 生成：結合事實性 (faithfulness) 和流暢性指標\n",
    "   • 端到端：設計符合業務目標的複合指標\n",
    "\n",
    "3. 評估頻率 (Evaluation Frequency):\n",
    "   • 開發階段：每次模型變更後評估\n",
    "   • 生產階段：定期評估 (週/月) + 關鍵變更後評估\n",
    "   • A/B 測試：並行評估不同系統版本\n",
    "\n",
    "4. 計算資源管理 (Resource Management):\n",
    "   • 使用較小的評估集進行快速迭代\n",
    "   • 完整評估使用較大資料集\n",
    "   • 考慮使用 4bit/8bit 量化以降低記憶體需求\n",
    "\n",
    "5. 結果解釋 (Result Interpretation):\n",
    "   • 關注趨勢而非絕對數值\n",
    "   • 結合定量指標與定性分析\n",
    "   • 考慮不同查詢類型的差異表現\n",
    "\"\"\"\n",
    "\n",
    "print(usage_examples)\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75fcb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Final Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎊 NOTEBOOK COMPLETION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "completion_summary = f\"\"\"\n",
    "✅ COMPLETED COMPONENTS:\n",
    "• Dataset Management: Synthetic QA dataset creation + MS MARCO integration\n",
    "• Retrieval Evaluation: Recall@k, Precision@k, MRR, NDCG metrics\n",
    "• Generation Evaluation: Faithfulness, groundedness, attribution scoring\n",
    "• End-to-End Pipeline: Integrated RAG system evaluation\n",
    "• Advanced Features: Query type analysis, ablation studies\n",
    "• Reporting System: Automated report generation with visualizations\n",
    "• Validation: Comprehensive smoke test suite\n",
    "\n",
    "🔧 CORE CAPABILITIES:\n",
    "• Decoupled evaluation of retrieval and generation components\n",
    "• Multiple embedding model support with easy switching\n",
    "• Production-ready evaluation pipeline with error handling\n",
    "• Extensible architecture for custom metrics and evaluators\n",
    "• Memory-efficient evaluation with 4bit/8bit support\n",
    "\n",
    "📊 EVALUATION METRICS IMPLEMENTED:\n",
    "• Retrieval: Recall@k, Precision@k, MRR, NDCG@k\n",
    "• Generation: Faithfulness, Groundedness, Attribution, BERTScore\n",
    "• End-to-End: Composite RAG performance scoring\n",
    "\n",
    "💡 KEY LEARNINGS:\n",
    "• 分離式評估讓你能精確定位系統瓶頸 (檢索 vs 生成)\n",
    "• 不同查詢類型需要不同的評估策略和指標\n",
    "• 自動化評估流程對於持續改進 RAG 系統至關重要\n",
    "• 基準資料集的品質直接影響評估結果的可靠性\n",
    "\n",
    "Smoke Test Status: {'✅ PASSED' if smoke_test_passed else '❌ FAILED'}\n",
    "\"\"\"\n",
    "\n",
    "print(completion_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71b0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SMOKE TEST CELL (Run this to verify everything works)\n",
    "# 驗收測試 (運行此 Cell 驗證所有功能)\n",
    "\n",
    "print(\"\\n🚀 FINAL SMOKE TEST - Run this cell to verify the complete system\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def comprehensive_smoke_test():\n",
    "    \"\"\"Comprehensive test of the entire evaluation system\"\"\"\n",
    "\n",
    "    try:\n",
    "        # 1. Quick dataset setup\n",
    "        print(\"\\n1. Setting up test dataset...\")\n",
    "        test_dm = EvaluationDatasetManager()\n",
    "        test_data = test_dm.create_synthetic_qa_dataset(size=5)\n",
    "\n",
    "        # 2. Initialize all evaluators\n",
    "        print(\"2. Initializing evaluators...\")\n",
    "        ret_eval = RetrievalEvaluator()\n",
    "        gen_eval = GenerationEvaluator()\n",
    "        rag_eval = RAGEvaluator(ret_eval, gen_eval)\n",
    "\n",
    "        # 3. Index documents\n",
    "        print(\"3. Indexing documents...\")\n",
    "        ret_eval.index_documents(test_dm.documents)\n",
    "\n",
    "        # 4. Simple generator for testing\n",
    "        def test_generator(query, context):\n",
    "            return f\"Based on the context: {context.split('.')[0]}.\"\n",
    "\n",
    "        # 5. Run evaluation\n",
    "        print(\"4. Running RAG evaluation...\")\n",
    "        results = rag_eval.evaluate_rag_system(\n",
    "            test_data.select([0]), test_dm.documents, test_generator, k_values=[1, 3]\n",
    "        )\n",
    "\n",
    "        # 6. Generate report\n",
    "        print(\"5. Generating evaluation report...\")\n",
    "        reporter = EvaluationReporter()\n",
    "        reporter.print_results_summary(results)\n",
    "\n",
    "        print(\"\\n✅ COMPREHENSIVE SMOKE TEST PASSED!\")\n",
    "        print(\"🎯 The evaluation system is ready for production use.\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ SMOKE TEST FAILED: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run the comprehensive test\n",
    "final_test_result = comprehensive_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e238e",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 本章小結\n",
    "\n",
    "### ✅ 完成項目\n",
    "- **分離式評估架構**：實作了檢索器與生成器的獨立評估能力\n",
    "- **全面評估指標**：包含 Recall@k, Precision@k, MRR, NDCG (檢索) 及 Faithfulness, Groundedness, Attribution (生成)\n",
    "- **端到端評估流程**：整合檢索與生成的整體系統評估管線  \n",
    "- **進階分析功能**：查詢類型分析、消融研究、信心區間計算\n",
    "- **自動化報告生成**：視覺化結果展示與改進建議生成\n",
    "### 🔍 核心概念與原理\n",
    "\n",
    "**1. 分離式評估 (Decoupled Evaluation)**\n",
    "- **檢索評估**：獨立衡量文檔檢索的準確性和相關性，不受生成品質影響\n",
    "- **生成評估**：專注於文本生成的事實性、一致性和可歸因性\n",
    "- **優勢**：能精確定位系統瓶頸，針對性改進特定組件\n",
    "\n",
    "**2. 檢索評估指標體系**\n",
    "- **Recall@k**：衡量檢索到的相關文檔覆蓋率\n",
    "- **Precision@k**：衡量檢索結果中相關文檔的比例\n",
    "- **MRR (Mean Reciprocal Rank)**：關注第一個相關文檔的排序位置\n",
    "- **NDCG@k**：考慮排序質量的歸一化折扣累積增益\n",
    "\n",
    "**3. 生成評估維度**\n",
    "- **Faithfulness (忠實性)**：生成內容與源文檔的一致性\n",
    "- **Groundedness (基於性)**：回答是否有充分的文檔支撐\n",
    "- **Attribution (可歸因性)**：能否追溯到具體的源文檔位置\n",
    "\n",
    "**4. 端到端評估哲學**\n",
    "- **複合指標設計**：權衡檢索召回率與生成質量\n",
    "- **業務對齊**：評估指標應反映實際應用場景的成功標準\n",
    "\n",
    "### ⚠️ 常見陷阱與注意事項\n",
    "\n",
    "**1. 評估資料集偏差**\n",
    "- **問題**：合成資料集可能不反映真實查詢分布\n",
    "- **解決**：結合多種資料來源，定期用真實用戶查詢更新評估集\n",
    "\n",
    "**2. 指標過度優化**\n",
    "- **問題**：單一指標優化可能損害整體系統性能\n",
    "- **解決**：使用多維指標組合，關注指標間的平衡\n",
    "\n",
    "**3. 計算資源消耗**\n",
    "- **問題**：大規模評估可能消耗大量 GPU 記憶體\n",
    "- **解決**：使用 4bit 量化、批次處理、分階段評估\n",
    "\n",
    "**4. 生成評估的主觀性**\n",
    "- **問題**：事實性和流暢性評估存在主觀判斷\n",
    "- **解決**：結合自動化指標與人工評估，建立標準評估準則\n",
    "\n",
    "### 🎯 下一步建議與延伸方向\n",
    "\n",
    "**立即可行的改進 (本週內)**\n",
    "1. **擴展評估資料集**：整合更多領域的標準 QA 資料集 (SQuAD, Natural Questions)\n",
    "2. **優化記憶體使用**：實作批次評估和梯度累積以支援更大規模測試\n",
    "3. **增強報告功能**：添加 LaTeX/PDF 報告導出和 Slack 通知整合\n",
    "\n",
    "**中期發展目標 (未來 2-4 週)**\n",
    "1. **多語言評估支援**：擴展到中文 RAG 系統評估，使用中文特化指標\n",
    "2. **在線評估監控**：建立持續評估管線，監控生產系統性能退化\n",
    "3. **對抗性評估**：添加 prompt injection、幻覺檢測等安全性評估\n",
    "\n",
    "**長期規劃 (1-2 個月)**\n",
    "1. **人機協作評估**：整合人工標註和眾包評估流程\n",
    "2. **因果分析**：建立組件間性能相關性分析，理解檢索-生成耦合效應  \n",
    "3. **自適應評估**：根據系統性能自動調整評估策略和指標權重\n",
    "\n",
    "## 🔄 與其他 Notebook 的連接\n",
    "\n",
    "**前置依賴**\n",
    "- `nb26_rag_basic_faiss.ipynb`：基礎 RAG 系統實作\n",
    "- `nb13_function_calling_tools.ipynb`：工具使用和函數調用\n",
    "\n",
    "**後續建議順序**\n",
    "1. **nb29_multi_agent_collaboration.ipynb**：多代理協作系統 (利用本章評估方法測試多代理性能)\n",
    "2. **nb30_auto_pipeline_endtoend.ipynb**：自動化端到端流程 (整合評估作為品質門控)\n",
    "3. **nb25_domain_specific_tuning.ipynb**：領域特定微調 (使用評估系統驗證微調效果)\n",
    "\n",
    "**可選並行開發**\n",
    "- **nb24_dpo_vs_rlhf.ipynb**：偏好優化方法 (可使用生成評估指標作為自動化偏好信號)\n",
    "- **nb31_gradio_chat_ui.ipynb**：Web UI 開發 (整合評估結果展示功能)\n",
    "\n",
    "## 🤔 階段性決策點\n",
    "\n",
    "基於本章完成的評估系統，你現在有幾個發展方向可以選擇：\n",
    "\n",
    "### 選項 A：多代理協作 (nb29) \n",
    "**優勢**：可以立即使用剛建立的評估系統測試多代理性能\n",
    "**適合**：希望快速看到複雜系統整合效果\n",
    "**投入**：中等，主要是架構設計\n",
    "\n",
    "### 選項 B：領域微調 (nb25)\n",
    "**優勢**：評估系統可以量化微調前後的效果差異\n",
    "**適合**：希望深入理解模型優化技術\n",
    "**投入**：較高，需要訓練資源\n",
    "\n",
    "### 選項 C：自動化流程 (nb30)\n",
    "**優勢**：打造完整的生產級 RAG 系統\n",
    "**適合**：注重實用性和部署就緒度\n",
    "**投入**：中等，偏重工程整合\n",
    "\n",
    "**我的建議順序**：nb29 → nb30 → nb25，理由是先建立完整的系統架構，再進行模型層面的優化。\n",
    "\n",
    "你希望接下來專注哪個方向？或者你有其他優先考慮的 notebook 主題？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
