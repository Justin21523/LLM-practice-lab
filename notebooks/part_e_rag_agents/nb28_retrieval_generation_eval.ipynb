{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === E3: Retrieval-Generation Evaluation System ===\n",
    "# æª¢ç´¢ç”Ÿæˆåˆ†é›¢è©•ä¼°ç³»çµ± (Decoupled RAG Evaluation)\n",
    "\n",
    "# Cell 1: Environment Setup and Shared Cache\n",
    "import os, pathlib, torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Shared cache bootstrap\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "try:\n",
    "    import evaluate\n",
    "    import bert_score\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import faiss\n",
    "\n",
    "    print(\"[Packages] All evaluation packages loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"[Error] Missing package: {e}\")\n",
    "    print(\"Run: pip install evaluate bert-score sentence-transformers faiss-cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31992f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 2: Evaluation Dataset Preparation\n",
    "# è©•ä¼°è³‡æ–™é›†æº–å‚™\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "\n",
    "class EvaluationDatasetManager:\n",
    "    \"\"\"Manages evaluation datasets for RAG systems\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.eval_data = None\n",
    "        self.documents = None\n",
    "\n",
    "    def create_synthetic_qa_dataset(self, size: int = 50) -> Dataset:\n",
    "        \"\"\"Create synthetic QA dataset for evaluation\"\"\"\n",
    "\n",
    "        # Sample documents (technology domain)\n",
    "        documents = [\n",
    "            \"Python is a high-level programming language known for its simplicity and readability. It was created by Guido van Rossum in 1991.\",\n",
    "            \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed.\",\n",
    "            \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\",\n",
    "            \"Natural language processing (NLP) is a field of AI that focuses on the interaction between computers and human language.\",\n",
    "            \"Computer vision is a field of AI that trains computers to interpret and understand visual information from the world.\",\n",
    "            \"Transformers are a type of neural network architecture that has revolutionized natural language processing tasks.\",\n",
    "            \"BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google.\",\n",
    "            \"GPT (Generative Pre-trained Transformer) is an autoregressive language model that generates human-like text.\",\n",
    "            \"Retrieval-Augmented Generation (RAG) combines information retrieval with text generation for better factual accuracy.\",\n",
    "            \"Vector databases store high-dimensional vectors and enable efficient similarity search for AI applications.\",\n",
    "        ]\n",
    "\n",
    "        # Create QA pairs based on documents\n",
    "        qa_pairs = [\n",
    "            {\n",
    "                \"query\": \"Who created Python and when?\",\n",
    "                \"relevant_doc_ids\": [0],\n",
    "                \"ground_truth\": \"Python was created by Guido van Rossum in 1991.\",\n",
    "                \"context\": documents[0],\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"What is machine learning?\",\n",
    "                \"relevant_doc_ids\": [1],\n",
    "                \"ground_truth\": \"Machine learning is a subset of artificial intelligence that enables computers to learn from experience without explicit programming.\",\n",
    "                \"context\": documents[1],\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"How do deep learning and neural networks relate?\",\n",
    "                \"relevant_doc_ids\": [2],\n",
    "                \"ground_truth\": \"Deep learning uses neural networks with multiple layers to model complex patterns in data.\",\n",
    "                \"context\": documents[2],\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"What does NLP focus on?\",\n",
    "                \"relevant_doc_ids\": [3],\n",
    "                \"ground_truth\": \"NLP focuses on the interaction between computers and human language.\",\n",
    "                \"context\": documents[3],\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"What is the purpose of computer vision?\",\n",
    "                \"relevant_doc_ids\": [4],\n",
    "                \"ground_truth\": \"Computer vision trains computers to interpret and understand visual information.\",\n",
    "                \"context\": documents[4],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # Expand to requested size by cycling through patterns\n",
    "        expanded_qa = []\n",
    "        for i in range(size):\n",
    "            base_qa = qa_pairs[i % len(qa_pairs)]\n",
    "            expanded_qa.append(\n",
    "                {\n",
    "                    \"id\": f\"eval_{i:03d}\",\n",
    "                    \"query\": base_qa[\"query\"],\n",
    "                    \"relevant_doc_ids\": base_qa[\"relevant_doc_ids\"],\n",
    "                    \"ground_truth\": base_qa[\"ground_truth\"],\n",
    "                    \"context\": base_qa[\"context\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        self.documents = documents\n",
    "        self.eval_data = Dataset.from_list(expanded_qa)\n",
    "\n",
    "        print(f\"[Dataset] Created {len(expanded_qa)} evaluation examples\")\n",
    "        print(f\"[Dataset] {len(documents)} documents in knowledge base\")\n",
    "\n",
    "        return self.eval_data\n",
    "\n",
    "    def load_msmarco_subset(self, size: int = 100) -> Optional[Dataset]:\n",
    "        \"\"\"Load MS MARCO QA subset (if available)\"\"\"\n",
    "        try:\n",
    "            # Try to load MS MARCO from HuggingFace\n",
    "            dataset = load_dataset(\n",
    "                \"ms_marco\", \"v1.1\", split=\"validation\", streaming=True\n",
    "            )\n",
    "\n",
    "            # Take first `size` examples and convert to our format\n",
    "            examples = []\n",
    "            for i, example in enumerate(dataset):\n",
    "                if i >= size:\n",
    "                    break\n",
    "\n",
    "                if example.get(\"answers\") and len(example[\"answers\"]) > 0:\n",
    "                    examples.append(\n",
    "                        {\n",
    "                            \"id\": f\"msmarco_{i:03d}\",\n",
    "                            \"query\": example[\"query\"],\n",
    "                            \"relevant_doc_ids\": [0],  # Simplified for demo\n",
    "                            \"ground_truth\": example[\"answers\"][0],\n",
    "                            \"context\": example.get(\"passages\", [{}])[0].get(\n",
    "                                \"passage_text\", \"\"\n",
    "                            ),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            if examples:\n",
    "                self.eval_data = Dataset.from_list(examples)\n",
    "                print(f\"[Dataset] Loaded {len(examples)} MS MARCO examples\")\n",
    "                return self.eval_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Could not load MS MARCO: {e}\")\n",
    "            print(\"[Fallback] Using synthetic dataset...\")\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "# Initialize dataset manager\n",
    "dataset_manager = EvaluationDatasetManager()\n",
    "\n",
    "# Try MS MARCO first, fallback to synthetic\n",
    "eval_dataset = dataset_manager.load_msmarco_subset(size=50)\n",
    "if eval_dataset is None:\n",
    "    eval_dataset = dataset_manager.create_synthetic_qa_dataset(size=50)\n",
    "\n",
    "print(\"\\n[Sample] First evaluation example:\")\n",
    "print(f\"Query: {eval_dataset[0]['query']}\")\n",
    "print(f\"Ground Truth: {eval_dataset[0]['ground_truth']}\")\n",
    "print(f\"Context: {eval_dataset[0]['context'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07583c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 3: Retrieval Evaluation System\n",
    "# æª¢ç´¢è©•ä¼°ç³»çµ±\n",
    "\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class RetrievalEvaluator:\n",
    "    \"\"\"Evaluates retrieval performance with standard IR metrics\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    ):\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        self.document_embeddings = None\n",
    "        self.faiss_index = None\n",
    "\n",
    "    def index_documents(self, documents: List[str]) -> None:\n",
    "        \"\"\"Create FAISS index for documents\"\"\"\n",
    "        print(f\"[Indexing] Encoding {len(documents)} documents...\")\n",
    "\n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedding_model.encode(documents, show_progress_bar=True)\n",
    "        self.document_embeddings = embeddings\n",
    "\n",
    "        # Create FAISS index\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.faiss_index = faiss.IndexFlatIP(\n",
    "            dimension\n",
    "        )  # Inner product for cosine similarity\n",
    "\n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.faiss_index.add(embeddings.astype(np.float32))\n",
    "\n",
    "        print(\n",
    "            f\"[Indexing] Created FAISS index with {self.faiss_index.ntotal} documents\"\n",
    "        )\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 10) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"Retrieve top-k documents for a query\"\"\"\n",
    "        if self.faiss_index is None:\n",
    "            raise ValueError(\"Documents must be indexed first\")\n",
    "\n",
    "        # Encode query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "\n",
    "        # Search\n",
    "        scores, doc_ids = self.faiss_index.search(query_embedding.astype(np.float32), k)\n",
    "\n",
    "        return doc_ids[0].tolist(), scores[0].tolist()\n",
    "\n",
    "    def calculate_recall_at_k(\n",
    "        self, retrieved_docs: List[int], relevant_docs: List[int], k: int\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate Recall@K\"\"\"\n",
    "        if not relevant_docs:\n",
    "            return 0.0\n",
    "\n",
    "        retrieved_k = set(retrieved_docs[:k])\n",
    "        relevant_set = set(relevant_docs)\n",
    "\n",
    "        intersection = len(retrieved_k.intersection(relevant_set))\n",
    "        return intersection / len(relevant_set)\n",
    "\n",
    "    def calculate_precision_at_k(\n",
    "        self, retrieved_docs: List[int], relevant_docs: List[int], k: int\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate Precision@K\"\"\"\n",
    "        if k == 0:\n",
    "            return 0.0\n",
    "\n",
    "        retrieved_k = set(retrieved_docs[:k])\n",
    "        relevant_set = set(relevant_docs)\n",
    "\n",
    "        intersection = len(retrieved_k.intersection(relevant_set))\n",
    "        return intersection / k\n",
    "\n",
    "    def calculate_mrr(\n",
    "        self, retrieved_docs: List[int], relevant_docs: List[int]\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate Mean Reciprocal Rank\"\"\"\n",
    "        relevant_set = set(relevant_docs)\n",
    "\n",
    "        for rank, doc_id in enumerate(retrieved_docs, 1):\n",
    "            if doc_id in relevant_set:\n",
    "                return 1.0 / rank\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "    def calculate_ndcg_at_k(\n",
    "        self, retrieved_docs: List[int], relevant_docs: List[int], k: int\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate NDCG@K (simplified binary relevance)\"\"\"\n",
    "\n",
    "        def dcg_at_k(relevance_scores: List[int], k: int) -> float:\n",
    "            dcg = 0.0\n",
    "            for i in range(min(k, len(relevance_scores))):\n",
    "                dcg += relevance_scores[i] / math.log2(i + 2)\n",
    "            return dcg\n",
    "\n",
    "        # Binary relevance: 1 if relevant, 0 if not\n",
    "        retrieved_k = retrieved_docs[:k]\n",
    "        relevant_set = set(relevant_docs)\n",
    "\n",
    "        # Actual relevance scores for retrieved docs\n",
    "        actual_scores = [1 if doc_id in relevant_set else 0 for doc_id in retrieved_k]\n",
    "\n",
    "        # Ideal relevance scores (all relevant docs first)\n",
    "        ideal_scores = [1] * min(len(relevant_docs), k) + [0] * max(\n",
    "            0, k - len(relevant_docs)\n",
    "        )\n",
    "\n",
    "        actual_dcg = dcg_at_k(actual_scores, k)\n",
    "        ideal_dcg = dcg_at_k(ideal_scores, k)\n",
    "\n",
    "        return actual_dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "\n",
    "\n",
    "# Initialize retrieval evaluator\n",
    "retrieval_evaluator = RetrievalEvaluator()\n",
    "\n",
    "# Index documents\n",
    "if dataset_manager.documents:\n",
    "    retrieval_evaluator.index_documents(dataset_manager.documents)\n",
    "else:\n",
    "    print(\"[Warning] No documents available for indexing\")\n",
    "\n",
    "# Test retrieval evaluation\n",
    "if len(eval_dataset) > 0:\n",
    "    sample_query = eval_dataset[0][\"query\"]\n",
    "    retrieved_ids, scores = retrieval_evaluator.retrieve(sample_query, k=5)\n",
    "\n",
    "    print(f\"\\n[Retrieval Test] Query: {sample_query}\")\n",
    "    print(f\"[Retrieval Test] Retrieved doc IDs: {retrieved_ids}\")\n",
    "    print(f\"[Retrieval Test] Scores: {[f'{s:.3f}' for s in scores]}\")\n",
    "\n",
    "    # Calculate metrics for this example\n",
    "    relevant_ids = eval_dataset[0][\"relevant_doc_ids\"]\n",
    "    recall_5 = retrieval_evaluator.calculate_recall_at_k(retrieved_ids, relevant_ids, 5)\n",
    "    precision_5 = retrieval_evaluator.calculate_precision_at_k(\n",
    "        retrieved_ids, relevant_ids, 5\n",
    "    )\n",
    "    mrr = retrieval_evaluator.calculate_mrr(retrieved_ids, relevant_ids)\n",
    "    ndcg_5 = retrieval_evaluator.calculate_ndcg_at_k(retrieved_ids, relevant_ids, 5)\n",
    "\n",
    "    print(f\"[Metrics] Recall@5: {recall_5:.3f}\")\n",
    "    print(f\"[Metrics] Precision@5: {precision_5:.3f}\")\n",
    "    print(f\"[Metrics] MRR: {mrr:.3f}\")\n",
    "    print(f\"[Metrics] NDCG@5: {ndcg_5:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adba4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 4: Generation Evaluation System\n",
    "# ç”Ÿæˆè©•ä¼°ç³»çµ±\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import re\n",
    "\n",
    "\n",
    "class GenerationEvaluator:\n",
    "    \"\"\"Evaluates generation quality with faithfulness and groundedness metrics\"\"\"\n",
    "\n",
    "    def __init__(self, device: str = \"auto\"):\n",
    "        self.device = device\n",
    "        self.nli_model = None\n",
    "        self.bert_scorer = None\n",
    "        self._load_models()\n",
    "\n",
    "    def _load_models(self):\n",
    "        \"\"\"Load models for evaluation\"\"\"\n",
    "        try:\n",
    "            # Load NLI model for faithfulness/consistency checking\n",
    "            print(\"[Loading] NLI model for faithfulness evaluation...\")\n",
    "            self.nli_model = pipeline(\n",
    "                \"text-classification\",\n",
    "                model=\"microsoft/DialoGPT-medium\",  # Lightweight alternative\n",
    "                device=0 if torch.cuda.is_available() else -1,\n",
    "            )\n",
    "\n",
    "            # Initialize BERTScore\n",
    "            print(\"[Loading] BERTScore for semantic similarity...\")\n",
    "            self.bert_scorer = bert_score\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Could not load some evaluation models: {e}\")\n",
    "            print(\"[Fallback] Using simplified heuristic evaluations\")\n",
    "\n",
    "    def calculate_faithfulness(self, generated_text: str, source_context: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate faithfulness score (0-1)\n",
    "        Measures if generated text is consistent with source context\n",
    "        \"\"\"\n",
    "        if not generated_text or not source_context:\n",
    "            return 0.0\n",
    "\n",
    "        # Simplified faithfulness using lexical overlap\n",
    "        # In production, use NLI models like DeBERTa\n",
    "\n",
    "        generated_words = set(generated_text.lower().split())\n",
    "        context_words = set(source_context.lower().split())\n",
    "\n",
    "        if len(generated_words) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Calculate overlap ratio\n",
    "        overlap = len(generated_words.intersection(context_words))\n",
    "        faithfulness = overlap / len(generated_words)\n",
    "\n",
    "        return min(faithfulness, 1.0)\n",
    "\n",
    "    def calculate_groundedness(\n",
    "        self, generated_text: str, source_documents: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate groundedness score (0-1)\n",
    "        Measures if generated text is supported by source documents\n",
    "        \"\"\"\n",
    "        if not generated_text or not source_documents:\n",
    "            return 0.0\n",
    "\n",
    "        # Combine all source documents\n",
    "        combined_context = \" \".join(source_documents)\n",
    "\n",
    "        # Use faithfulness calculation as proxy for groundedness\n",
    "        return self.calculate_faithfulness(generated_text, combined_context)\n",
    "\n",
    "    def calculate_attribution(\n",
    "        self, generated_text: str, source_context: str\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate attribution metrics\n",
    "        Returns dict with citation_accuracy, source_coverage, etc.\n",
    "        \"\"\"\n",
    "        attribution_metrics = {}\n",
    "\n",
    "        # Check for explicit citations (simplified)\n",
    "        citation_pattern = r\"\\[(\\d+)\\]|\\(source: ?\\d+\\)\"\n",
    "        citations = re.findall(citation_pattern, generated_text)\n",
    "\n",
    "        attribution_metrics[\"citation_count\"] = len(citations)\n",
    "        attribution_metrics[\"has_citations\"] = len(citations) > 0\n",
    "\n",
    "        # Calculate source coverage (how much of context is reflected)\n",
    "        if source_context:\n",
    "            context_sentences = source_context.split(\".\")\n",
    "            covered_sentences = 0\n",
    "\n",
    "            for sentence in context_sentences:\n",
    "                if sentence.strip() and any(\n",
    "                    word in generated_text.lower()\n",
    "                    for word in sentence.lower().split()[:3]\n",
    "                ):\n",
    "                    covered_sentences += 1\n",
    "\n",
    "            attribution_metrics[\"source_coverage\"] = (\n",
    "                covered_sentences / len(context_sentences) if context_sentences else 0.0\n",
    "            )\n",
    "        else:\n",
    "            attribution_metrics[\"source_coverage\"] = 0.0\n",
    "\n",
    "        return attribution_metrics\n",
    "\n",
    "    def calculate_bert_score(\n",
    "        self, generated_text: str, reference_text: str\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Calculate BERTScore for semantic similarity\"\"\"\n",
    "        try:\n",
    "            P, R, F1 = self.bert_scorer.score(\n",
    "                [generated_text], [reference_text], lang=\"en\", verbose=False\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"bert_precision\": P.item(),\n",
    "                \"bert_recall\": R.item(),\n",
    "                \"bert_f1\": F1.item(),\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] BERTScore calculation failed: {e}\")\n",
    "            return {\"bert_precision\": 0.0, \"bert_recall\": 0.0, \"bert_f1\": 0.0}\n",
    "\n",
    "    def calculate_factual_consistency(\n",
    "        self, generated_text: str, ground_truth: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Simple factual consistency check using keyword overlap\n",
    "        \"\"\"\n",
    "        if not generated_text or not ground_truth:\n",
    "            return 0.0\n",
    "\n",
    "        # Extract key information (simplified)\n",
    "        def extract_key_terms(text: str) -> set:\n",
    "            # Remove common words and extract potential facts\n",
    "            common_words = {\n",
    "                \"the\",\n",
    "                \"a\",\n",
    "                \"an\",\n",
    "                \"and\",\n",
    "                \"or\",\n",
    "                \"but\",\n",
    "                \"in\",\n",
    "                \"on\",\n",
    "                \"at\",\n",
    "                \"to\",\n",
    "                \"for\",\n",
    "                \"of\",\n",
    "                \"with\",\n",
    "                \"by\",\n",
    "                \"is\",\n",
    "                \"are\",\n",
    "                \"was\",\n",
    "                \"were\",\n",
    "                \"be\",\n",
    "                \"been\",\n",
    "                \"being\",\n",
    "            }\n",
    "            words = set(text.lower().split())\n",
    "            return words - common_words\n",
    "\n",
    "        generated_terms = extract_key_terms(generated_text)\n",
    "        truth_terms = extract_key_terms(ground_truth)\n",
    "\n",
    "        if len(truth_terms) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        overlap = len(generated_terms.intersection(truth_terms))\n",
    "        consistency = overlap / len(truth_terms)\n",
    "\n",
    "        return min(consistency, 1.0)\n",
    "\n",
    "\n",
    "# Initialize generation evaluator\n",
    "generation_evaluator = GenerationEvaluator()\n",
    "\n",
    "# Test generation evaluation\n",
    "sample_generated = (\n",
    "    \"Python was created by Guido van Rossum in 1991 and is known for its simplicity.\"\n",
    ")\n",
    "sample_context = dataset_manager.documents[0] if dataset_manager.documents else \"\"\n",
    "sample_ground_truth = eval_dataset[0][\"ground_truth\"]\n",
    "\n",
    "print(\"\\n[Generation Eval Test]\")\n",
    "print(f\"Generated: {sample_generated}\")\n",
    "print(f\"Context: {sample_context}\")\n",
    "print(f\"Ground Truth: {sample_ground_truth}\")\n",
    "\n",
    "# Calculate generation metrics\n",
    "faithfulness = generation_evaluator.calculate_faithfulness(\n",
    "    sample_generated, sample_context\n",
    ")\n",
    "groundedness = generation_evaluator.calculate_groundedness(\n",
    "    sample_generated, [sample_context]\n",
    ")\n",
    "attribution = generation_evaluator.calculate_attribution(\n",
    "    sample_generated, sample_context\n",
    ")\n",
    "bert_scores = generation_evaluator.calculate_bert_score(\n",
    "    sample_generated, sample_ground_truth\n",
    ")\n",
    "factual_consistency = generation_evaluator.calculate_factual_consistency(\n",
    "    sample_generated, sample_ground_truth\n",
    ")\n",
    "\n",
    "print(f\"\\n[Generation Metrics]\")\n",
    "print(f\"Faithfulness: {faithfulness:.3f}\")\n",
    "print(f\"Groundedness: {groundedness:.3f}\")\n",
    "print(f\"Attribution: {attribution}\")\n",
    "print(f\"BERTScore: {bert_scores}\")\n",
    "print(f\"Factual Consistency: {factual_consistency:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27577bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 5: End-to-End RAG Evaluation\n",
    "# ç«¯åˆ°ç«¯ RAG è©•ä¼°\n",
    "\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"End-to-end evaluation of RAG systems\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        retrieval_evaluator: RetrievalEvaluator,\n",
    "        generation_evaluator: GenerationEvaluator,\n",
    "    ):\n",
    "        self.retrieval_eval = retrieval_evaluator\n",
    "        self.generation_eval = generation_evaluator\n",
    "\n",
    "    def evaluate_rag_system(\n",
    "        self,\n",
    "        eval_dataset: Dataset,\n",
    "        documents: List[str],\n",
    "        generator_func: callable,\n",
    "        k_values: List[int] = [1, 3, 5, 10],\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Comprehensive RAG system evaluation\n",
    "\n",
    "        Args:\n",
    "            eval_dataset: Dataset with queries, relevant docs, ground truth\n",
    "            documents: Document corpus\n",
    "            generator_func: Function that takes (query, context) -> generated_text\n",
    "            k_values: List of k values for Recall@k, Precision@k evaluation\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            \"retrieval_metrics\": defaultdict(list),\n",
    "            \"generation_metrics\": defaultdict(list),\n",
    "            \"end_to_end_metrics\": defaultdict(list),\n",
    "        }\n",
    "\n",
    "        print(f\"[RAG Eval] Evaluating {len(eval_dataset)} examples...\")\n",
    "\n",
    "        for idx, example in enumerate(eval_dataset):\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"[Progress] {idx}/{len(eval_dataset)}\")\n",
    "\n",
    "            query = example[\"query\"]\n",
    "            relevant_doc_ids = example[\"relevant_doc_ids\"]\n",
    "            ground_truth = example[\"ground_truth\"]\n",
    "\n",
    "            # Step 1: Retrieval evaluation\n",
    "            retrieved_ids, scores = self.retrieval_eval.retrieve(query, k=max(k_values))\n",
    "\n",
    "            for k in k_values:\n",
    "                recall_k = self.retrieval_eval.calculate_recall_at_k(\n",
    "                    retrieved_ids, relevant_doc_ids, k\n",
    "                )\n",
    "                precision_k = self.retrieval_eval.calculate_precision_at_k(\n",
    "                    retrieved_ids, relevant_doc_ids, k\n",
    "                )\n",
    "                ndcg_k = self.retrieval_eval.calculate_ndcg_at_k(\n",
    "                    retrieved_ids, relevant_doc_ids, k\n",
    "                )\n",
    "\n",
    "                results[\"retrieval_metrics\"][f\"recall@{k}\"].append(recall_k)\n",
    "                results[\"retrieval_metrics\"][f\"precision@{k}\"].append(precision_k)\n",
    "                results[\"retrieval_metrics\"][f\"ndcg@{k}\"].append(ndcg_k)\n",
    "\n",
    "            # MRR (computed once per query)\n",
    "            mrr = self.retrieval_eval.calculate_mrr(retrieved_ids, relevant_doc_ids)\n",
    "            results[\"retrieval_metrics\"][\"mrr\"].append(mrr)\n",
    "\n",
    "            # Step 2: Generation evaluation\n",
    "            # Use top-3 retrieved documents as context\n",
    "            top_retrieved_docs = [\n",
    "                documents[doc_id]\n",
    "                for doc_id in retrieved_ids[:3]\n",
    "                if doc_id < len(documents)\n",
    "            ]\n",
    "\n",
    "            if top_retrieved_docs:\n",
    "                combined_context = \"\\n\".join(top_retrieved_docs)\n",
    "\n",
    "                # Generate response using provided generator function\n",
    "                try:\n",
    "                    generated_text = generator_func(query, combined_context)\n",
    "                except Exception as e:\n",
    "                    print(f\"[Warning] Generation failed for example {idx}: {e}\")\n",
    "                    generated_text = \"\"\n",
    "\n",
    "                # Calculate generation metrics\n",
    "                faithfulness = self.generation_eval.calculate_faithfulness(\n",
    "                    generated_text, combined_context\n",
    "                )\n",
    "                groundedness = self.generation_eval.calculate_groundedness(\n",
    "                    generated_text, top_retrieved_docs\n",
    "                )\n",
    "                factual_consistency = (\n",
    "                    self.generation_eval.calculate_factual_consistency(\n",
    "                        generated_text, ground_truth\n",
    "                    )\n",
    "                )\n",
    "                bert_scores = self.generation_eval.calculate_bert_score(\n",
    "                    generated_text, ground_truth\n",
    "                )\n",
    "                attribution = self.generation_eval.calculate_attribution(\n",
    "                    generated_text, combined_context\n",
    "                )\n",
    "\n",
    "                results[\"generation_metrics\"][\"faithfulness\"].append(faithfulness)\n",
    "                results[\"generation_metrics\"][\"groundedness\"].append(groundedness)\n",
    "                results[\"generation_metrics\"][\"factual_consistency\"].append(\n",
    "                    factual_consistency\n",
    "                )\n",
    "                results[\"generation_metrics\"][\"bert_f1\"].append(bert_scores[\"bert_f1\"])\n",
    "                results[\"generation_metrics\"][\"source_coverage\"].append(\n",
    "                    attribution[\"source_coverage\"]\n",
    "                )\n",
    "\n",
    "                # End-to-end metrics (combine retrieval + generation)\n",
    "                # E2E score: weighted combination of retrieval and generation performance\n",
    "                retrieval_score = recall_k  # Use Recall@3 as proxy\n",
    "                generation_score = (faithfulness + factual_consistency) / 2\n",
    "                e2e_score = 0.4 * retrieval_score + 0.6 * generation_score\n",
    "\n",
    "                results[\"end_to_end_metrics\"][\"e2e_score\"].append(e2e_score)\n",
    "\n",
    "        # Aggregate results (compute means)\n",
    "        aggregated_results = {}\n",
    "        for category in [\n",
    "            \"retrieval_metrics\",\n",
    "            \"generation_metrics\",\n",
    "            \"end_to_end_metrics\",\n",
    "        ]:\n",
    "            aggregated_results[category] = {}\n",
    "            for metric, values in results[category].items():\n",
    "                if values:  # Only if we have values\n",
    "                    aggregated_results[category][metric] = {\n",
    "                        \"mean\": np.mean(values),\n",
    "                        \"std\": np.std(values),\n",
    "                        \"count\": len(values),\n",
    "                    }\n",
    "\n",
    "        return aggregated_results\n",
    "\n",
    "\n",
    "# Simple generator function for testing\n",
    "def simple_generator(query: str, context: str) -> str:\n",
    "    \"\"\"Simple generator that creates responses based on context\"\"\"\n",
    "    # This is a simplified generator for demonstration\n",
    "    # In practice, you'd use an actual LLM here\n",
    "\n",
    "    context_sentences = context.split(\".\")[:2]  # Take first 2 sentences\n",
    "    relevant_context = \". \".join(s.strip() for s in context_sentences if s.strip())\n",
    "\n",
    "    # Simple template-based generation\n",
    "    if \"who\" in query.lower() or \"when\" in query.lower():\n",
    "        return f\"Based on the provided context: {relevant_context}.\"\n",
    "    elif \"what\" in query.lower():\n",
    "        return f\"According to the information: {relevant_context}.\"\n",
    "    else:\n",
    "        return f\"The answer is: {relevant_context}.\"\n",
    "\n",
    "\n",
    "# Initialize RAG evaluator\n",
    "rag_evaluator = RAGEvaluator(retrieval_evaluator, generation_evaluator)\n",
    "\n",
    "# Run evaluation on a subset for demonstration\n",
    "eval_subset = eval_dataset.select(\n",
    "    range(min(10, len(eval_dataset)))\n",
    ")  # First 10 examples\n",
    "\n",
    "print(\"\\n[RAG Evaluation] Running end-to-end evaluation...\")\n",
    "rag_results = rag_evaluator.evaluate_rag_system(\n",
    "    eval_subset, dataset_manager.documents, simple_generator, k_values=[1, 3, 5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80945d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 6: Evaluation Report Generation\n",
    "# è©•ä¼°å ±å‘Šç”Ÿæˆ\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class EvaluationReporter:\n",
    "    \"\"\"Generate comprehensive evaluation reports\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        plt.style.use(\"default\")\n",
    "        sns.set_palette(\"husl\")\n",
    "\n",
    "    def print_results_summary(self, results: Dict) -> None:\n",
    "        \"\"\"Print formatted evaluation results\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ðŸ” RAG SYSTEM EVALUATION REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Retrieval metrics\n",
    "        if \"retrieval_metrics\" in results:\n",
    "            print(\"\\nðŸ“Š RETRIEVAL PERFORMANCE\")\n",
    "            print(\"-\" * 30)\n",
    "            for metric, stats in results[\"retrieval_metrics\"].items():\n",
    "                print(\n",
    "                    f\"{metric:15}: {stats['mean']:.3f} Â± {stats['std']:.3f} (n={stats['count']})\"\n",
    "                )\n",
    "\n",
    "        # Generation metrics\n",
    "        if \"generation_metrics\" in results:\n",
    "            print(\"\\nâœï¸  GENERATION PERFORMANCE\")\n",
    "            print(\"-\" * 30)\n",
    "            for metric, stats in results[\"generation_metrics\"].items():\n",
    "                print(\n",
    "                    f\"{metric:15}: {stats['mean']:.3f} Â± {stats['std']:.3f} (n={stats['count']})\"\n",
    "                )\n",
    "\n",
    "        # End-to-end metrics\n",
    "        if \"end_to_end_metrics\" in results:\n",
    "            print(\"\\nðŸŽ¯ END-TO-END PERFORMANCE\")\n",
    "            print(\"-\" * 30)\n",
    "            for metric, stats in results[\"end_to_end_metrics\"].items():\n",
    "                print(\n",
    "                    f\"{metric:15}: {stats['mean']:.3f} Â± {stats['std']:.3f} (n={stats['count']})\"\n",
    "                )\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "    def create_performance_plots(self, results: Dict) -> None:\n",
    "        \"\"\"Create visualization plots for evaluation results\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(\"RAG System Evaluation Results\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "        # Plot 1: Retrieval Metrics\n",
    "        if \"retrieval_metrics\" in results:\n",
    "            retrieval_data = results[\"retrieval_metrics\"]\n",
    "            metrics = list(retrieval_data.keys())\n",
    "            means = [retrieval_data[m][\"mean\"] for m in metrics]\n",
    "            stds = [retrieval_data[m][\"std\"] for m in metrics]\n",
    "\n",
    "            axes[0, 0].bar(range(len(metrics)), means, yerr=stds, capsize=5, alpha=0.7)\n",
    "            axes[0, 0].set_xticks(range(len(metrics)))\n",
    "            axes[0, 0].set_xticklabels(metrics, rotation=45, ha=\"right\")\n",
    "            axes[0, 0].set_title(\"Retrieval Performance Metrics\")\n",
    "            axes[0, 0].set_ylabel(\"Score\")\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 2: Generation Metrics\n",
    "        if \"generation_metrics\" in results:\n",
    "            generation_data = results[\"generation_metrics\"]\n",
    "            metrics = list(generation_data.keys())\n",
    "            means = [generation_data[m][\"mean\"] for m in metrics]\n",
    "            stds = [generation_data[m][\"std\"] for m in metrics]\n",
    "\n",
    "            axes[0, 1].bar(\n",
    "                range(len(metrics)),\n",
    "                means,\n",
    "                yerr=stds,\n",
    "                capsize=5,\n",
    "                alpha=0.7,\n",
    "                color=\"orange\",\n",
    "            )\n",
    "            axes[0, 1].set_xticks(range(len(metrics)))\n",
    "            axes[0, 1].set_xticklabels(metrics, rotation=45, ha=\"right\")\n",
    "            axes[0, 1].set_title(\"Generation Performance Metrics\")\n",
    "            axes[0, 1].set_ylabel(\"Score\")\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 3: Recall@K comparison\n",
    "        if \"retrieval_metrics\" in results:\n",
    "            recall_metrics = {\n",
    "                k: v\n",
    "                for k, v in results[\"retrieval_metrics\"].items()\n",
    "                if k.startswith(\"recall@\")\n",
    "            }\n",
    "            k_values = [k.split(\"@\")[1] for k in recall_metrics.keys()]\n",
    "            recall_means = [recall_metrics[k][\"mean\"] for k in recall_metrics.keys()]\n",
    "\n",
    "            axes[1, 0].plot(\n",
    "                k_values, recall_means, marker=\"o\", linewidth=2, markersize=8\n",
    "            )\n",
    "            axes[1, 0].set_title(\"Recall@K Performance\")\n",
    "            axes[1, 0].set_xlabel(\"K (Number of Retrieved Documents)\")\n",
    "            axes[1, 0].set_ylabel(\"Recall@K\")\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot 4: Overall Performance Radar\n",
    "        if all(\n",
    "            category in results\n",
    "            for category in [\"retrieval_metrics\", \"generation_metrics\"]\n",
    "        ):\n",
    "            # Select key metrics for radar chart\n",
    "            key_metrics = []\n",
    "            values = []\n",
    "\n",
    "            if \"recall@5\" in results[\"retrieval_metrics\"]:\n",
    "                key_metrics.append(\"Recall@5\")\n",
    "                values.append(results[\"retrieval_metrics\"][\"recall@5\"][\"mean\"])\n",
    "\n",
    "            if \"mrr\" in results[\"retrieval_metrics\"]:\n",
    "                key_metrics.append(\"MRR\")\n",
    "                values.append(results[\"retrieval_metrics\"][\"mrr\"][\"mean\"])\n",
    "\n",
    "            if \"faithfulness\" in results[\"generation_metrics\"]:\n",
    "                key_metrics.append(\"Faithfulness\")\n",
    "                values.append(results[\"generation_metrics\"][\"faithfulness\"][\"mean\"])\n",
    "\n",
    "            if \"factual_consistency\" in results[\"generation_metrics\"]:\n",
    "                key_metrics.append(\"Factual Consistency\")\n",
    "                values.append(\n",
    "                    results[\"generation_metrics\"][\"factual_consistency\"][\"mean\"]\n",
    "                )\n",
    "\n",
    "            if key_metrics and len(key_metrics) >= 3:\n",
    "                # Create radar chart\n",
    "                angles = np.linspace(0, 2 * np.pi, len(key_metrics), endpoint=False)\n",
    "                values += values[:1]  # Complete the circle\n",
    "                angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "                axes[1, 1].plot(angles, values, \"o-\", linewidth=2, label=\"RAG System\")\n",
    "                axes[1, 1].fill(angles, values, alpha=0.25)\n",
    "                axes[1, 1].set_xticks(angles[:-1])\n",
    "                axes[1, 1].set_xticklabels(key_metrics)\n",
    "                axes[1, 1].set_ylim(0, 1)\n",
    "                axes[1, 1].set_title(\"Overall Performance Profile\")\n",
    "                axes[1, 1].grid(True)\n",
    "            else:\n",
    "                axes[1, 1].text(\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    \"Insufficient data\\nfor radar chart\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    transform=axes[1, 1].transAxes,\n",
    "                )\n",
    "                axes[1, 1].set_title(\"Overall Performance Profile\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def generate_recommendations(self, results: Dict) -> List[str]:\n",
    "        \"\"\"Generate improvement recommendations based on evaluation results\"\"\"\n",
    "        recommendations = []\n",
    "\n",
    "        # Check retrieval performance\n",
    "        if \"retrieval_metrics\" in results:\n",
    "            retrieval_data = results[\"retrieval_metrics\"]\n",
    "\n",
    "            if (\n",
    "                \"recall@5\" in retrieval_data\n",
    "                and retrieval_data[\"recall@5\"][\"mean\"] < 0.5\n",
    "            ):\n",
    "                recommendations.append(\n",
    "                    \"ðŸ” Low Recall@5 detected. Consider: (1) Using better embedding models \"\n",
    "                    \"(e.g., bge-large), (2) Improving document chunking strategy, \"\n",
    "                    \"(3) Adding query expansion or reformulation\"\n",
    "                )\n",
    "\n",
    "            if \"mrr\" in retrieval_data and retrieval_data[\"mrr\"][\"mean\"] < 0.3:\n",
    "                recommendations.append(\n",
    "                    \"ðŸ“Š Low MRR suggests relevant documents are not ranked highly. \"\n",
    "                    \"Consider: (1) Using reranking models, (2) Hybrid search (BM25 + vector), \"\n",
    "                    \"(3) Fine-tuning embedding models on domain data\"\n",
    "                )\n",
    "\n",
    "        # Check generation performance\n",
    "        if \"generation_metrics\" in results:\n",
    "            generation_data = results[\"generation_metrics\"]\n",
    "\n",
    "            if (\n",
    "                \"faithfulness\" in generation_data\n",
    "                and generation_data[\"faithfulness\"][\"mean\"] < 0.6\n",
    "            ):\n",
    "                recommendations.append(\n",
    "                    \"âœï¸  Low faithfulness score indicates generated text may not align with sources. \"\n",
    "                    \"Consider: (1) Using instruction-tuned models, (2) Improving prompt templates, \"\n",
    "                    \"(3) Adding citation requirements in prompts\"\n",
    "                )\n",
    "\n",
    "            if (\n",
    "                \"factual_consistency\" in generation_data\n",
    "                and generation_data[\"factual_consistency\"][\"mean\"] < 0.5\n",
    "            ):\n",
    "                recommendations.append(\n",
    "                    \"âŒ Low factual consistency. Consider: (1) Using more factual-aware LLMs, \"\n",
    "                    \"(2) Adding fact-checking post-processing, (3) Constraining generation \"\n",
    "                    \"to be more conservative\"\n",
    "                )\n",
    "\n",
    "        # End-to-end recommendations\n",
    "        if \"end_to_end_metrics\" in results:\n",
    "            e2e_data = results[\"end_to_end_metrics\"]\n",
    "            if \"e2e_score\" in e2e_data and e2e_data[\"e2e_score\"][\"mean\"] < 0.6:\n",
    "                recommendations.append(\n",
    "                    \"ðŸŽ¯ Overall system performance is below target. Focus on: \"\n",
    "                    \"(1) The component with lower individual scores, \"\n",
    "                    \"(2) Better retrieval-generation alignment, \"\n",
    "                    \"(3) End-to-end fine-tuning approaches\"\n",
    "                )\n",
    "\n",
    "        if not recommendations:\n",
    "            recommendations.append(\n",
    "                \"âœ… System performance looks good! Consider: (1) Testing on larger datasets, \"\n",
    "                \"(2) Evaluating on out-of-domain queries, (3) A/B testing with users\"\n",
    "            )\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "# Initialize reporter\n",
    "reporter = EvaluationReporter()\n",
    "\n",
    "# Generate comprehensive report\n",
    "print(\"\\nðŸ”Ž Generating Evaluation Report...\")\n",
    "reporter.print_results_summary(rag_results)\n",
    "\n",
    "# Create visualizations\n",
    "print(\"\\nðŸ“Š Creating Performance Visualizations...\")\n",
    "reporter.create_performance_plots(rag_results)\n",
    "\n",
    "# Generate recommendations\n",
    "print(\"\\nðŸ’¡ IMPROVEMENT RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "recommendations = reporter.generate_recommendations(rag_results)\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"\\n{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f12035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 7: Advanced Evaluation Features\n",
    "# é€²éšŽè©•ä¼°åŠŸèƒ½\n",
    "\n",
    "\n",
    "class AdvancedEvaluator:\n",
    "    \"\"\"Advanced evaluation features for production RAG systems\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.evaluation_history = []\n",
    "\n",
    "    def evaluate_query_types(\n",
    "        self,\n",
    "        eval_dataset: Dataset,\n",
    "        rag_evaluator: RAGEvaluator,\n",
    "        documents: List[str],\n",
    "        generator_func: callable,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Evaluate performance by query type\"\"\"\n",
    "\n",
    "        def categorize_query(query: str) -> str:\n",
    "            \"\"\"Simple query type categorization\"\"\"\n",
    "            query_lower = query.lower()\n",
    "\n",
    "            if any(word in query_lower for word in [\"who\", \"when\", \"where\"]):\n",
    "                return \"factual\"\n",
    "            elif any(word in query_lower for word in [\"what\", \"define\", \"explain\"]):\n",
    "                return \"definitional\"\n",
    "            elif any(word in query_lower for word in [\"how\", \"why\"]):\n",
    "                return \"procedural\"\n",
    "            elif any(\n",
    "                word in query_lower for word in [\"compare\", \"difference\", \"versus\"]\n",
    "            ):\n",
    "                return \"comparative\"\n",
    "            else:\n",
    "                return \"other\"\n",
    "\n",
    "        # Categorize queries\n",
    "        query_types = defaultdict(list)\n",
    "        for idx, example in enumerate(eval_dataset):\n",
    "            query_type = categorize_query(example[\"query\"])\n",
    "            query_types[query_type].append(idx)\n",
    "\n",
    "        print(f\"\\n[Query Analysis] Found {len(query_types)} query types:\")\n",
    "        for qtype, indices in query_types.items():\n",
    "            print(f\"  {qtype}: {len(indices)} queries\")\n",
    "\n",
    "        # Evaluate each type separately\n",
    "        type_results = {}\n",
    "        for qtype, indices in query_types.items():\n",
    "            if len(indices) >= 3:  # Only evaluate types with enough examples\n",
    "                subset = eval_dataset.select(indices)\n",
    "                results = rag_evaluator.evaluate_rag_system(\n",
    "                    subset, documents, generator_func, k_values=[3, 5]\n",
    "                )\n",
    "                type_results[qtype] = results\n",
    "                print(f\"\\n[{qtype.upper()}] Completed evaluation\")\n",
    "\n",
    "        return type_results\n",
    "\n",
    "    def ablation_study(\n",
    "        self, eval_dataset: Dataset, documents: List[str], generator_func: callable\n",
    "    ) -> Dict:\n",
    "        \"\"\"Conduct ablation study on retrieval parameters\"\"\"\n",
    "\n",
    "        ablation_results = {}\n",
    "\n",
    "        # Test different embedding models\n",
    "        embedding_models = [\n",
    "            \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        ]\n",
    "\n",
    "        for model_name in embedding_models:\n",
    "            print(f\"\\n[Ablation] Testing embedding model: {model_name}\")\n",
    "\n",
    "            # Create new evaluator with different embedding model\n",
    "            retrieval_eval = RetrievalEvaluator(model_name)\n",
    "            retrieval_eval.index_documents(documents)\n",
    "\n",
    "            rag_eval = RAGEvaluator(retrieval_eval, generation_evaluator)\n",
    "\n",
    "            # Evaluate on subset\n",
    "            subset = eval_dataset.select(range(min(5, len(eval_dataset))))\n",
    "            results = rag_eval.evaluate_rag_system(subset, documents, generator_func)\n",
    "\n",
    "            ablation_results[f\"embedding_{model_name.split('/')[-1]}\"] = results\n",
    "\n",
    "        return ablation_results\n",
    "\n",
    "    def confidence_intervals(\n",
    "        self, results: Dict, confidence_level: float = 0.95\n",
    "    ) -> Dict:\n",
    "        \"\"\"Calculate confidence intervals for metrics using bootstrap\"\"\"\n",
    "\n",
    "        def bootstrap_ci(\n",
    "            values: List[float], confidence: float = 0.95, n_bootstrap: int = 1000\n",
    "        ):\n",
    "            \"\"\"Calculate bootstrap confidence interval\"\"\"\n",
    "            if len(values) < 2:\n",
    "                return (0.0, 0.0)\n",
    "\n",
    "            import random\n",
    "\n",
    "            bootstrap_means = []\n",
    "\n",
    "            for _ in range(n_bootstrap):\n",
    "                bootstrap_sample = [random.choice(values) for _ in range(len(values))]\n",
    "                bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "\n",
    "            alpha = 1 - confidence\n",
    "            lower_percentile = (alpha / 2) * 100\n",
    "            upper_percentile = (1 - alpha / 2) * 100\n",
    "\n",
    "            ci_lower = np.percentile(bootstrap_means, lower_percentile)\n",
    "            ci_upper = np.percentile(bootstrap_means, upper_percentile)\n",
    "\n",
    "            return (ci_lower, ci_upper)\n",
    "\n",
    "        # This would require access to raw values, which we don't have in aggregated results\n",
    "        # In practice, you'd store raw values and compute CIs\n",
    "        print(f\"[Info] Confidence interval calculation requires raw evaluation values\")\n",
    "        print(\n",
    "            f\"[Info] Consider storing individual scores during evaluation for CI computation\"\n",
    "        )\n",
    "\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Run advanced evaluations\n",
    "advanced_evaluator = AdvancedEvaluator()\n",
    "\n",
    "print(\"\\nðŸ”¬ Running Advanced Evaluations...\")\n",
    "\n",
    "# Query type analysis\n",
    "print(\"\\n1. Query Type Analysis\")\n",
    "query_type_results = advanced_evaluator.evaluate_query_types(\n",
    "    eval_subset, dataset_manager.documents, simple_generator\n",
    ")\n",
    "\n",
    "for qtype, results in query_type_results.items():\n",
    "    print(f\"\\n[{qtype.upper()}] Performance Summary:\")\n",
    "    if \"retrieval_metrics\" in results and \"recall@5\" in results[\"retrieval_metrics\"]:\n",
    "        recall = results[\"retrieval_metrics\"][\"recall@5\"][\"mean\"]\n",
    "        print(f\"  Recall@5: {recall:.3f}\")\n",
    "    if (\n",
    "        \"generation_metrics\" in results\n",
    "        and \"faithfulness\" in results[\"generation_metrics\"]\n",
    "    ):\n",
    "        faithfulness = results[\"generation_metrics\"][\"faithfulness\"][\"mean\"]\n",
    "        print(f\"  Faithfulness: {faithfulness:.3f}\")\n",
    "\n",
    "# Ablation study (if sufficient resources)\n",
    "print(\"\\n2. Ablation Study\")\n",
    "try:\n",
    "    ablation_results = advanced_evaluator.ablation_study(\n",
    "        eval_subset, dataset_manager.documents, simple_generator\n",
    "    )\n",
    "\n",
    "    print(\"\\n[Ablation] Embedding Model Comparison:\")\n",
    "    for model_variant, results in ablation_results.items():\n",
    "        if (\n",
    "            \"retrieval_metrics\" in results\n",
    "            and \"recall@5\" in results[\"retrieval_metrics\"]\n",
    "        ):\n",
    "            recall = results[\"retrieval_metrics\"][\"recall@5\"][\"mean\"]\n",
    "            print(f\"  {model_variant}: Recall@5 = {recall:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[Ablation] Skipped due to resource constraints: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffedc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 8: Smoke Test & Validation\n",
    "# é©—æ”¶æ¸¬è©¦\n",
    "\n",
    "\n",
    "def run_evaluation_smoke_test():\n",
    "    \"\"\"Quick smoke test for evaluation pipeline\"\"\"\n",
    "\n",
    "    print(\"\\nðŸ§ª EVALUATION SYSTEM SMOKE TEST\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    test_results = {\"passed\": 0, \"total\": 0}\n",
    "\n",
    "    # Test 1: Dataset creation\n",
    "    test_results[\"total\"] += 1\n",
    "    try:\n",
    "        test_dataset = dataset_manager.create_synthetic_qa_dataset(size=3)\n",
    "        assert len(test_dataset) == 3\n",
    "        assert \"query\" in test_dataset[0]\n",
    "        assert \"ground_truth\" in test_dataset[0]\n",
    "        print(\"âœ… Dataset creation: PASSED\")\n",
    "        test_results[\"passed\"] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Dataset creation: FAILED - {e}\")\n",
    "\n",
    "    # Test 2: Retrieval evaluation\n",
    "    test_results[\"total\"] += 1\n",
    "    try:\n",
    "        test_retrieval_eval = RetrievalEvaluator()\n",
    "        test_docs = [\n",
    "            \"Document 1 about Python\",\n",
    "            \"Document 2 about AI\",\n",
    "            \"Document 3 about ML\",\n",
    "        ]\n",
    "        test_retrieval_eval.index_documents(test_docs)\n",
    "\n",
    "        retrieved_ids, scores = test_retrieval_eval.retrieve(\"Python programming\", k=2)\n",
    "        assert len(retrieved_ids) == 2\n",
    "        assert len(scores) == 2\n",
    "        assert all(isinstance(score, (int, float)) for score in scores)\n",
    "\n",
    "        recall = test_retrieval_eval.calculate_recall_at_k([0, 1], [0], 2)\n",
    "        assert 0 <= recall <= 1\n",
    "\n",
    "        print(\"âœ… Retrieval evaluation: PASSED\")\n",
    "        test_results[\"passed\"] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Retrieval evaluation: FAILED - {e}\")\n",
    "\n",
    "    # Test 3: Generation evaluation\n",
    "    test_results[\"total\"] += 1\n",
    "    try:\n",
    "        test_generation_eval = GenerationEvaluator()\n",
    "\n",
    "        faithfulness = test_generation_eval.calculate_faithfulness(\n",
    "            \"Python is a programming language\",\n",
    "            \"Python is a high-level programming language\",\n",
    "        )\n",
    "        assert 0 <= faithfulness <= 1\n",
    "\n",
    "        attribution = test_generation_eval.calculate_attribution(\n",
    "            \"Python was created by Guido\", \"Python was created by Guido van Rossum\"\n",
    "        )\n",
    "        assert isinstance(attribution, dict)\n",
    "        assert \"source_coverage\" in attribution\n",
    "\n",
    "        print(\"âœ… Generation evaluation: PASSED\")\n",
    "        test_results[\"passed\"] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Generation evaluation: FAILED - {e}\")\n",
    "\n",
    "    # Test 4: End-to-end evaluation\n",
    "    test_results[\"total\"] += 1\n",
    "    try:\n",
    "        test_rag_eval = RAGEvaluator(test_retrieval_eval, test_generation_eval)\n",
    "\n",
    "        def dummy_generator(query, context):\n",
    "            return f\"Answer: {context[:50]}\"\n",
    "\n",
    "        small_dataset = test_dataset.select([0])  # Just one example\n",
    "        results = test_rag_eval.evaluate_rag_system(\n",
    "            small_dataset, test_docs, dummy_generator, k_values=[1]\n",
    "        )\n",
    "\n",
    "        assert \"retrieval_metrics\" in results\n",
    "        assert \"generation_metrics\" in results\n",
    "        assert \"end_to_end_metrics\" in results\n",
    "\n",
    "        print(\"âœ… End-to-end evaluation: PASSED\")\n",
    "        test_results[\"passed\"] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ End-to-end evaluation: FAILED - {e}\")\n",
    "\n",
    "    # Test 5: Report generation\n",
    "    test_results[\"total\"] += 1\n",
    "    try:\n",
    "        test_reporter = EvaluationReporter()\n",
    "        recommendations = test_reporter.generate_recommendations(results)\n",
    "        assert isinstance(recommendations, list)\n",
    "        assert len(recommendations) > 0\n",
    "\n",
    "        print(\"âœ… Report generation: PASSED\")\n",
    "        test_results[\"passed\"] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Report generation: FAILED - {e}\")\n",
    "\n",
    "    # Final results\n",
    "    print(\n",
    "        f\"\\nðŸ“Š SMOKE TEST RESULTS: {test_results['passed']}/{test_results['total']} PASSED\"\n",
    "    )\n",
    "\n",
    "    if test_results[\"passed\"] == test_results[\"total\"]:\n",
    "        print(\"ðŸŽ‰ All tests passed! Evaluation system is ready.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"âš ï¸  Some tests failed. Check the error messages above.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_passed = run_evaluation_smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aed62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 9: Usage Examples & Best Practices\n",
    "# ä½¿ç”¨ç¯„ä¾‹èˆ‡æœ€ä½³å¯¦è¸\n",
    "\n",
    "print(\"\\nðŸ“š USAGE EXAMPLES & BEST PRACTICES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "usage_examples = \"\"\"\n",
    "# 1. åŸºæœ¬è©•ä¼°æµç¨‹ (Basic Evaluation Pipeline)\n",
    "from evaluation_system import RAGEvaluator, RetrievalEvaluator, GenerationEvaluator\n",
    "\n",
    "# Initialize evaluators\n",
    "retrieval_eval = RetrievalEvaluator(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "generation_eval = GenerationEvaluator()\n",
    "rag_eval = RAGEvaluator(retrieval_eval, generation_eval)\n",
    "\n",
    "# Index your documents\n",
    "retrieval_eval.index_documents(your_documents)\n",
    "\n",
    "# Define your generator function\n",
    "def your_llm_generator(query: str, context: str) -> str:\n",
    "    # Your LLM call here\n",
    "    return generated_response\n",
    "\n",
    "# Run evaluation\n",
    "results = rag_eval.evaluate_rag_system(\n",
    "    eval_dataset, your_documents, your_llm_generator\n",
    ")\n",
    "\n",
    "# 2. è‡ªè¨‚è©•ä¼°æŒ‡æ¨™ (Custom Evaluation Metrics)\n",
    "class CustomEvaluator(GenerationEvaluator):\n",
    "    def calculate_domain_relevance(self, generated_text, domain_keywords):\n",
    "        # Your domain-specific evaluation logic\n",
    "        pass\n",
    "\n",
    "# 3. æ‰¹é‡æ¨¡åž‹æ¯”è¼ƒ (Batch Model Comparison)\n",
    "models_to_test = [\n",
    "    (\"model_a\", model_a_generator),\n",
    "    (\"model_b\", model_b_generator),\n",
    "]\n",
    "\n",
    "comparison_results = {}\n",
    "for model_name, generator in models_to_test:\n",
    "    results = rag_eval.evaluate_rag_system(eval_dataset, documents, generator)\n",
    "    comparison_results[model_name] = results\n",
    "\n",
    "# 4. æŒçºŒè©•ä¼°ç›£æŽ§ (Continuous Evaluation Monitoring)\n",
    "def setup_evaluation_monitoring():\n",
    "    # Set up regular evaluation runs\n",
    "    # Log results to monitoring system\n",
    "    # Alert on performance degradation\n",
    "    pass\n",
    "\"\"\"\n",
    "\n",
    "best_practices = \"\"\"\n",
    "ðŸŽ¯ EVALUATION BEST PRACTICES:\n",
    "\n",
    "1. è³‡æ–™é›†å“è³ª (Dataset Quality):\n",
    "   â€¢ ä½¿ç”¨å¤šæ¨£åŒ–çš„æŸ¥è©¢é¡žåž‹ (factual, definitional, comparative)\n",
    "   â€¢ ç¢ºä¿ ground truth çš„æº–ç¢ºæ€§èˆ‡å®Œæ•´æ€§\n",
    "   â€¢ å®šæœŸæ›´æ–°è©•ä¼°è³‡æ–™é›†ä»¥åæ˜ å¯¦éš›ä½¿ç”¨æƒ…æ³\n",
    "\n",
    "2. æŒ‡æ¨™é¸æ“‡ (Metric Selection):\n",
    "   â€¢ æª¢ç´¢ï¼šå„ªå…ˆ Recall@k å’Œ MRRï¼Œé—œæ³¨ç›¸é—œæ–‡æª”çš„æŽ’åº\n",
    "   â€¢ ç”Ÿæˆï¼šçµåˆäº‹å¯¦æ€§ (faithfulness) å’Œæµæš¢æ€§æŒ‡æ¨™\n",
    "   â€¢ ç«¯åˆ°ç«¯ï¼šè¨­è¨ˆç¬¦åˆæ¥­å‹™ç›®æ¨™çš„è¤‡åˆæŒ‡æ¨™\n",
    "\n",
    "3. è©•ä¼°é »çŽ‡ (Evaluation Frequency):\n",
    "   â€¢ é–‹ç™¼éšŽæ®µï¼šæ¯æ¬¡æ¨¡åž‹è®Šæ›´å¾Œè©•ä¼°\n",
    "   â€¢ ç”Ÿç”¢éšŽæ®µï¼šå®šæœŸè©•ä¼° (é€±/æœˆ) + é—œéµè®Šæ›´å¾Œè©•ä¼°\n",
    "   â€¢ A/B æ¸¬è©¦ï¼šä¸¦è¡Œè©•ä¼°ä¸åŒç³»çµ±ç‰ˆæœ¬\n",
    "\n",
    "4. è¨ˆç®—è³‡æºç®¡ç† (Resource Management):\n",
    "   â€¢ ä½¿ç”¨è¼ƒå°çš„è©•ä¼°é›†é€²è¡Œå¿«é€Ÿè¿­ä»£\n",
    "   â€¢ å®Œæ•´è©•ä¼°ä½¿ç”¨è¼ƒå¤§è³‡æ–™é›†\n",
    "   â€¢ è€ƒæ…®ä½¿ç”¨ 4bit/8bit é‡åŒ–ä»¥é™ä½Žè¨˜æ†¶é«”éœ€æ±‚\n",
    "\n",
    "5. çµæžœè§£é‡‹ (Result Interpretation):\n",
    "   â€¢ é—œæ³¨è¶¨å‹¢è€Œéžçµ•å°æ•¸å€¼\n",
    "   â€¢ çµåˆå®šé‡æŒ‡æ¨™èˆ‡å®šæ€§åˆ†æž\n",
    "   â€¢ è€ƒæ…®ä¸åŒæŸ¥è©¢é¡žåž‹çš„å·®ç•°è¡¨ç¾\n",
    "\"\"\"\n",
    "\n",
    "print(usage_examples)\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75fcb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Final Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽŠ NOTEBOOK COMPLETION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "completion_summary = f\"\"\"\n",
    "âœ… COMPLETED COMPONENTS:\n",
    "â€¢ Dataset Management: Synthetic QA dataset creation + MS MARCO integration\n",
    "â€¢ Retrieval Evaluation: Recall@k, Precision@k, MRR, NDCG metrics\n",
    "â€¢ Generation Evaluation: Faithfulness, groundedness, attribution scoring\n",
    "â€¢ End-to-End Pipeline: Integrated RAG system evaluation\n",
    "â€¢ Advanced Features: Query type analysis, ablation studies\n",
    "â€¢ Reporting System: Automated report generation with visualizations\n",
    "â€¢ Validation: Comprehensive smoke test suite\n",
    "\n",
    "ðŸ”§ CORE CAPABILITIES:\n",
    "â€¢ Decoupled evaluation of retrieval and generation components\n",
    "â€¢ Multiple embedding model support with easy switching\n",
    "â€¢ Production-ready evaluation pipeline with error handling\n",
    "â€¢ Extensible architecture for custom metrics and evaluators\n",
    "â€¢ Memory-efficient evaluation with 4bit/8bit support\n",
    "\n",
    "ðŸ“Š EVALUATION METRICS IMPLEMENTED:\n",
    "â€¢ Retrieval: Recall@k, Precision@k, MRR, NDCG@k\n",
    "â€¢ Generation: Faithfulness, Groundedness, Attribution, BERTScore\n",
    "â€¢ End-to-End: Composite RAG performance scoring\n",
    "\n",
    "ðŸ’¡ KEY LEARNINGS:\n",
    "â€¢ åˆ†é›¢å¼è©•ä¼°è®“ä½ èƒ½ç²¾ç¢ºå®šä½ç³»çµ±ç“¶é ¸ (æª¢ç´¢ vs ç”Ÿæˆ)\n",
    "â€¢ ä¸åŒæŸ¥è©¢é¡žåž‹éœ€è¦ä¸åŒçš„è©•ä¼°ç­–ç•¥å’ŒæŒ‡æ¨™\n",
    "â€¢ è‡ªå‹•åŒ–è©•ä¼°æµç¨‹å°æ–¼æŒçºŒæ”¹é€² RAG ç³»çµ±è‡³é—œé‡è¦\n",
    "â€¢ åŸºæº–è³‡æ–™é›†çš„å“è³ªç›´æŽ¥å½±éŸ¿è©•ä¼°çµæžœçš„å¯é æ€§\n",
    "\n",
    "Smoke Test Status: {'âœ… PASSED' if smoke_test_passed else 'âŒ FAILED'}\n",
    "\"\"\"\n",
    "\n",
    "print(completion_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71b0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SMOKE TEST CELL (Run this to verify everything works)\n",
    "# é©—æ”¶æ¸¬è©¦ (é‹è¡Œæ­¤ Cell é©—è­‰æ‰€æœ‰åŠŸèƒ½)\n",
    "\n",
    "print(\"\\nðŸš€ FINAL SMOKE TEST - Run this cell to verify the complete system\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def comprehensive_smoke_test():\n",
    "    \"\"\"Comprehensive test of the entire evaluation system\"\"\"\n",
    "\n",
    "    try:\n",
    "        # 1. Quick dataset setup\n",
    "        print(\"\\n1. Setting up test dataset...\")\n",
    "        test_dm = EvaluationDatasetManager()\n",
    "        test_data = test_dm.create_synthetic_qa_dataset(size=5)\n",
    "\n",
    "        # 2. Initialize all evaluators\n",
    "        print(\"2. Initializing evaluators...\")\n",
    "        ret_eval = RetrievalEvaluator()\n",
    "        gen_eval = GenerationEvaluator()\n",
    "        rag_eval = RAGEvaluator(ret_eval, gen_eval)\n",
    "\n",
    "        # 3. Index documents\n",
    "        print(\"3. Indexing documents...\")\n",
    "        ret_eval.index_documents(test_dm.documents)\n",
    "\n",
    "        # 4. Simple generator for testing\n",
    "        def test_generator(query, context):\n",
    "            return f\"Based on the context: {context.split('.')[0]}.\"\n",
    "\n",
    "        # 5. Run evaluation\n",
    "        print(\"4. Running RAG evaluation...\")\n",
    "        results = rag_eval.evaluate_rag_system(\n",
    "            test_data.select([0]), test_dm.documents, test_generator, k_values=[1, 3]\n",
    "        )\n",
    "\n",
    "        # 6. Generate report\n",
    "        print(\"5. Generating evaluation report...\")\n",
    "        reporter = EvaluationReporter()\n",
    "        reporter.print_results_summary(results)\n",
    "\n",
    "        print(\"\\nâœ… COMPREHENSIVE SMOKE TEST PASSED!\")\n",
    "        print(\"ðŸŽ¯ The evaluation system is ready for production use.\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ SMOKE TEST FAILED: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run the comprehensive test\n",
    "final_test_result = comprehensive_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e238e",
   "metadata": {},
   "source": [
    "\n",
    "## 6. æœ¬ç« å°çµ\n",
    "\n",
    "### âœ… å®Œæˆé …ç›®\n",
    "- **åˆ†é›¢å¼è©•ä¼°æž¶æ§‹**ï¼šå¯¦ä½œäº†æª¢ç´¢å™¨èˆ‡ç”Ÿæˆå™¨çš„ç¨ç«‹è©•ä¼°èƒ½åŠ›\n",
    "- **å…¨é¢è©•ä¼°æŒ‡æ¨™**ï¼šåŒ…å« Recall@k, Precision@k, MRR, NDCG (æª¢ç´¢) åŠ Faithfulness, Groundedness, Attribution (ç”Ÿæˆ)\n",
    "- **ç«¯åˆ°ç«¯è©•ä¼°æµç¨‹**ï¼šæ•´åˆæª¢ç´¢èˆ‡ç”Ÿæˆçš„æ•´é«”ç³»çµ±è©•ä¼°ç®¡ç·š  \n",
    "- **é€²éšŽåˆ†æžåŠŸèƒ½**ï¼šæŸ¥è©¢é¡žåž‹åˆ†æžã€æ¶ˆèžç ”ç©¶ã€ä¿¡å¿ƒå€é–“è¨ˆç®—\n",
    "- **è‡ªå‹•åŒ–å ±å‘Šç”Ÿæˆ**ï¼šè¦–è¦ºåŒ–çµæžœå±•ç¤ºèˆ‡æ”¹é€²å»ºè­°ç”Ÿæˆ\n",
    "### ðŸ” æ ¸å¿ƒæ¦‚å¿µèˆ‡åŽŸç†\n",
    "\n",
    "**1. åˆ†é›¢å¼è©•ä¼° (Decoupled Evaluation)**\n",
    "- **æª¢ç´¢è©•ä¼°**ï¼šç¨ç«‹è¡¡é‡æ–‡æª”æª¢ç´¢çš„æº–ç¢ºæ€§å’Œç›¸é—œæ€§ï¼Œä¸å—ç”Ÿæˆå“è³ªå½±éŸ¿\n",
    "- **ç”Ÿæˆè©•ä¼°**ï¼šå°ˆæ³¨æ–¼æ–‡æœ¬ç”Ÿæˆçš„äº‹å¯¦æ€§ã€ä¸€è‡´æ€§å’Œå¯æ­¸å› æ€§\n",
    "- **å„ªå‹¢**ï¼šèƒ½ç²¾ç¢ºå®šä½ç³»çµ±ç“¶é ¸ï¼Œé‡å°æ€§æ”¹é€²ç‰¹å®šçµ„ä»¶\n",
    "\n",
    "**2. æª¢ç´¢è©•ä¼°æŒ‡æ¨™é«”ç³»**\n",
    "- **Recall@k**ï¼šè¡¡é‡æª¢ç´¢åˆ°çš„ç›¸é—œæ–‡æª”è¦†è“‹çŽ‡\n",
    "- **Precision@k**ï¼šè¡¡é‡æª¢ç´¢çµæžœä¸­ç›¸é—œæ–‡æª”çš„æ¯”ä¾‹\n",
    "- **MRR (Mean Reciprocal Rank)**ï¼šé—œæ³¨ç¬¬ä¸€å€‹ç›¸é—œæ–‡æª”çš„æŽ’åºä½ç½®\n",
    "- **NDCG@k**ï¼šè€ƒæ…®æŽ’åºè³ªé‡çš„æ­¸ä¸€åŒ–æŠ˜æ‰£ç´¯ç©å¢žç›Š\n",
    "\n",
    "**3. ç”Ÿæˆè©•ä¼°ç¶­åº¦**\n",
    "- **Faithfulness (å¿ å¯¦æ€§)**ï¼šç”Ÿæˆå…§å®¹èˆ‡æºæ–‡æª”çš„ä¸€è‡´æ€§\n",
    "- **Groundedness (åŸºæ–¼æ€§)**ï¼šå›žç­”æ˜¯å¦æœ‰å……åˆ†çš„æ–‡æª”æ”¯æ’\n",
    "- **Attribution (å¯æ­¸å› æ€§)**ï¼šèƒ½å¦è¿½æº¯åˆ°å…·é«”çš„æºæ–‡æª”ä½ç½®\n",
    "\n",
    "**4. ç«¯åˆ°ç«¯è©•ä¼°å“²å­¸**\n",
    "- **è¤‡åˆæŒ‡æ¨™è¨­è¨ˆ**ï¼šæ¬Šè¡¡æª¢ç´¢å¬å›žçŽ‡èˆ‡ç”Ÿæˆè³ªé‡\n",
    "- **æ¥­å‹™å°é½Š**ï¼šè©•ä¼°æŒ‡æ¨™æ‡‰åæ˜ å¯¦éš›æ‡‰ç”¨å ´æ™¯çš„æˆåŠŸæ¨™æº–\n",
    "\n",
    "### âš ï¸ å¸¸è¦‹é™·é˜±èˆ‡æ³¨æ„äº‹é …\n",
    "\n",
    "**1. è©•ä¼°è³‡æ–™é›†åå·®**\n",
    "- **å•é¡Œ**ï¼šåˆæˆè³‡æ–™é›†å¯èƒ½ä¸åæ˜ çœŸå¯¦æŸ¥è©¢åˆ†å¸ƒ\n",
    "- **è§£æ±º**ï¼šçµåˆå¤šç¨®è³‡æ–™ä¾†æºï¼Œå®šæœŸç”¨çœŸå¯¦ç”¨æˆ¶æŸ¥è©¢æ›´æ–°è©•ä¼°é›†\n",
    "\n",
    "**2. æŒ‡æ¨™éŽåº¦å„ªåŒ–**\n",
    "- **å•é¡Œ**ï¼šå–®ä¸€æŒ‡æ¨™å„ªåŒ–å¯èƒ½æå®³æ•´é«”ç³»çµ±æ€§èƒ½\n",
    "- **è§£æ±º**ï¼šä½¿ç”¨å¤šç¶­æŒ‡æ¨™çµ„åˆï¼Œé—œæ³¨æŒ‡æ¨™é–“çš„å¹³è¡¡\n",
    "\n",
    "**3. è¨ˆç®—è³‡æºæ¶ˆè€—**\n",
    "- **å•é¡Œ**ï¼šå¤§è¦æ¨¡è©•ä¼°å¯èƒ½æ¶ˆè€—å¤§é‡ GPU è¨˜æ†¶é«”\n",
    "- **è§£æ±º**ï¼šä½¿ç”¨ 4bit é‡åŒ–ã€æ‰¹æ¬¡è™•ç†ã€åˆ†éšŽæ®µè©•ä¼°\n",
    "\n",
    "**4. ç”Ÿæˆè©•ä¼°çš„ä¸»è§€æ€§**\n",
    "- **å•é¡Œ**ï¼šäº‹å¯¦æ€§å’Œæµæš¢æ€§è©•ä¼°å­˜åœ¨ä¸»è§€åˆ¤æ–·\n",
    "- **è§£æ±º**ï¼šçµåˆè‡ªå‹•åŒ–æŒ‡æ¨™èˆ‡äººå·¥è©•ä¼°ï¼Œå»ºç«‹æ¨™æº–è©•ä¼°æº–å‰‡\n",
    "\n",
    "### ðŸŽ¯ ä¸‹ä¸€æ­¥å»ºè­°èˆ‡å»¶ä¼¸æ–¹å‘\n",
    "\n",
    "**ç«‹å³å¯è¡Œçš„æ”¹é€² (æœ¬é€±å…§)**\n",
    "1. **æ“´å±•è©•ä¼°è³‡æ–™é›†**ï¼šæ•´åˆæ›´å¤šé ˜åŸŸçš„æ¨™æº– QA è³‡æ–™é›† (SQuAD, Natural Questions)\n",
    "2. **å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨**ï¼šå¯¦ä½œæ‰¹æ¬¡è©•ä¼°å’Œæ¢¯åº¦ç´¯ç©ä»¥æ”¯æ´æ›´å¤§è¦æ¨¡æ¸¬è©¦\n",
    "3. **å¢žå¼·å ±å‘ŠåŠŸèƒ½**ï¼šæ·»åŠ  LaTeX/PDF å ±å‘Šå°Žå‡ºå’Œ Slack é€šçŸ¥æ•´åˆ\n",
    "\n",
    "**ä¸­æœŸç™¼å±•ç›®æ¨™ (æœªä¾† 2-4 é€±)**\n",
    "1. **å¤šèªžè¨€è©•ä¼°æ”¯æ´**ï¼šæ“´å±•åˆ°ä¸­æ–‡ RAG ç³»çµ±è©•ä¼°ï¼Œä½¿ç”¨ä¸­æ–‡ç‰¹åŒ–æŒ‡æ¨™\n",
    "2. **åœ¨ç·šè©•ä¼°ç›£æŽ§**ï¼šå»ºç«‹æŒçºŒè©•ä¼°ç®¡ç·šï¼Œç›£æŽ§ç”Ÿç”¢ç³»çµ±æ€§èƒ½é€€åŒ–\n",
    "3. **å°æŠ—æ€§è©•ä¼°**ï¼šæ·»åŠ  prompt injectionã€å¹»è¦ºæª¢æ¸¬ç­‰å®‰å…¨æ€§è©•ä¼°\n",
    "\n",
    "**é•·æœŸè¦åŠƒ (1-2 å€‹æœˆ)**\n",
    "1. **äººæ©Ÿå”ä½œè©•ä¼°**ï¼šæ•´åˆäººå·¥æ¨™è¨»å’Œçœ¾åŒ…è©•ä¼°æµç¨‹\n",
    "2. **å› æžœåˆ†æž**ï¼šå»ºç«‹çµ„ä»¶é–“æ€§èƒ½ç›¸é—œæ€§åˆ†æžï¼Œç†è§£æª¢ç´¢-ç”Ÿæˆè€¦åˆæ•ˆæ‡‰  \n",
    "3. **è‡ªé©æ‡‰è©•ä¼°**ï¼šæ ¹æ“šç³»çµ±æ€§èƒ½è‡ªå‹•èª¿æ•´è©•ä¼°ç­–ç•¥å’ŒæŒ‡æ¨™æ¬Šé‡\n",
    "\n",
    "## ðŸ”„ èˆ‡å…¶ä»– Notebook çš„é€£æŽ¥\n",
    "\n",
    "**å‰ç½®ä¾è³´**\n",
    "- `nb26_rag_basic_faiss.ipynb`ï¼šåŸºç¤Ž RAG ç³»çµ±å¯¦ä½œ\n",
    "- `nb13_function_calling_tools.ipynb`ï¼šå·¥å…·ä½¿ç”¨å’Œå‡½æ•¸èª¿ç”¨\n",
    "\n",
    "**å¾ŒçºŒå»ºè­°é †åº**\n",
    "1. **nb29_multi_agent_collaboration.ipynb**ï¼šå¤šä»£ç†å”ä½œç³»çµ± (åˆ©ç”¨æœ¬ç« è©•ä¼°æ–¹æ³•æ¸¬è©¦å¤šä»£ç†æ€§èƒ½)\n",
    "2. **nb30_auto_pipeline_endtoend.ipynb**ï¼šè‡ªå‹•åŒ–ç«¯åˆ°ç«¯æµç¨‹ (æ•´åˆè©•ä¼°ä½œç‚ºå“è³ªé–€æŽ§)\n",
    "3. **nb25_domain_specific_tuning.ipynb**ï¼šé ˜åŸŸç‰¹å®šå¾®èª¿ (ä½¿ç”¨è©•ä¼°ç³»çµ±é©—è­‰å¾®èª¿æ•ˆæžœ)\n",
    "\n",
    "**å¯é¸ä¸¦è¡Œé–‹ç™¼**\n",
    "- **nb24_dpo_vs_rlhf.ipynb**ï¼šåå¥½å„ªåŒ–æ–¹æ³• (å¯ä½¿ç”¨ç”Ÿæˆè©•ä¼°æŒ‡æ¨™ä½œç‚ºè‡ªå‹•åŒ–åå¥½ä¿¡è™Ÿ)\n",
    "- **nb31_gradio_chat_ui.ipynb**ï¼šWeb UI é–‹ç™¼ (æ•´åˆè©•ä¼°çµæžœå±•ç¤ºåŠŸèƒ½)\n",
    "\n",
    "## ðŸ¤” éšŽæ®µæ€§æ±ºç­–é»ž\n",
    "\n",
    "åŸºæ–¼æœ¬ç« å®Œæˆçš„è©•ä¼°ç³»çµ±ï¼Œä½ ç¾åœ¨æœ‰å¹¾å€‹ç™¼å±•æ–¹å‘å¯ä»¥é¸æ“‡ï¼š\n",
    "\n",
    "### é¸é … Aï¼šå¤šä»£ç†å”ä½œ (nb29) \n",
    "**å„ªå‹¢**ï¼šå¯ä»¥ç«‹å³ä½¿ç”¨å‰›å»ºç«‹çš„è©•ä¼°ç³»çµ±æ¸¬è©¦å¤šä»£ç†æ€§èƒ½\n",
    "**é©åˆ**ï¼šå¸Œæœ›å¿«é€Ÿçœ‹åˆ°è¤‡é›œç³»çµ±æ•´åˆæ•ˆæžœ\n",
    "**æŠ•å…¥**ï¼šä¸­ç­‰ï¼Œä¸»è¦æ˜¯æž¶æ§‹è¨­è¨ˆ\n",
    "\n",
    "### é¸é … Bï¼šé ˜åŸŸå¾®èª¿ (nb25)\n",
    "**å„ªå‹¢**ï¼šè©•ä¼°ç³»çµ±å¯ä»¥é‡åŒ–å¾®èª¿å‰å¾Œçš„æ•ˆæžœå·®ç•°\n",
    "**é©åˆ**ï¼šå¸Œæœ›æ·±å…¥ç†è§£æ¨¡åž‹å„ªåŒ–æŠ€è¡“\n",
    "**æŠ•å…¥**ï¼šè¼ƒé«˜ï¼Œéœ€è¦è¨“ç·´è³‡æº\n",
    "\n",
    "### é¸é … Cï¼šè‡ªå‹•åŒ–æµç¨‹ (nb30)\n",
    "**å„ªå‹¢**ï¼šæ‰“é€ å®Œæ•´çš„ç”Ÿç”¢ç´š RAG ç³»çµ±\n",
    "**é©åˆ**ï¼šæ³¨é‡å¯¦ç”¨æ€§å’Œéƒ¨ç½²å°±ç·’åº¦\n",
    "**æŠ•å…¥**ï¼šä¸­ç­‰ï¼Œåé‡å·¥ç¨‹æ•´åˆ\n",
    "\n",
    "**æˆ‘çš„å»ºè­°é †åº**ï¼šnb29 â†’ nb30 â†’ nb25ï¼Œç†ç”±æ˜¯å…ˆå»ºç«‹å®Œæ•´çš„ç³»çµ±æž¶æ§‹ï¼Œå†é€²è¡Œæ¨¡åž‹å±¤é¢çš„å„ªåŒ–ã€‚\n",
    "\n",
    "ä½ å¸Œæœ›æŽ¥ä¸‹ä¾†å°ˆæ³¨å“ªå€‹æ–¹å‘ï¼Ÿæˆ–è€…ä½ æœ‰å…¶ä»–å„ªå…ˆè€ƒæ…®çš„ notebook ä¸»é¡Œï¼Ÿ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
