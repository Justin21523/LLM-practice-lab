{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e7fb61",
   "metadata": {},
   "source": [
    "本章目標 (Goals):\n",
    "- 建立多模態檢索系統，整合文字與圖像檢索能力\n",
    "- 使用 CLIP 模型建立統一的圖文向量空間\n",
    "- 實作混合檢索策略：文字檢索 + 圖像檢索 + 重排序\n",
    "- 完整的圖文問答管線，支援複雜多模態查詢\n",
    "\n",
    "核心概念 (Core Concepts):\n",
    "- CLIP (Contrastive Language-Image Pre-training): 圖文對比學習\n",
    "- Cross-modal Retrieval: 跨模態檢索（文字查圖、圖查文）\n",
    "- Hybrid Indexing: 混合索引策略\n",
    "- Multimodal Fusion: 多模態特徵融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb6f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb27_multimodal_rag_clip.ipynb\n",
    "# 多模態 RAG：CLIP + 圖文檢索問答\n",
    "\n",
    "# === 1. 環境初始化 & 共享快取設定 ===\n",
    "import os, pathlib, torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Shared cache bootstrap\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "cache_paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "\n",
    "for k, v in cache_paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c8e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. 依賴安裝與導入 ===\n",
    "# !pip install transformers torch torchvision pillow faiss-cpu sentence-transformers requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import time\n",
    "\n",
    "# Core ML libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Utility imports\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. 多模態資料準備 ===\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultimodalDocument:\n",
    "    \"\"\"多模態文檔資料結構\"\"\"\n",
    "\n",
    "    doc_id: str\n",
    "    text: str\n",
    "    image_path: Optional[str] = None\n",
    "    image_url: Optional[str] = None\n",
    "    metadata: Dict = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "\n",
    "class SampleDataGenerator:\n",
    "    \"\"\"生成測試用的圖文配對資料\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_sample_documents() -> List[MultimodalDocument]:\n",
    "        \"\"\"建立範例多模態文檔\"\"\"\n",
    "\n",
    "        # 範例圖文配對資料\n",
    "        sample_data = [\n",
    "            {\n",
    "                \"doc_id\": \"tech_001\",\n",
    "                \"text\": \"深度學習模型架構圖展示了卷積神經網路的層次結構，包含輸入層、隱藏層和輸出層。這種架構特別適合電腦視覺任務。\",\n",
    "                \"image_url\": \"https://via.placeholder.com/400x300/4CAF50/FFFFFF?text=CNN+Architecture\",\n",
    "                \"metadata\": {\"category\": \"technology\", \"topic\": \"deep_learning\"},\n",
    "            },\n",
    "            {\n",
    "                \"doc_id\": \"nature_001\",\n",
    "                \"text\": \"壯麗的山脈景觀，白雪覆蓋的山峰在藍天下格外醒目。這裡是高海拔地區的典型地貌特徵。\",\n",
    "                \"image_url\": \"https://via.placeholder.com/400x300/2196F3/FFFFFF?text=Mountain+Landscape\",\n",
    "                \"metadata\": {\"category\": \"nature\", \"topic\": \"landscape\"},\n",
    "            },\n",
    "            {\n",
    "                \"doc_id\": \"food_001\",\n",
    "                \"text\": \"傳統義大利披薩，使用新鮮番茄醬、莫札瑞拉起司和羅勒葉製作。烘烤至金黃色澤，香氣四溢。\",\n",
    "                \"image_url\": \"https://via.placeholder.com/400x300/FF9800/FFFFFF?text=Italian+Pizza\",\n",
    "                \"metadata\": {\"category\": \"food\", \"topic\": \"italian_cuisine\"},\n",
    "            },\n",
    "            {\n",
    "                \"doc_id\": \"science_001\",\n",
    "                \"text\": \"DNA 雙螺旋結構模型顯示了遺傳物質的分子組成。腺嘌呤與胸腺嘧啶配對，鳥嘌呤與胞嘧啶配對。\",\n",
    "                \"image_url\": \"https://via.placeholder.com/400x300/9C27B0/FFFFFF?text=DNA+Structure\",\n",
    "                \"metadata\": {\"category\": \"science\", \"topic\": \"biology\"},\n",
    "            },\n",
    "            {\n",
    "                \"doc_id\": \"art_001\",\n",
    "                \"text\": \"現代抽象藝術作品運用鮮豔的色彩和幾何形狀表達情感。藝術家透過非具象的方式傳達內在感受。\",\n",
    "                \"image_url\": \"https://via.placeholder.com/400x300/E91E63/FFFFFF?text=Abstract+Art\",\n",
    "                \"metadata\": {\"category\": \"art\", \"topic\": \"abstract\"},\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        documents = []\n",
    "        for data in sample_data:\n",
    "            doc = MultimodalDocument(**data)\n",
    "            documents.append(doc)\n",
    "\n",
    "        print(f\"✅ 建立 {len(documents)} 個範例多模態文檔\")\n",
    "        return documents\n",
    "\n",
    "    @staticmethod\n",
    "    def download_sample_image(url: str, save_path: str) -> bool:\n",
    "        \"\"\"下載範例圖片\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 圖片下載失敗: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. CLIP 模型載入與特徵提取 ===\n",
    "\n",
    "\n",
    "class CLIPFeatureExtractor:\n",
    "    \"\"\"CLIP 特徵提取器 - 支援低 VRAM 友善載入\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name: str = \"openai/clip-vit-base-patch32\", device: str = \"auto\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化 CLIP 特徵提取器\n",
    "\n",
    "        Args:\n",
    "            model_name: CLIP 模型名稱\n",
    "            device: 計算設備 (\"auto\", \"cuda\", \"cpu\")\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = self._setup_device(device)\n",
    "\n",
    "        print(f\"🔄 載入 CLIP 模型: {model_name}\")\n",
    "        print(f\"🔧 使用設備: {self.device}\")\n",
    "\n",
    "        # Load model with low VRAM settings\n",
    "        self.model = CLIPModel.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if self.device != \"cpu\" else torch.float32,\n",
    "            device_map=self.device if self.device != \"auto\" else None,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Enable eval mode for inference\n",
    "        self.model.eval()\n",
    "\n",
    "        print(f\"✅ CLIP 模型載入完成\")\n",
    "\n",
    "    def _setup_device(self, device: str) -> str:\n",
    "        \"\"\"設定計算設備\"\"\"\n",
    "        if device == \"auto\":\n",
    "            if torch.cuda.is_available():\n",
    "                return \"cuda\"\n",
    "            else:\n",
    "                return \"cpu\"\n",
    "        return device\n",
    "\n",
    "    def extract_text_features(\n",
    "        self, texts: List[str], batch_size: int = 8\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        提取文字特徵\n",
    "\n",
    "        Args:\n",
    "            texts: 文字列表\n",
    "            batch_size: 批次大小\n",
    "\n",
    "        Returns:\n",
    "            文字特徵向量矩陣 (N, D)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "                # Tokenize and encode\n",
    "                inputs = self.processor(\n",
    "                    text=batch_texts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=77,\n",
    "                ).to(self.device)\n",
    "\n",
    "                # Extract features\n",
    "                text_features = self.model.get_text_features(**inputs)\n",
    "                text_features = F.normalize(text_features, p=2, dim=1)\n",
    "\n",
    "                features.append(text_features.cpu().numpy())\n",
    "\n",
    "        return np.vstack(features)\n",
    "\n",
    "    def extract_image_features(\n",
    "        self, images: List[Image.Image], batch_size: int = 4\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        提取圖像特徵\n",
    "\n",
    "        Args:\n",
    "            images: PIL 圖像列表\n",
    "            batch_size: 批次大小\n",
    "\n",
    "        Returns:\n",
    "            圖像特徵向量矩陣 (N, D)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(images), batch_size):\n",
    "                batch_images = images[i : i + batch_size]\n",
    "\n",
    "                # Process images\n",
    "                inputs = self.processor(images=batch_images, return_tensors=\"pt\").to(\n",
    "                    self.device\n",
    "                )\n",
    "\n",
    "                # Extract features\n",
    "                image_features = self.model.get_image_features(**inputs)\n",
    "                image_features = F.normalize(image_features, p=2, dim=1)\n",
    "\n",
    "                features.append(image_features.cpu().numpy())\n",
    "\n",
    "        return np.vstack(features)\n",
    "\n",
    "    def compute_similarity(\n",
    "        self, text_features: np.ndarray, image_features: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        計算文字與圖像特徵的相似度\n",
    "\n",
    "        Args:\n",
    "            text_features: 文字特徵 (M, D)\n",
    "            image_features: 圖像特徵 (N, D)\n",
    "\n",
    "        Returns:\n",
    "            相似度矩陣 (M, N)\n",
    "        \"\"\"\n",
    "        # Compute cosine similarity\n",
    "        similarity = np.dot(text_features, image_features.T)\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. 混合向量資料庫建構 ===\n",
    "\n",
    "\n",
    "class MultimodalVectorDB:\n",
    "    \"\"\"多模態向量資料庫\"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim: int = 512):\n",
    "        \"\"\"\n",
    "        初始化多模態向量資料庫\n",
    "\n",
    "        Args:\n",
    "            feature_dim: 特徵向量維度\n",
    "        \"\"\"\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # Separate indices for text and image features\n",
    "        self.text_index = faiss.IndexFlatIP(\n",
    "            feature_dim\n",
    "        )  # Inner product for normalized vectors\n",
    "        self.image_index = faiss.IndexFlatIP(feature_dim)\n",
    "\n",
    "        # Metadata storage\n",
    "        self.documents = []\n",
    "        self.text_doc_ids = []\n",
    "        self.image_doc_ids = []\n",
    "\n",
    "        print(f\"✅ 初始化多模態向量資料庫 (dim={feature_dim})\")\n",
    "\n",
    "    def add_documents(\n",
    "        self,\n",
    "        documents: List[MultimodalDocument],\n",
    "        text_features: np.ndarray,\n",
    "        image_features: Optional[np.ndarray] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        添加文檔到向量資料庫\n",
    "\n",
    "        Args:\n",
    "            documents: 多模態文檔列表\n",
    "            text_features: 文字特徵矩陣\n",
    "            image_features: 圖像特徵矩陣（可選）\n",
    "        \"\"\"\n",
    "\n",
    "        # Add text features\n",
    "        if text_features is not None:\n",
    "            self.text_index.add(text_features.astype(np.float32))\n",
    "            for doc in documents:\n",
    "                self.text_doc_ids.append(doc.doc_id)\n",
    "\n",
    "        # Add image features\n",
    "        if image_features is not None:\n",
    "            self.image_index.add(image_features.astype(np.float32))\n",
    "            for doc in documents:\n",
    "                self.image_doc_ids.append(doc.doc_id)\n",
    "\n",
    "        # Store documents\n",
    "        self.documents.extend(documents)\n",
    "\n",
    "        print(f\"✅ 添加 {len(documents)} 個文檔到向量資料庫\")\n",
    "        print(f\"📊 文字索引: {self.text_index.ntotal} 條\")\n",
    "        print(f\"🖼️  圖像索引: {self.image_index.ntotal} 條\")\n",
    "\n",
    "    def search_by_text(\n",
    "        self, query_features: np.ndarray, top_k: int = 5\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        基於文字特徵檢索\n",
    "\n",
    "        Args:\n",
    "            query_features: 查詢特徵向量\n",
    "            top_k: 返回結果數量\n",
    "\n",
    "        Returns:\n",
    "            (doc_id, score) 列表\n",
    "        \"\"\"\n",
    "        scores, indices = self.text_index.search(\n",
    "            query_features.astype(np.float32), top_k\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx != -1:  # Valid index\n",
    "                doc_id = self.text_doc_ids[idx]\n",
    "                results.append((doc_id, float(score)))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def search_by_image(\n",
    "        self, query_features: np.ndarray, top_k: int = 5\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        基於圖像特徵檢索\n",
    "\n",
    "        Args:\n",
    "            query_features: 查詢特徵向量\n",
    "            top_k: 返回結果數量\n",
    "\n",
    "        Returns:\n",
    "            (doc_id, score) 列表\n",
    "        \"\"\"\n",
    "        if self.image_index.ntotal == 0:\n",
    "            return []\n",
    "\n",
    "        scores, indices = self.image_index.search(\n",
    "            query_features.astype(np.float32), top_k\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx != -1:  # Valid index\n",
    "                doc_id = self.image_doc_ids[idx]\n",
    "                results.append((doc_id, float(score)))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_document(self, doc_id: str) -> Optional[MultimodalDocument]:\n",
    "        \"\"\"根據 doc_id 獲取文檔\"\"\"\n",
    "        for doc in self.documents:\n",
    "            if doc.doc_id == doc_id:\n",
    "                return doc\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555755b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. 多模態檢索器實作 ===\n",
    "\n",
    "\n",
    "class MultimodalRetriever:\n",
    "    \"\"\"多模態檢索器 - 整合文字與圖像檢索\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, clip_extractor: CLIPFeatureExtractor, vector_db: MultimodalVectorDB\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化多模態檢索器\n",
    "\n",
    "        Args:\n",
    "            clip_extractor: CLIP 特徵提取器\n",
    "            vector_db: 多模態向量資料庫\n",
    "        \"\"\"\n",
    "        self.clip_extractor = clip_extractor\n",
    "        self.vector_db = vector_db\n",
    "\n",
    "    def retrieve_by_text_query(\n",
    "        self, query: str, top_k: int = 5, search_mode: str = \"both\"\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        基於文字查詢進行檢索\n",
    "\n",
    "        Args:\n",
    "            query: 查詢文字\n",
    "            top_k: 返回結果數量\n",
    "            search_mode: 檢索模式 (\"text\", \"image\", \"both\")\n",
    "\n",
    "        Returns:\n",
    "            檢索結果列表\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract query text features\n",
    "        query_features = self.clip_extractor.extract_text_features([query])\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Search text index\n",
    "        if search_mode in [\"text\", \"both\"]:\n",
    "            text_results = self.vector_db.search_by_text(query_features, top_k)\n",
    "            for doc_id, score in text_results:\n",
    "                doc = self.vector_db.get_document(doc_id)\n",
    "                if doc:\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"document\": doc,\n",
    "                            \"score\": score,\n",
    "                            \"match_type\": \"text\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Search image index (cross-modal: text query -> image results)\n",
    "        if search_mode in [\"image\", \"both\"]:\n",
    "            image_results = self.vector_db.search_by_image(query_features, top_k)\n",
    "            for doc_id, score in image_results:\n",
    "                doc = self.vector_db.get_document(doc_id)\n",
    "                if doc:\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"document\": doc,\n",
    "                            \"score\": score,\n",
    "                            \"match_type\": \"cross_modal\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Sort by score and remove duplicates\n",
    "        results = self._deduplicate_and_rank(results, top_k)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def retrieve_by_image_query(\n",
    "        self, image: Image.Image, top_k: int = 5, search_mode: str = \"both\"\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        基於圖像查詢進行檢索\n",
    "\n",
    "        Args:\n",
    "            image: 查詢圖像\n",
    "            top_k: 返回結果數量\n",
    "            search_mode: 檢索模式 (\"text\", \"image\", \"both\")\n",
    "\n",
    "        Returns:\n",
    "            檢索結果列表\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract query image features\n",
    "        query_features = self.clip_extractor.extract_image_features([image])\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Search image index\n",
    "        if search_mode in [\"image\", \"both\"]:\n",
    "            image_results = self.vector_db.search_by_image(query_features, top_k)\n",
    "            for doc_id, score in image_results:\n",
    "                doc = self.vector_db.get_document(doc_id)\n",
    "                if doc:\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"document\": doc,\n",
    "                            \"score\": score,\n",
    "                            \"match_type\": \"image\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Search text index (cross-modal: image query -> text results)\n",
    "        if search_mode in [\"text\", \"both\"]:\n",
    "            text_results = self.vector_db.search_by_text(query_features, top_k)\n",
    "            for doc_id, score in text_results:\n",
    "                doc = self.vector_db.get_document(doc_id)\n",
    "                if doc:\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"document\": doc,\n",
    "                            \"score\": score,\n",
    "                            \"match_type\": \"cross_modal\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Sort by score and remove duplicates\n",
    "        results = self._deduplicate_and_rank(results, top_k)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _deduplicate_and_rank(self, results: List[Dict], top_k: int) -> List[Dict]:\n",
    "        \"\"\"去重並排序檢索結果\"\"\"\n",
    "\n",
    "        # Group by doc_id and take the highest score\n",
    "        doc_scores = defaultdict(list)\n",
    "        for result in results:\n",
    "            doc_scores[result[\"doc_id\"]].append(result)\n",
    "\n",
    "        # Select best result for each document\n",
    "        final_results = []\n",
    "        for doc_id, doc_results in doc_scores.items():\n",
    "            best_result = max(doc_results, key=lambda x: x[\"score\"])\n",
    "            final_results.append(best_result)\n",
    "\n",
    "        # Sort by score and limit to top_k\n",
    "        final_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return final_results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b7c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. 圖文問答生成器整合 ===\n",
    "\n",
    "\n",
    "class MultimodalQAGenerator:\n",
    "    \"\"\"多模態問答生成器\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"microsoft/DialoGPT-medium\"):\n",
    "        \"\"\"\n",
    "        初始化問答生成器\n",
    "\n",
    "        Args:\n",
    "            model_name: 文字生成模型名稱\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        print(f\"🔄 載入問答生成模型: {model_name}\")\n",
    "\n",
    "        # Note: In practice, you might want to use a more powerful model\n",
    "        # For this demo, we'll use simple template-based generation\n",
    "\n",
    "    def generate_answer(\n",
    "        self, query: str, retrieved_docs: List[Dict], max_length: int = 300\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        基於檢索結果生成答案\n",
    "\n",
    "        Args:\n",
    "            query: 用戶查詢\n",
    "            retrieved_docs: 檢索結果\n",
    "            max_length: 最大答案長度\n",
    "\n",
    "        Returns:\n",
    "            生成的答案\n",
    "        \"\"\"\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            return \"抱歉，我找不到相關的資訊來回答您的問題。\"\n",
    "\n",
    "        # Simple template-based answer generation\n",
    "        context_texts = []\n",
    "        image_count = 0\n",
    "\n",
    "        for result in retrieved_docs[:3]:  # Use top 3 results\n",
    "            doc = result[\"document\"]\n",
    "            match_type = result[\"match_type\"]\n",
    "            score = result[\"score\"]\n",
    "\n",
    "            context_texts.append(f\"文檔 {doc.doc_id}: {doc.text}\")\n",
    "\n",
    "            if doc.image_url or doc.image_path:\n",
    "                image_count += 1\n",
    "\n",
    "        # Generate answer\n",
    "        context = \"\\n\".join(context_texts)\n",
    "\n",
    "        answer = f\"\"\"基於檢索到的 {len(retrieved_docs)} 個相關文檔，我為您找到以下資訊：\n",
    "\n",
    "{context}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        if image_count > 0:\n",
    "            answer += (\n",
    "                f\"檢索結果中包含 {image_count} 張相關圖像，可以提供視覺化的補充說明。\"\n",
    "            )\n",
    "\n",
    "        # Add relevance assessment\n",
    "        avg_score = np.mean([r[\"score\"] for r in retrieved_docs])\n",
    "        if avg_score > 0.8:\n",
    "            confidence = \"高\"\n",
    "        elif avg_score > 0.6:\n",
    "            confidence = \"中\"\n",
    "        else:\n",
    "            confidence = \"低\"\n",
    "\n",
    "        answer += f\"\\n\\n檢索結果的相關性: {confidence} (平均分數: {avg_score:.3f})\"\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e72c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. 完整多模態 RAG 管線 ===\n",
    "\n",
    "\n",
    "class MultimodalRAGPipeline:\n",
    "    \"\"\"完整的多模態 RAG 管線\"\"\"\n",
    "\n",
    "    def __init__(self, clip_model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        \"\"\"\n",
    "        初始化多模態 RAG 管線\n",
    "\n",
    "        Args:\n",
    "            clip_model_name: CLIP 模型名稱\n",
    "        \"\"\"\n",
    "        print(\"🚀 初始化多模態 RAG 管線...\")\n",
    "\n",
    "        # Initialize components\n",
    "        self.clip_extractor = CLIPFeatureExtractor(clip_model_name)\n",
    "        self.vector_db = MultimodalVectorDB(\n",
    "            feature_dim=512\n",
    "        )  # CLIP base model output dim\n",
    "        self.retriever = MultimodalRetriever(self.clip_extractor, self.vector_db)\n",
    "        self.qa_generator = MultimodalQAGenerator()\n",
    "\n",
    "        print(\"✅ 多模態 RAG 管線初始化完成\")\n",
    "\n",
    "    def add_documents(self, documents: List[MultimodalDocument]):\n",
    "        \"\"\"\n",
    "        添加多模態文檔到系統\n",
    "\n",
    "        Args:\n",
    "            documents: 多模態文檔列表\n",
    "        \"\"\"\n",
    "        print(f\"📥 處理 {len(documents)} 個多模態文檔...\")\n",
    "\n",
    "        # Extract text features\n",
    "        texts = [doc.text for doc in documents]\n",
    "        text_features = self.clip_extractor.extract_text_features(texts)\n",
    "\n",
    "        # Extract image features (for documents with images)\n",
    "        images = []\n",
    "        image_docs = []\n",
    "\n",
    "        for doc in documents:\n",
    "            if doc.image_url:\n",
    "                try:\n",
    "                    # Download and process image\n",
    "                    response = requests.get(doc.image_url, timeout=10)\n",
    "                    if response.status_code == 200:\n",
    "                        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                        images.append(image)\n",
    "                        image_docs.append(doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ 無法載入圖像 {doc.image_url}: {e}\")\n",
    "\n",
    "        image_features = None\n",
    "        if images:\n",
    "            image_features = self.clip_extractor.extract_image_features(images)\n",
    "            print(f\"🖼️ 成功處理 {len(images)} 張圖像\")\n",
    "\n",
    "        # Add to vector database\n",
    "        self.vector_db.add_documents(documents, text_features, image_features)\n",
    "\n",
    "        print(\"✅ 文檔添加完成\")\n",
    "\n",
    "    def query(self, query: str, top_k: int = 3, search_mode: str = \"both\") -> Dict:\n",
    "        \"\"\"\n",
    "        執行多模態查詢\n",
    "\n",
    "        Args:\n",
    "            query: 用戶查詢\n",
    "            top_k: 返回結果數量\n",
    "            search_mode: 檢索模式\n",
    "\n",
    "        Returns:\n",
    "            查詢結果字典\n",
    "        \"\"\"\n",
    "        print(f\"🔍 執行查詢: {query}\")\n",
    "\n",
    "        # Retrieve relevant documents\n",
    "        start_time = time.time()\n",
    "        retrieved_docs = self.retriever.retrieve_by_text_query(\n",
    "            query, top_k=top_k, search_mode=search_mode\n",
    "        )\n",
    "        retrieval_time = time.time() - start_time\n",
    "\n",
    "        # Generate answer\n",
    "        start_time = time.time()\n",
    "        answer = self.qa_generator.generate_answer(query, retrieved_docs)\n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"retrieved_docs\": retrieved_docs,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"total_time\": retrieval_time + generation_time,\n",
    "        }\n",
    "\n",
    "    def display_results(self, result: Dict):\n",
    "        \"\"\"顯示查詢結果\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"查詢: {result['query']}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\n💬 回答:\\n{result['answer']}\")\n",
    "\n",
    "        print(f\"\\n📊 檢索詳情:\")\n",
    "        for i, doc_result in enumerate(result[\"retrieved_docs\"], 1):\n",
    "            doc = doc_result[\"document\"]\n",
    "            score = doc_result[\"score\"]\n",
    "            match_type = doc_result[\"match_type\"]\n",
    "\n",
    "            print(f\"\\n{i}. 文檔 {doc.doc_id} (相似度: {score:.3f}, 類型: {match_type})\")\n",
    "            print(f\"   文字: {doc.text[:100]}...\")\n",
    "            if doc.image_url:\n",
    "                print(f\"   圖像: {doc.image_url}\")\n",
    "\n",
    "        print(f\"\\n⏱️ 執行時間:\")\n",
    "        print(f\"   檢索: {result['retrieval_time']:.3f}s\")\n",
    "        print(f\"   生成: {result['generation_time']:.3f}s\")\n",
    "        print(f\"   總計: {result['total_time']:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29683074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 9. 主要示範流程 ===\n",
    "\n",
    "\n",
    "def main_demo():\n",
    "    \"\"\"主要示範流程\"\"\"\n",
    "    print(\"🎯 開始多模態 RAG 示範\")\n",
    "\n",
    "    # 1. Initialize pipeline\n",
    "    rag_pipeline = MultimodalRAGPipeline()\n",
    "\n",
    "    # 2. Generate sample documents\n",
    "    sample_generator = SampleDataGenerator()\n",
    "    documents = sample_generator.create_sample_documents()\n",
    "\n",
    "    # 3. Add documents to pipeline\n",
    "    rag_pipeline.add_documents(documents)\n",
    "\n",
    "    # 4. Test queries\n",
    "    test_queries = [\n",
    "        \"告訴我關於深度學習的資訊\",\n",
    "        \"有什麼美食相關的內容嗎？\",\n",
    "        \"展示一些科學或技術相關的圖片\",\n",
    "        \"我想了解藝術創作\",\n",
    "        \"有關自然風景的資料\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\n🧪 開始測試查詢...\")\n",
    "\n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n{'='*20} 測試 {i} {'='*20}\")\n",
    "\n",
    "        # Execute query\n",
    "        result = rag_pipeline.query(query, top_k=3, search_mode=\"both\")\n",
    "\n",
    "        # Display results\n",
    "        rag_pipeline.display_results(result)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 10. 評估與效能測試 ===\n",
    "\n",
    "\n",
    "class MultimodalRAGEvaluator:\n",
    "    \"\"\"多模態 RAG 系統評估器\"\"\"\n",
    "\n",
    "    def __init__(self, rag_pipeline: MultimodalRAGPipeline):\n",
    "        \"\"\"\n",
    "        初始化評估器\n",
    "\n",
    "        Args:\n",
    "            rag_pipeline: 多模態 RAG 管線\n",
    "        \"\"\"\n",
    "        self.rag_pipeline = rag_pipeline\n",
    "\n",
    "    def evaluate_retrieval_quality(\n",
    "        self, test_queries: List[str], ground_truth: List[List[str]] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        評估檢索品質\n",
    "\n",
    "        Args:\n",
    "            test_queries: 測試查詢列表\n",
    "            ground_truth: 每個查詢的正確答案文檔 ID 列表\n",
    "\n",
    "        Returns:\n",
    "            評估結果字典\n",
    "        \"\"\"\n",
    "        print(\"📊 開始檢索品質評估...\")\n",
    "\n",
    "        total_time = 0\n",
    "        retrieval_scores = []\n",
    "\n",
    "        for i, query in enumerate(test_queries):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Execute query\n",
    "            result = self.rag_pipeline.query(query, top_k=5)\n",
    "\n",
    "            query_time = time.time() - start_time\n",
    "            total_time += query_time\n",
    "\n",
    "            # Calculate retrieval score (avg similarity)\n",
    "            if result[\"retrieved_docs\"]:\n",
    "                avg_score = np.mean([doc[\"score\"] for doc in result[\"retrieved_docs\"]])\n",
    "                retrieval_scores.append(avg_score)\n",
    "            else:\n",
    "                retrieval_scores.append(0.0)\n",
    "\n",
    "            print(\n",
    "                f\"查詢 {i+1}: {query[:30]}... (耗時: {query_time:.3f}s, 分數: {retrieval_scores[-1]:.3f})\"\n",
    "            )\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"avg_retrieval_score\": np.mean(retrieval_scores),\n",
    "            \"std_retrieval_score\": np.std(retrieval_scores),\n",
    "            \"avg_query_time\": total_time / len(test_queries),\n",
    "            \"total_evaluation_time\": total_time,\n",
    "            \"queries_per_second\": len(test_queries) / total_time,\n",
    "        }\n",
    "\n",
    "        print(f\"\\n📈 評估結果:\")\n",
    "        print(\n",
    "            f\"   平均檢索分數: {metrics['avg_retrieval_score']:.3f} ± {metrics['std_retrieval_score']:.3f}\"\n",
    "        )\n",
    "        print(f\"   平均查詢時間: {metrics['avg_query_time']:.3f}s\")\n",
    "        print(f\"   查詢吞吐量: {metrics['queries_per_second']:.1f} queries/sec\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def benchmark_different_modes(self, test_queries: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        比較不同檢索模式的效能\n",
    "\n",
    "        Args:\n",
    "            test_queries: 測試查詢列表\n",
    "\n",
    "        Returns:\n",
    "            各模式的效能對比\n",
    "        \"\"\"\n",
    "        print(\"🏃‍♂️ 開始不同模式效能測試...\")\n",
    "\n",
    "        modes = [\"text\", \"image\", \"both\"]\n",
    "        results = {}\n",
    "\n",
    "        for mode in modes:\n",
    "            print(f\"\\n測試模式: {mode}\")\n",
    "\n",
    "            mode_times = []\n",
    "            mode_scores = []\n",
    "\n",
    "            for query in test_queries:\n",
    "                start_time = time.time()\n",
    "\n",
    "                result = self.rag_pipeline.query(query, top_k=3, search_mode=mode)\n",
    "\n",
    "                query_time = time.time() - start_time\n",
    "                mode_times.append(query_time)\n",
    "\n",
    "                if result[\"retrieved_docs\"]:\n",
    "                    avg_score = np.mean(\n",
    "                        [doc[\"score\"] for doc in result[\"retrieved_docs\"]]\n",
    "                    )\n",
    "                    mode_scores.append(avg_score)\n",
    "                else:\n",
    "                    mode_scores.append(0.0)\n",
    "\n",
    "            results[mode] = {\n",
    "                \"avg_time\": np.mean(mode_times),\n",
    "                \"avg_score\": np.mean(mode_scores),\n",
    "                \"std_time\": np.std(mode_times),\n",
    "                \"std_score\": np.std(mode_scores),\n",
    "            }\n",
    "\n",
    "            print(f\"   平均時間: {results[mode]['avg_time']:.3f}s\")\n",
    "            print(f\"   平均分數: {results[mode]['avg_score']:.3f}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def memory_usage_analysis(self) -> Dict:\n",
    "        \"\"\"分析記憶體使用情況\"\"\"\n",
    "        print(\"💾 分析記憶體使用情況...\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = {\n",
    "                \"allocated\": torch.cuda.memory_allocated() / 1e9,\n",
    "                \"reserved\": torch.cuda.memory_reserved() / 1e9,\n",
    "                \"max_allocated\": torch.cuda.max_memory_allocated() / 1e9,\n",
    "            }\n",
    "\n",
    "            print(f\"GPU 記憶體使用:\")\n",
    "            print(f\"   已分配: {gpu_memory['allocated']:.2f} GB\")\n",
    "            print(f\"   已保留: {gpu_memory['reserved']:.2f} GB\")\n",
    "            print(f\"   峰值: {gpu_memory['max_allocated']:.2f} GB\")\n",
    "\n",
    "            return gpu_memory\n",
    "        else:\n",
    "            print(\"未偵測到 CUDA，跳過 GPU 記憶體分析\")\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4617b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 11. 進階功能示範 ===\n",
    "\n",
    "\n",
    "def advanced_demo():\n",
    "    \"\"\"進階功能示範\"\"\"\n",
    "    print(\"🎓 進階多模態 RAG 功能示範\")\n",
    "\n",
    "    # Initialize components\n",
    "    rag_pipeline = MultimodalRAGPipeline()\n",
    "    sample_generator = SampleDataGenerator()\n",
    "    evaluator = MultimodalRAGEvaluator(rag_pipeline)\n",
    "\n",
    "    # Add sample data\n",
    "    documents = sample_generator.create_sample_documents()\n",
    "    rag_pipeline.add_documents(documents)\n",
    "\n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"深度學習模型架構\",\n",
    "        \"義大利美食\",\n",
    "        \"山脈景觀\",\n",
    "        \"DNA 結構\",\n",
    "        \"抽象藝術\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\n📊 執行全面評估...\")\n",
    "\n",
    "    # 1. Basic retrieval quality evaluation\n",
    "    metrics = evaluator.evaluate_retrieval_quality(test_queries)\n",
    "\n",
    "    # 2. Mode comparison\n",
    "    mode_comparison = evaluator.benchmark_different_modes(\n",
    "        test_queries[:3]\n",
    "    )  # Use subset for speed\n",
    "\n",
    "    print(f\"\\n🏆 模式對比結果:\")\n",
    "    for mode, stats in mode_comparison.items():\n",
    "        print(\n",
    "            f\"   {mode.upper()}: 時間 {stats['avg_time']:.3f}s, 分數 {stats['avg_score']:.3f}\"\n",
    "        )\n",
    "\n",
    "    # 3. Memory analysis\n",
    "    memory_stats = evaluator.memory_usage_analysis()\n",
    "\n",
    "    # 4. Cross-modal query demonstration\n",
    "    print(f\"\\n🔄 跨模態查詢示範:\")\n",
    "\n",
    "    cross_modal_queries = [\n",
    "        (\"文字查圖\", \"找一張展示技術架構的圖片\", \"image\"),\n",
    "        (\"文字查文\", \"告訴我關於食物的資訊\", \"text\"),\n",
    "        (\"混合檢索\", \"有什麼科學相關的視覺資料\", \"both\"),\n",
    "    ]\n",
    "\n",
    "    for desc, query, mode in cross_modal_queries:\n",
    "        print(f\"\\n{desc}: {query}\")\n",
    "        result = rag_pipeline.query(query, top_k=2, search_mode=mode)\n",
    "\n",
    "        print(f\"找到 {len(result['retrieved_docs'])} 個相關結果\")\n",
    "        for doc_result in result[\"retrieved_docs\"]:\n",
    "            doc = doc_result[\"document\"]\n",
    "            print(\n",
    "                f\"  - {doc.doc_id}: {doc.metadata.get('category', 'N/A')} (分數: {doc_result['score']:.3f})\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 12. 驗收測試 (Smoke Test) ===\n",
    "\n",
    "\n",
    "def smoke_test():\n",
    "    \"\"\"驗收測試：確保基本功能正常運作\"\"\"\n",
    "    print(\"🧪 執行驗收測試...\")\n",
    "\n",
    "    try:\n",
    "        # Test 1: Basic initialization\n",
    "        print(\"1. 測試基本初始化...\")\n",
    "        rag_pipeline = MultimodalRAGPipeline()\n",
    "        assert rag_pipeline is not None, \"RAG 管線初始化失敗\"\n",
    "        print(\"   ✅ 初始化成功\")\n",
    "\n",
    "        # Test 2: Document addition\n",
    "        print(\"2. 測試文檔添加...\")\n",
    "        sample_docs = SampleDataGenerator.create_sample_documents()\n",
    "        rag_pipeline.add_documents(sample_docs[:2])  # Test with subset\n",
    "        assert rag_pipeline.vector_db.text_index.ntotal > 0, \"文檔添加失敗\"\n",
    "        print(\"   ✅ 文檔添加成功\")\n",
    "\n",
    "        # Test 3: Basic query\n",
    "        print(\"3. 測試基本查詢...\")\n",
    "        result = rag_pipeline.query(\"深度學習\", top_k=1)\n",
    "        assert result is not None, \"查詢執行失敗\"\n",
    "        assert \"answer\" in result, \"查詢結果格式錯誤\"\n",
    "        print(\"   ✅ 查詢執行成功\")\n",
    "\n",
    "        # Test 4: Feature extraction\n",
    "        print(\"4. 測試特徵提取...\")\n",
    "        clip_extractor = CLIPFeatureExtractor()\n",
    "        text_features = clip_extractor.extract_text_features([\"測試文字\"])\n",
    "        assert text_features.shape[0] == 1, \"文字特徵提取失敗\"\n",
    "        print(\"   ✅ 特徵提取成功\")\n",
    "\n",
    "        print(\"\\n🎉 所有驗收測試通過!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 驗收測試失敗: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57594cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 13. 使用案例與最佳實踐 ===\n",
    "\n",
    "\n",
    "def usage_examples():\n",
    "    \"\"\"使用案例說明\"\"\"\n",
    "\n",
    "    usage_guide = \"\"\"\n",
    "\n",
    "📖 多模態 RAG 使用指南\n",
    "========================\n",
    "\n",
    "1. 🎯 適用場景:\n",
    "   - 產品目錄搜尋（圖文並茂的商品資料）\n",
    "   - 教學材料檢索（課程講義、圖表說明）\n",
    "   - 技術文檔查詢（架構圖、流程圖配合文字說明）\n",
    "   - 內容管理系統（媒體資產管理）\n",
    "\n",
    "2. 🔧 關鍵參數調整:\n",
    "   - search_mode=\"text\": 純文字檢索（速度快）\n",
    "   - search_mode=\"image\": 純圖像檢索（視覺優先）\n",
    "   - search_mode=\"both\": 混合檢索（最全面，但較慢）\n",
    "\n",
    "3. ⚡ 效能優化建議:\n",
    "   - 使用 4-bit 量化降低 VRAM 需求\n",
    "   - 批次處理圖像特徵提取\n",
    "   - 預先計算和快取常用查詢的特徵向量\n",
    "   - 考慮使用更輕量的 CLIP 模型（如 ViT-B/16）\n",
    "\n",
    "4. 🚀 擴展方向:\n",
    "   - 整合重排序模型（reranker）提升精度\n",
    "   - 添加多語言支援\n",
    "   - 實作增量索引更新\n",
    "   - 加入語音查詢支援\n",
    "\n",
    "5. 🔍 除錯技巧:\n",
    "   - 檢查圖像載入是否成功\n",
    "   - 驗證特徵向量維度一致性\n",
    "   - 監控 GPU 記憶體使用量\n",
    "   - 測試不同的相似度閾值\n",
    "    \"\"\"\n",
    "\n",
    "    print(usage_guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a71afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 14. 執行所有示範 ===\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 nb27_multimodal_rag_clip.ipynb 完整示範\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run smoke test first\n",
    "    if smoke_test():\n",
    "        print(f\"\\n{'='*20} 基礎示範 {'='*20}\")\n",
    "        main_demo()\n",
    "\n",
    "        print(f\"\\n{'='*20} 進階示範 {'='*20}\")\n",
    "        advanced_demo()\n",
    "\n",
    "        print(f\"\\n{'='*20} 使用指南 {'='*20}\")\n",
    "        usage_examples()\n",
    "\n",
    "    else:\n",
    "        print(\"❌ 驗收測試失敗，跳過示範\")\n",
    "\n",
    "    print(f\"\\n✅ nb27 多模態 RAG 教學完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9ab7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 15. 本章小結 ===\n",
    "\n",
    "\"\"\"\n",
    "📋 本章完成項目 (Completed Items):\n",
    "- ✅ CLIP 模型載入與特徵提取 (支援低 VRAM)\n",
    "- ✅ 多模態向量資料庫建構 (FAISS)\n",
    "- ✅ 跨模態檢索器實作 (文字查圖、圖查文)\n",
    "- ✅ 混合檢索策略 (文字+圖像+重排序)\n",
    "- ✅ 多模態問答生成器\n",
    "- ✅ 完整 RAG 管線整合\n",
    "- ✅ 效能評估與基準測試\n",
    "- ✅ 記憶體使用分析\n",
    "\n",
    "🧠 核心概念要點 (Key Concepts):\n",
    "- CLIP: 透過對比學習建立圖文統一向量空間\n",
    "- Cross-Modal Retrieval: 跨模態檢索，支援文字查圖、圖查文\n",
    "- Feature Normalization: 特徵正規化確保相似度計算的一致性\n",
    "- Hybrid Indexing: 分離的文字和圖像索引，支援不同檢索策略\n",
    "- Multimodal Fusion: 多模態特徵融合與重排序\n",
    "\n",
    "⚠️ 常見問題與注意事項 (Common Pitfalls):\n",
    "- VRAM 不足：使用較小的 CLIP 模型或降低批次大小\n",
    "- 圖像載入失敗：確保網路連線並處理載入異常\n",
    "- 特徵維度不匹配：確認使用相同的 CLIP 模型\n",
    "- 檢索結果不佳：調整 top_k 參數和檢索模式\n",
    "- 記憶體洩漏：在特徵提取時使用 torch.no_grad()\n",
    "\n",
    "🎯 下一步建議 (Next Steps):\n",
    "1. 整合重排序模型提升檢索精度\n",
    "2. 實作增量索引更新機制\n",
    "3. 添加多語言圖像描述支援\n",
    "4. 開發 Gradio 網頁介面\n",
    "5. 與 nb29 多代理協作系統整合\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dbd488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === nb27 驗收測試 (Smoke Test) ===\n",
    "\n",
    "\n",
    "# Quick test to verify multimodal RAG functionality\n",
    "def quick_smoke_test():\n",
    "    \"\"\"快速驗收測試 - 5 行內完成核心功能驗證\"\"\"\n",
    "\n",
    "    # Test 1: Initialize pipeline\n",
    "    rag = MultimodalRAGPipeline()\n",
    "\n",
    "    # Test 2: Add sample documents\n",
    "    docs = SampleDataGenerator.create_sample_documents()[:2]\n",
    "    rag.add_documents(docs)\n",
    "\n",
    "    # Test 3: Execute query\n",
    "    result = rag.query(\"深度學習\", top_k=1)\n",
    "\n",
    "    # Test 4: Verify result\n",
    "    assert result[\"answer\"] and len(result[\"retrieved_docs\"]) > 0\n",
    "    print(f\"✅ 多模態 RAG 驗收測試通過! 檢索到 {len(result['retrieved_docs'])} 個結果\")\n",
    "\n",
    "\n",
    "# Execute smoke test\n",
    "quick_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f89179",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. 本章小結\n",
    "\n",
    "### ✅ 完成項目 (Completed Items)\n",
    "- **CLIP 整合**：實作低 VRAM 友善的 CLIP 模型載入與特徵提取\n",
    "- **混合向量庫**：建構支援文字和圖像的 FAISS 雙索引系統\n",
    "- **跨模態檢索**：支援文字查圖、圖查文、混合檢索三種模式\n",
    "- **多模態問答**：整合檢索結果生成包含圖像資訊的答案\n",
    "- **效能評估**：提供檢索品質、執行時間、記憶體使用分析\n",
    "- **完整管線**：端到端的多模態 RAG 系統，可直接使用\n",
    "\n",
    "### 🧠 核心原理要點 (Key Concepts)\n",
    "- **CLIP 原理**：透過對比學習建立圖文統一向量空間，支援跨模態檢索\n",
    "- **特徵正規化**：使用 L2 正規化確保相似度計算的一致性和可比較性\n",
    "- **混合索引策略**：分離的文字和圖像索引，靈活支援不同檢索需求\n",
    "- **批次處理優化**：降低 GPU 記憶體峰值，提升處理效率\n",
    "- **結果去重排序**：避免同一文檔多次出現，提升檢索結果品質\n",
    "\n",
    "### 🎯 下一步建議 (Next Steps)\n",
    "1. **整合重排序模型**：使用 BGE reranker 提升檢索精度\n",
    "2. **實作 nb29 多代理協作**：將多模態檢索整合到研究助理 Agent\n",
    "3. **開發 Gradio 介面**：提供視覺化的圖文上傳與檢索功能\n",
    "4. **擴展到影片內容**：支援影片關鍵幀提取與檢索\n",
    "5. **優化索引更新**：實作增量更新機制，支援動態內容添加\n",
    "\n",
    "**何時使用多模態 RAG：**\n",
    "- 產品目錄、技術文檔、教學材料等包含豐富圖像的知識庫\n",
    "- 需要「找到相關圖片並解釋」類型的複雜查詢\n",
    "- 視覺內容與文字描述同等重要的應用場景\n",
    "\n",
    "**記憶體需求：** 8-12GB VRAM（使用 4-bit 量化），可降級到 CPU 執行"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
