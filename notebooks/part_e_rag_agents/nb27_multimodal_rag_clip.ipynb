{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e7fb61",
   "metadata": {},
   "source": [
    "æœ¬ç« ç›®æ¨™ (Goals):\n",
    "- å»ºç«‹å¤šæ¨¡æ…‹æª¢ç´¢ç³»çµ±ï¼Œæ•´åˆæ–‡å­—èˆ‡åœ–åƒæª¢ç´¢èƒ½åŠ›\n",
    "- ä½¿ç”¨ CLIP æ¨¡å‹å»ºç«‹çµ±ä¸€çš„åœ–æ–‡å‘é‡ç©ºé–“\n",
    "- å¯¦ä½œæ··åˆæª¢ç´¢ç­–ç•¥ï¼šæ–‡å­—æª¢ç´¢ + åœ–åƒæª¢ç´¢ + é‡æ’åº\n",
    "- å®Œæ•´çš„åœ–æ–‡å•ç­”ç®¡ç·šï¼Œæ”¯æ´è¤‡é›œå¤šæ¨¡æ…‹æŸ¥è©¢\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µ (Core Concepts):\n",
    "- CLIP (Contrastive Language-Image Pre-training): åœ–æ–‡å°æ¯”å­¸ç¿’\n",
    "- Cross-modal Retrieval: è·¨æ¨¡æ…‹æª¢ç´¢ï¼ˆæ–‡å­—æŸ¥åœ–ã€åœ–æŸ¥æ–‡ï¼‰\n",
    "- Hybrid Indexing: æ··åˆç´¢å¼•ç­–ç•¥\n",
    "- Multimodal Fusion: å¤šæ¨¡æ…‹ç‰¹å¾µèåˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb6f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb27_multimodal_rag_clip.ipynb\n",
    "# å¤šæ¨¡æ…‹ RAGï¼šCLIP + åœ–æ–‡æª¢ç´¢å•ç­”\n",
    "\n",
    "# === 1. ç’°å¢ƒåˆå§‹åŒ– & å…±äº«å¿«å–è¨­å®š ===\n",
    "import os, pathlib, torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Shared cache bootstrap\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "cache_paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "\n",
    "for k, v in cache_paths.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c8e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. ä¾è³´å®‰è£èˆ‡å°å…¥ ===\n",
    "# !pip install transformers torch torchvision pillow faiss-cpu sentence-transformers requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import time\n",
    "\n",
    "# Core ML libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Utility imports\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. å¤šæ¨¡æ…‹è³‡æ–™æº–å‚™ ===\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultimodalDocument:\n",
    "    \"\"\"å¤šæ¨¡æ…‹æ–‡æª”è³‡æ–™çµæ§‹\"\"\"\n",
    "\n",
    "    doc_id: str\n",
    "    text: str\n",
    "    image_path: Optional[str] = None\n",
    "    image_url: Optional[str] = None\n",
    "    metadata: Dict = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "\n",
    "class SampleDataGenerator:\n",
    "    \"\"\"ç”Ÿæˆæ¸¬è©¦ç”¨çš„åœ–æ–‡é…å°è³‡æ–™\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_sample_documents() -> List[MultimodalDocument]:\n",
    "        \"\"\"å»ºç«‹ç¯„ä¾‹å¤šæ¨¡æ…‹æ–‡æª”\"\"\"\n",
    "\n",
    "        # ç¯„ä¾‹åœ–æ–‡é…å°è³‡æ–™\n",
    "        sample_data = [\n",
    "            {\n",
    "                \"doc_id\": \"tech_001\",\n",
    "                \"text\": \"æ·±åº¦å­¸ç¿’æ¨¡å‹æ¶æ§‹åœ–å±•ç¤ºäº†å·ç©ç¥ç¶“ç¶²è·¯çš„å±¤æ¬¡çµæ§‹ï¼ŒåŒ…å«è¼¸å…¥å±¤ã€éš±è—å±¤å’Œè¼¸å‡ºå±¤ã€‚é€™ç¨®æ¶æ§‹ç‰¹åˆ¥é©åˆé›»è…¦è¦–è¦ºä»»å‹™ã€‚\",\n",
    "                \"image_url\": \"https://via.placeholder.com/400x300/4CAF50/FFFFFF?text=CNN+Architecture\",\n",
    "                \"metadata\": {\"category\": \"technology\", \"topic\": \"deep_learning\"},\n",
    "            },\n",
    "            {\n",
    "                \"doc_id\": \"nature_001\",\n",
    "                \"text\": \"å£¯éº—çš„å±±è„ˆæ™¯è§€ï¼Œç™½é›ªè¦†è“‹çš„å±±å³°åœ¨è—å¤©ä¸‹æ ¼å¤–é†’ç›®ã€‚é€™è£¡æ˜¯é«˜æµ·æ‹”åœ°å€çš„å…¸å‹åœ°è²Œç‰¹å¾µã€‚\",\n",
    "                \"image_url\": \"https://via.placeholder.com/400x300/2196F3/FFFFFF?text=Mountain+Landscape\",\n",
    "                \"metadata\": {\"category\": \"nature\", \"topic\": \"landscape\"},\n",
    "            },\n",
    "            {\n",
    "                \"doc_id\": \"food_001\",\n",
    "                \"text\": \"å‚³çµ±ç¾©å¤§åˆ©æŠ«è–©ï¼Œä½¿ç”¨æ–°é®®ç•ªèŒ„é†¬ã€è«æœ­ç‘æ‹‰èµ·å¸å’Œç¾…å‹’è‘‰è£½ä½œã€‚çƒ˜çƒ¤è‡³é‡‘é»ƒè‰²æ¾¤ï¼Œé¦™æ°£å››æº¢ã€‚\",\n",
    "                \"image_url\": \"https://via.placeholder.com/400x300/FF9800/FFFFFF?text=Italian+Pizza\",\n",
    "                \"metadata\": {\"category\": \"food\", \"topic\": \"italian_cuisine\"},\n",
    "            },\n",
    "            {\n",
    "                \"doc_id\": \"science_001\",\n",
    "                \"text\": \"DNA é›™èºæ—‹çµæ§‹æ¨¡å‹é¡¯ç¤ºäº†éºå‚³ç‰©è³ªçš„åˆ†å­çµ„æˆã€‚è…ºå˜Œå‘¤èˆ‡èƒ¸è…ºå˜§å•¶é…å°ï¼Œé³¥å˜Œå‘¤èˆ‡èƒå˜§å•¶é…å°ã€‚\",\n",
    "                \"image_url\": \"https://via.placeholder.com/400x300/9C27B0/FFFFFF?text=DNA+Structure\",\n",
    "                \"metadata\": {\"category\": \"science\", \"topic\": \"biology\"},\n",
    "            },\n",
    "            {\n",
    "                \"doc_id\": \"art_001\",\n",
    "                \"text\": \"ç¾ä»£æŠ½è±¡è—è¡“ä½œå“é‹ç”¨é®®è±”çš„è‰²å½©å’Œå¹¾ä½•å½¢ç‹€è¡¨é”æƒ…æ„Ÿã€‚è—è¡“å®¶é€ééå…·è±¡çš„æ–¹å¼å‚³é”å…§åœ¨æ„Ÿå—ã€‚\",\n",
    "                \"image_url\": \"https://via.placeholder.com/400x300/E91E63/FFFFFF?text=Abstract+Art\",\n",
    "                \"metadata\": {\"category\": \"art\", \"topic\": \"abstract\"},\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        documents = []\n",
    "        for data in sample_data:\n",
    "            doc = MultimodalDocument(**data)\n",
    "            documents.append(doc)\n",
    "\n",
    "        print(f\"âœ… å»ºç«‹ {len(documents)} å€‹ç¯„ä¾‹å¤šæ¨¡æ…‹æ–‡æª”\")\n",
    "        return documents\n",
    "\n",
    "    @staticmethod\n",
    "    def download_sample_image(url: str, save_path: str) -> bool:\n",
    "        \"\"\"ä¸‹è¼‰ç¯„ä¾‹åœ–ç‰‡\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åœ–ç‰‡ä¸‹è¼‰å¤±æ•—: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. CLIP æ¨¡å‹è¼‰å…¥èˆ‡ç‰¹å¾µæå– ===\n",
    "\n",
    "\n",
    "class CLIPFeatureExtractor:\n",
    "    \"\"\"CLIP ç‰¹å¾µæå–å™¨ - æ”¯æ´ä½ VRAM å‹å–„è¼‰å…¥\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name: str = \"openai/clip-vit-base-patch32\", device: str = \"auto\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– CLIP ç‰¹å¾µæå–å™¨\n",
    "\n",
    "        Args:\n",
    "            model_name: CLIP æ¨¡å‹åç¨±\n",
    "            device: è¨ˆç®—è¨­å‚™ (\"auto\", \"cuda\", \"cpu\")\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = self._setup_device(device)\n",
    "\n",
    "        print(f\"ğŸ”„ è¼‰å…¥ CLIP æ¨¡å‹: {model_name}\")\n",
    "        print(f\"ğŸ”§ ä½¿ç”¨è¨­å‚™: {self.device}\")\n",
    "\n",
    "        # Load model with low VRAM settings\n",
    "        self.model = CLIPModel.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if self.device != \"cpu\" else torch.float32,\n",
    "            device_map=self.device if self.device != \"auto\" else None,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Enable eval mode for inference\n",
    "        self.model.eval()\n",
    "\n",
    "        print(f\"âœ… CLIP æ¨¡å‹è¼‰å…¥å®Œæˆ\")\n",
    "\n",
    "    def _setup_device(self, device: str) -> str:\n",
    "        \"\"\"è¨­å®šè¨ˆç®—è¨­å‚™\"\"\"\n",
    "        if device == \"auto\":\n",
    "            if torch.cuda.is_available():\n",
    "                return \"cuda\"\n",
    "            else:\n",
    "                return \"cpu\"\n",
    "        return device\n",
    "\n",
    "    def extract_text_features(\n",
    "        self, texts: List[str], batch_size: int = 8\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        æå–æ–‡å­—ç‰¹å¾µ\n",
    "\n",
    "        Args:\n",
    "            texts: æ–‡å­—åˆ—è¡¨\n",
    "            batch_size: æ‰¹æ¬¡å¤§å°\n",
    "\n",
    "        Returns:\n",
    "            æ–‡å­—ç‰¹å¾µå‘é‡çŸ©é™£ (N, D)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "                # Tokenize and encode\n",
    "                inputs = self.processor(\n",
    "                    text=batch_texts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=77,\n",
    "                ).to(self.device)\n",
    "\n",
    "                # Extract features\n",
    "                text_features = self.model.get_text_features(**inputs)\n",
    "                text_features = F.normalize(text_features, p=2, dim=1)\n",
    "\n",
    "                features.append(text_features.cpu().numpy())\n",
    "\n",
    "        return np.vstack(features)\n",
    "\n",
    "    def extract_image_features(\n",
    "        self, images: List[Image.Image], batch_size: int = 4\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        æå–åœ–åƒç‰¹å¾µ\n",
    "\n",
    "        Args:\n",
    "            images: PIL åœ–åƒåˆ—è¡¨\n",
    "            batch_size: æ‰¹æ¬¡å¤§å°\n",
    "\n",
    "        Returns:\n",
    "            åœ–åƒç‰¹å¾µå‘é‡çŸ©é™£ (N, D)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(images), batch_size):\n",
    "                batch_images = images[i : i + batch_size]\n",
    "\n",
    "                # Process images\n",
    "                inputs = self.processor(images=batch_images, return_tensors=\"pt\").to(\n",
    "                    self.device\n",
    "                )\n",
    "\n",
    "                # Extract features\n",
    "                image_features = self.model.get_image_features(**inputs)\n",
    "                image_features = F.normalize(image_features, p=2, dim=1)\n",
    "\n",
    "                features.append(image_features.cpu().numpy())\n",
    "\n",
    "        return np.vstack(features)\n",
    "\n",
    "    def compute_similarity(\n",
    "        self, text_features: np.ndarray, image_features: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æ–‡å­—èˆ‡åœ–åƒç‰¹å¾µçš„ç›¸ä¼¼åº¦\n",
    "\n",
    "        Args:\n",
    "            text_features: æ–‡å­—ç‰¹å¾µ (M, D)\n",
    "            image_features: åœ–åƒç‰¹å¾µ (N, D)\n",
    "\n",
    "        Returns:\n",
    "            ç›¸ä¼¼åº¦çŸ©é™£ (M, N)\n",
    "        \"\"\"\n",
    "        # Compute cosine similarity\n",
    "        similarity = np.dot(text_features, image_features.T)\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. æ··åˆå‘é‡è³‡æ–™åº«å»ºæ§‹ ===\n",
    "\n",
    "\n",
    "class MultimodalVectorDB:\n",
    "    \"\"\"å¤šæ¨¡æ…‹å‘é‡è³‡æ–™åº«\"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim: int = 512):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å¤šæ¨¡æ…‹å‘é‡è³‡æ–™åº«\n",
    "\n",
    "        Args:\n",
    "            feature_dim: ç‰¹å¾µå‘é‡ç¶­åº¦\n",
    "        \"\"\"\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # Separate indices for text and image features\n",
    "        self.text_index = faiss.IndexFlatIP(\n",
    "            feature_dim\n",
    "        )  # Inner product for normalized vectors\n",
    "        self.image_index = faiss.IndexFlatIP(feature_dim)\n",
    "\n",
    "        # Metadata storage\n",
    "        self.documents = []\n",
    "        self.text_doc_ids = []\n",
    "        self.image_doc_ids = []\n",
    "\n",
    "        print(f\"âœ… åˆå§‹åŒ–å¤šæ¨¡æ…‹å‘é‡è³‡æ–™åº« (dim={feature_dim})\")\n",
    "\n",
    "    def add_documents(\n",
    "        self,\n",
    "        documents: List[MultimodalDocument],\n",
    "        text_features: np.ndarray,\n",
    "        image_features: Optional[np.ndarray] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        æ·»åŠ æ–‡æª”åˆ°å‘é‡è³‡æ–™åº«\n",
    "\n",
    "        Args:\n",
    "            documents: å¤šæ¨¡æ…‹æ–‡æª”åˆ—è¡¨\n",
    "            text_features: æ–‡å­—ç‰¹å¾µçŸ©é™£\n",
    "            image_features: åœ–åƒç‰¹å¾µçŸ©é™£ï¼ˆå¯é¸ï¼‰\n",
    "        \"\"\"\n",
    "\n",
    "        # Add text features\n",
    "        if text_features is not None:\n",
    "            self.text_index.add(text_features.astype(np.float32))\n",
    "            for doc in documents:\n",
    "                self.text_doc_ids.append(doc.doc_id)\n",
    "\n",
    "        # Add image features\n",
    "        if image_features is not None:\n",
    "            self.image_index.add(image_features.astype(np.float32))\n",
    "            for doc in documents:\n",
    "                self.image_doc_ids.append(doc.doc_id)\n",
    "\n",
    "        # Store documents\n",
    "        self.documents.extend(documents)\n",
    "\n",
    "        print(f\"âœ… æ·»åŠ  {len(documents)} å€‹æ–‡æª”åˆ°å‘é‡è³‡æ–™åº«\")\n",
    "        print(f\"ğŸ“Š æ–‡å­—ç´¢å¼•: {self.text_index.ntotal} æ¢\")\n",
    "        print(f\"ğŸ–¼ï¸  åœ–åƒç´¢å¼•: {self.image_index.ntotal} æ¢\")\n",
    "\n",
    "    def search_by_text(\n",
    "        self, query_features: np.ndarray, top_k: int = 5\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        åŸºæ–¼æ–‡å­—ç‰¹å¾µæª¢ç´¢\n",
    "\n",
    "        Args:\n",
    "            query_features: æŸ¥è©¢ç‰¹å¾µå‘é‡\n",
    "            top_k: è¿”å›çµæœæ•¸é‡\n",
    "\n",
    "        Returns:\n",
    "            (doc_id, score) åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        scores, indices = self.text_index.search(\n",
    "            query_features.astype(np.float32), top_k\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx != -1:  # Valid index\n",
    "                doc_id = self.text_doc_ids[idx]\n",
    "                results.append((doc_id, float(score)))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def search_by_image(\n",
    "        self, query_features: np.ndarray, top_k: int = 5\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        åŸºæ–¼åœ–åƒç‰¹å¾µæª¢ç´¢\n",
    "\n",
    "        Args:\n",
    "            query_features: æŸ¥è©¢ç‰¹å¾µå‘é‡\n",
    "            top_k: è¿”å›çµæœæ•¸é‡\n",
    "\n",
    "        Returns:\n",
    "            (doc_id, score) åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        if self.image_index.ntotal == 0:\n",
    "            return []\n",
    "\n",
    "        scores, indices = self.image_index.search(\n",
    "            query_features.astype(np.float32), top_k\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx != -1:  # Valid index\n",
    "                doc_id = self.image_doc_ids[idx]\n",
    "                results.append((doc_id, float(score)))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_document(self, doc_id: str) -> Optional[MultimodalDocument]:\n",
    "        \"\"\"æ ¹æ“š doc_id ç²å–æ–‡æª”\"\"\"\n",
    "        for doc in self.documents:\n",
    "            if doc.doc_id == doc_id:\n",
    "                return doc\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555755b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. å¤šæ¨¡æ…‹æª¢ç´¢å™¨å¯¦ä½œ ===\n",
    "\n",
    "\n",
    "class MultimodalRetriever:\n",
    "    \"\"\"å¤šæ¨¡æ…‹æª¢ç´¢å™¨ - æ•´åˆæ–‡å­—èˆ‡åœ–åƒæª¢ç´¢\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, clip_extractor: CLIPFeatureExtractor, vector_db: MultimodalVectorDB\n",
    "    ):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å¤šæ¨¡æ…‹æª¢ç´¢å™¨\n",
    "\n",
    "        Args:\n",
    "            clip_extractor: CLIP ç‰¹å¾µæå–å™¨\n",
    "            vector_db: å¤šæ¨¡æ…‹å‘é‡è³‡æ–™åº«\n",
    "        \"\"\"\n",
    "        self.clip_extractor = clip_extractor\n",
    "        self.vector_db = vector_db\n",
    "\n",
    "    def retrieve_by_text_query(\n",
    "        self, query: str, top_k: int = 5, search_mode: str = \"both\"\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        åŸºæ–¼æ–‡å­—æŸ¥è©¢é€²è¡Œæª¢ç´¢\n",
    "\n",
    "        Args:\n",
    "            query: æŸ¥è©¢æ–‡å­—\n",
    "            top_k: è¿”å›çµæœæ•¸é‡\n",
    "            search_mode: æª¢ç´¢æ¨¡å¼ (\"text\", \"image\", \"both\")\n",
    "\n",
    "        Returns:\n",
    "            æª¢ç´¢çµæœåˆ—è¡¨\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract query text features\n",
    "        query_features = self.clip_extractor.extract_text_features([query])\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Search text index\n",
    "        if search_mode in [\"text\", \"both\"]:\n",
    "            text_results = self.vector_db.search_by_text(query_features, top_k)\n",
    "            for doc_id, score in text_results:\n",
    "                doc = self.vector_db.get_document(doc_id)\n",
    "                if doc:\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"document\": doc,\n",
    "                            \"score\": score,\n",
    "                            \"match_type\": \"text\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Search image index (cross-modal: text query -> image results)\n",
    "        if search_mode in [\"image\", \"both\"]:\n",
    "            image_results = self.vector_db.search_by_image(query_features, top_k)\n",
    "            for doc_id, score in image_results:\n",
    "                doc = self.vector_db.get_document(doc_id)\n",
    "                if doc:\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"document\": doc,\n",
    "                            \"score\": score,\n",
    "                            \"match_type\": \"cross_modal\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Sort by score and remove duplicates\n",
    "        results = self._deduplicate_and_rank(results, top_k)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def retrieve_by_image_query(\n",
    "        self, image: Image.Image, top_k: int = 5, search_mode: str = \"both\"\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        åŸºæ–¼åœ–åƒæŸ¥è©¢é€²è¡Œæª¢ç´¢\n",
    "\n",
    "        Args:\n",
    "            image: æŸ¥è©¢åœ–åƒ\n",
    "            top_k: è¿”å›çµæœæ•¸é‡\n",
    "            search_mode: æª¢ç´¢æ¨¡å¼ (\"text\", \"image\", \"both\")\n",
    "\n",
    "        Returns:\n",
    "            æª¢ç´¢çµæœåˆ—è¡¨\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract query image features\n",
    "        query_features = self.clip_extractor.extract_image_features([image])\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Search image index\n",
    "        if search_mode in [\"image\", \"both\"]:\n",
    "            image_results = self.vector_db.search_by_image(query_features, top_k)\n",
    "            for doc_id, score in image_results:\n",
    "                doc = self.vector_db.get_document(doc_id)\n",
    "                if doc:\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"document\": doc,\n",
    "                            \"score\": score,\n",
    "                            \"match_type\": \"image\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Search text index (cross-modal: image query -> text results)\n",
    "        if search_mode in [\"text\", \"both\"]:\n",
    "            text_results = self.vector_db.search_by_text(query_features, top_k)\n",
    "            for doc_id, score in text_results:\n",
    "                doc = self.vector_db.get_document(doc_id)\n",
    "                if doc:\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"document\": doc,\n",
    "                            \"score\": score,\n",
    "                            \"match_type\": \"cross_modal\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Sort by score and remove duplicates\n",
    "        results = self._deduplicate_and_rank(results, top_k)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _deduplicate_and_rank(self, results: List[Dict], top_k: int) -> List[Dict]:\n",
    "        \"\"\"å»é‡ä¸¦æ’åºæª¢ç´¢çµæœ\"\"\"\n",
    "\n",
    "        # Group by doc_id and take the highest score\n",
    "        doc_scores = defaultdict(list)\n",
    "        for result in results:\n",
    "            doc_scores[result[\"doc_id\"]].append(result)\n",
    "\n",
    "        # Select best result for each document\n",
    "        final_results = []\n",
    "        for doc_id, doc_results in doc_scores.items():\n",
    "            best_result = max(doc_results, key=lambda x: x[\"score\"])\n",
    "            final_results.append(best_result)\n",
    "\n",
    "        # Sort by score and limit to top_k\n",
    "        final_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return final_results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b7c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. åœ–æ–‡å•ç­”ç”Ÿæˆå™¨æ•´åˆ ===\n",
    "\n",
    "\n",
    "class MultimodalQAGenerator:\n",
    "    \"\"\"å¤šæ¨¡æ…‹å•ç­”ç”Ÿæˆå™¨\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"microsoft/DialoGPT-medium\"):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å•ç­”ç”Ÿæˆå™¨\n",
    "\n",
    "        Args:\n",
    "            model_name: æ–‡å­—ç”Ÿæˆæ¨¡å‹åç¨±\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        print(f\"ğŸ”„ è¼‰å…¥å•ç­”ç”Ÿæˆæ¨¡å‹: {model_name}\")\n",
    "\n",
    "        # Note: In practice, you might want to use a more powerful model\n",
    "        # For this demo, we'll use simple template-based generation\n",
    "\n",
    "    def generate_answer(\n",
    "        self, query: str, retrieved_docs: List[Dict], max_length: int = 300\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        åŸºæ–¼æª¢ç´¢çµæœç”Ÿæˆç­”æ¡ˆ\n",
    "\n",
    "        Args:\n",
    "            query: ç”¨æˆ¶æŸ¥è©¢\n",
    "            retrieved_docs: æª¢ç´¢çµæœ\n",
    "            max_length: æœ€å¤§ç­”æ¡ˆé•·åº¦\n",
    "\n",
    "        Returns:\n",
    "            ç”Ÿæˆçš„ç­”æ¡ˆ\n",
    "        \"\"\"\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            return \"æŠ±æ­‰ï¼Œæˆ‘æ‰¾ä¸åˆ°ç›¸é—œçš„è³‡è¨Šä¾†å›ç­”æ‚¨çš„å•é¡Œã€‚\"\n",
    "\n",
    "        # Simple template-based answer generation\n",
    "        context_texts = []\n",
    "        image_count = 0\n",
    "\n",
    "        for result in retrieved_docs[:3]:  # Use top 3 results\n",
    "            doc = result[\"document\"]\n",
    "            match_type = result[\"match_type\"]\n",
    "            score = result[\"score\"]\n",
    "\n",
    "            context_texts.append(f\"æ–‡æª” {doc.doc_id}: {doc.text}\")\n",
    "\n",
    "            if doc.image_url or doc.image_path:\n",
    "                image_count += 1\n",
    "\n",
    "        # Generate answer\n",
    "        context = \"\\n\".join(context_texts)\n",
    "\n",
    "        answer = f\"\"\"åŸºæ–¼æª¢ç´¢åˆ°çš„ {len(retrieved_docs)} å€‹ç›¸é—œæ–‡æª”ï¼Œæˆ‘ç‚ºæ‚¨æ‰¾åˆ°ä»¥ä¸‹è³‡è¨Šï¼š\n",
    "\n",
    "{context}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        if image_count > 0:\n",
    "            answer += (\n",
    "                f\"æª¢ç´¢çµæœä¸­åŒ…å« {image_count} å¼µç›¸é—œåœ–åƒï¼Œå¯ä»¥æä¾›è¦–è¦ºåŒ–çš„è£œå……èªªæ˜ã€‚\"\n",
    "            )\n",
    "\n",
    "        # Add relevance assessment\n",
    "        avg_score = np.mean([r[\"score\"] for r in retrieved_docs])\n",
    "        if avg_score > 0.8:\n",
    "            confidence = \"é«˜\"\n",
    "        elif avg_score > 0.6:\n",
    "            confidence = \"ä¸­\"\n",
    "        else:\n",
    "            confidence = \"ä½\"\n",
    "\n",
    "        answer += f\"\\n\\næª¢ç´¢çµæœçš„ç›¸é—œæ€§: {confidence} (å¹³å‡åˆ†æ•¸: {avg_score:.3f})\"\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e72c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. å®Œæ•´å¤šæ¨¡æ…‹ RAG ç®¡ç·š ===\n",
    "\n",
    "\n",
    "class MultimodalRAGPipeline:\n",
    "    \"\"\"å®Œæ•´çš„å¤šæ¨¡æ…‹ RAG ç®¡ç·š\"\"\"\n",
    "\n",
    "    def __init__(self, clip_model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å¤šæ¨¡æ…‹ RAG ç®¡ç·š\n",
    "\n",
    "        Args:\n",
    "            clip_model_name: CLIP æ¨¡å‹åç¨±\n",
    "        \"\"\"\n",
    "        print(\"ğŸš€ åˆå§‹åŒ–å¤šæ¨¡æ…‹ RAG ç®¡ç·š...\")\n",
    "\n",
    "        # Initialize components\n",
    "        self.clip_extractor = CLIPFeatureExtractor(clip_model_name)\n",
    "        self.vector_db = MultimodalVectorDB(\n",
    "            feature_dim=512\n",
    "        )  # CLIP base model output dim\n",
    "        self.retriever = MultimodalRetriever(self.clip_extractor, self.vector_db)\n",
    "        self.qa_generator = MultimodalQAGenerator()\n",
    "\n",
    "        print(\"âœ… å¤šæ¨¡æ…‹ RAG ç®¡ç·šåˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "    def add_documents(self, documents: List[MultimodalDocument]):\n",
    "        \"\"\"\n",
    "        æ·»åŠ å¤šæ¨¡æ…‹æ–‡æª”åˆ°ç³»çµ±\n",
    "\n",
    "        Args:\n",
    "            documents: å¤šæ¨¡æ…‹æ–‡æª”åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ“¥ è™•ç† {len(documents)} å€‹å¤šæ¨¡æ…‹æ–‡æª”...\")\n",
    "\n",
    "        # Extract text features\n",
    "        texts = [doc.text for doc in documents]\n",
    "        text_features = self.clip_extractor.extract_text_features(texts)\n",
    "\n",
    "        # Extract image features (for documents with images)\n",
    "        images = []\n",
    "        image_docs = []\n",
    "\n",
    "        for doc in documents:\n",
    "            if doc.image_url:\n",
    "                try:\n",
    "                    # Download and process image\n",
    "                    response = requests.get(doc.image_url, timeout=10)\n",
    "                    if response.status_code == 200:\n",
    "                        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                        images.append(image)\n",
    "                        image_docs.append(doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ ç„¡æ³•è¼‰å…¥åœ–åƒ {doc.image_url}: {e}\")\n",
    "\n",
    "        image_features = None\n",
    "        if images:\n",
    "            image_features = self.clip_extractor.extract_image_features(images)\n",
    "            print(f\"ğŸ–¼ï¸ æˆåŠŸè™•ç† {len(images)} å¼µåœ–åƒ\")\n",
    "\n",
    "        # Add to vector database\n",
    "        self.vector_db.add_documents(documents, text_features, image_features)\n",
    "\n",
    "        print(\"âœ… æ–‡æª”æ·»åŠ å®Œæˆ\")\n",
    "\n",
    "    def query(self, query: str, top_k: int = 3, search_mode: str = \"both\") -> Dict:\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œå¤šæ¨¡æ…‹æŸ¥è©¢\n",
    "\n",
    "        Args:\n",
    "            query: ç”¨æˆ¶æŸ¥è©¢\n",
    "            top_k: è¿”å›çµæœæ•¸é‡\n",
    "            search_mode: æª¢ç´¢æ¨¡å¼\n",
    "\n",
    "        Returns:\n",
    "            æŸ¥è©¢çµæœå­—å…¸\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ” åŸ·è¡ŒæŸ¥è©¢: {query}\")\n",
    "\n",
    "        # Retrieve relevant documents\n",
    "        start_time = time.time()\n",
    "        retrieved_docs = self.retriever.retrieve_by_text_query(\n",
    "            query, top_k=top_k, search_mode=search_mode\n",
    "        )\n",
    "        retrieval_time = time.time() - start_time\n",
    "\n",
    "        # Generate answer\n",
    "        start_time = time.time()\n",
    "        answer = self.qa_generator.generate_answer(query, retrieved_docs)\n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"retrieved_docs\": retrieved_docs,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"total_time\": retrieval_time + generation_time,\n",
    "        }\n",
    "\n",
    "    def display_results(self, result: Dict):\n",
    "        \"\"\"é¡¯ç¤ºæŸ¥è©¢çµæœ\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"æŸ¥è©¢: {result['query']}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nğŸ’¬ å›ç­”:\\n{result['answer']}\")\n",
    "\n",
    "        print(f\"\\nğŸ“Š æª¢ç´¢è©³æƒ…:\")\n",
    "        for i, doc_result in enumerate(result[\"retrieved_docs\"], 1):\n",
    "            doc = doc_result[\"document\"]\n",
    "            score = doc_result[\"score\"]\n",
    "            match_type = doc_result[\"match_type\"]\n",
    "\n",
    "            print(f\"\\n{i}. æ–‡æª” {doc.doc_id} (ç›¸ä¼¼åº¦: {score:.3f}, é¡å‹: {match_type})\")\n",
    "            print(f\"   æ–‡å­—: {doc.text[:100]}...\")\n",
    "            if doc.image_url:\n",
    "                print(f\"   åœ–åƒ: {doc.image_url}\")\n",
    "\n",
    "        print(f\"\\nâ±ï¸ åŸ·è¡Œæ™‚é–“:\")\n",
    "        print(f\"   æª¢ç´¢: {result['retrieval_time']:.3f}s\")\n",
    "        print(f\"   ç”Ÿæˆ: {result['generation_time']:.3f}s\")\n",
    "        print(f\"   ç¸½è¨ˆ: {result['total_time']:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29683074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 9. ä¸»è¦ç¤ºç¯„æµç¨‹ ===\n",
    "\n",
    "\n",
    "def main_demo():\n",
    "    \"\"\"ä¸»è¦ç¤ºç¯„æµç¨‹\"\"\"\n",
    "    print(\"ğŸ¯ é–‹å§‹å¤šæ¨¡æ…‹ RAG ç¤ºç¯„\")\n",
    "\n",
    "    # 1. Initialize pipeline\n",
    "    rag_pipeline = MultimodalRAGPipeline()\n",
    "\n",
    "    # 2. Generate sample documents\n",
    "    sample_generator = SampleDataGenerator()\n",
    "    documents = sample_generator.create_sample_documents()\n",
    "\n",
    "    # 3. Add documents to pipeline\n",
    "    rag_pipeline.add_documents(documents)\n",
    "\n",
    "    # 4. Test queries\n",
    "    test_queries = [\n",
    "        \"å‘Šè¨´æˆ‘é—œæ–¼æ·±åº¦å­¸ç¿’çš„è³‡è¨Š\",\n",
    "        \"æœ‰ä»€éº¼ç¾é£Ÿç›¸é—œçš„å…§å®¹å—ï¼Ÿ\",\n",
    "        \"å±•ç¤ºä¸€äº›ç§‘å­¸æˆ–æŠ€è¡“ç›¸é—œçš„åœ–ç‰‡\",\n",
    "        \"æˆ‘æƒ³äº†è§£è—è¡“å‰µä½œ\",\n",
    "        \"æœ‰é—œè‡ªç„¶é¢¨æ™¯çš„è³‡æ–™\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\nğŸ§ª é–‹å§‹æ¸¬è©¦æŸ¥è©¢...\")\n",
    "\n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n{'='*20} æ¸¬è©¦ {i} {'='*20}\")\n",
    "\n",
    "        # Execute query\n",
    "        result = rag_pipeline.query(query, top_k=3, search_mode=\"both\")\n",
    "\n",
    "        # Display results\n",
    "        rag_pipeline.display_results(result)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 10. è©•ä¼°èˆ‡æ•ˆèƒ½æ¸¬è©¦ ===\n",
    "\n",
    "\n",
    "class MultimodalRAGEvaluator:\n",
    "    \"\"\"å¤šæ¨¡æ…‹ RAG ç³»çµ±è©•ä¼°å™¨\"\"\"\n",
    "\n",
    "    def __init__(self, rag_pipeline: MultimodalRAGPipeline):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–è©•ä¼°å™¨\n",
    "\n",
    "        Args:\n",
    "            rag_pipeline: å¤šæ¨¡æ…‹ RAG ç®¡ç·š\n",
    "        \"\"\"\n",
    "        self.rag_pipeline = rag_pipeline\n",
    "\n",
    "    def evaluate_retrieval_quality(\n",
    "        self, test_queries: List[str], ground_truth: List[List[str]] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        è©•ä¼°æª¢ç´¢å“è³ª\n",
    "\n",
    "        Args:\n",
    "            test_queries: æ¸¬è©¦æŸ¥è©¢åˆ—è¡¨\n",
    "            ground_truth: æ¯å€‹æŸ¥è©¢çš„æ­£ç¢ºç­”æ¡ˆæ–‡æª” ID åˆ—è¡¨\n",
    "\n",
    "        Returns:\n",
    "            è©•ä¼°çµæœå­—å…¸\n",
    "        \"\"\"\n",
    "        print(\"ğŸ“Š é–‹å§‹æª¢ç´¢å“è³ªè©•ä¼°...\")\n",
    "\n",
    "        total_time = 0\n",
    "        retrieval_scores = []\n",
    "\n",
    "        for i, query in enumerate(test_queries):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Execute query\n",
    "            result = self.rag_pipeline.query(query, top_k=5)\n",
    "\n",
    "            query_time = time.time() - start_time\n",
    "            total_time += query_time\n",
    "\n",
    "            # Calculate retrieval score (avg similarity)\n",
    "            if result[\"retrieved_docs\"]:\n",
    "                avg_score = np.mean([doc[\"score\"] for doc in result[\"retrieved_docs\"]])\n",
    "                retrieval_scores.append(avg_score)\n",
    "            else:\n",
    "                retrieval_scores.append(0.0)\n",
    "\n",
    "            print(\n",
    "                f\"æŸ¥è©¢ {i+1}: {query[:30]}... (è€—æ™‚: {query_time:.3f}s, åˆ†æ•¸: {retrieval_scores[-1]:.3f})\"\n",
    "            )\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"avg_retrieval_score\": np.mean(retrieval_scores),\n",
    "            \"std_retrieval_score\": np.std(retrieval_scores),\n",
    "            \"avg_query_time\": total_time / len(test_queries),\n",
    "            \"total_evaluation_time\": total_time,\n",
    "            \"queries_per_second\": len(test_queries) / total_time,\n",
    "        }\n",
    "\n",
    "        print(f\"\\nğŸ“ˆ è©•ä¼°çµæœ:\")\n",
    "        print(\n",
    "            f\"   å¹³å‡æª¢ç´¢åˆ†æ•¸: {metrics['avg_retrieval_score']:.3f} Â± {metrics['std_retrieval_score']:.3f}\"\n",
    "        )\n",
    "        print(f\"   å¹³å‡æŸ¥è©¢æ™‚é–“: {metrics['avg_query_time']:.3f}s\")\n",
    "        print(f\"   æŸ¥è©¢ååé‡: {metrics['queries_per_second']:.1f} queries/sec\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def benchmark_different_modes(self, test_queries: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        æ¯”è¼ƒä¸åŒæª¢ç´¢æ¨¡å¼çš„æ•ˆèƒ½\n",
    "\n",
    "        Args:\n",
    "            test_queries: æ¸¬è©¦æŸ¥è©¢åˆ—è¡¨\n",
    "\n",
    "        Returns:\n",
    "            å„æ¨¡å¼çš„æ•ˆèƒ½å°æ¯”\n",
    "        \"\"\"\n",
    "        print(\"ğŸƒâ€â™‚ï¸ é–‹å§‹ä¸åŒæ¨¡å¼æ•ˆèƒ½æ¸¬è©¦...\")\n",
    "\n",
    "        modes = [\"text\", \"image\", \"both\"]\n",
    "        results = {}\n",
    "\n",
    "        for mode in modes:\n",
    "            print(f\"\\næ¸¬è©¦æ¨¡å¼: {mode}\")\n",
    "\n",
    "            mode_times = []\n",
    "            mode_scores = []\n",
    "\n",
    "            for query in test_queries:\n",
    "                start_time = time.time()\n",
    "\n",
    "                result = self.rag_pipeline.query(query, top_k=3, search_mode=mode)\n",
    "\n",
    "                query_time = time.time() - start_time\n",
    "                mode_times.append(query_time)\n",
    "\n",
    "                if result[\"retrieved_docs\"]:\n",
    "                    avg_score = np.mean(\n",
    "                        [doc[\"score\"] for doc in result[\"retrieved_docs\"]]\n",
    "                    )\n",
    "                    mode_scores.append(avg_score)\n",
    "                else:\n",
    "                    mode_scores.append(0.0)\n",
    "\n",
    "            results[mode] = {\n",
    "                \"avg_time\": np.mean(mode_times),\n",
    "                \"avg_score\": np.mean(mode_scores),\n",
    "                \"std_time\": np.std(mode_times),\n",
    "                \"std_score\": np.std(mode_scores),\n",
    "            }\n",
    "\n",
    "            print(f\"   å¹³å‡æ™‚é–“: {results[mode]['avg_time']:.3f}s\")\n",
    "            print(f\"   å¹³å‡åˆ†æ•¸: {results[mode]['avg_score']:.3f}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def memory_usage_analysis(self) -> Dict:\n",
    "        \"\"\"åˆ†æè¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³\"\"\"\n",
    "        print(\"ğŸ’¾ åˆ†æè¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³...\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = {\n",
    "                \"allocated\": torch.cuda.memory_allocated() / 1e9,\n",
    "                \"reserved\": torch.cuda.memory_reserved() / 1e9,\n",
    "                \"max_allocated\": torch.cuda.max_memory_allocated() / 1e9,\n",
    "            }\n",
    "\n",
    "            print(f\"GPU è¨˜æ†¶é«”ä½¿ç”¨:\")\n",
    "            print(f\"   å·²åˆ†é…: {gpu_memory['allocated']:.2f} GB\")\n",
    "            print(f\"   å·²ä¿ç•™: {gpu_memory['reserved']:.2f} GB\")\n",
    "            print(f\"   å³°å€¼: {gpu_memory['max_allocated']:.2f} GB\")\n",
    "\n",
    "            return gpu_memory\n",
    "        else:\n",
    "            print(\"æœªåµæ¸¬åˆ° CUDAï¼Œè·³é GPU è¨˜æ†¶é«”åˆ†æ\")\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4617b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 11. é€²éšåŠŸèƒ½ç¤ºç¯„ ===\n",
    "\n",
    "\n",
    "def advanced_demo():\n",
    "    \"\"\"é€²éšåŠŸèƒ½ç¤ºç¯„\"\"\"\n",
    "    print(\"ğŸ“ é€²éšå¤šæ¨¡æ…‹ RAG åŠŸèƒ½ç¤ºç¯„\")\n",
    "\n",
    "    # Initialize components\n",
    "    rag_pipeline = MultimodalRAGPipeline()\n",
    "    sample_generator = SampleDataGenerator()\n",
    "    evaluator = MultimodalRAGEvaluator(rag_pipeline)\n",
    "\n",
    "    # Add sample data\n",
    "    documents = sample_generator.create_sample_documents()\n",
    "    rag_pipeline.add_documents(documents)\n",
    "\n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"æ·±åº¦å­¸ç¿’æ¨¡å‹æ¶æ§‹\",\n",
    "        \"ç¾©å¤§åˆ©ç¾é£Ÿ\",\n",
    "        \"å±±è„ˆæ™¯è§€\",\n",
    "        \"DNA çµæ§‹\",\n",
    "        \"æŠ½è±¡è—è¡“\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\nğŸ“Š åŸ·è¡Œå…¨é¢è©•ä¼°...\")\n",
    "\n",
    "    # 1. Basic retrieval quality evaluation\n",
    "    metrics = evaluator.evaluate_retrieval_quality(test_queries)\n",
    "\n",
    "    # 2. Mode comparison\n",
    "    mode_comparison = evaluator.benchmark_different_modes(\n",
    "        test_queries[:3]\n",
    "    )  # Use subset for speed\n",
    "\n",
    "    print(f\"\\nğŸ† æ¨¡å¼å°æ¯”çµæœ:\")\n",
    "    for mode, stats in mode_comparison.items():\n",
    "        print(\n",
    "            f\"   {mode.upper()}: æ™‚é–“ {stats['avg_time']:.3f}s, åˆ†æ•¸ {stats['avg_score']:.3f}\"\n",
    "        )\n",
    "\n",
    "    # 3. Memory analysis\n",
    "    memory_stats = evaluator.memory_usage_analysis()\n",
    "\n",
    "    # 4. Cross-modal query demonstration\n",
    "    print(f\"\\nğŸ”„ è·¨æ¨¡æ…‹æŸ¥è©¢ç¤ºç¯„:\")\n",
    "\n",
    "    cross_modal_queries = [\n",
    "        (\"æ–‡å­—æŸ¥åœ–\", \"æ‰¾ä¸€å¼µå±•ç¤ºæŠ€è¡“æ¶æ§‹çš„åœ–ç‰‡\", \"image\"),\n",
    "        (\"æ–‡å­—æŸ¥æ–‡\", \"å‘Šè¨´æˆ‘é—œæ–¼é£Ÿç‰©çš„è³‡è¨Š\", \"text\"),\n",
    "        (\"æ··åˆæª¢ç´¢\", \"æœ‰ä»€éº¼ç§‘å­¸ç›¸é—œçš„è¦–è¦ºè³‡æ–™\", \"both\"),\n",
    "    ]\n",
    "\n",
    "    for desc, query, mode in cross_modal_queries:\n",
    "        print(f\"\\n{desc}: {query}\")\n",
    "        result = rag_pipeline.query(query, top_k=2, search_mode=mode)\n",
    "\n",
    "        print(f\"æ‰¾åˆ° {len(result['retrieved_docs'])} å€‹ç›¸é—œçµæœ\")\n",
    "        for doc_result in result[\"retrieved_docs\"]:\n",
    "            doc = doc_result[\"document\"]\n",
    "            print(\n",
    "                f\"  - {doc.doc_id}: {doc.metadata.get('category', 'N/A')} (åˆ†æ•¸: {doc_result['score']:.3f})\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 12. é©—æ”¶æ¸¬è©¦ (Smoke Test) ===\n",
    "\n",
    "\n",
    "def smoke_test():\n",
    "    \"\"\"é©—æ”¶æ¸¬è©¦ï¼šç¢ºä¿åŸºæœ¬åŠŸèƒ½æ­£å¸¸é‹ä½œ\"\"\"\n",
    "    print(\"ğŸ§ª åŸ·è¡Œé©—æ”¶æ¸¬è©¦...\")\n",
    "\n",
    "    try:\n",
    "        # Test 1: Basic initialization\n",
    "        print(\"1. æ¸¬è©¦åŸºæœ¬åˆå§‹åŒ–...\")\n",
    "        rag_pipeline = MultimodalRAGPipeline()\n",
    "        assert rag_pipeline is not None, \"RAG ç®¡ç·šåˆå§‹åŒ–å¤±æ•—\"\n",
    "        print(\"   âœ… åˆå§‹åŒ–æˆåŠŸ\")\n",
    "\n",
    "        # Test 2: Document addition\n",
    "        print(\"2. æ¸¬è©¦æ–‡æª”æ·»åŠ ...\")\n",
    "        sample_docs = SampleDataGenerator.create_sample_documents()\n",
    "        rag_pipeline.add_documents(sample_docs[:2])  # Test with subset\n",
    "        assert rag_pipeline.vector_db.text_index.ntotal > 0, \"æ–‡æª”æ·»åŠ å¤±æ•—\"\n",
    "        print(\"   âœ… æ–‡æª”æ·»åŠ æˆåŠŸ\")\n",
    "\n",
    "        # Test 3: Basic query\n",
    "        print(\"3. æ¸¬è©¦åŸºæœ¬æŸ¥è©¢...\")\n",
    "        result = rag_pipeline.query(\"æ·±åº¦å­¸ç¿’\", top_k=1)\n",
    "        assert result is not None, \"æŸ¥è©¢åŸ·è¡Œå¤±æ•—\"\n",
    "        assert \"answer\" in result, \"æŸ¥è©¢çµæœæ ¼å¼éŒ¯èª¤\"\n",
    "        print(\"   âœ… æŸ¥è©¢åŸ·è¡ŒæˆåŠŸ\")\n",
    "\n",
    "        # Test 4: Feature extraction\n",
    "        print(\"4. æ¸¬è©¦ç‰¹å¾µæå–...\")\n",
    "        clip_extractor = CLIPFeatureExtractor()\n",
    "        text_features = clip_extractor.extract_text_features([\"æ¸¬è©¦æ–‡å­—\"])\n",
    "        assert text_features.shape[0] == 1, \"æ–‡å­—ç‰¹å¾µæå–å¤±æ•—\"\n",
    "        print(\"   âœ… ç‰¹å¾µæå–æˆåŠŸ\")\n",
    "\n",
    "        print(\"\\nğŸ‰ æ‰€æœ‰é©—æ”¶æ¸¬è©¦é€šé!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ é©—æ”¶æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57594cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 13. ä½¿ç”¨æ¡ˆä¾‹èˆ‡æœ€ä½³å¯¦è¸ ===\n",
    "\n",
    "\n",
    "def usage_examples():\n",
    "    \"\"\"ä½¿ç”¨æ¡ˆä¾‹èªªæ˜\"\"\"\n",
    "\n",
    "    usage_guide = \"\"\"\n",
    "\n",
    "ğŸ“– å¤šæ¨¡æ…‹ RAG ä½¿ç”¨æŒ‡å—\n",
    "========================\n",
    "\n",
    "1. ğŸ¯ é©ç”¨å ´æ™¯:\n",
    "   - ç”¢å“ç›®éŒ„æœå°‹ï¼ˆåœ–æ–‡ä¸¦èŒ‚çš„å•†å“è³‡æ–™ï¼‰\n",
    "   - æ•™å­¸ææ–™æª¢ç´¢ï¼ˆèª²ç¨‹è¬›ç¾©ã€åœ–è¡¨èªªæ˜ï¼‰\n",
    "   - æŠ€è¡“æ–‡æª”æŸ¥è©¢ï¼ˆæ¶æ§‹åœ–ã€æµç¨‹åœ–é…åˆæ–‡å­—èªªæ˜ï¼‰\n",
    "   - å…§å®¹ç®¡ç†ç³»çµ±ï¼ˆåª’é«”è³‡ç”¢ç®¡ç†ï¼‰\n",
    "\n",
    "2. ğŸ”§ é—œéµåƒæ•¸èª¿æ•´:\n",
    "   - search_mode=\"text\": ç´”æ–‡å­—æª¢ç´¢ï¼ˆé€Ÿåº¦å¿«ï¼‰\n",
    "   - search_mode=\"image\": ç´”åœ–åƒæª¢ç´¢ï¼ˆè¦–è¦ºå„ªå…ˆï¼‰\n",
    "   - search_mode=\"both\": æ··åˆæª¢ç´¢ï¼ˆæœ€å…¨é¢ï¼Œä½†è¼ƒæ…¢ï¼‰\n",
    "\n",
    "3. âš¡ æ•ˆèƒ½å„ªåŒ–å»ºè­°:\n",
    "   - ä½¿ç”¨ 4-bit é‡åŒ–é™ä½ VRAM éœ€æ±‚\n",
    "   - æ‰¹æ¬¡è™•ç†åœ–åƒç‰¹å¾µæå–\n",
    "   - é å…ˆè¨ˆç®—å’Œå¿«å–å¸¸ç”¨æŸ¥è©¢çš„ç‰¹å¾µå‘é‡\n",
    "   - è€ƒæ…®ä½¿ç”¨æ›´è¼•é‡çš„ CLIP æ¨¡å‹ï¼ˆå¦‚ ViT-B/16ï¼‰\n",
    "\n",
    "4. ğŸš€ æ“´å±•æ–¹å‘:\n",
    "   - æ•´åˆé‡æ’åºæ¨¡å‹ï¼ˆrerankerï¼‰æå‡ç²¾åº¦\n",
    "   - æ·»åŠ å¤šèªè¨€æ”¯æ´\n",
    "   - å¯¦ä½œå¢é‡ç´¢å¼•æ›´æ–°\n",
    "   - åŠ å…¥èªéŸ³æŸ¥è©¢æ”¯æ´\n",
    "\n",
    "5. ğŸ” é™¤éŒ¯æŠ€å·§:\n",
    "   - æª¢æŸ¥åœ–åƒè¼‰å…¥æ˜¯å¦æˆåŠŸ\n",
    "   - é©—è­‰ç‰¹å¾µå‘é‡ç¶­åº¦ä¸€è‡´æ€§\n",
    "   - ç›£æ§ GPU è¨˜æ†¶é«”ä½¿ç”¨é‡\n",
    "   - æ¸¬è©¦ä¸åŒçš„ç›¸ä¼¼åº¦é–¾å€¼\n",
    "    \"\"\"\n",
    "\n",
    "    print(usage_guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a71afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 14. åŸ·è¡Œæ‰€æœ‰ç¤ºç¯„ ===\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ nb27_multimodal_rag_clip.ipynb å®Œæ•´ç¤ºç¯„\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run smoke test first\n",
    "    if smoke_test():\n",
    "        print(f\"\\n{'='*20} åŸºç¤ç¤ºç¯„ {'='*20}\")\n",
    "        main_demo()\n",
    "\n",
    "        print(f\"\\n{'='*20} é€²éšç¤ºç¯„ {'='*20}\")\n",
    "        advanced_demo()\n",
    "\n",
    "        print(f\"\\n{'='*20} ä½¿ç”¨æŒ‡å— {'='*20}\")\n",
    "        usage_examples()\n",
    "\n",
    "    else:\n",
    "        print(\"âŒ é©—æ”¶æ¸¬è©¦å¤±æ•—ï¼Œè·³éç¤ºç¯„\")\n",
    "\n",
    "    print(f\"\\nâœ… nb27 å¤šæ¨¡æ…‹ RAG æ•™å­¸å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9ab7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 15. æœ¬ç« å°çµ ===\n",
    "\n",
    "\"\"\"\n",
    "ğŸ“‹ æœ¬ç« å®Œæˆé …ç›® (Completed Items):\n",
    "- âœ… CLIP æ¨¡å‹è¼‰å…¥èˆ‡ç‰¹å¾µæå– (æ”¯æ´ä½ VRAM)\n",
    "- âœ… å¤šæ¨¡æ…‹å‘é‡è³‡æ–™åº«å»ºæ§‹ (FAISS)\n",
    "- âœ… è·¨æ¨¡æ…‹æª¢ç´¢å™¨å¯¦ä½œ (æ–‡å­—æŸ¥åœ–ã€åœ–æŸ¥æ–‡)\n",
    "- âœ… æ··åˆæª¢ç´¢ç­–ç•¥ (æ–‡å­—+åœ–åƒ+é‡æ’åº)\n",
    "- âœ… å¤šæ¨¡æ…‹å•ç­”ç”Ÿæˆå™¨\n",
    "- âœ… å®Œæ•´ RAG ç®¡ç·šæ•´åˆ\n",
    "- âœ… æ•ˆèƒ½è©•ä¼°èˆ‡åŸºæº–æ¸¬è©¦\n",
    "- âœ… è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ\n",
    "\n",
    "ğŸ§  æ ¸å¿ƒæ¦‚å¿µè¦é» (Key Concepts):\n",
    "- CLIP: é€éå°æ¯”å­¸ç¿’å»ºç«‹åœ–æ–‡çµ±ä¸€å‘é‡ç©ºé–“\n",
    "- Cross-Modal Retrieval: è·¨æ¨¡æ…‹æª¢ç´¢ï¼Œæ”¯æ´æ–‡å­—æŸ¥åœ–ã€åœ–æŸ¥æ–‡\n",
    "- Feature Normalization: ç‰¹å¾µæ­£è¦åŒ–ç¢ºä¿ç›¸ä¼¼åº¦è¨ˆç®—çš„ä¸€è‡´æ€§\n",
    "- Hybrid Indexing: åˆ†é›¢çš„æ–‡å­—å’Œåœ–åƒç´¢å¼•ï¼Œæ”¯æ´ä¸åŒæª¢ç´¢ç­–ç•¥\n",
    "- Multimodal Fusion: å¤šæ¨¡æ…‹ç‰¹å¾µèåˆèˆ‡é‡æ’åº\n",
    "\n",
    "âš ï¸ å¸¸è¦‹å•é¡Œèˆ‡æ³¨æ„äº‹é … (Common Pitfalls):\n",
    "- VRAM ä¸è¶³ï¼šä½¿ç”¨è¼ƒå°çš„ CLIP æ¨¡å‹æˆ–é™ä½æ‰¹æ¬¡å¤§å°\n",
    "- åœ–åƒè¼‰å…¥å¤±æ•—ï¼šç¢ºä¿ç¶²è·¯é€£ç·šä¸¦è™•ç†è¼‰å…¥ç•°å¸¸\n",
    "- ç‰¹å¾µç¶­åº¦ä¸åŒ¹é…ï¼šç¢ºèªä½¿ç”¨ç›¸åŒçš„ CLIP æ¨¡å‹\n",
    "- æª¢ç´¢çµæœä¸ä½³ï¼šèª¿æ•´ top_k åƒæ•¸å’Œæª¢ç´¢æ¨¡å¼\n",
    "- è¨˜æ†¶é«”æ´©æ¼ï¼šåœ¨ç‰¹å¾µæå–æ™‚ä½¿ç”¨ torch.no_grad()\n",
    "\n",
    "ğŸ¯ ä¸‹ä¸€æ­¥å»ºè­° (Next Steps):\n",
    "1. æ•´åˆé‡æ’åºæ¨¡å‹æå‡æª¢ç´¢ç²¾åº¦\n",
    "2. å¯¦ä½œå¢é‡ç´¢å¼•æ›´æ–°æ©Ÿåˆ¶\n",
    "3. æ·»åŠ å¤šèªè¨€åœ–åƒæè¿°æ”¯æ´\n",
    "4. é–‹ç™¼ Gradio ç¶²é ä»‹é¢\n",
    "5. èˆ‡ nb29 å¤šä»£ç†å”ä½œç³»çµ±æ•´åˆ\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dbd488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === nb27 é©—æ”¶æ¸¬è©¦ (Smoke Test) ===\n",
    "\n",
    "\n",
    "# Quick test to verify multimodal RAG functionality\n",
    "def quick_smoke_test():\n",
    "    \"\"\"å¿«é€Ÿé©—æ”¶æ¸¬è©¦ - 5 è¡Œå…§å®Œæˆæ ¸å¿ƒåŠŸèƒ½é©—è­‰\"\"\"\n",
    "\n",
    "    # Test 1: Initialize pipeline\n",
    "    rag = MultimodalRAGPipeline()\n",
    "\n",
    "    # Test 2: Add sample documents\n",
    "    docs = SampleDataGenerator.create_sample_documents()[:2]\n",
    "    rag.add_documents(docs)\n",
    "\n",
    "    # Test 3: Execute query\n",
    "    result = rag.query(\"æ·±åº¦å­¸ç¿’\", top_k=1)\n",
    "\n",
    "    # Test 4: Verify result\n",
    "    assert result[\"answer\"] and len(result[\"retrieved_docs\"]) > 0\n",
    "    print(f\"âœ… å¤šæ¨¡æ…‹ RAG é©—æ”¶æ¸¬è©¦é€šé! æª¢ç´¢åˆ° {len(result['retrieved_docs'])} å€‹çµæœ\")\n",
    "\n",
    "\n",
    "# Execute smoke test\n",
    "quick_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f89179",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. æœ¬ç« å°çµ\n",
    "\n",
    "### âœ… å®Œæˆé …ç›® (Completed Items)\n",
    "- **CLIP æ•´åˆ**ï¼šå¯¦ä½œä½ VRAM å‹å–„çš„ CLIP æ¨¡å‹è¼‰å…¥èˆ‡ç‰¹å¾µæå–\n",
    "- **æ··åˆå‘é‡åº«**ï¼šå»ºæ§‹æ”¯æ´æ–‡å­—å’Œåœ–åƒçš„ FAISS é›™ç´¢å¼•ç³»çµ±\n",
    "- **è·¨æ¨¡æ…‹æª¢ç´¢**ï¼šæ”¯æ´æ–‡å­—æŸ¥åœ–ã€åœ–æŸ¥æ–‡ã€æ··åˆæª¢ç´¢ä¸‰ç¨®æ¨¡å¼\n",
    "- **å¤šæ¨¡æ…‹å•ç­”**ï¼šæ•´åˆæª¢ç´¢çµæœç”ŸæˆåŒ…å«åœ–åƒè³‡è¨Šçš„ç­”æ¡ˆ\n",
    "- **æ•ˆèƒ½è©•ä¼°**ï¼šæä¾›æª¢ç´¢å“è³ªã€åŸ·è¡Œæ™‚é–“ã€è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ\n",
    "- **å®Œæ•´ç®¡ç·š**ï¼šç«¯åˆ°ç«¯çš„å¤šæ¨¡æ…‹ RAG ç³»çµ±ï¼Œå¯ç›´æ¥ä½¿ç”¨\n",
    "\n",
    "### ğŸ§  æ ¸å¿ƒåŸç†è¦é» (Key Concepts)\n",
    "- **CLIP åŸç†**ï¼šé€éå°æ¯”å­¸ç¿’å»ºç«‹åœ–æ–‡çµ±ä¸€å‘é‡ç©ºé–“ï¼Œæ”¯æ´è·¨æ¨¡æ…‹æª¢ç´¢\n",
    "- **ç‰¹å¾µæ­£è¦åŒ–**ï¼šä½¿ç”¨ L2 æ­£è¦åŒ–ç¢ºä¿ç›¸ä¼¼åº¦è¨ˆç®—çš„ä¸€è‡´æ€§å’Œå¯æ¯”è¼ƒæ€§\n",
    "- **æ··åˆç´¢å¼•ç­–ç•¥**ï¼šåˆ†é›¢çš„æ–‡å­—å’Œåœ–åƒç´¢å¼•ï¼Œéˆæ´»æ”¯æ´ä¸åŒæª¢ç´¢éœ€æ±‚\n",
    "- **æ‰¹æ¬¡è™•ç†å„ªåŒ–**ï¼šé™ä½ GPU è¨˜æ†¶é«”å³°å€¼ï¼Œæå‡è™•ç†æ•ˆç‡\n",
    "- **çµæœå»é‡æ’åº**ï¼šé¿å…åŒä¸€æ–‡æª”å¤šæ¬¡å‡ºç¾ï¼Œæå‡æª¢ç´¢çµæœå“è³ª\n",
    "\n",
    "### ğŸ¯ ä¸‹ä¸€æ­¥å»ºè­° (Next Steps)\n",
    "1. **æ•´åˆé‡æ’åºæ¨¡å‹**ï¼šä½¿ç”¨ BGE reranker æå‡æª¢ç´¢ç²¾åº¦\n",
    "2. **å¯¦ä½œ nb29 å¤šä»£ç†å”ä½œ**ï¼šå°‡å¤šæ¨¡æ…‹æª¢ç´¢æ•´åˆåˆ°ç ”ç©¶åŠ©ç† Agent\n",
    "3. **é–‹ç™¼ Gradio ä»‹é¢**ï¼šæä¾›è¦–è¦ºåŒ–çš„åœ–æ–‡ä¸Šå‚³èˆ‡æª¢ç´¢åŠŸèƒ½\n",
    "4. **æ“´å±•åˆ°å½±ç‰‡å…§å®¹**ï¼šæ”¯æ´å½±ç‰‡é—œéµå¹€æå–èˆ‡æª¢ç´¢\n",
    "5. **å„ªåŒ–ç´¢å¼•æ›´æ–°**ï¼šå¯¦ä½œå¢é‡æ›´æ–°æ©Ÿåˆ¶ï¼Œæ”¯æ´å‹•æ…‹å…§å®¹æ·»åŠ \n",
    "\n",
    "**ä½•æ™‚ä½¿ç”¨å¤šæ¨¡æ…‹ RAGï¼š**\n",
    "- ç”¢å“ç›®éŒ„ã€æŠ€è¡“æ–‡æª”ã€æ•™å­¸ææ–™ç­‰åŒ…å«è±å¯Œåœ–åƒçš„çŸ¥è­˜åº«\n",
    "- éœ€è¦ã€Œæ‰¾åˆ°ç›¸é—œåœ–ç‰‡ä¸¦è§£é‡‹ã€é¡å‹çš„è¤‡é›œæŸ¥è©¢\n",
    "- è¦–è¦ºå…§å®¹èˆ‡æ–‡å­—æè¿°åŒç­‰é‡è¦çš„æ‡‰ç”¨å ´æ™¯\n",
    "\n",
    "**è¨˜æ†¶é«”éœ€æ±‚ï¼š** 8-12GB VRAMï¼ˆä½¿ç”¨ 4-bit é‡åŒ–ï¼‰ï¼Œå¯é™ç´šåˆ° CPU åŸ·è¡Œ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
