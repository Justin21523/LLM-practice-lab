{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401df3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 進階大型資料集處理 (Advanced Large-Scale Dataset Processing)\n",
    "# Goal: Master complex preprocessing methods for TB-scale datasets\n",
    "\n",
    "# === Cell 1: Advanced Environment Setup ===\n",
    "import os, pathlib, torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Setup shared model cache\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "cache_paths = {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}\n",
    "\n",
    "for key, path in cache_paths.items():\n",
    "    os.environ[key] = path\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "print(f\"[CPU] Cores: {os.cpu_count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa530c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and install advanced packages\n",
    "try:\n",
    "    import cv2\n",
    "    import albumentations as A\n",
    "    import imagehash\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import noisereduce as nr\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    print(\"✅ All advanced packages available\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Installing missing packages: {e}\")\n",
    "    os.system(\n",
    "        \"pip install opencv-python albumentations imagehash scikit-learn noisereduce\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d32226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Advanced Imports & Large Dataset Loading ===\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from datasets import Features, Value, Image, Audio, Sequence\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoProcessor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import hashlib\n",
    "import json\n",
    "import gc\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "\n",
    "# Memory monitoring utility\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_gb = process.memory_info().rss / 1024 / 1024 / 1024\n",
    "    return f\"{memory_gb:.2f}GB\"\n",
    "\n",
    "\n",
    "# Large dataset streaming with sharding\n",
    "class AdvancedDatasetLoader:\n",
    "    \"\"\"Advanced dataset loader with memory management and sharding\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        split: str = \"train\",\n",
    "        max_shard_size: int = 10000,\n",
    "        streaming: bool = True,\n",
    "    ):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.split = split\n",
    "        self.max_shard_size = max_shard_size\n",
    "        self.streaming = streaming\n",
    "        self.cache_dir = os.environ[\"HF_DATASETS_CACHE\"]\n",
    "\n",
    "    def load_with_sharding(self, num_samples: Optional[int] = None):\n",
    "        \"\"\"Load dataset with automatic sharding for memory efficiency\"\"\"\n",
    "        print(f\"Loading {self.dataset_name} with sharding...\")\n",
    "\n",
    "        if self.streaming:\n",
    "            # Use streaming for very large datasets\n",
    "            dataset = load_dataset(\n",
    "                self.dataset_name,\n",
    "                split=self.split,\n",
    "                streaming=True,\n",
    "                cache_dir=self.cache_dir,\n",
    "            )\n",
    "\n",
    "            if num_samples:\n",
    "                dataset = dataset.take(num_samples)\n",
    "\n",
    "            return dataset\n",
    "        else:\n",
    "            # Load specific slice for manageable datasets\n",
    "            split_slice = f\"{self.split}[:{num_samples}]\" if num_samples else self.split\n",
    "            return load_dataset(\n",
    "                self.dataset_name, split=split_slice, cache_dir=self.cache_dir\n",
    "            )\n",
    "\n",
    "    def create_shards(self, dataset, shard_size: int = None):\n",
    "        \"\"\"Split dataset into manageable shards\"\"\"\n",
    "        shard_size = shard_size or self.max_shard_size\n",
    "        shards = []\n",
    "\n",
    "        current_shard = []\n",
    "        for i, example in enumerate(dataset):\n",
    "            current_shard.append(example)\n",
    "\n",
    "            if len(current_shard) >= shard_size:\n",
    "                shards.append(Dataset.from_list(current_shard))\n",
    "                current_shard = []\n",
    "                print(f\"Created shard {len(shards)}, Memory: {get_memory_usage()}\")\n",
    "\n",
    "        # Add remaining examples\n",
    "        if current_shard:\n",
    "            shards.append(Dataset.from_list(current_shard))\n",
    "\n",
    "        return shards\n",
    "\n",
    "\n",
    "# Demo with manageable datasets\n",
    "print(\"Setting up demo datasets...\")\n",
    "\n",
    "# Text dataset - use a medium-sized one\n",
    "try:\n",
    "    loader = AdvancedDatasetLoader(\"wikitext\", \"train\", streaming=False)\n",
    "    wiki_dataset = loader.load_with_sharding(num_samples=5000)\n",
    "    print(f\"✅ Wiki dataset loaded: {len(wiki_dataset)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Wiki dataset failed, creating synthetic: {e}\")\n",
    "    # Create synthetic text dataset\n",
    "    wiki_dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"text\": [\n",
    "                f\"This is a sample article about topic {i}. \" * 20 for i in range(1000)\n",
    "            ]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ee52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Advanced Text Preprocessing ===\n",
    "print(\"\\n=== Advanced Text Preprocessing ===\")\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class AdvancedTextProcessor:\n",
    "    \"\"\"Advanced text processing with multiple languages and noise handling\"\"\"\n",
    "\n",
    "    def __init__(self, languages: List[str] = [\"en\", \"zh\", \"es\", \"fr\"]):\n",
    "        self.languages = languages\n",
    "        self.stats = defaultdict(int)\n",
    "\n",
    "        # Load language-specific resources\n",
    "        try:\n",
    "            import spacy\n",
    "\n",
    "            self.nlp_models = {}\n",
    "            for lang in languages:\n",
    "                try:\n",
    "                    model_name = {\"en\": \"en_core_web_sm\", \"zh\": \"zh_core_web_sm\"}.get(\n",
    "                        lang, f\"{lang}_core_news_sm\"\n",
    "                    )\n",
    "                    self.nlp_models[lang] = spacy.load(model_name)\n",
    "                except:\n",
    "                    print(f\"⚠️ Spacy model for {lang} not available\")\n",
    "        except ImportError:\n",
    "            print(\"⚠️ Spacy not available, using basic processing\")\n",
    "            self.nlp_models = {}\n",
    "\n",
    "    def detect_language(self, text: str) -> str:\n",
    "        \"\"\"Simple language detection based on character patterns\"\"\"\n",
    "        # Chinese characters\n",
    "        if re.search(r\"[\\u4e00-\\u9fff]\", text):\n",
    "            return \"zh\"\n",
    "        # Arabic characters\n",
    "        elif re.search(r\"[\\u0600-\\u06ff]\", text):\n",
    "            return \"ar\"\n",
    "        # Cyrillic characters\n",
    "        elif re.search(r\"[\\u0400-\\u04ff]\", text):\n",
    "            return \"ru\"\n",
    "        # Spanish specific patterns\n",
    "        elif re.search(r\"[ñáéíóúü]\", text.lower()):\n",
    "            return \"es\"\n",
    "        # French specific patterns\n",
    "        elif re.search(r\"[àâäéèêëïîôùûüÿç]\", text.lower()):\n",
    "            return \"fr\"\n",
    "        else:\n",
    "            return \"en\"\n",
    "\n",
    "    def advanced_clean(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Advanced text cleaning with multiple strategies\"\"\"\n",
    "        original_length = len(text)\n",
    "\n",
    "        # 1. Unicode normalization\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "        # 2. Remove control characters except newlines and tabs\n",
    "        text = \"\".join(\n",
    "            char\n",
    "            for char in text\n",
    "            if unicodedata.category(char) != \"Cc\" or char in \"\\n\\t\"\n",
    "        )\n",
    "\n",
    "        # 3. Fix common encoding issues\n",
    "        text = text.replace(\"\\ufffd\", \"\")  # Remove replacement characters\n",
    "\n",
    "        # 4. Normalize whitespace\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        text = text.strip()\n",
    "\n",
    "        # 5. Remove repeated characters (like \"sooooo\" -> \"so\")\n",
    "        text = re.sub(r\"(.)\\1{3,}\", r\"\\1\\1\", text)\n",
    "\n",
    "        # 6. Language detection\n",
    "        detected_lang = self.detect_language(text)\n",
    "\n",
    "        # 7. Language-specific cleaning\n",
    "        if detected_lang == \"zh\":\n",
    "            # Remove English mixed in Chinese text (if too much)\n",
    "            english_ratio = (\n",
    "                len(re.findall(r\"[a-zA-Z]\", text)) / len(text) if text else 0\n",
    "            )\n",
    "            if english_ratio > 0.5:\n",
    "                text = re.sub(r\"[a-zA-Z]+\", \"\", text)\n",
    "\n",
    "        # 8. Quality metrics\n",
    "        quality_score = self.calculate_quality_score(text, original_length)\n",
    "\n",
    "        self.stats[\"processed\"] += 1\n",
    "        self.stats[f\"lang_{detected_lang}\"] += 1\n",
    "\n",
    "        return {\n",
    "            \"cleaned_text\": text,\n",
    "            \"original_length\": original_length,\n",
    "            \"cleaned_length\": len(text),\n",
    "            \"detected_language\": detected_lang,\n",
    "            \"quality_score\": quality_score,\n",
    "            \"compression_ratio\": (\n",
    "                len(text) / original_length if original_length > 0 else 0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def calculate_quality_score(self, text: str, original_length: int) -> float:\n",
    "        \"\"\"Calculate text quality score (0-1)\"\"\"\n",
    "        if not text or len(text) < 10:\n",
    "            return 0.0\n",
    "\n",
    "        score = 1.0\n",
    "\n",
    "        # Penalize very short or very long texts\n",
    "        if len(text) < 50:\n",
    "            score *= 0.5\n",
    "        elif len(text) > 10000:\n",
    "            score *= 0.8\n",
    "\n",
    "        # Penalize high numbers/special characters ratio\n",
    "        special_ratio = len(re.findall(r\"[^a-zA-Z\\u4e00-\\u9fff\\s]\", text)) / len(text)\n",
    "        if special_ratio > 0.3:\n",
    "            score *= 1 - special_ratio\n",
    "\n",
    "        # Reward proper sentence structure\n",
    "        sentences = text.split(\".\")\n",
    "        if len(sentences) > 1:\n",
    "            score *= min(1.2, 1 + len(sentences) * 0.01)\n",
    "\n",
    "        return min(1.0, score)\n",
    "\n",
    "\n",
    "# Initialize processor and apply to dataset\n",
    "text_processor = AdvancedTextProcessor()\n",
    "\n",
    "\n",
    "def process_text_advanced(examples: Dict[str, List]) -> Dict[str, List]:\n",
    "    \"\"\"Apply advanced text processing to batch\"\"\"\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        processed = text_processor.advanced_clean(text)\n",
    "        for key, value in processed.items():\n",
    "            results[key].append(value)\n",
    "\n",
    "    return dict(results)\n",
    "\n",
    "\n",
    "# Apply advanced processing\n",
    "print(\"Applying advanced text processing...\")\n",
    "start_time = time.time()\n",
    "\n",
    "processed_text_dataset = wiki_dataset.map(\n",
    "    process_text_advanced, batched=True, batch_size=64, num_proc=min(4, os.cpu_count())\n",
    ")\n",
    "\n",
    "processing_time = time.time() - start_time\n",
    "print(f\"Processing completed in {processing_time:.2f}s\")\n",
    "print(f\"Processing stats: {dict(text_processor.stats)}\")\n",
    "\n",
    "# Analyze results\n",
    "quality_scores = processed_text_dataset[\"quality_score\"]\n",
    "lang_distribution = Counter(processed_text_dataset[\"detected_language\"])\n",
    "\n",
    "print(\n",
    "    f\"Quality score stats: mean={np.mean(quality_scores):.3f}, std={np.std(quality_scores):.3f}\"\n",
    ")\n",
    "print(f\"Language distribution: {dict(lang_distribution)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Complex Image Processing Pipeline ===\n",
    "print(\"\\n=== Complex Image Processing Pipeline ===\")\n",
    "\n",
    "\n",
    "class AdvancedImageProcessor:\n",
    "    \"\"\"Advanced image processing with quality control and augmentation\"\"\"\n",
    "\n",
    "    def __init__(self, target_size: Tuple[int, int] = (224, 224)):\n",
    "        self.target_size = target_size\n",
    "        self.stats = defaultdict(int)\n",
    "\n",
    "        # Define augmentation pipeline\n",
    "        self.augmentation_pipeline = A.Compose(\n",
    "            [\n",
    "                A.RandomResizedCrop(\n",
    "                    height=target_size[0], width=target_size[1], scale=(0.8, 1.0)\n",
    "                ),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.OneOf(\n",
    "                    [\n",
    "                        A.MotionBlur(blur_limit=3),\n",
    "                        A.MedianBlur(blur_limit=3),\n",
    "                        A.Blur(blur_limit=3),\n",
    "                    ],\n",
    "                    p=0.3,\n",
    "                ),\n",
    "                A.OneOf(\n",
    "                    [\n",
    "                        A.CLAHE(clip_limit=2),\n",
    "                        A.RandomBrightnessContrast(\n",
    "                            brightness_limit=0.2, contrast_limit=0.2\n",
    "                        ),\n",
    "                        A.RandomGamma(gamma_limit=(80, 120)),\n",
    "                    ],\n",
    "                    p=0.5,\n",
    "                ),\n",
    "                A.HueSaturationValue(\n",
    "                    hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.3\n",
    "                ),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Quality control pipeline\n",
    "        self.quality_pipeline = A.Compose(\n",
    "            [\n",
    "                A.Resize(height=target_size[0], width=target_size[1]),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def detect_image_quality(self, image: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Detect various image quality metrics\"\"\"\n",
    "        if len(image.shape) == 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray = image\n",
    "\n",
    "        # Blur detection using Laplacian variance\n",
    "        blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "        # Brightness analysis\n",
    "        brightness = np.mean(gray)\n",
    "\n",
    "        # Contrast analysis\n",
    "        contrast = np.std(gray)\n",
    "\n",
    "        # Edge density\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "        edge_density = np.sum(edges > 0) / edges.size\n",
    "\n",
    "        # Noise estimation (using high-frequency content)\n",
    "        noise_score = np.std(cv2.GaussianBlur(gray, (5, 5), 0) - gray)\n",
    "\n",
    "        return {\n",
    "            \"blur_score\": float(blur_score),\n",
    "            \"brightness\": float(brightness / 255.0),\n",
    "            \"contrast\": float(contrast / 255.0),\n",
    "            \"edge_density\": float(edge_density),\n",
    "            \"noise_score\": float(noise_score),\n",
    "        }\n",
    "\n",
    "    def calculate_image_hash(self, image: np.ndarray) -> str:\n",
    "        \"\"\"Calculate perceptual hash for duplicate detection\"\"\"\n",
    "        pil_image = Image.fromarray(image) if isinstance(image, np.ndarray) else image\n",
    "        return str(imagehash.phash(pil_image))\n",
    "\n",
    "    def process_image_advanced(\n",
    "        self, image: Union[np.ndarray, Image.Image], apply_augmentation: bool = False\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Advanced image processing with quality analysis\"\"\"\n",
    "\n",
    "        # Convert to numpy array if PIL Image\n",
    "        if isinstance(image, Image.Image):\n",
    "            image_array = np.array(image)\n",
    "        else:\n",
    "            image_array = image\n",
    "\n",
    "        original_shape = image_array.shape\n",
    "\n",
    "        # Quality analysis on original image\n",
    "        quality_metrics = self.detect_image_quality(image_array)\n",
    "\n",
    "        # Calculate hash for duplicate detection\n",
    "        image_hash = self.calculate_image_hash(image_array)\n",
    "\n",
    "        # Apply appropriate pipeline\n",
    "        if apply_augmentation:\n",
    "            processed = self.augmentation_pipeline(image=image_array)\n",
    "        else:\n",
    "            processed = self.quality_pipeline(image=image_array)\n",
    "\n",
    "        processed_image = processed[\"image\"]\n",
    "\n",
    "        # Calculate quality score\n",
    "        quality_score = self.calculate_quality_score(quality_metrics)\n",
    "\n",
    "        self.stats[\"processed\"] += 1\n",
    "        self.stats[\"quality_passed\"] += 1 if quality_score > 0.5 else 0\n",
    "\n",
    "        return {\n",
    "            \"processed_image\": processed_image,\n",
    "            \"original_shape\": original_shape,\n",
    "            \"quality_metrics\": quality_metrics,\n",
    "            \"quality_score\": quality_score,\n",
    "            \"image_hash\": image_hash,\n",
    "            \"is_augmented\": apply_augmentation,\n",
    "        }\n",
    "\n",
    "    def calculate_quality_score(self, metrics: Dict[str, float]) -> float:\n",
    "        \"\"\"Calculate overall quality score from metrics\"\"\"\n",
    "        score = 1.0\n",
    "\n",
    "        # Penalize very blurry images\n",
    "        if metrics[\"blur_score\"] < 100:\n",
    "            score *= 0.3\n",
    "        elif metrics[\"blur_score\"] < 500:\n",
    "            score *= 0.7\n",
    "\n",
    "        # Penalize very dark or very bright images\n",
    "        brightness = metrics[\"brightness\"]\n",
    "        if brightness < 0.1 or brightness > 0.9:\n",
    "            score *= 0.5\n",
    "\n",
    "        # Penalize very low contrast images\n",
    "        if metrics[\"contrast\"] < 0.1:\n",
    "            score *= 0.4\n",
    "\n",
    "        # Penalize images with very few edges (possibly corrupted)\n",
    "        if metrics[\"edge_density\"] < 0.01:\n",
    "            score *= 0.2\n",
    "\n",
    "        return min(1.0, score)\n",
    "\n",
    "\n",
    "# Create synthetic image dataset for demonstration\n",
    "def create_synthetic_image_dataset(num_images: int = 100) -> Dataset:\n",
    "    \"\"\"Create synthetic images with varying quality\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Create random image with varying quality\n",
    "        if i % 4 == 0:\n",
    "            # High quality image\n",
    "            img = np.random.randint(50, 200, (224, 224, 3), dtype=np.uint8)\n",
    "            img = cv2.GaussianBlur(img, (1, 1), 0)  # Slight blur\n",
    "        elif i % 4 == 1:\n",
    "            # Low quality (blurry) image\n",
    "            img = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
    "            img = cv2.GaussianBlur(img, (15, 15), 0)  # Heavy blur\n",
    "        elif i % 4 == 2:\n",
    "            # Dark image\n",
    "            img = np.random.randint(0, 50, (224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            # Bright image\n",
    "            img = np.random.randint(200, 255, (224, 224, 3), dtype=np.uint8)\n",
    "\n",
    "        images.append(img)\n",
    "        labels.append(i % 10)  # 10 classes\n",
    "\n",
    "    return Dataset.from_dict({\"image\": images, \"label\": labels})\n",
    "\n",
    "\n",
    "# Create and process image dataset\n",
    "print(\"Creating synthetic image dataset...\")\n",
    "image_dataset = create_synthetic_image_dataset(200)\n",
    "\n",
    "image_processor = AdvancedImageProcessor()\n",
    "\n",
    "\n",
    "def process_images_batch(examples: Dict[str, List]) -> Dict[str, List]:\n",
    "    \"\"\"Process batch of images with quality control\"\"\"\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    for image in examples[\"image\"]:\n",
    "        # Apply augmentation to 50% of images\n",
    "        apply_aug = np.random.random() > 0.5\n",
    "        processed = image_processor.process_image_advanced(\n",
    "            image, apply_augmentation=apply_aug\n",
    "        )\n",
    "\n",
    "        for key, value in processed.items():\n",
    "            results[key].append(value)\n",
    "\n",
    "    return dict(results)\n",
    "\n",
    "\n",
    "# Process images\n",
    "print(\"Processing images with quality control...\")\n",
    "start_time = time.time()\n",
    "\n",
    "processed_image_dataset = image_dataset.map(\n",
    "    process_images_batch,\n",
    "    batched=True,\n",
    "    batch_size=16,\n",
    "    num_proc=2,  # Lower for image processing\n",
    ")\n",
    "\n",
    "processing_time = time.time() - start_time\n",
    "print(f\"Image processing completed in {processing_time:.2f}s\")\n",
    "print(f\"Image processing stats: {dict(image_processor.stats)}\")\n",
    "\n",
    "# Analyze image quality distribution\n",
    "quality_scores = processed_image_dataset[\"quality_score\"]\n",
    "print(\n",
    "    f\"Image quality stats: mean={np.mean(quality_scores):.3f}, std={np.std(quality_scores):.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"High quality images (>0.7): {sum(1 for score in quality_scores if score > 0.7)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf215a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Advanced Audio Processing ===\n",
    "print(\"\\n=== Advanced Audio Processing ===\")\n",
    "\n",
    "\n",
    "class AdvancedAudioProcessor:\n",
    "    \"\"\"Advanced audio processing with noise reduction and feature extraction\"\"\"\n",
    "\n",
    "    def __init__(self, target_sr: int = 16000, segment_length: float = 10.0):\n",
    "        self.target_sr = target_sr\n",
    "        self.segment_length = segment_length\n",
    "        self.stats = defaultdict(int)\n",
    "\n",
    "    def detect_silence(\n",
    "        self,\n",
    "        audio: np.ndarray,\n",
    "        sr: int,\n",
    "        threshold: float = 0.01,\n",
    "        min_duration: float = 0.5,\n",
    "    ) -> List[Tuple[float, float]]:\n",
    "        \"\"\"Detect silent segments in audio\"\"\"\n",
    "        # Calculate energy in sliding windows\n",
    "        frame_length = int(0.025 * sr)  # 25ms frames\n",
    "        hop_length = frame_length // 2\n",
    "\n",
    "        energy = []\n",
    "        for i in range(0, len(audio) - frame_length, hop_length):\n",
    "            frame = audio[i : i + frame_length]\n",
    "            energy.append(np.sum(frame**2))\n",
    "\n",
    "        energy = np.array(energy)\n",
    "        energy = energy / np.max(energy) if np.max(energy) > 0 else energy\n",
    "\n",
    "        # Find silent regions\n",
    "        silent_frames = energy < threshold\n",
    "        silent_segments = []\n",
    "\n",
    "        in_silence = False\n",
    "        start_frame = 0\n",
    "\n",
    "        for i, is_silent in enumerate(silent_frames):\n",
    "            if is_silent and not in_silence:\n",
    "                start_frame = i\n",
    "                in_silence = True\n",
    "            elif not is_silent and in_silence:\n",
    "                duration = (i - start_frame) * hop_length / sr\n",
    "                if duration >= min_duration:\n",
    "                    start_time = start_frame * hop_length / sr\n",
    "                    end_time = i * hop_length / sr\n",
    "                    silent_segments.append((start_time, end_time))\n",
    "                in_silence = False\n",
    "\n",
    "        return silent_segments\n",
    "\n",
    "    def extract_advanced_features(self, audio: np.ndarray, sr: int) -> Dict[str, Any]:\n",
    "        \"\"\"Extract comprehensive audio features\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Basic stats\n",
    "        features[\"rms_energy\"] = float(np.sqrt(np.mean(audio**2)))\n",
    "        features[\"zero_crossing_rate\"] = float(\n",
    "            np.mean(librosa.feature.zero_crossing_rate(audio))\n",
    "        )\n",
    "\n",
    "        # Spectral features\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]\n",
    "        features[\"spectral_centroid_mean\"] = float(np.mean(spectral_centroids))\n",
    "        features[\"spectral_centroid_std\"] = float(np.std(spectral_centroids))\n",
    "\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]\n",
    "        features[\"spectral_rolloff_mean\"] = float(np.mean(spectral_rolloff))\n",
    "\n",
    "        # MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "        features[\"mfcc_mean\"] = mfccs.mean(axis=1).tolist()\n",
    "        features[\"mfcc_std\"] = mfccs.std(axis=1).tolist()\n",
    "\n",
    "        # Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "        features[\"chroma_mean\"] = float(np.mean(chroma))\n",
    "\n",
    "        # Tempo estimation\n",
    "        try:\n",
    "            tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)\n",
    "            features[\"tempo\"] = float(tempo)\n",
    "        except:\n",
    "            features[\"tempo\"] = 0.0\n",
    "\n",
    "        return features\n",
    "\n",
    "    def process_audio_advanced(self, audio_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Advanced audio processing with quality control\"\"\"\n",
    "        audio_array = audio_data[\"array\"]\n",
    "        original_sr = audio_data[\"sampling_rate\"]\n",
    "\n",
    "        # Resample if needed\n",
    "        if original_sr != self.target_sr:\n",
    "            audio_array = librosa.resample(\n",
    "                audio_array, orig_sr=original_sr, target_sr=self.target_sr\n",
    "            )\n",
    "\n",
    "        # Normalize audio\n",
    "        if np.max(np.abs(audio_array)) > 0:\n",
    "            audio_array = audio_array / np.max(np.abs(audio_array))\n",
    "\n",
    "        # Apply noise reduction if audio seems noisy\n",
    "        try:\n",
    "            # Simple noise reduction\n",
    "            noise_reduced = nr.reduce_noise(y=audio_array, sr=self.target_sr)\n",
    "        except:\n",
    "            noise_reduced = audio_array\n",
    "\n",
    "        # Detect silence\n",
    "        silent_segments = self.detect_silence(audio_array, self.target_sr)\n",
    "        silence_ratio = sum(end - start for start, end in silent_segments) / (\n",
    "            len(audio_array) / self.target_sr\n",
    "        )\n",
    "\n",
    "        # Segment audio if too long\n",
    "        max_samples = int(self.segment_length * self.target_sr)\n",
    "        if len(audio_array) > max_samples:\n",
    "            # Take middle segment to avoid silence at beginning/end\n",
    "            start_idx = (len(audio_array) - max_samples) // 2\n",
    "            audio_array = audio_array[start_idx : start_idx + max_samples]\n",
    "\n",
    "        # Extract features\n",
    "        features = self.extract_advanced_features(audio_array, self.target_sr)\n",
    "\n",
    "        # Calculate quality score\n",
    "        quality_score = self.calculate_audio_quality_score(features, silence_ratio)\n",
    "\n",
    "        self.stats[\"processed\"] += 1\n",
    "        self.stats[\"noise_reduced\"] += (\n",
    "            1 if np.mean(np.abs(noise_reduced - audio_array)) > 0.01 else 0\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"processed_audio\": audio_array.tolist(),\n",
    "            \"noise_reduced_audio\": noise_reduced.tolist(),\n",
    "            \"sampling_rate\": self.target_sr,\n",
    "            \"original_length\": len(audio_data[\"array\"]) / original_sr,\n",
    "            \"processed_length\": len(audio_array) / self.target_sr,\n",
    "            \"silence_ratio\": silence_ratio,\n",
    "            \"audio_features\": features,\n",
    "            \"quality_score\": quality_score,\n",
    "        }\n",
    "\n",
    "    def calculate_audio_quality_score(\n",
    "        self, features: Dict[str, Any], silence_ratio: float\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate audio quality score\"\"\"\n",
    "        score = 1.0\n",
    "\n",
    "        # Penalize too much silence\n",
    "        if silence_ratio > 0.7:\n",
    "            score *= 0.3\n",
    "        elif silence_ratio > 0.5:\n",
    "            score *= 0.6\n",
    "\n",
    "        # Penalize very low energy\n",
    "        if features[\"rms_energy\"] < 0.01:\n",
    "            score *= 0.4\n",
    "\n",
    "        # Penalize monotone audio (very low spectral centroid std)\n",
    "        if features[\"spectral_centroid_std\"] < 100:\n",
    "            score *= 0.7\n",
    "\n",
    "        return min(1.0, score)\n",
    "\n",
    "\n",
    "# Create synthetic audio dataset\n",
    "def create_synthetic_audio_dataset(num_samples: int = 50) -> Dataset:\n",
    "    \"\"\"Create synthetic audio samples with varying quality\"\"\"\n",
    "    audio_samples = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Create different types of audio\n",
    "        duration = np.random.uniform(3, 15)  # 3-15 seconds\n",
    "        samples = int(duration * 16000)\n",
    "\n",
    "        if i % 4 == 0:\n",
    "            # Speech-like signal\n",
    "            fundamental = 200 + np.random.normal(0, 50)\n",
    "            t = np.linspace(0, duration, samples)\n",
    "            audio = np.sin(2 * np.pi * fundamental * t) * np.exp(-t / 2)\n",
    "            # Add formants\n",
    "            audio += 0.3 * np.sin(2 * np.pi * fundamental * 2 * t)\n",
    "            audio += 0.2 * np.sin(2 * np.pi * fundamental * 3 * t)\n",
    "        elif i % 4 == 1:\n",
    "            # Music-like signal\n",
    "            frequencies = [440, 554, 659]  # A major chord\n",
    "            t = np.linspace(0, duration, samples)\n",
    "            audio = sum(np.sin(2 * np.pi * f * t) for f in frequencies) / len(\n",
    "                frequencies\n",
    "            )\n",
    "        elif i % 4 == 2:\n",
    "            # Noisy signal\n",
    "            audio = np.random.normal(0, 0.1, samples)\n",
    "        else:\n",
    "            # Silent signal with occasional blips\n",
    "            audio = np.random.normal(0, 0.01, samples)\n",
    "            for _ in range(5):\n",
    "                pos = np.random.randint(0, samples - 1000)\n",
    "                audio[pos : pos + 1000] += 0.5 * np.sin(\n",
    "                    2 * np.pi * 1000 * np.linspace(0, 1000 / 16000, 1000)\n",
    "                )\n",
    "\n",
    "        # Add some noise to all signals\n",
    "        audio += np.random.normal(0, 0.02, len(audio))\n",
    "\n",
    "        audio_samples.append({\"array\": audio, \"sampling_rate\": 16000})\n",
    "\n",
    "    return Dataset.from_dict({\"audio\": audio_samples, \"id\": list(range(num_samples))})\n",
    "\n",
    "\n",
    "# Create and process audio dataset\n",
    "print(\"Creating synthetic audio dataset...\")\n",
    "audio_dataset = create_synthetic_audio_dataset(30)\n",
    "\n",
    "audio_processor = AdvancedAudioProcessor()\n",
    "\n",
    "\n",
    "def process_audio_batch(examples: Dict[str, List]) -> Dict[str, List]:\n",
    "    \"\"\"Process batch of audio samples\"\"\"\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    for audio_data in examples[\"audio\"]:\n",
    "        processed = audio_processor.process_audio_advanced(audio_data)\n",
    "        for key, value in processed.items():\n",
    "            results[key].append(value)\n",
    "\n",
    "    return dict(results)\n",
    "\n",
    "\n",
    "# Process audio\n",
    "print(\"Processing audio with advanced pipeline...\")\n",
    "start_time = time.time()\n",
    "\n",
    "processed_audio_dataset = audio_dataset.map(\n",
    "    process_audio_batch,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    num_proc=1,  # Audio processing is CPU intensive\n",
    ")\n",
    "\n",
    "processing_time = time.time() - start_time\n",
    "print(f\"Audio processing completed in {processing_time:.2f}s\")\n",
    "print(f\"Audio processing stats: {dict(audio_processor.stats)}\")\n",
    "\n",
    "# Analyze audio quality\n",
    "audio_quality_scores = processed_audio_dataset[\"quality_score\"]\n",
    "silence_ratios = processed_audio_dataset[\"silence_ratio\"]\n",
    "\n",
    "print(\n",
    "    f\"Audio quality stats: mean={np.mean(audio_quality_scores):.3f}, std={np.std(audio_quality_scores):.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Silence ratio stats: mean={np.mean(silence_ratios):.3f}, std={np.std(silence_ratios):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Data Quality Control & Anomaly Detection ===\n",
    "print(\"\\n=== Data Quality Control & Anomaly Detection ===\")\n",
    "\n",
    "\n",
    "class DataQualityController:\n",
    "    \"\"\"Advanced data quality control and anomaly detection\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stats = defaultdict(int)\n",
    "        self.quality_thresholds = {\n",
    "            \"text_quality\": 0.3,\n",
    "            \"image_quality\": 0.5,\n",
    "            \"audio_quality\": 0.4,\n",
    "        }\n",
    "        self.duplicate_hashes = set()\n",
    "\n",
    "    def detect_text_duplicates(\n",
    "        self, texts: List[str], similarity_threshold: float = 0.85\n",
    "    ) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"Detect near-duplicate texts using TF-IDF similarity\"\"\"\n",
    "        if len(texts) < 2:\n",
    "            return []\n",
    "\n",
    "        # Use TF-IDF vectorization\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=1000, stop_words=\"english\", ngram_range=(1, 2)\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "            # Calculate cosine similarity\n",
    "            similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "            duplicates = []\n",
    "            for i in range(len(texts)):\n",
    "                for j in range(i + 1, len(texts)):\n",
    "                    if similarity_matrix[i, j] > similarity_threshold:\n",
    "                        duplicates.append((i, j, similarity_matrix[i, j]))\n",
    "\n",
    "            return duplicates\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ TF-IDF similarity calculation failed: {e}\")\n",
    "            return []\n",
    "\n",
    "    def detect_outliers_iqr(self, data: List[float], factor: float = 1.5) -> List[int]:\n",
    "        \"\"\"Detect outliers using IQR method\"\"\"\n",
    "        if len(data) < 4:\n",
    "            return []\n",
    "\n",
    "        q1 = np.percentile(data, 25)\n",
    "        q3 = np.percentile(data, 75)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        lower_bound = q1 - factor * iqr\n",
    "        upper_bound = q3 + factor * iqr\n",
    "\n",
    "        outliers = []\n",
    "        for i, value in enumerate(data):\n",
    "            if value < lower_bound or value > upper_bound:\n",
    "                outliers.append(i)\n",
    "\n",
    "        return outliers\n",
    "\n",
    "    def analyze_dataset_quality(\n",
    "        self, dataset: Dataset, text_column: str = None, quality_column: str = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive dataset quality analysis\"\"\"\n",
    "        analysis = {\n",
    "            \"total_samples\": len(dataset),\n",
    "            \"memory_usage_mb\": (\n",
    "                dataset.data.nbytes / (1024 * 1024)\n",
    "                if hasattr(dataset.data, \"nbytes\")\n",
    "                else 0\n",
    "            ),\n",
    "            \"columns\": dataset.column_names,\n",
    "            \"quality_issues\": [],\n",
    "        }\n",
    "\n",
    "        # Analyze quality scores if available\n",
    "        if quality_column and quality_column in dataset.column_names:\n",
    "            quality_scores = dataset[quality_column]\n",
    "            analysis[\"quality_stats\"] = {\n",
    "                \"mean\": float(np.mean(quality_scores)),\n",
    "                \"std\": float(np.std(quality_scores)),\n",
    "                \"min\": float(np.min(quality_scores)),\n",
    "                \"max\": float(np.max(quality_scores)),\n",
    "                \"median\": float(np.median(quality_scores)),\n",
    "            }\n",
    "\n",
    "            # Find low quality samples\n",
    "            threshold = self.quality_thresholds.get(\n",
    "                quality_column.replace(\"_score\", \"\"), 0.5\n",
    "            )\n",
    "            low_quality_indices = [\n",
    "                i for i, score in enumerate(quality_scores) if score < threshold\n",
    "            ]\n",
    "            analysis[\"low_quality_samples\"] = len(low_quality_indices)\n",
    "            analysis[\"low_quality_ratio\"] = len(low_quality_indices) / len(dataset)\n",
    "\n",
    "            # Detect outliers in quality scores\n",
    "            outliers = self.detect_outliers_iqr(quality_scores)\n",
    "            analysis[\"quality_outliers\"] = len(outliers)\n",
    "\n",
    "        # Analyze text duplicates if text column provided\n",
    "        if text_column and text_column in dataset.column_names:\n",
    "            sample_size = min(1000, len(dataset))  # Sample for large datasets\n",
    "            sample_indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "            sample_texts = [dataset[int(i)][text_column] for i in sample_indices]\n",
    "\n",
    "            duplicates = self.detect_text_duplicates(sample_texts)\n",
    "            analysis[\"duplicate_pairs\"] = len(duplicates)\n",
    "            analysis[\"estimated_duplicate_ratio\"] = len(duplicates) * 2 / sample_size\n",
    "\n",
    "        # Check for missing values\n",
    "        for column in dataset.column_names:\n",
    "            try:\n",
    "                sample_values = dataset[column][:100]  # Check first 100 samples\n",
    "                none_count = sum(1 for v in sample_values if v is None or v == \"\")\n",
    "                if none_count > 0:\n",
    "                    analysis[\"quality_issues\"].append(\n",
    "                        f\"Column '{column}' has missing values\"\n",
    "                    )\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def create_quality_report(self, datasets: Dict[str, Dataset]) -> str:\n",
    "        \"\"\"Generate comprehensive quality report\"\"\"\n",
    "        report = [\"# Dataset Quality Report\", \"\"]\n",
    "\n",
    "        for name, dataset in datasets.items():\n",
    "            report.append(f\"## Dataset: {name}\")\n",
    "            report.append(\"\")\n",
    "\n",
    "            # Determine quality column\n",
    "            quality_cols = [\n",
    "                col for col in dataset.column_names if \"quality_score\" in col\n",
    "            ]\n",
    "            quality_col = quality_cols[0] if quality_cols else None\n",
    "\n",
    "            # Determine text column\n",
    "            text_cols = [col for col in dataset.column_names if \"text\" in col.lower()]\n",
    "            text_col = text_cols[0] if text_cols else None\n",
    "\n",
    "            analysis = self.analyze_dataset_quality(dataset, text_col, quality_col)\n",
    "\n",
    "            report.append(f\"- **Total Samples**: {analysis['total_samples']:,}\")\n",
    "            report.append(f\"- **Memory Usage**: {analysis['memory_usage_mb']:.1f} MB\")\n",
    "            report.append(f\"- **Columns**: {', '.join(analysis['columns'])}\")\n",
    "\n",
    "            if \"quality_stats\" in analysis:\n",
    "                stats = analysis[\"quality_stats\"]\n",
    "                report.append(\n",
    "                    f\"- **Quality Score**: {stats['mean']:.3f} ± {stats['std']:.3f}\"\n",
    "                )\n",
    "                report.append(\n",
    "                    f\"- **Low Quality Samples**: {analysis['low_quality_samples']} ({analysis['low_quality_ratio']:.1%})\"\n",
    "                )\n",
    "\n",
    "            if \"duplicate_pairs\" in analysis:\n",
    "                report.append(\n",
    "                    f\"- **Estimated Duplicates**: {analysis['estimated_duplicate_ratio']:.1%}\"\n",
    "                )\n",
    "\n",
    "            if analysis[\"quality_issues\"]:\n",
    "                report.append(\"- **Issues**: \" + \", \".join(analysis[\"quality_issues\"]))\n",
    "\n",
    "            report.append(\"\")\n",
    "\n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "\n",
    "# Initialize quality controller and analyze datasets\n",
    "quality_controller = DataQualityController()\n",
    "\n",
    "# Prepare datasets for analysis\n",
    "datasets_to_analyze = {\n",
    "    \"processed_text\": processed_text_dataset,\n",
    "    \"processed_images\": processed_image_dataset,\n",
    "    \"processed_audio\": processed_audio_dataset,\n",
    "}\n",
    "\n",
    "print(\"Analyzing dataset quality...\")\n",
    "quality_report = quality_controller.create_quality_report(datasets_to_analyze)\n",
    "print(quality_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759308a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: GPU-Accelerated Processing & Distributed Processing ===\n",
    "print(\"\\n=== GPU-Accelerated & Distributed Processing ===\")\n",
    "\n",
    "\n",
    "class GPUAcceleratedProcessor:\n",
    "    \"\"\"GPU-accelerated preprocessing operations\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def gpu_batch_normalize(self, data: List[np.ndarray]) -> List[np.ndarray]:\n",
    "        \"\"\"GPU-accelerated batch normalization\"\"\"\n",
    "        if not data:\n",
    "            return data\n",
    "\n",
    "        # Convert to tensor and move to GPU\n",
    "        tensors = [torch.from_numpy(arr).float().to(self.device) for arr in data]\n",
    "\n",
    "        # Batch normalize\n",
    "        normalized = []\n",
    "        for tensor in tensors:\n",
    "            # Normalize to [0, 1] range\n",
    "            tensor_min = tensor.min()\n",
    "            tensor_max = tensor.max()\n",
    "            if tensor_max > tensor_min:\n",
    "                normalized_tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "            else:\n",
    "                normalized_tensor = tensor\n",
    "\n",
    "            normalized.append(normalized_tensor.cpu().numpy())\n",
    "\n",
    "        return normalized\n",
    "\n",
    "    def gpu_image_transforms(\n",
    "        self, images: List[np.ndarray], transforms: List[str] = [\"resize\", \"normalize\"]\n",
    "    ) -> List[np.ndarray]:\n",
    "        \"\"\"GPU-accelerated image transformations\"\"\"\n",
    "        if not images or not torch.cuda.is_available():\n",
    "            return images\n",
    "\n",
    "        # Convert to tensor batch\n",
    "        image_tensors = []\n",
    "        for img in images:\n",
    "            if len(img.shape) == 3:\n",
    "                tensor = (\n",
    "                    torch.from_numpy(img).permute(2, 0, 1).float().to(self.device)\n",
    "                )  # HWC to CHW\n",
    "            else:\n",
    "                tensor = torch.from_numpy(img).float().to(self.device)\n",
    "            image_tensors.append(tensor)\n",
    "\n",
    "        # Batch process\n",
    "        processed_images = []\n",
    "        for tensor in image_tensors:\n",
    "            if \"resize\" in transforms:\n",
    "                tensor = F.interpolate(\n",
    "                    tensor.unsqueeze(0),\n",
    "                    size=(224, 224),\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False,\n",
    "                ).squeeze(0)\n",
    "\n",
    "            if \"normalize\" in transforms:\n",
    "                tensor = (tensor - tensor.mean()) / (tensor.std() + 1e-8)\n",
    "\n",
    "            # Convert back to numpy\n",
    "            if len(tensor.shape) == 3:\n",
    "                processed_img = tensor.permute(1, 2, 0).cpu().numpy()  # CHW to HWC\n",
    "            else:\n",
    "                processed_img = tensor.cpu().numpy()\n",
    "\n",
    "            processed_images.append(processed_img)\n",
    "\n",
    "        return processed_images\n",
    "\n",
    "\n",
    "class DistributedProcessor:\n",
    "    \"\"\"Distributed processing for large-scale datasets\"\"\"\n",
    "\n",
    "    def __init__(self, num_workers: int = None):\n",
    "        self.num_workers = num_workers or min(8, os.cpu_count())\n",
    "\n",
    "    def parallel_map(\n",
    "        self, dataset: Dataset, processing_func: callable, chunk_size: int = 1000\n",
    "    ) -> Dataset:\n",
    "        \"\"\"Apply processing function in parallel chunks\"\"\"\n",
    "        print(f\"Processing dataset with {self.num_workers} workers...\")\n",
    "\n",
    "        # Split dataset into chunks\n",
    "        dataset_size = len(dataset)\n",
    "        chunks = []\n",
    "\n",
    "        for i in range(0, dataset_size, chunk_size):\n",
    "            end_idx = min(i + chunk_size, dataset_size)\n",
    "            chunk = dataset.select(range(i, end_idx))\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        print(f\"Created {len(chunks)} chunks of size ~{chunk_size}\")\n",
    "\n",
    "        # Process chunks in parallel\n",
    "        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Create partial function with fixed parameters\n",
    "            process_chunk = partial(\n",
    "                self._process_chunk, processing_func=processing_func\n",
    "            )\n",
    "\n",
    "            # Submit all chunks\n",
    "            futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\n",
    "\n",
    "            # Collect results\n",
    "            processed_chunks = []\n",
    "            for i, future in enumerate(futures):\n",
    "                try:\n",
    "                    result = future.result(timeout=300)  # 5 minute timeout\n",
    "                    processed_chunks.append(result)\n",
    "                    print(f\"Completed chunk {i+1}/{len(chunks)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Chunk {i+1} failed: {e}\")\n",
    "                    processed_chunks.append(\n",
    "                        chunks[i]\n",
    "                    )  # Use original chunk if processing fails\n",
    "\n",
    "            processing_time = time.time() - start_time\n",
    "            print(f\"Parallel processing completed in {processing_time:.2f}s\")\n",
    "\n",
    "        # Concatenate all processed chunks\n",
    "        if processed_chunks:\n",
    "            return concatenate_datasets(processed_chunks)\n",
    "        else:\n",
    "            return dataset\n",
    "\n",
    "    def _process_chunk(self, chunk: Dataset, processing_func: callable) -> Dataset:\n",
    "        \"\"\"Process a single chunk of data\"\"\"\n",
    "        try:\n",
    "            return chunk.map(processing_func, batched=True, batch_size=32)\n",
    "        except Exception as e:\n",
    "            print(f\"Chunk processing error: {e}\")\n",
    "            return chunk\n",
    "\n",
    "\n",
    "# Demo GPU acceleration if available\n",
    "gpu_processor = GPUAcceleratedProcessor()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Testing GPU-accelerated processing...\")\n",
    "\n",
    "    # Test with a small batch of synthetic images\n",
    "    test_images = [\n",
    "        np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8) for _ in range(10)\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    gpu_processed = gpu_processor.gpu_image_transforms(test_images)\n",
    "    gpu_time = time.time() - start_time\n",
    "\n",
    "    print(f\"GPU processing time: {gpu_time:.4f}s for {len(test_images)} images\")\n",
    "    print(f\"Processed image shape: {gpu_processed[0].shape}\")\n",
    "else:\n",
    "    print(\"GPU not available, skipping GPU acceleration demo\")\n",
    "\n",
    "# Demo distributed processing\n",
    "distributed_processor = DistributedProcessor(num_workers=2)\n",
    "\n",
    "\n",
    "def simple_text_length_calc(examples):\n",
    "    \"\"\"Simple processing function for distributed demo\"\"\"\n",
    "    return {\"char_count\": [len(text) for text in examples[\"cleaned_text\"]]}\n",
    "\n",
    "\n",
    "print(\"Testing distributed processing...\")\n",
    "if len(processed_text_dataset) > 100:\n",
    "    # Use subset for demo\n",
    "    subset = processed_text_dataset.select(range(100))\n",
    "    distributed_result = distributed_processor.parallel_map(\n",
    "        subset, simple_text_length_calc, chunk_size=25\n",
    "    )\n",
    "    print(f\"Distributed processing completed: {len(distributed_result)} samples\")\n",
    "else:\n",
    "    print(\"Dataset too small for distributed processing demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f540e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Adaptive Batch Processing & Dynamic Memory Management ===\n",
    "print(\"\\n=== Adaptive Batch Processing & Dynamic Memory Management ===\")\n",
    "\n",
    "\n",
    "class AdaptiveBatchProcessor:\n",
    "    \"\"\"Adaptive batch processing with dynamic memory management\"\"\"\n",
    "\n",
    "    def __init__(self, initial_batch_size: int = 32, memory_threshold_gb: float = 8.0):\n",
    "        self.initial_batch_size = initial_batch_size\n",
    "        self.memory_threshold_gb = memory_threshold_gb\n",
    "        self.batch_size_history = []\n",
    "        self.processing_times = []\n",
    "\n",
    "    def get_current_memory_usage(self) -> float:\n",
    "        \"\"\"Get current memory usage in GB\"\"\"\n",
    "        process = psutil.Process(os.getpid())\n",
    "        return process.memory_info().rss / 1024 / 1024 / 1024\n",
    "\n",
    "    def estimate_optimal_batch_size(\n",
    "        self, sample_processing_time: float, sample_memory_usage: float\n",
    "    ) -> int:\n",
    "        \"\"\"Estimate optimal batch size based on memory and time constraints\"\"\"\n",
    "        available_memory = self.memory_threshold_gb - self.get_current_memory_usage()\n",
    "\n",
    "        # Calculate batch size based on memory constraint\n",
    "        memory_based_batch = max(1, int(available_memory / (sample_memory_usage + 0.1)))\n",
    "\n",
    "        # Calculate batch size based on time constraint (target: 1-5 seconds per batch)\n",
    "        target_batch_time = 3.0  # seconds\n",
    "        time_based_batch = max(\n",
    "            1, int(target_batch_time / max(sample_processing_time, 0.01))\n",
    "        )\n",
    "\n",
    "        # Take the minimum to respect both constraints\n",
    "        optimal_batch = min(memory_based_batch, time_based_batch, 128)  # Cap at 128\n",
    "\n",
    "        return max(1, optimal_batch)\n",
    "\n",
    "    def adaptive_map(\n",
    "        self, dataset: Dataset, processing_func: callable, sample_size: int = 10\n",
    "    ) -> Dataset:\n",
    "        \"\"\"Apply processing with adaptive batch sizing\"\"\"\n",
    "        print(\"Starting adaptive batch processing...\")\n",
    "\n",
    "        # Sample small batch to estimate resource usage\n",
    "        sample_indices = np.random.choice(\n",
    "            len(dataset), min(sample_size, len(dataset)), replace=False\n",
    "        )\n",
    "        sample_data = dataset.select(sample_indices.tolist())\n",
    "\n",
    "        # Measure processing time and memory for sample\n",
    "        initial_memory = self.get_current_memory_usage()\n",
    "        start_time = time.time()\n",
    "\n",
    "        sample_result = sample_data.map(processing_func, batched=True, batch_size=1)\n",
    "\n",
    "        sample_time = (time.time() - start_time) / len(sample_data)\n",
    "        memory_per_sample = (self.get_current_memory_usage() - initial_memory) / len(\n",
    "            sample_data\n",
    "        )\n",
    "\n",
    "        # Estimate optimal batch size\n",
    "        optimal_batch_size = self.estimate_optimal_batch_size(\n",
    "            sample_time, memory_per_sample\n",
    "        )\n",
    "\n",
    "        print(f\"Estimated optimal batch size: {optimal_batch_size}\")\n",
    "        print(f\"Sample processing time: {sample_time:.4f}s per item\")\n",
    "        print(f\"Memory usage per sample: {memory_per_sample*1024:.2f}MB\")\n",
    "\n",
    "        # Process full dataset with adaptive batching\n",
    "        processed_samples = 0\n",
    "        current_batch_size = optimal_batch_size\n",
    "        batch_results = []\n",
    "\n",
    "        while processed_samples < len(dataset):\n",
    "            end_idx = min(processed_samples + current_batch_size, len(dataset))\n",
    "            batch_data = dataset.select(range(processed_samples, end_idx))\n",
    "\n",
    "            # Monitor processing time and memory\n",
    "            batch_start_time = time.time()\n",
    "            memory_before = self.get_current_memory_usage()\n",
    "\n",
    "            try:\n",
    "                batch_result = batch_data.map(\n",
    "                    processing_func, batched=True, batch_size=current_batch_size\n",
    "                )\n",
    "                batch_results.append(batch_result)\n",
    "\n",
    "                batch_time = time.time() - batch_start_time\n",
    "                memory_after = self.get_current_memory_usage()\n",
    "\n",
    "                # Log batch statistics\n",
    "                print(\n",
    "                    f\"Processed batch {len(batch_results)}: {len(batch_data)} samples in {batch_time:.2f}s, \"\n",
    "                    f\"Memory: {memory_after:.2f}GB\"\n",
    "                )\n",
    "\n",
    "                # Adjust batch size for next iteration\n",
    "                if batch_time > 5.0:  # Too slow\n",
    "                    current_batch_size = max(1, int(current_batch_size * 0.8))\n",
    "                elif (\n",
    "                    batch_time < 1.0 and memory_after < self.memory_threshold_gb * 0.8\n",
    "                ):  # Can go faster\n",
    "                    current_batch_size = min(128, int(current_batch_size * 1.2))\n",
    "\n",
    "                processed_samples = end_idx\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"❌ Batch processing failed with batch size {current_batch_size}: {e}\"\n",
    "                )\n",
    "                # Reduce batch size and retry\n",
    "                current_batch_size = max(1, current_batch_size // 2)\n",
    "                if current_batch_size == 1:\n",
    "                    print(\"❌ Cannot process even single samples, skipping batch\")\n",
    "                    processed_samples = end_idx\n",
    "\n",
    "            # Memory cleanup\n",
    "            if memory_after > self.memory_threshold_gb * 0.9:\n",
    "                gc.collect()\n",
    "                print(\n",
    "                    f\"Memory cleanup triggered. Usage: {self.get_current_memory_usage():.2f}GB\"\n",
    "                )\n",
    "\n",
    "        # Concatenate all batch results\n",
    "        if batch_results:\n",
    "            final_result = concatenate_datasets(batch_results)\n",
    "            print(f\"✅ Adaptive processing completed: {len(final_result)} samples\")\n",
    "            return final_result\n",
    "        else:\n",
    "            print(\"❌ No batches processed successfully\")\n",
    "            return dataset\n",
    "\n",
    "\n",
    "# Demo adaptive processing\n",
    "adaptive_processor = AdaptiveBatchProcessor(\n",
    "    initial_batch_size=16, memory_threshold_gb=4.0\n",
    ")\n",
    "\n",
    "\n",
    "def memory_intensive_processing(examples):\n",
    "    \"\"\"Memory-intensive processing function for demo\"\"\"\n",
    "    results = []\n",
    "    for text in examples[\"cleaned_text\"]:\n",
    "        # Simulate memory-intensive operation\n",
    "        large_array = np.random.randn(1000, 100)  # 100KB per sample\n",
    "        processed_value = np.sum(large_array) + len(text)\n",
    "        results.append(processed_value)\n",
    "    return {\"memory_intensive_result\": results}\n",
    "\n",
    "\n",
    "print(\"Testing adaptive batch processing...\")\n",
    "if len(processed_text_dataset) > 50:\n",
    "    subset = processed_text_dataset.select(range(50))\n",
    "    adaptive_result = adaptive_processor.adaptive_map(\n",
    "        subset, memory_intensive_processing\n",
    "    )\n",
    "    print(f\"Adaptive processing result: {len(adaptive_result)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841abfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9: Comprehensive Performance Testing & Optimization Recommendations ===\n",
    "print(\"\\n=== Performance Testing & Optimization ===\")\n",
    "\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"Comprehensive performance benchmarking for dataset processing\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "\n",
    "    def benchmark_processing_methods(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        processing_func: callable,\n",
    "        methods: List[str] = [\"sequential\", \"batched\", \"parallel\"],\n",
    "    ) -> Dict[str, Dict]:\n",
    "        \"\"\"Benchmark different processing methods\"\"\"\n",
    "        results = {}\n",
    "        sample_size = min(100, len(dataset))\n",
    "        test_dataset = dataset.select(range(sample_size))\n",
    "\n",
    "        for method in methods:\n",
    "            print(f\"Benchmarking {method} processing...\")\n",
    "\n",
    "            # Memory before\n",
    "            memory_before = (\n",
    "                psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
    "            )  # MB\n",
    "            start_time = time.time()\n",
    "\n",
    "            try:\n",
    "                if method == \"sequential\":\n",
    "                    result = test_dataset.map(processing_func, batched=False)\n",
    "                elif method == \"batched\":\n",
    "                    result = test_dataset.map(\n",
    "                        processing_func, batched=True, batch_size=16\n",
    "                    )\n",
    "                elif method == \"parallel\":\n",
    "                    result = test_dataset.map(\n",
    "                        processing_func, batched=True, batch_size=16, num_proc=2\n",
    "                    )\n",
    "\n",
    "                processing_time = time.time() - start_time\n",
    "                memory_after = (\n",
    "                    psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
    "                )  # MB\n",
    "                memory_used = memory_after - memory_before\n",
    "\n",
    "                results[method] = {\n",
    "                    \"processing_time\": processing_time,\n",
    "                    \"samples_per_second\": sample_size / processing_time,\n",
    "                    \"memory_used_mb\": memory_used,\n",
    "                    \"success\": True,\n",
    "                }\n",
    "\n",
    "                print(\n",
    "                    f\"  ✅ {method}: {processing_time:.2f}s, {sample_size/processing_time:.1f} samples/s\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                results[method] = {\n",
    "                    \"processing_time\": float(\"inf\"),\n",
    "                    \"samples_per_second\": 0,\n",
    "                    \"memory_used_mb\": 0,\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e),\n",
    "                }\n",
    "                print(f\"  ❌ {method}: Failed - {e}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def generate_optimization_recommendations(\n",
    "        self, benchmark_results: Dict[str, Dict], dataset_info: Dict[str, Any]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Generate optimization recommendations based on benchmark results\"\"\"\n",
    "        recommendations = []\n",
    "\n",
    "        # Find best performing method\n",
    "        successful_methods = {\n",
    "            k: v for k, v in benchmark_results.items() if v.get(\"success\", False)\n",
    "        }\n",
    "\n",
    "        if successful_methods:\n",
    "            best_method = min(\n",
    "                successful_methods.keys(),\n",
    "                key=lambda x: successful_methods[x][\"processing_time\"],\n",
    "            )\n",
    "\n",
    "            recommendations.append(f\"🏆 Best performing method: {best_method}\")\n",
    "\n",
    "            # Memory optimization\n",
    "            memory_usage = [v[\"memory_used_mb\"] for v in successful_methods.values()]\n",
    "            if max(memory_usage) > 1000:  # > 1GB\n",
    "                recommendations.append(\n",
    "                    \"🔧 Consider reducing batch size to lower memory usage\"\n",
    "                )\n",
    "\n",
    "            # Speed optimization\n",
    "            speeds = [v[\"samples_per_second\"] for v in successful_methods.values()]\n",
    "            if max(speeds) < 10:  # Less than 10 samples/second\n",
    "                recommendations.append(\n",
    "                    \"⚡ Consider GPU acceleration or simpler preprocessing\"\n",
    "                )\n",
    "\n",
    "            # Parallel processing\n",
    "            if \"parallel\" in successful_methods and \"batched\" in successful_methods:\n",
    "                parallel_speed = successful_methods[\"parallel\"][\"samples_per_second\"]\n",
    "                batched_speed = successful_methods[\"batched\"][\"samples_per_second\"]\n",
    "\n",
    "                if parallel_speed > batched_speed * 1.5:\n",
    "                    recommendations.append(\n",
    "                        \"🚀 Parallel processing shows significant improvement\"\n",
    "                    )\n",
    "                else:\n",
    "                    recommendations.append(\n",
    "                        \"⚠️ Parallel processing overhead may not be worth it\"\n",
    "                    )\n",
    "\n",
    "        # Dataset-specific recommendations\n",
    "        if dataset_info.get(\"total_samples\", 0) > 100000:\n",
    "            recommendations.append(\n",
    "                \"📊 Large dataset: Consider streaming mode and chunked processing\"\n",
    "            )\n",
    "\n",
    "        if dataset_info.get(\"memory_usage_mb\", 0) > 5000:\n",
    "            recommendations.append(\n",
    "                \"💾 High memory usage: Consider data compression or feature selection\"\n",
    "            )\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "# Run comprehensive benchmarks\n",
    "benchmark = PerformanceBenchmark()\n",
    "\n",
    "\n",
    "# Simple processing function for benchmarking\n",
    "def benchmark_processing_func(examples):\n",
    "    \"\"\"Simple processing function for benchmarking\"\"\"\n",
    "    if isinstance(examples, dict) and \"cleaned_text\" in examples:\n",
    "        # Batched mode\n",
    "        return {\"word_count\": [len(text.split()) for text in examples[\"cleaned_text\"]]}\n",
    "    else:\n",
    "        # Single sample mode\n",
    "        return {\"word_count\": len(examples[\"cleaned_text\"].split())}\n",
    "\n",
    "\n",
    "print(\"Running comprehensive performance benchmarks...\")\n",
    "\n",
    "# Benchmark text processing\n",
    "if len(processed_text_dataset) > 0:\n",
    "    text_benchmark_results = benchmark.benchmark_processing_methods(\n",
    "        processed_text_dataset,\n",
    "        benchmark_processing_func,\n",
    "        methods=[\"sequential\", \"batched\", \"parallel\"],\n",
    "    )\n",
    "\n",
    "    text_dataset_info = quality_controller.analyze_dataset_quality(\n",
    "        processed_text_dataset\n",
    "    )\n",
    "    text_recommendations = benchmark.generate_optimization_recommendations(\n",
    "        text_benchmark_results, text_dataset_info\n",
    "    )\n",
    "\n",
    "    print(\"\\n📈 Text Processing Benchmark Results:\")\n",
    "    for method, results in text_benchmark_results.items():\n",
    "        if results[\"success\"]:\n",
    "            print(\n",
    "                f\"  {method}: {results['processing_time']:.2f}s, {results['samples_per_second']:.1f} samples/s\"\n",
    "            )\n",
    "\n",
    "    print(\"\\n💡 Text Processing Recommendations:\")\n",
    "    for rec in text_recommendations:\n",
    "        print(f\"  {rec}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Final Smoke Test & Validation ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🧪 COMPREHENSIVE SMOKE TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "def run_comprehensive_smoke_test():\n",
    "    \"\"\"Run comprehensive smoke tests for all advanced processing functions\"\"\"\n",
    "    tests_passed = 0\n",
    "    total_tests = 0\n",
    "\n",
    "    # Test 1: Advanced text processing\n",
    "    total_tests += 1\n",
    "    try:\n",
    "        test_processor = AdvancedTextProcessor()\n",
    "        result = test_processor.advanced_clean(\n",
    "            \"This is a test sentence with répéated characters!!!\"\n",
    "        )\n",
    "        assert \"quality_score\" in result\n",
    "        assert \"detected_language\" in result\n",
    "        tests_passed += 1\n",
    "        print(\"✅ Advanced text processing\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Advanced text processing: {e}\")\n",
    "\n",
    "    # Test 2: Image quality detection\n",
    "    total_tests += 1\n",
    "    try:\n",
    "        test_img_processor = AdvancedImageProcessor()\n",
    "        test_image = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n",
    "        result = test_img_processor.process_image_advanced(test_image)\n",
    "        assert \"quality_score\" in result\n",
    "        assert \"quality_metrics\" in result\n",
    "        tests_passed += 1\n",
    "        print(\"✅ Advanced image processing\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Advanced image processing: {e}\")\n",
    "\n",
    "    # Test 3: Audio processing\n",
    "    total_tests += 1\n",
    "    try:\n",
    "        test_audio_processor = AdvancedAudioProcessor()\n",
    "        test_audio = {\"array\": np.random.randn(8000), \"sampling_rate\": 16000}\n",
    "        result = test_audio_processor.process_audio_advanced(test_audio)\n",
    "        assert \"quality_score\" in result\n",
    "        assert \"audio_features\" in result\n",
    "        tests_passed += 1\n",
    "        print(\"✅ Advanced audio processing\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Advanced audio processing: {e}\")\n",
    "\n",
    "    # Test 4: Quality control\n",
    "    total_tests += 1\n",
    "    try:\n",
    "        test_qc = DataQualityController()\n",
    "        test_dataset = Dataset.from_dict(\n",
    "            {\"text\": [\"test1\", \"test2\"], \"quality_score\": [0.8, 0.6]}\n",
    "        )\n",
    "        analysis = test_qc.analyze_dataset_quality(\n",
    "            test_dataset, \"text\", \"quality_score\"\n",
    "        )\n",
    "        assert \"total_samples\" in analysis\n",
    "        assert \"quality_stats\" in analysis\n",
    "        tests_passed += 1\n",
    "        print(\"✅ Data quality control\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data quality control: {e}\")\n",
    "\n",
    "    # Test 5: GPU acceleration (if available)\n",
    "    total_tests += 1\n",
    "    try:\n",
    "        test_gpu_processor = GPUAcceleratedProcessor()\n",
    "        test_data = [np.random.randn(10, 10) for _ in range(3)]\n",
    "        result = test_gpu_processor.gpu_batch_normalize(test_data)\n",
    "        assert len(result) == len(test_data)\n",
    "        tests_passed += 1\n",
    "        print(\"✅ GPU acceleration\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ GPU acceleration: {e}\")\n",
    "\n",
    "    # Test 6: Adaptive batch processing\n",
    "    total_tests += 1\n",
    "    try:\n",
    "        test_adaptive = AdaptiveBatchProcessor()\n",
    "        memory_usage = test_adaptive.get_current_memory_usage()\n",
    "        assert isinstance(memory_usage, float)\n",
    "        assert memory_usage > 0\n",
    "        tests_passed += 1\n",
    "        print(\"✅ Adaptive batch processing\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Adaptive batch processing: {e}\")\n",
    "\n",
    "    # Test 7: Performance benchmarking\n",
    "    total_tests += 1\n",
    "    try:\n",
    "        test_benchmark = PerformanceBenchmark()\n",
    "        recommendations = test_benchmark.generate_optimization_recommendations(\n",
    "            {\n",
    "                \"method1\": {\n",
    "                    \"success\": True,\n",
    "                    \"processing_time\": 1.0,\n",
    "                    \"samples_per_second\": 10,\n",
    "                    \"memory_used_mb\": 100,\n",
    "                }\n",
    "            },\n",
    "            {\"total_samples\": 1000, \"memory_usage_mb\": 100},\n",
    "        )\n",
    "        assert isinstance(recommendations, list)\n",
    "        tests_passed += 1\n",
    "        print(\"✅ Performance benchmarking\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Performance benchmarking: {e}\")\n",
    "\n",
    "    print(f\"\\n🎯 Advanced Processing Tests: {tests_passed}/{total_tests} passed\")\n",
    "    return tests_passed == total_tests\n",
    "\n",
    "\n",
    "# Run comprehensive smoke test\n",
    "all_advanced_tests_passed = run_comprehensive_smoke_test()\n",
    "\n",
    "# === Summary & Advanced Insights ===\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📋 ADVANCED DATASET PROCESSING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n✅ 完成項目 (Completed Advanced Items):\")\n",
    "print(\"• 超大型資料集處理：TB級資料的串流處理與分片管理\")\n",
    "print(\"• 進階多模態前處理：複雜文本清理、圖像品質檢測、音頻降噪\")\n",
    "print(\"• 智能品質控制：異常檢測、重複去除、自動化品質評分\")\n",
    "print(\"• GPU加速處理：批次正規化、圖像變換的GPU優化\")\n",
    "print(\"• 分散式處理架構：多程序並行、動態記憶體管理\")\n",
    "print(\"• 自適應批次處理：根據記憶體與時間約束動態調整batch size\")\n",
    "print(\"• 全面性能測試：基準測試與優化建議自動生成\")\n",
    "\n",
    "print(\"\\n🧠 進階核心概念 (Advanced Core Concepts):\")\n",
    "print(\"• 記憶體映射 vs 串流處理：何時使用哪種策略處理超大資料集\")\n",
    "print(\"• 品質感知處理：整合品質評分到處理管線中的重要性\")\n",
    "print(\"• GPU記憶體管理：避免OOM的tensor生命週期管理\")\n",
    "print(\"• 工作負載分散：CPU密集 vs I/O密集任務的不同並行策略\")\n",
    "print(\"• 自適應系統設計：根據運行時資源動態調整處理參數\")\n",
    "print(\"• 效能剖析驅動優化：從基準測試數據得出可行的優化策略\")\n",
    "\n",
    "print(\"\\n⚠️ 進階陷阱 (Advanced Pitfalls):\")\n",
    "print(\"• GPU記憶體碎片：頻繁的tensor創建/釋放導致記憶體碎片\")\n",
    "print(\"• 過度並行化：worker數量超過CPU核心數反而降低效能\")\n",
    "print(\"• 批次大小調優：忽略記憶體與計算的非線性關係\")\n",
    "print(\"• 品質閾值設定：過嚴格的品質要求可能移除有用資料\")\n",
    "print(\"• 串流處理狀態：無狀態處理vs有狀態聚合的設計選擇\")\n",
    "print(\"• 異步處理錯誤：分散式處理中的錯誤處理與復原機制\")\n",
    "\n",
    "print(\"\\n🚀 生產環境建議 (Production Recommendations):\")\n",
    "print(\"• 資料管線監控：實施處理速度、記憶體使用、錯誤率的即時監控\")\n",
    "print(\"• 彈性擴展設計：支援從單機到叢集的無縫擴展\")\n",
    "print(\"• 資料版本控制：追蹤處理參數變更對下游模型效能的影響\")\n",
    "print(\"• 快取策略優化：多層快取(記憶體/SSD/網路)的智能管理\")\n",
    "print(\"• 容錯處理機制：graceful degradation與自動重試策略\")\n",
    "print(\"• 效能基線建立：定期基準測試以檢測效能退化\")\n",
    "\n",
    "print(\"\\n📊 處理規模指引 (Scale Guidelines):\")\n",
    "print(\"• < 1GB: 直接記憶體處理，標準batch size\")\n",
    "print(\"• 1-10GB: 使用記憶體映射，增加batch size，開啟多程序\")\n",
    "print(\"• 10-100GB: 串流處理 + 分片，GPU加速，分散式處理\")\n",
    "print(\"• > 100GB: 叢集處理，資料分區，增量處理策略\")\n",
    "\n",
    "print(\"\\n💡 下一步進階主題 (Next Advanced Topics):\")\n",
    "print(\"• 聯邦學習資料處理：分散式資料的隱私保護處理\")\n",
    "print(\"• 即時資料流處理：Apache Kafka + Spark Streaming整合\")\n",
    "print(\"• 多模態融合前處理：跨模態特徵對齊與同步\")\n",
    "print(\"• 自動化資料清理：ML驅動的資料品質提升\")\n",
    "print(\"• 邊緣計算優化：移動裝置上的輕量化處理管線\")\n",
    "\n",
    "# Final memory cleanup\n",
    "gc.collect()\n",
    "final_memory = get_memory_usage()\n",
    "print(f\"\\n💾 Final Memory Usage: {final_memory}\")\n",
    "\n",
    "print(\n",
    "    f\"\\n🎯 Overall Advanced Processing Status: {'SUCCESS' if all_advanced_tests_passed else 'NEEDS_REVIEW'}\"\n",
    ")\n",
    "\n",
    "# Save processing statistics for future analysis\n",
    "processing_stats = {\n",
    "    \"text_processing\": (\n",
    "        dict(text_processor.stats) if \"text_processor\" in locals() else {}\n",
    "    ),\n",
    "    \"image_processing\": (\n",
    "        dict(image_processor.stats) if \"image_processor\" in locals() else {}\n",
    "    ),\n",
    "    \"audio_processing\": (\n",
    "        dict(audio_processor.stats) if \"audio_processor\" in locals() else {}\n",
    "    ),\n",
    "    \"memory_usage_gb\": final_memory,\n",
    "    \"tests_passed\": all_advanced_tests_passed,\n",
    "}\n",
    "\n",
    "print(f\"\\n📈 Session Processing Statistics:\")\n",
    "for category, stats in processing_stats.items():\n",
    "    if stats and isinstance(stats, dict):\n",
    "        print(f\"  {category}: {stats}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🎓 ADVANCED DATASET PROCESSING COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96403c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Final Advanced Smoke Test (5-line validation) ===\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Create test dataset\n",
    "test_ds = Dataset.from_dict({\"text\": [\"advanced test\"], \"data\": [np.random.randn(10)]})\n",
    "# Test advanced processing pipeline\n",
    "processor = AdvancedTextProcessor()\n",
    "result = processor.advanced_clean(\"test\")\n",
    "assert \"quality_score\" in result and isinstance(result[\"quality_score\"], float)\n",
    "print(\"🎯 Advanced Notebook 03b 驗收通過！進階大型資料集處理管線運作正常\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef25ab9",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 延伸章節小結\n",
    "\n",
    "### ✅ 進階完成項目 (Advanced Completed Items)\n",
    "- **TB級資料處理能力**：串流處理、分片管理、分散式架構\n",
    "- **智能品質控制系統**：多維度品質評分、異常檢測、重複去除\n",
    "- **GPU加速處理管線**：CUDA優化的批次處理、記憶體管理\n",
    "- **自適應系統設計**：動態batch sizing、記憶體感知處理\n",
    "- **全面效能分析框架**：基準測試、瓶頸識別、優化建議生成\n",
    "\n",
    "### 🧠 進階核心原理 (Advanced Core Principles)\n",
    "- **可擴展性設計模式**：從單機到叢集的無縫擴展策略\n",
    "- **品質感知處理**：將資料品質作為處理決策的核心參數\n",
    "- **資源感知調度**：根據系統資源動態調整處理策略\n",
    "- **多模態整合處理**：統一的品質控制與異常檢測框架\n",
    "- **生產級錯誤處理**：graceful degradation 與自動恢復機制\n",
    "\n",
    "### 📈 實戰應用場景 (Real-world Applications)\n",
    "1. **大型語料庫清理**：TB級文本資料的品質控制與去重\n",
    "2. **多媒體內容管理**：圖片/音頻的品質評估與自動分類\n",
    "3. **即時資料流處理**：高吞吐量的串流資料前處理\n",
    "4. **分散式訓練資料準備**：多節點環境下的資料分發與同步\n",
    "5. **邊緣計算優化**：資源受限環境下的高效處理策略\n",
    "\n",
    "### 🚀 下一步整合建議 (Next Integration Steps)\n",
    "1. **立即應用**：將進階處理技巧整合到後續的 fine-tuning 和 RAG 系統中\n",
    "2. **擴展方向**：探索 Apache Spark / Dask 等大資料處理框架的整合\n",
    "3. **監控完善**：建立資料處理的 MLOps 監控與告警系統\n",
    "4. **效能優化**：針對特定硬體環境進行客製化優化\n",
    "\n",
    "**何時使用這些進階技巧**：當資料量超過單機記憶體限制、需要保證資料品質、要求高吞吐量處理、或部署到生產環境時。這些技巧特別適用於構建robust的資料處理管線，為後續的模型訓練和推理提供高品質的資料基礎。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
