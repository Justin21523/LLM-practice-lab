{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bdac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb01_tensor_autograd.ipynb\n",
    "# PyTorch Tensor Operations & Autograd Fundamentals\n",
    "\n",
    "# Cell 1: Shared Cache Bootstrap & Environment Setup\n",
    "import os, pathlib, torch\n",
    "import numpy as np\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )\n",
    "print(f\"[PyTorch] Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5138e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Basic Tensor Creation & Operations\n",
    "# åŸºæœ¬å¼µé‡å‰µå»ºèˆ‡æ“ä½œ\n",
    "\n",
    "def demonstrate_tensor_basics():\n",
    "    \"\"\"Demonstrate basic tensor creation and operations\"\"\"\n",
    "\n",
    "    # Create tensors with different methods\n",
    "    print(\"=== Tensor Creation ===\")\n",
    "\n",
    "    # From Python lists\n",
    "    data = [[1, 2], [3, 4]]\n",
    "    x_data = torch.tensor(data, dtype=torch.float32)\n",
    "    print(f\"From list: {x_data}\")\n",
    "\n",
    "    # From NumPy arrays\n",
    "    np_array = np.array(data)\n",
    "    x_np = torch.from_numpy(np_array).float()\n",
    "    print(f\"From numpy: {x_np}\")\n",
    "\n",
    "    # Specific creation functions\n",
    "    x_ones = torch.ones(2, 3)\n",
    "    x_zeros = torch.zeros_like(x_ones)\n",
    "    x_rand = torch.randn(2, 3)  # Normal distribution\n",
    "\n",
    "    print(f\"Ones: \\n{x_ones}\")\n",
    "    print(f\"Zeros: \\n{x_zeros}\")\n",
    "    print(f\"Random: \\n{x_rand}\")\n",
    "\n",
    "    # Tensor properties\n",
    "    print(f\"\\n=== Tensor Properties ===\")\n",
    "    print(f\"Shape: {x_rand.shape}\")\n",
    "    print(f\"Dtype: {x_rand.dtype}\")\n",
    "    print(f\"Device: {x_rand.device}\")\n",
    "\n",
    "    return x_rand\n",
    "\n",
    "\n",
    "# Run basic demo\n",
    "sample_tensor = demonstrate_tensor_basics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5afb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Tensor Operations & Broadcasting\n",
    "# å¼µé‡é‹ç®—èˆ‡å»£æ’­æ©Ÿåˆ¶\n",
    "\n",
    "\n",
    "def demonstrate_tensor_operations():\n",
    "    \"\"\"Show various tensor operations and broadcasting\"\"\"\n",
    "\n",
    "    print(\"=== Arithmetic Operations ===\")\n",
    "\n",
    "    a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "    b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "    # Element-wise operations\n",
    "    print(f\"A: \\n{a}\")\n",
    "    print(f\"B: \\n{b}\")\n",
    "    print(f\"A + B: \\n{a + b}\")\n",
    "    print(f\"A * B (element-wise): \\n{a * b}\")\n",
    "    print(f\"A @ B (matrix multiplication): \\n{a @ b}\")\n",
    "\n",
    "    # Broadcasting examples\n",
    "    print(f\"\\n=== Broadcasting ===\")\n",
    "    c = torch.tensor([1, 2])  # Shape: (2,)\n",
    "    print(f\"A + c (broadcasting): \\n{a + c}\")\n",
    "\n",
    "    # Reduction operations\n",
    "    print(f\"\\n=== Reductions ===\")\n",
    "    print(f\"Sum: {a.sum()}\")\n",
    "    print(f\"Mean: {a.mean()}\")\n",
    "    print(f\"Max: {a.max()}\")\n",
    "    print(f\"Argmax: {a.argmax()}\")\n",
    "\n",
    "    # Reshape operations\n",
    "    print(f\"\\n=== Reshaping ===\")\n",
    "    x = torch.arange(12).float()\n",
    "    print(f\"Original: {x}\")\n",
    "    print(f\"Reshaped (3x4): \\n{x.view(3, 4)}\")\n",
    "    print(f\"Reshaped (2x6): \\n{x.reshape(2, 6)}\")\n",
    "\n",
    "    return a, b\n",
    "\n",
    "\n",
    "# Run operations demo\n",
    "tensor_a, tensor_b = demonstrate_tensor_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Autograd Fundamentals\n",
    "# è‡ªå‹•å¾®åˆ†åŸºç¤\n",
    "\n",
    "\n",
    "def demonstrate_autograd_basics():\n",
    "    \"\"\"Demonstrate automatic differentiation with simple examples\"\"\"\n",
    "\n",
    "    print(\"=== Autograd Basics ===\")\n",
    "\n",
    "    # Create tensors with gradient tracking\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "    # Simple function: z = x^2 + 2*x*y + y^2\n",
    "    z = x**2 + 2 * x * y + y**2\n",
    "\n",
    "    print(f\"x = {x.item()}, y = {y.item()}\")\n",
    "    print(f\"z = xÂ² + 2xy + yÂ² = {z.item()}\")\n",
    "\n",
    "    # Compute gradients\n",
    "    z.backward()\n",
    "\n",
    "    print(f\"âˆ‚z/âˆ‚x = {x.grad.item()}\")  # Should be 2x + 2y = 4 + 6 = 10\n",
    "    print(f\"âˆ‚z/âˆ‚y = {y.grad.item()}\")  # Should be 2x + 2y = 4 + 6 = 10\n",
    "\n",
    "    # Verify manual calculation\n",
    "    manual_dz_dx = 2 * x.item() + 2 * y.item()\n",
    "    manual_dz_dy = 2 * x.item() + 2 * y.item()\n",
    "    print(f\"Manual âˆ‚z/âˆ‚x = {manual_dz_dx}\")\n",
    "    print(f\"Manual âˆ‚z/âˆ‚y = {manual_dz_dy}\")\n",
    "\n",
    "\n",
    "# Run autograd demo\n",
    "demonstrate_autograd_basics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Gradient Computation for Vectors/Matrices\n",
    "# å‘é‡/çŸ©é™£çš„æ¢¯åº¦è¨ˆç®—\n",
    "\n",
    "\n",
    "def demonstrate_vector_gradients():\n",
    "    \"\"\"Show gradient computation for vector/matrix operations\"\"\"\n",
    "\n",
    "    print(\"=== Vector/Matrix Gradients ===\")\n",
    "\n",
    "    # Vector input\n",
    "    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "    # Vector function: f(x) = sum(x^2) = xâ‚Â² + xâ‚‚Â² + xâ‚ƒÂ²\n",
    "    f = torch.sum(x**2)\n",
    "\n",
    "    print(f\"x = {x}\")\n",
    "    print(f\"f(x) = sum(xÂ²) = {f.item()}\")\n",
    "\n",
    "    f.backward()\n",
    "    print(f\"âˆ‡f = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, âˆ‚f/âˆ‚xâ‚ƒ] = {x.grad}\")\n",
    "    print(f\"Expected: [2xâ‚, 2xâ‚‚, 2xâ‚ƒ] = {2*x.detach()}\")\n",
    "\n",
    "    # Matrix example\n",
    "    print(f\"\\n=== Matrix Gradients ===\")\n",
    "    W = torch.randn(2, 3, requires_grad=True)\n",
    "    x_input = torch.randn(3, 1)\n",
    "\n",
    "    # Linear transformation + loss\n",
    "    y = W @ x_input  # Matrix multiplication\n",
    "    loss = torch.sum(y**2)  # Simple quadratic loss\n",
    "\n",
    "    print(f\"W shape: {W.shape}\")\n",
    "    print(f\"x shape: {x_input.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    loss.backward()\n",
    "    print(f\"Gradient W.grad shape: {W.grad.shape}\")\n",
    "    print(f\"Gradient norm: {torch.norm(W.grad).item():.4f}\")\n",
    "\n",
    "\n",
    "# Run vector gradients demo\n",
    "demonstrate_vector_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141242dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Gradient Flow in Neural Network Context\n",
    "# ç¥ç¶“ç¶²è·¯æƒ…å¢ƒä¸‹çš„æ¢¯åº¦æµ\n",
    "\n",
    "\n",
    "def demonstrate_neural_network_gradients():\n",
    "    \"\"\"Simulate simple neural network forward and backward pass\"\"\"\n",
    "\n",
    "    print(\"=== Simple Neural Network Gradients ===\")\n",
    "\n",
    "    # Input data (batch_size=4, features=3)\n",
    "    X = torch.randn(4, 3)\n",
    "    y_true = torch.randn(4, 1)  # Target values\n",
    "\n",
    "    # Network parameters\n",
    "    W1 = torch.randn(3, 5, requires_grad=True)  # Input to hidden\n",
    "    b1 = torch.zeros(5, requires_grad=True)  # Hidden bias\n",
    "    W2 = torch.randn(5, 1, requires_grad=True)  # Hidden to output\n",
    "    b2 = torch.zeros(1, requires_grad=True)  # Output bias\n",
    "\n",
    "    print(f\"Input shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y_true.shape}\")\n",
    "\n",
    "    # Forward pass\n",
    "    hidden = torch.relu(X @ W1 + b1)  # ReLU activation\n",
    "    output = hidden @ W2 + b2  # Linear output\n",
    "\n",
    "    # Mean Squared Error loss\n",
    "    loss = torch.mean((output - y_true) ** 2)\n",
    "\n",
    "    print(f\"Hidden shape: {hidden.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Loss: {loss.item():.6f}\")\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Check gradients exist\n",
    "    print(f\"\\n=== Gradient Information ===\")\n",
    "    print(f\"W1.grad exists: {W1.grad is not None}\")\n",
    "    print(f\"W1.grad shape: {W1.grad.shape}\")\n",
    "    print(f\"W1.grad norm: {torch.norm(W1.grad).item():.6f}\")\n",
    "\n",
    "    print(f\"W2.grad exists: {W2.grad is not None}\")\n",
    "    print(f\"W2.grad shape: {W2.grad.shape}\")\n",
    "    print(f\"W2.grad norm: {torch.norm(W2.grad).item():.6f}\")\n",
    "\n",
    "    # Gradient descent step (manual)\n",
    "    learning_rate = 0.01\n",
    "    with torch.no_grad():\n",
    "        W1 -= learning_rate * W1.grad\n",
    "        W2 -= learning_rate * W2.grad\n",
    "        b1 -= learning_rate * b1.grad\n",
    "        b2 -= learning_rate * b2.grad\n",
    "\n",
    "    print(f\"Applied gradient descent with lr={learning_rate}\")\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Run neural network demo\n",
    "initial_loss = demonstrate_neural_network_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Device Management (CPU/GPU)\n",
    "# è¨­å‚™ç®¡ç† (CPU/GPU)\n",
    "\n",
    "\n",
    "def demonstrate_device_management():\n",
    "    \"\"\"Show how to work with different devices\"\"\"\n",
    "\n",
    "    print(\"=== Device Management ===\")\n",
    "\n",
    "    # Check available devices\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create tensors on specific devices\n",
    "    x_cpu = torch.randn(1000, 1000)\n",
    "    print(f\"CPU tensor device: {x_cpu.device}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        x_gpu = x_cpu.to(device)\n",
    "        print(f\"GPU tensor device: {x_gpu.device}\")\n",
    "\n",
    "        # Timing comparison for matrix multiplication\n",
    "        import time\n",
    "\n",
    "        # CPU timing\n",
    "        start_time = time.time()\n",
    "        result_cpu = x_cpu @ x_cpu.T\n",
    "        cpu_time = time.time() - start_time\n",
    "\n",
    "        # GPU timing (with synchronization)\n",
    "        start_time = time.time()\n",
    "        result_gpu = x_gpu @ x_gpu.T\n",
    "        torch.cuda.synchronize()  # Wait for GPU computation\n",
    "        gpu_time = time.time() - start_time\n",
    "\n",
    "        print(f\"CPU matmul time: {cpu_time:.4f}s\")\n",
    "        print(f\"GPU matmul time: {gpu_time:.4f}s\")\n",
    "        print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "    else:\n",
    "        print(\"CUDA not available, using CPU only\")\n",
    "\n",
    "\n",
    "# Run device management demo\n",
    "demonstrate_device_management()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb278ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Smoke Test - Verification\n",
    "# é©—æ”¶æ¸¬è©¦\n",
    "\n",
    "\n",
    "def smoke_test():\n",
    "    \"\"\"Quick verification that all concepts work correctly\"\"\"\n",
    "\n",
    "    print(\"=== Smoke Test ===\")\n",
    "\n",
    "    # Test 1: Basic tensor creation and operations\n",
    "    x = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
    "    y = torch.sum(x**2)\n",
    "    y.backward()\n",
    "\n",
    "    expected_grad = torch.tensor([2, 4, 6], dtype=torch.float32)\n",
    "    assert torch.allclose(x.grad, expected_grad), \"Gradient computation failed\"\n",
    "    print(\"âœ“ Basic autograd works\")\n",
    "\n",
    "    # Test 2: Matrix operations\n",
    "    A = torch.randn(2, 3, requires_grad=True)\n",
    "    B = torch.randn(3, 2)\n",
    "    C = A @ B\n",
    "    loss = torch.sum(C)\n",
    "    loss.backward()\n",
    "\n",
    "    assert A.grad is not None, \"Matrix gradient is None\"\n",
    "    assert A.grad.shape == A.shape, \"Gradient shape mismatch\"\n",
    "    print(\"âœ“ Matrix gradients work\")\n",
    "\n",
    "    # Test 3: Device placement (if GPU available)\n",
    "    if torch.cuda.is_available():\n",
    "        x_gpu = torch.randn(10, device=\"cuda\")\n",
    "        assert x_gpu.device.type == \"cuda\", \"GPU placement failed\"\n",
    "        print(\"âœ“ GPU placement works\")\n",
    "    else:\n",
    "        print(\"âœ“ CPU-only mode works\")\n",
    "\n",
    "    # Test 4: Shared cache is configured\n",
    "    assert \"TORCH_HOME\" in os.environ, \"TORCH_HOME not set\"\n",
    "    assert os.path.exists(os.environ[\"TORCH_HOME\"]), \"TORCH_HOME directory missing\"\n",
    "    print(\"âœ“ Shared cache configured\")\n",
    "\n",
    "    print(\"\\nğŸ‰ All smoke tests passed!\")\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Summary and Next Steps\n",
    "print(\n",
    "    \"\"\"\n",
    "=== æœ¬ç« å®Œæˆæ‘˜è¦ (Chapter Summary) ===\n",
    "\n",
    "âœ… å®Œæˆé …ç›® (Completed Items):\n",
    "- PyTorch å¼µé‡åŸºæœ¬æ“ä½œ (Basic tensor operations)\n",
    "- è‡ªå‹•å¾®åˆ†æ©Ÿåˆ¶ç†è§£ (Autograd mechanism understanding)\n",
    "- æ¢¯åº¦è¨ˆç®—èˆ‡åå‘å‚³æ’­ (Gradient computation & backpropagation)\n",
    "- è¨­å‚™ç®¡ç† (CPU/GPU) (Device management)\n",
    "- å…±äº«å¿«å–é…ç½® (Shared cache setup)\n",
    "\n",
    "ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µ (Core Concepts):\n",
    "- Tensor: PyTorch çš„åŸºæœ¬æ•¸æ“šçµæ§‹ï¼Œæ”¯æ´ GPU åŠ é€Ÿ\n",
    "- Autograd: è‡ªå‹•å¾®åˆ†ç³»çµ±ï¼Œè¿½è¹¤é‹ç®—åœ–ä¸¦è¨ˆç®—æ¢¯åº¦\n",
    "- requires_grad=True: å•Ÿç”¨æ¢¯åº¦è¿½è¹¤çš„é—œéµåƒæ•¸\n",
    "- .backward(): åŸ·è¡Œåå‘å‚³æ’­çš„æ–¹æ³•\n",
    "- Device placement: åœ¨ CPU/GPU é–“ç§»å‹•å¼µé‡\n",
    "\n",
    "âš ï¸ å¸¸è¦‹é™·é˜± (Common Pitfalls):\n",
    "- å¿˜è¨˜è¨­å®š requires_grad=True å°è‡´ç„¡æ³•è¨ˆç®—æ¢¯åº¦\n",
    "- åœ¨ä¸åŒè¨­å‚™é–“æ“ä½œå¼µé‡æœƒå ±éŒ¯\n",
    "- æ¢¯åº¦æœƒç´¯ç©ï¼Œéœ€è¦æ‰‹å‹•æ¸…é›¶ (.zero_grad())\n",
    "- è¨ˆç®—åœ–åœ¨ .backward() å¾Œæœƒè¢«é‡‹æ”¾\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥ (Next Steps):\n",
    "- å­¸ç¿’ nn.Module å’Œè‡ªè¨‚å±¤ (Custom layers)\n",
    "- ç†è§£è¨“ç·´è¿´åœˆçµæ§‹ (Training loop structure)\n",
    "- æŒæ¡å„ªåŒ–å™¨ä½¿ç”¨ (Optimizer usage)\n",
    "- å¯¦ä½œå®Œæ•´çš„ç¥ç¶“ç¶²è·¯è¨“ç·´æµç¨‹\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6da1e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## æœ¬ç« å°çµ\n",
    "\n",
    "### âœ… å®Œæˆé …ç›®\n",
    "- **å…±äº«å¿«å–ç³»çµ±**ï¼šå»ºç«‹çµ±ä¸€çš„æ¨¡å‹å’Œæ•¸æ“šå­˜æ”¾æ©Ÿåˆ¶\n",
    "- **PyTorch åŸºç¤**ï¼šå¼µé‡æ“ä½œã€å»£æ’­ã€é‡å¡‘ç­‰æ ¸å¿ƒæ¦‚å¿µ\n",
    "- **è‡ªå‹•å¾®åˆ†**ï¼šå¾ç°¡å–®å‡½æ•¸åˆ°ç¥ç¶“ç¶²è·¯çš„æ¢¯åº¦è¨ˆç®—\n",
    "- **è¨­å‚™ç®¡ç†**ï¼šCPU/GPU åˆ‡æ›å’Œæ€§èƒ½å„ªåŒ–\n",
    "- **å¯¦ç”¨ç¯„ä¾‹**ï¼šå¯ç›´æ¥é‹è¡Œçš„æœ€å°å¯è¡Œä»£ç¢¼\n",
    "\n",
    "### ğŸ”‘ åŸç†è¦é»\n",
    "- **è¨ˆç®—åœ– (Computation Graph)**ï¼šPyTorch å‹•æ…‹å»ºæ§‹é‹ç®—åœ–ä¾†è¿½è¹¤æ¢¯åº¦\n",
    "- **åå‘å‚³æ’­ (Backpropagation)**ï¼šé€šééˆå¼æ³•å‰‡è‡ªå‹•è¨ˆç®—æ‰€æœ‰åƒæ•¸çš„æ¢¯åº¦\n",
    "- **è¨˜æ†¶é«”æ•ˆç‡**ï¼šé©ç•¶ä½¿ç”¨ `with torch.no_grad()` ä¾†ç¯€çœè¨˜æ†¶é«”\n",
    "- **è¨­å‚™ä¸€è‡´æ€§**ï¼šç¢ºä¿æ‰€æœ‰å¼µé‡åœ¨ç›¸åŒè¨­å‚™ä¸Šé€²è¡Œé‹ç®—\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å»ºè­°\n",
    "1. **ç«‹å³è¡Œå‹•**ï¼šé€²å…¥ `nb02_nn_module_training.ipynb` å­¸ç¿’æ¨¡çµ„åŒ–ç¶²è·¯è¨­è¨ˆ\n",
    "2. **æ·±åŒ–ç†è§£**ï¼šå˜—è©¦ä¿®æ”¹ç¯„ä¾‹ä¸­çš„å‡½æ•¸ï¼Œè§€å¯Ÿæ¢¯åº¦è®ŠåŒ–\n",
    "3. **æ•ˆèƒ½å„ªåŒ–**ï¼šå¯¦é©—ä¸åŒçš„è³‡æ–™é¡å‹ï¼ˆfloat16/float32ï¼‰å°é€Ÿåº¦çš„å½±éŸ¿\n",
    "4. **å¯¦éš›æ‡‰ç”¨**ï¼šæº–å‚™å­¸ç¿’å¦‚ä½•å°‡é€™äº›åŸºç¤æ¦‚å¿µæ‡‰ç”¨åˆ°å¯¦éš›çš„æ©Ÿå™¨å­¸ç¿’ä»»å‹™ä¸­\n",
    "\n",
    "**æº–å‚™å¥½é€²å…¥ä¸‹ä¸€å€‹ notebookï¼š`nb02_nn_module_training.ipynb` äº†å—ï¼Ÿæˆ‘å€‘å°‡å­¸ç¿’å¦‚ä½•å»ºæ§‹å¯é‡ç”¨çš„ç¥ç¶“ç¶²è·¯æ¨¡çµ„å’Œå®Œæ•´çš„è¨“ç·´æµç¨‹ã€‚**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
