{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bdac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb01_tensor_autograd.ipynb\n",
    "# PyTorch Tensor Operations & Autograd Fundamentals\n",
    "\n",
    "# Cell 1: Shared Cache Bootstrap & Environment Setup\n",
    "import os, pathlib, torch\n",
    "import numpy as np\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )\n",
    "print(f\"[PyTorch] Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5138e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Basic Tensor Creation & Operations\n",
    "# 基本張量創建與操作\n",
    "\n",
    "def demonstrate_tensor_basics():\n",
    "    \"\"\"Demonstrate basic tensor creation and operations\"\"\"\n",
    "\n",
    "    # Create tensors with different methods\n",
    "    print(\"=== Tensor Creation ===\")\n",
    "\n",
    "    # From Python lists\n",
    "    data = [[1, 2], [3, 4]]\n",
    "    x_data = torch.tensor(data, dtype=torch.float32)\n",
    "    print(f\"From list: {x_data}\")\n",
    "\n",
    "    # From NumPy arrays\n",
    "    np_array = np.array(data)\n",
    "    x_np = torch.from_numpy(np_array).float()\n",
    "    print(f\"From numpy: {x_np}\")\n",
    "\n",
    "    # Specific creation functions\n",
    "    x_ones = torch.ones(2, 3)\n",
    "    x_zeros = torch.zeros_like(x_ones)\n",
    "    x_rand = torch.randn(2, 3)  # Normal distribution\n",
    "\n",
    "    print(f\"Ones: \\n{x_ones}\")\n",
    "    print(f\"Zeros: \\n{x_zeros}\")\n",
    "    print(f\"Random: \\n{x_rand}\")\n",
    "\n",
    "    # Tensor properties\n",
    "    print(f\"\\n=== Tensor Properties ===\")\n",
    "    print(f\"Shape: {x_rand.shape}\")\n",
    "    print(f\"Dtype: {x_rand.dtype}\")\n",
    "    print(f\"Device: {x_rand.device}\")\n",
    "\n",
    "    return x_rand\n",
    "\n",
    "\n",
    "# Run basic demo\n",
    "sample_tensor = demonstrate_tensor_basics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5afb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Tensor Operations & Broadcasting\n",
    "# 張量運算與廣播機制\n",
    "\n",
    "\n",
    "def demonstrate_tensor_operations():\n",
    "    \"\"\"Show various tensor operations and broadcasting\"\"\"\n",
    "\n",
    "    print(\"=== Arithmetic Operations ===\")\n",
    "\n",
    "    a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "    b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "    # Element-wise operations\n",
    "    print(f\"A: \\n{a}\")\n",
    "    print(f\"B: \\n{b}\")\n",
    "    print(f\"A + B: \\n{a + b}\")\n",
    "    print(f\"A * B (element-wise): \\n{a * b}\")\n",
    "    print(f\"A @ B (matrix multiplication): \\n{a @ b}\")\n",
    "\n",
    "    # Broadcasting examples\n",
    "    print(f\"\\n=== Broadcasting ===\")\n",
    "    c = torch.tensor([1, 2])  # Shape: (2,)\n",
    "    print(f\"A + c (broadcasting): \\n{a + c}\")\n",
    "\n",
    "    # Reduction operations\n",
    "    print(f\"\\n=== Reductions ===\")\n",
    "    print(f\"Sum: {a.sum()}\")\n",
    "    print(f\"Mean: {a.mean()}\")\n",
    "    print(f\"Max: {a.max()}\")\n",
    "    print(f\"Argmax: {a.argmax()}\")\n",
    "\n",
    "    # Reshape operations\n",
    "    print(f\"\\n=== Reshaping ===\")\n",
    "    x = torch.arange(12).float()\n",
    "    print(f\"Original: {x}\")\n",
    "    print(f\"Reshaped (3x4): \\n{x.view(3, 4)}\")\n",
    "    print(f\"Reshaped (2x6): \\n{x.reshape(2, 6)}\")\n",
    "\n",
    "    return a, b\n",
    "\n",
    "\n",
    "# Run operations demo\n",
    "tensor_a, tensor_b = demonstrate_tensor_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Autograd Fundamentals\n",
    "# 自動微分基礎\n",
    "\n",
    "\n",
    "def demonstrate_autograd_basics():\n",
    "    \"\"\"Demonstrate automatic differentiation with simple examples\"\"\"\n",
    "\n",
    "    print(\"=== Autograd Basics ===\")\n",
    "\n",
    "    # Create tensors with gradient tracking\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "    # Simple function: z = x^2 + 2*x*y + y^2\n",
    "    z = x**2 + 2 * x * y + y**2\n",
    "\n",
    "    print(f\"x = {x.item()}, y = {y.item()}\")\n",
    "    print(f\"z = x² + 2xy + y² = {z.item()}\")\n",
    "\n",
    "    # Compute gradients\n",
    "    z.backward()\n",
    "\n",
    "    print(f\"∂z/∂x = {x.grad.item()}\")  # Should be 2x + 2y = 4 + 6 = 10\n",
    "    print(f\"∂z/∂y = {y.grad.item()}\")  # Should be 2x + 2y = 4 + 6 = 10\n",
    "\n",
    "    # Verify manual calculation\n",
    "    manual_dz_dx = 2 * x.item() + 2 * y.item()\n",
    "    manual_dz_dy = 2 * x.item() + 2 * y.item()\n",
    "    print(f\"Manual ∂z/∂x = {manual_dz_dx}\")\n",
    "    print(f\"Manual ∂z/∂y = {manual_dz_dy}\")\n",
    "\n",
    "\n",
    "# Run autograd demo\n",
    "demonstrate_autograd_basics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Gradient Computation for Vectors/Matrices\n",
    "# 向量/矩陣的梯度計算\n",
    "\n",
    "\n",
    "def demonstrate_vector_gradients():\n",
    "    \"\"\"Show gradient computation for vector/matrix operations\"\"\"\n",
    "\n",
    "    print(\"=== Vector/Matrix Gradients ===\")\n",
    "\n",
    "    # Vector input\n",
    "    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "    # Vector function: f(x) = sum(x^2) = x₁² + x₂² + x₃²\n",
    "    f = torch.sum(x**2)\n",
    "\n",
    "    print(f\"x = {x}\")\n",
    "    print(f\"f(x) = sum(x²) = {f.item()}\")\n",
    "\n",
    "    f.backward()\n",
    "    print(f\"∇f = [∂f/∂x₁, ∂f/∂x₂, ∂f/∂x₃] = {x.grad}\")\n",
    "    print(f\"Expected: [2x₁, 2x₂, 2x₃] = {2*x.detach()}\")\n",
    "\n",
    "    # Matrix example\n",
    "    print(f\"\\n=== Matrix Gradients ===\")\n",
    "    W = torch.randn(2, 3, requires_grad=True)\n",
    "    x_input = torch.randn(3, 1)\n",
    "\n",
    "    # Linear transformation + loss\n",
    "    y = W @ x_input  # Matrix multiplication\n",
    "    loss = torch.sum(y**2)  # Simple quadratic loss\n",
    "\n",
    "    print(f\"W shape: {W.shape}\")\n",
    "    print(f\"x shape: {x_input.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    loss.backward()\n",
    "    print(f\"Gradient W.grad shape: {W.grad.shape}\")\n",
    "    print(f\"Gradient norm: {torch.norm(W.grad).item():.4f}\")\n",
    "\n",
    "\n",
    "# Run vector gradients demo\n",
    "demonstrate_vector_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141242dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Gradient Flow in Neural Network Context\n",
    "# 神經網路情境下的梯度流\n",
    "\n",
    "\n",
    "def demonstrate_neural_network_gradients():\n",
    "    \"\"\"Simulate simple neural network forward and backward pass\"\"\"\n",
    "\n",
    "    print(\"=== Simple Neural Network Gradients ===\")\n",
    "\n",
    "    # Input data (batch_size=4, features=3)\n",
    "    X = torch.randn(4, 3)\n",
    "    y_true = torch.randn(4, 1)  # Target values\n",
    "\n",
    "    # Network parameters\n",
    "    W1 = torch.randn(3, 5, requires_grad=True)  # Input to hidden\n",
    "    b1 = torch.zeros(5, requires_grad=True)  # Hidden bias\n",
    "    W2 = torch.randn(5, 1, requires_grad=True)  # Hidden to output\n",
    "    b2 = torch.zeros(1, requires_grad=True)  # Output bias\n",
    "\n",
    "    print(f\"Input shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y_true.shape}\")\n",
    "\n",
    "    # Forward pass\n",
    "    hidden = torch.relu(X @ W1 + b1)  # ReLU activation\n",
    "    output = hidden @ W2 + b2  # Linear output\n",
    "\n",
    "    # Mean Squared Error loss\n",
    "    loss = torch.mean((output - y_true) ** 2)\n",
    "\n",
    "    print(f\"Hidden shape: {hidden.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Loss: {loss.item():.6f}\")\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Check gradients exist\n",
    "    print(f\"\\n=== Gradient Information ===\")\n",
    "    print(f\"W1.grad exists: {W1.grad is not None}\")\n",
    "    print(f\"W1.grad shape: {W1.grad.shape}\")\n",
    "    print(f\"W1.grad norm: {torch.norm(W1.grad).item():.6f}\")\n",
    "\n",
    "    print(f\"W2.grad exists: {W2.grad is not None}\")\n",
    "    print(f\"W2.grad shape: {W2.grad.shape}\")\n",
    "    print(f\"W2.grad norm: {torch.norm(W2.grad).item():.6f}\")\n",
    "\n",
    "    # Gradient descent step (manual)\n",
    "    learning_rate = 0.01\n",
    "    with torch.no_grad():\n",
    "        W1 -= learning_rate * W1.grad\n",
    "        W2 -= learning_rate * W2.grad\n",
    "        b1 -= learning_rate * b1.grad\n",
    "        b2 -= learning_rate * b2.grad\n",
    "\n",
    "    print(f\"Applied gradient descent with lr={learning_rate}\")\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Run neural network demo\n",
    "initial_loss = demonstrate_neural_network_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Device Management (CPU/GPU)\n",
    "# 設備管理 (CPU/GPU)\n",
    "\n",
    "\n",
    "def demonstrate_device_management():\n",
    "    \"\"\"Show how to work with different devices\"\"\"\n",
    "\n",
    "    print(\"=== Device Management ===\")\n",
    "\n",
    "    # Check available devices\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create tensors on specific devices\n",
    "    x_cpu = torch.randn(1000, 1000)\n",
    "    print(f\"CPU tensor device: {x_cpu.device}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        x_gpu = x_cpu.to(device)\n",
    "        print(f\"GPU tensor device: {x_gpu.device}\")\n",
    "\n",
    "        # Timing comparison for matrix multiplication\n",
    "        import time\n",
    "\n",
    "        # CPU timing\n",
    "        start_time = time.time()\n",
    "        result_cpu = x_cpu @ x_cpu.T\n",
    "        cpu_time = time.time() - start_time\n",
    "\n",
    "        # GPU timing (with synchronization)\n",
    "        start_time = time.time()\n",
    "        result_gpu = x_gpu @ x_gpu.T\n",
    "        torch.cuda.synchronize()  # Wait for GPU computation\n",
    "        gpu_time = time.time() - start_time\n",
    "\n",
    "        print(f\"CPU matmul time: {cpu_time:.4f}s\")\n",
    "        print(f\"GPU matmul time: {gpu_time:.4f}s\")\n",
    "        print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "    else:\n",
    "        print(\"CUDA not available, using CPU only\")\n",
    "\n",
    "\n",
    "# Run device management demo\n",
    "demonstrate_device_management()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb278ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Smoke Test - Verification\n",
    "# 驗收測試\n",
    "\n",
    "\n",
    "def smoke_test():\n",
    "    \"\"\"Quick verification that all concepts work correctly\"\"\"\n",
    "\n",
    "    print(\"=== Smoke Test ===\")\n",
    "\n",
    "    # Test 1: Basic tensor creation and operations\n",
    "    x = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
    "    y = torch.sum(x**2)\n",
    "    y.backward()\n",
    "\n",
    "    expected_grad = torch.tensor([2, 4, 6], dtype=torch.float32)\n",
    "    assert torch.allclose(x.grad, expected_grad), \"Gradient computation failed\"\n",
    "    print(\"✓ Basic autograd works\")\n",
    "\n",
    "    # Test 2: Matrix operations\n",
    "    A = torch.randn(2, 3, requires_grad=True)\n",
    "    B = torch.randn(3, 2)\n",
    "    C = A @ B\n",
    "    loss = torch.sum(C)\n",
    "    loss.backward()\n",
    "\n",
    "    assert A.grad is not None, \"Matrix gradient is None\"\n",
    "    assert A.grad.shape == A.shape, \"Gradient shape mismatch\"\n",
    "    print(\"✓ Matrix gradients work\")\n",
    "\n",
    "    # Test 3: Device placement (if GPU available)\n",
    "    if torch.cuda.is_available():\n",
    "        x_gpu = torch.randn(10, device=\"cuda\")\n",
    "        assert x_gpu.device.type == \"cuda\", \"GPU placement failed\"\n",
    "        print(\"✓ GPU placement works\")\n",
    "    else:\n",
    "        print(\"✓ CPU-only mode works\")\n",
    "\n",
    "    # Test 4: Shared cache is configured\n",
    "    assert \"TORCH_HOME\" in os.environ, \"TORCH_HOME not set\"\n",
    "    assert os.path.exists(os.environ[\"TORCH_HOME\"]), \"TORCH_HOME directory missing\"\n",
    "    print(\"✓ Shared cache configured\")\n",
    "\n",
    "    print(\"\\n🎉 All smoke tests passed!\")\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Summary and Next Steps\n",
    "print(\n",
    "    \"\"\"\n",
    "=== 本章完成摘要 (Chapter Summary) ===\n",
    "\n",
    "✅ 完成項目 (Completed Items):\n",
    "- PyTorch 張量基本操作 (Basic tensor operations)\n",
    "- 自動微分機制理解 (Autograd mechanism understanding)\n",
    "- 梯度計算與反向傳播 (Gradient computation & backpropagation)\n",
    "- 設備管理 (CPU/GPU) (Device management)\n",
    "- 共享快取配置 (Shared cache setup)\n",
    "\n",
    "🔑 核心概念 (Core Concepts):\n",
    "- Tensor: PyTorch 的基本數據結構，支援 GPU 加速\n",
    "- Autograd: 自動微分系統，追蹤運算圖並計算梯度\n",
    "- requires_grad=True: 啟用梯度追蹤的關鍵參數\n",
    "- .backward(): 執行反向傳播的方法\n",
    "- Device placement: 在 CPU/GPU 間移動張量\n",
    "\n",
    "⚠️ 常見陷阱 (Common Pitfalls):\n",
    "- 忘記設定 requires_grad=True 導致無法計算梯度\n",
    "- 在不同設備間操作張量會報錯\n",
    "- 梯度會累積，需要手動清零 (.zero_grad())\n",
    "- 計算圖在 .backward() 後會被釋放\n",
    "\n",
    "🚀 下一步 (Next Steps):\n",
    "- 學習 nn.Module 和自訂層 (Custom layers)\n",
    "- 理解訓練迴圈結構 (Training loop structure)\n",
    "- 掌握優化器使用 (Optimizer usage)\n",
    "- 實作完整的神經網路訓練流程\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6da1e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 本章小結\n",
    "\n",
    "### ✅ 完成項目\n",
    "- **共享快取系統**：建立統一的模型和數據存放機制\n",
    "- **PyTorch 基礎**：張量操作、廣播、重塑等核心概念\n",
    "- **自動微分**：從簡單函數到神經網路的梯度計算\n",
    "- **設備管理**：CPU/GPU 切換和性能優化\n",
    "- **實用範例**：可直接運行的最小可行代碼\n",
    "\n",
    "### 🔑 原理要點\n",
    "- **計算圖 (Computation Graph)**：PyTorch 動態建構運算圖來追蹤梯度\n",
    "- **反向傳播 (Backpropagation)**：通過鏈式法則自動計算所有參數的梯度\n",
    "- **記憶體效率**：適當使用 `with torch.no_grad()` 來節省記憶體\n",
    "- **設備一致性**：確保所有張量在相同設備上進行運算\n",
    "\n",
    "### 🚀 下一步建議\n",
    "1. **立即行動**：進入 `nb02_nn_module_training.ipynb` 學習模組化網路設計\n",
    "2. **深化理解**：嘗試修改範例中的函數，觀察梯度變化\n",
    "3. **效能優化**：實驗不同的資料類型（float16/float32）對速度的影響\n",
    "4. **實際應用**：準備學習如何將這些基礎概念應用到實際的機器學習任務中\n",
    "\n",
    "**準備好進入下一個 notebook：`nb02_nn_module_training.ipynb` 了嗎？我們將學習如何建構可重用的神經網路模組和完整的訓練流程。**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
