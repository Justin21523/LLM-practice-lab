{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6442511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: Shared Cache Bootstrap ===\n",
    "import os, pathlib, torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import time\n",
    "\n",
    "# Shared cache setup\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Device] Using: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Custom nn.Module - SimpleMLP Implementation ===\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Multi-Layer Perceptron for binary classification\n",
    "    Features: configurable hidden layers, dropout, batch normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dims: List[int],\n",
    "        output_dim: int = 1,\n",
    "        dropout_rate: float = 0.2,\n",
    "        use_batch_norm: bool = True,\n",
    "    ):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "\n",
    "        # Build layers dynamically\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "\n",
    "            # Batch normalization (optional)\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "            # Activation function\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "            # Dropout (optional)\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "\n",
    "        # Combine all layers\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Xavier/He initialization for better convergence\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "    def get_num_parameters(self) -> int:\n",
    "        \"\"\"Count total trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Test the model architecture\n",
    "model = SimpleMLP(input_dim=20, hidden_dims=[128, 64, 32], output_dim=1)\n",
    "print(f\"[Model] Architecture: {model}\")\n",
    "print(f\"[Model] Total parameters: {model.get_num_parameters():,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(32, 20)  # batch_size=32, input_dim=20\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "    print(f\"[Test] Input shape: {dummy_input.shape}\")\n",
    "    print(f\"[Test] Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Data Loading & Preprocessing ===\n",
    "def create_synthetic_dataset(\n",
    "    n_samples: int = 1000,\n",
    "    n_features: int = 20,\n",
    "    n_informative: int = 15,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Create synthetic binary classification dataset\n",
    "    Returns normalized features and labels as tensors\n",
    "    \"\"\"\n",
    "    # Generate synthetic data\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=n_informative,\n",
    "        n_redundant=0,\n",
    "        n_clusters_per_class=1,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_tensor = torch.FloatTensor(X_normalized)\n",
    "    y_tensor = torch.FloatTensor(y).unsqueeze(1)  # Add dimension for BCEWithLogitsLoss\n",
    "\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "\n",
    "def create_data_loaders(\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    batch_size: int = 32,\n",
    "    train_ratio: float = 0.8,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create train and validation data loaders with proper splitting\n",
    "    \"\"\"\n",
    "    # Set seed for reproducible splitting\n",
    "    torch.manual_seed(random_state)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = TensorDataset(X, y)\n",
    "\n",
    "    # Split into train and validation\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"[Data] Total samples: {len(dataset)}\")\n",
    "    print(f\"[Data] Train samples: {len(train_dataset)}\")\n",
    "    print(f\"[Data] Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# Generate dataset\n",
    "X, y = create_synthetic_dataset(n_samples=1000, n_features=20)\n",
    "train_loader, val_loader = create_data_loaders(X, y, batch_size=32)\n",
    "\n",
    "print(f\"[Dataset] Feature shape: {X.shape}\")\n",
    "print(f\"[Dataset] Label shape: {y.shape}\")\n",
    "print(f\"[Dataset] Class distribution: {torch.bincount(y.long().squeeze())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0950359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Manual Training Loop Implementation ===\n",
    "def train_epoch_manual(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Manual implementation of training loop for one epoch\n",
    "    Returns training metrics\n",
    "    \"\"\"\n",
    "    model.train()  # Set to training mode\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy (for binary classification)\n",
    "        predicted = (torch.sigmoid(output) > 0.5).float()\n",
    "        correct_predictions += (predicted == target).sum().item()\n",
    "        total_samples += target.size(0)\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    return {\"loss\": avg_loss, \"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "def validate_epoch_manual(\n",
    "    model: nn.Module, val_loader: DataLoader, criterion: nn.Module, device: torch.device\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Manual implementation of validation loop for one epoch\n",
    "    Returns validation metrics\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data, target in val_loader:\n",
    "            # Move data to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = (torch.sigmoid(output) > 0.5).float()\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    return {\"loss\": avg_loss, \"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# Test manual training loop\n",
    "model = SimpleMLP(input_dim=20, hidden_dims=[64, 32], output_dim=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"[Manual Training] Testing one epoch...\")\n",
    "train_metrics = train_epoch_manual(model, train_loader, criterion, optimizer, device)\n",
    "val_metrics = validate_epoch_manual(model, val_loader, criterion, device)\n",
    "\n",
    "print(\n",
    "    f\"Train - Loss: {train_metrics['loss']:.4f}, Accuracy: {train_metrics['accuracy']:.4f}\"\n",
    ")\n",
    "print(f\"Val - Loss: {val_metrics['loss']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Reusable Trainer Class Design ===\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Reusable trainer class for PyTorch models\n",
    "    Supports: early stopping, learning rate scheduling, gradient clipping, checkpointing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        criterion: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        device: torch.device,\n",
    "        scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,\n",
    "        gradient_clip_val: Optional[float] = None,\n",
    "        early_stopping_patience: int = 10,\n",
    "    ):\n",
    "\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        self.gradient_clip_val = gradient_clip_val\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "\n",
    "        # Training history\n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_accuracy\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_accuracy\": [],\n",
    "            \"learning_rates\": [],\n",
    "        }\n",
    "\n",
    "        # Early stopping variables\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.patience_counter = 0\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (optional)\n",
    "            if self.gradient_clip_val is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), self.gradient_clip_val\n",
    "                )\n",
    "\n",
    "            # Update parameters\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(output) > 0.5).float()\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss / len(train_loader),\n",
    "            \"accuracy\": correct_predictions / total_samples,\n",
    "        }\n",
    "\n",
    "    def validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                predicted = (torch.sigmoid(output) > 0.5).float()\n",
    "                correct_predictions += (predicted == target).sum().item()\n",
    "                total_samples += target.size(0)\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss / len(val_loader),\n",
    "            \"accuracy\": correct_predictions / total_samples,\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        epochs: int,\n",
    "        verbose: bool = True,\n",
    "    ) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Train the model for multiple epochs\n",
    "        Returns training history\n",
    "        \"\"\"\n",
    "        print(f\"[Trainer] Starting training for {epochs} epochs...\")\n",
    "        print(\n",
    "            f\"[Trainer] Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\"\n",
    "        )\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Train and validate\n",
    "            train_metrics = self.train_epoch(train_loader)\n",
    "            val_metrics = self.validate_epoch(val_loader)\n",
    "\n",
    "            # Update learning rate scheduler\n",
    "            if self.scheduler is not None:\n",
    "                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    self.scheduler.step(val_metrics[\"loss\"])\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Record metrics\n",
    "            self.history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "            self.history[\"train_accuracy\"].append(train_metrics[\"accuracy\"])\n",
    "            self.history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "            self.history[\"val_accuracy\"].append(val_metrics[\"accuracy\"])\n",
    "            self.history[\"learning_rates\"].append(self.optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_metrics[\"loss\"] < self.best_val_loss:\n",
    "                self.best_val_loss = val_metrics[\"loss\"]\n",
    "                self.patience_counter = 0\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "\n",
    "            # Print progress\n",
    "            if verbose:\n",
    "                epoch_time = time.time() - start_time\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                    f\"Train Loss: {train_metrics['loss']:.4f} | \"\n",
    "                    f\"Train Acc: {train_metrics['accuracy']:.4f} | \"\n",
    "                    f\"Val Loss: {val_metrics['loss']:.4f} | \"\n",
    "                    f\"Val Acc: {val_metrics['accuracy']:.4f} | \"\n",
    "                    f\"LR: {self.optimizer.param_groups[0]['lr']:.6f} | \"\n",
    "                    f\"Time: {epoch_time:.2f}s\"\n",
    "                )\n",
    "\n",
    "            # Early stopping\n",
    "            if self.patience_counter >= self.early_stopping_patience:\n",
    "                print(f\"[Trainer] Early stopping triggered after {epoch+1} epochs\")\n",
    "                print(f\"[Trainer] Best validation loss: {self.best_val_loss:.4f}\")\n",
    "                break\n",
    "\n",
    "        # Load best model\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "            print(f\"[Trainer] Loaded best model (val_loss: {self.best_val_loss:.4f})\")\n",
    "\n",
    "        return self.history\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "        # Loss plot\n",
    "        axes[0, 0].plot(self.history[\"train_loss\"], label=\"Train Loss\", color=\"blue\")\n",
    "        axes[0, 0].plot(self.history[\"val_loss\"], label=\"Val Loss\", color=\"red\")\n",
    "        axes[0, 0].set_title(\"Loss\")\n",
    "        axes[0, 0].set_xlabel(\"Epoch\")\n",
    "        axes[0, 0].set_ylabel(\"Loss\")\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "\n",
    "        # Accuracy plot\n",
    "        axes[0, 1].plot(self.history[\"train_accuracy\"], label=\"Train Acc\", color=\"blue\")\n",
    "        axes[0, 1].plot(self.history[\"val_accuracy\"], label=\"Val Acc\", color=\"red\")\n",
    "        axes[0, 1].set_title(\"Accuracy\")\n",
    "        axes[0, 1].set_xlabel(\"Epoch\")\n",
    "        axes[0, 1].set_ylabel(\"Accuracy\")\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "\n",
    "        # Learning rate plot\n",
    "        axes[1, 0].plot(self.history[\"learning_rates\"], color=\"green\")\n",
    "        axes[1, 0].set_title(\"Learning Rate\")\n",
    "        axes[1, 0].set_xlabel(\"Epoch\")\n",
    "        axes[1, 0].set_ylabel(\"Learning Rate\")\n",
    "        axes[1, 0].grid(True)\n",
    "\n",
    "        # Combined metrics\n",
    "        axes[1, 1].plot(self.history[\"train_loss\"], label=\"Train Loss\", alpha=0.7)\n",
    "        axes[1, 1].plot(self.history[\"val_loss\"], label=\"Val Loss\", alpha=0.7)\n",
    "        ax2 = axes[1, 1].twinx()\n",
    "        ax2.plot(\n",
    "            self.history[\"train_accuracy\"], label=\"Train Acc\", color=\"orange\", alpha=0.7\n",
    "        )\n",
    "        ax2.plot(\n",
    "            self.history[\"val_accuracy\"], label=\"Val Acc\", color=\"purple\", alpha=0.7\n",
    "        )\n",
    "        axes[1, 1].set_title(\"Combined Metrics\")\n",
    "        axes[1, 1].set_xlabel(\"Epoch\")\n",
    "        axes[1, 1].set_ylabel(\"Loss\")\n",
    "        ax2.set_ylabel(\"Accuracy\")\n",
    "        axes[1, 1].legend(loc=\"upper left\")\n",
    "        ax2.legend(loc=\"upper right\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae700369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Model Training & Validation Experiment ===\n",
    "# Initialize fresh model and training components\n",
    "model = SimpleMLP(input_dim=20, hidden_dims=[128, 64, 32], output_dim=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Optional: Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    scheduler=scheduler,\n",
    "    gradient_clip_val=1.0,\n",
    "    early_stopping_patience=15,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING EXPERIMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = trainer.fit(\n",
    "    train_loader=train_loader, val_loader=val_loader, epochs=50, verbose=True\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "trainer.plot_training_history()\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "final_train_metrics = trainer.validate_epoch(train_loader)\n",
    "final_val_metrics = trainer.validate_epoch(val_loader)\n",
    "\n",
    "print(\n",
    "    f\"Final Train - Loss: {final_train_metrics['loss']:.4f}, Accuracy: {final_train_metrics['accuracy']:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Final Val   - Loss: {final_val_metrics['loss']:.4f}, Accuracy: {final_val_metrics['accuracy']:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dee64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Model Save/Load & Checkpoint Mechanism ===\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Utility class for saving and loading model checkpoints\n",
    "    Includes model state, optimizer state, and training metadata\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def save_checkpoint(\n",
    "        model: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        epoch: int,\n",
    "        metrics: Dict[str, float],\n",
    "        history: Dict[str, List[float]],\n",
    "        filepath: str,\n",
    "        metadata: Optional[Dict] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save complete training checkpoint\n",
    "        \"\"\"\n",
    "        checkpoint = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"metrics\": metrics,\n",
    "            \"history\": history,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model_info\": {\n",
    "                \"class_name\": model.__class__.__name__,\n",
    "                \"num_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        if metadata:\n",
    "            checkpoint[\"metadata\"] = metadata\n",
    "\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"[Checkpoint] Saved to: {filepath}\")\n",
    "\n",
    "        # Also save a human-readable summary\n",
    "        summary_path = filepath.replace(\".pth\", \"_summary.json\")\n",
    "        summary = {\n",
    "            \"model_class\": checkpoint[\"model_info\"][\"class_name\"],\n",
    "            \"num_parameters\": checkpoint[\"model_info\"][\"num_parameters\"],\n",
    "            \"epoch\": epoch,\n",
    "            \"metrics\": metrics,\n",
    "            \"timestamp\": checkpoint[\"timestamp\"],\n",
    "        }\n",
    "\n",
    "        with open(summary_path, \"w\") as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(\n",
    "        filepath: str,\n",
    "        model: nn.Module,\n",
    "        optimizer: Optional[optim.Optimizer] = None,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load training checkpoint\n",
    "        Returns: epoch, metrics, history\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "\n",
    "        # Load model state\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        # Load optimizer state if provided\n",
    "        if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "        print(f\"[Checkpoint] Loaded from: {filepath}\")\n",
    "        print(f\"[Checkpoint] Epoch: {checkpoint['epoch']}\")\n",
    "        print(f\"[Checkpoint] Timestamp: {checkpoint['timestamp']}\")\n",
    "\n",
    "        return checkpoint[\"epoch\"], checkpoint[\"metrics\"], checkpoint[\"history\"]\n",
    "\n",
    "\n",
    "# Save current model checkpoint\n",
    "checkpoint_dir = pathlib.Path(AI_CACHE_ROOT) / \"checkpoints\" / \"nb02_experiments\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "checkpoint_path = checkpoint_dir / f\"simple_mlp_best_{int(time.time())}.pth\"\n",
    "\n",
    "ModelCheckpoint.save_checkpoint(\n",
    "    model=trainer.model,\n",
    "    optimizer=trainer.optimizer,\n",
    "    epoch=len(trainer.history[\"train_loss\"]),\n",
    "    metrics=final_val_metrics,\n",
    "    history=trainer.history,\n",
    "    filepath=str(checkpoint_path),\n",
    "    metadata={\n",
    "        \"dataset_info\": \"synthetic_binary_classification\",\n",
    "        \"model_config\": {\n",
    "            \"input_dim\": 20,\n",
    "            \"hidden_dims\": [128, 64, 32],\n",
    "            \"output_dim\": 1,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "# Test loading checkpoint\n",
    "print(\"\\n[Test] Loading checkpoint...\")\n",
    "new_model = SimpleMLP(input_dim=20, hidden_dims=[128, 64, 32], output_dim=1)\n",
    "new_optimizer = optim.Adam(new_model.parameters(), lr=0.001)\n",
    "\n",
    "epoch, metrics, loaded_history = ModelCheckpoint.load_checkpoint(\n",
    "    checkpoint_path, new_model, new_optimizer, device\n",
    ")\n",
    "\n",
    "print(f\"[Test] Loaded model metrics: {metrics}\")\n",
    "print(f\"[Test] History length: {len(loaded_history['train_loss'])} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c7e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Advanced Techniques - Gradient Accumulation & Mixed Precision ===\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "class AdvancedTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Extended trainer with gradient accumulation and mixed precision support\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        criterion: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        device: torch.device,\n",
    "        scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,\n",
    "        gradient_clip_val: Optional[float] = None,\n",
    "        early_stopping_patience: int = 10,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        use_mixed_precision: bool = False,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device,\n",
    "            scheduler,\n",
    "            gradient_clip_val,\n",
    "            early_stopping_patience,\n",
    "        )\n",
    "\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.use_mixed_precision = use_mixed_precision\n",
    "\n",
    "        # Mixed precision scaler\n",
    "        self.scaler = (\n",
    "            GradScaler() if use_mixed_precision and device.type == \"cuda\" else None\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"[AdvancedTrainer] Gradient accumulation steps: {gradient_accumulation_steps}\"\n",
    "        )\n",
    "        print(f\"[AdvancedTrainer] Mixed precision: {use_mixed_precision}\")\n",
    "\n",
    "    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"Enhanced training with gradient accumulation and mixed precision\"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "            # Mixed precision forward pass\n",
    "            if self.use_mixed_precision and self.scaler is not None:\n",
    "                with autocast():\n",
    "                    output = self.model(data)\n",
    "                    loss = self.criterion(output, target)\n",
    "                    # Scale loss for gradient accumulation\n",
    "                    loss = loss / self.gradient_accumulation_steps\n",
    "\n",
    "                # Backward pass with gradient scaling\n",
    "                self.scaler.scale(loss).backward()\n",
    "\n",
    "                # Update weights every N steps\n",
    "                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
    "                    if self.gradient_clip_val is not None:\n",
    "                        self.scaler.unscale_(self.optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), self.gradient_clip_val\n",
    "                        )\n",
    "\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "            else:\n",
    "                # Standard training\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                loss = loss / self.gradient_accumulation_steps\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
    "                    if self.gradient_clip_val is not None:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), self.gradient_clip_val\n",
    "                        )\n",
    "\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "            # Accumulate metrics (use original loss scale)\n",
    "            total_loss += loss.item() * self.gradient_accumulation_steps\n",
    "            predicted = (torch.sigmoid(output) > 0.5).float()\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss / len(train_loader),\n",
    "            \"accuracy\": correct_predictions / total_samples,\n",
    "        }\n",
    "\n",
    "\n",
    "# Demonstrate advanced training (optional for GPU users)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ADVANCED TRAINING DEMO (GPU)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Create advanced trainer with gradient accumulation and mixed precision\n",
    "    advanced_model = SimpleMLP(input_dim=20, hidden_dims=[256, 128, 64], output_dim=1)\n",
    "    advanced_optimizer = optim.AdamW(\n",
    "        advanced_model.parameters(), lr=0.001, weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    advanced_trainer = AdvancedTrainer(\n",
    "        model=advanced_model,\n",
    "        criterion=nn.BCEWithLogitsLoss(),\n",
    "        optimizer=advanced_optimizer,\n",
    "        device=device,\n",
    "        gradient_accumulation_steps=4,  # Simulate larger batch size\n",
    "        use_mixed_precision=True,\n",
    "        gradient_clip_val=1.0,\n",
    "        early_stopping_patience=10,\n",
    "    )\n",
    "\n",
    "    # Quick training run\n",
    "    print(\n",
    "        \"[Advanced] Running 10 epochs with gradient accumulation + mixed precision...\"\n",
    "    )\n",
    "    advanced_history = advanced_trainer.fit(\n",
    "        train_loader, val_loader, epochs=10, verbose=True\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"\\n[Advanced] Skipping advanced features (CPU mode)\")\n",
    "    print(\"Advanced features (mixed precision) require CUDA-enabled GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f218c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9: Smoke Test - Verification ===\n",
    "def run_smoke_tests():\n",
    "    \"\"\"\n",
    "    Comprehensive smoke tests to verify all components work correctly\n",
    "    \"\"\"\n",
    "    print(\"🧪 RUNNING SMOKE TESTS...\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    test_results = []\n",
    "\n",
    "    # Test 1: Model creation and forward pass\n",
    "    try:\n",
    "        test_model = SimpleMLP(input_dim=10, hidden_dims=[32, 16], output_dim=1)\n",
    "        test_input = torch.randn(8, 10)\n",
    "        test_output = test_model(test_input)\n",
    "        assert test_output.shape == (\n",
    "            8,\n",
    "            1,\n",
    "        ), f\"Expected shape (8, 1), got {test_output.shape}\"\n",
    "        test_results.append(\"✅ Model creation and forward pass\")\n",
    "    except Exception as e:\n",
    "        test_results.append(f\"❌ Model creation failed: {e}\")\n",
    "\n",
    "    # Test 2: Parameter counting\n",
    "    try:\n",
    "        param_count = test_model.get_num_parameters()\n",
    "        assert param_count > 0, \"Parameter count should be positive\"\n",
    "        test_results.append(f\"✅ Parameter counting: {param_count:,} params\")\n",
    "    except Exception as e:\n",
    "        test_results.append(f\"❌ Parameter counting failed: {e}\")\n",
    "\n",
    "    # Test 3: Data loading\n",
    "    try:\n",
    "        X_test, y_test = create_synthetic_dataset(n_samples=100, n_features=10)\n",
    "        train_loader_test, val_loader_test = create_data_loaders(\n",
    "            X_test, y_test, batch_size=16\n",
    "        )\n",
    "        assert len(train_loader_test) > 0, \"Train loader should not be empty\"\n",
    "        assert len(val_loader_test) > 0, \"Val loader should not be empty\"\n",
    "        test_results.append(\"✅ Data loading and splitting\")\n",
    "    except Exception as e:\n",
    "        test_results.append(f\"❌ Data loading failed: {e}\")\n",
    "\n",
    "    # Test 4: Training loop\n",
    "    try:\n",
    "        smoke_trainer = Trainer(\n",
    "            model=test_model,\n",
    "            criterion=nn.BCEWithLogitsLoss(),\n",
    "            optimizer=optim.Adam(test_model.parameters(), lr=0.01),\n",
    "            device=torch.device(\"cpu\"),  # Force CPU for reliability\n",
    "            early_stopping_patience=5,\n",
    "        )\n",
    "\n",
    "        # Train for just 2 epochs\n",
    "        smoke_history = smoke_trainer.fit(\n",
    "            train_loader_test, val_loader_test, epochs=2, verbose=False\n",
    "        )\n",
    "        assert len(smoke_history[\"train_loss\"]) == 2, \"Should have 2 epochs of history\"\n",
    "        test_results.append(\"✅ Training loop execution\")\n",
    "    except Exception as e:\n",
    "        test_results.append(f\"❌ Training loop failed: {e}\")\n",
    "\n",
    "    # Test 5: Checkpoint save/load\n",
    "    try:\n",
    "        checkpoint_path = f\"/tmp/smoke_test_checkpoint_{int(time.time())}.pth\"\n",
    "\n",
    "        ModelCheckpoint.save_checkpoint(\n",
    "            model=test_model,\n",
    "            optimizer=smoke_trainer.optimizer,\n",
    "            epoch=2,\n",
    "            metrics={\"loss\": 0.5, \"accuracy\": 0.8},\n",
    "            history=smoke_history,\n",
    "            filepath=checkpoint_path,\n",
    "        )\n",
    "\n",
    "        # Load checkpoint\n",
    "        new_test_model = SimpleMLP(input_dim=10, hidden_dims=[32, 16], output_dim=1)\n",
    "        epoch, metrics, history = ModelCheckpoint.load_checkpoint(\n",
    "            checkpoint_path, new_test_model, device=torch.device(\"cpu\")\n",
    "        )\n",
    "\n",
    "        assert epoch == 2, f\"Expected epoch 2, got {epoch}\"\n",
    "        assert \"loss\" in metrics, \"Metrics should contain loss\"\n",
    "\n",
    "        # Cleanup\n",
    "        os.remove(checkpoint_path)\n",
    "        if os.path.exists(checkpoint_path.replace(\".pth\", \"_summary.json\")):\n",
    "            os.remove(checkpoint_path.replace(\".pth\", \"_summary.json\"))\n",
    "\n",
    "        test_results.append(\"✅ Checkpoint save/load\")\n",
    "    except Exception as e:\n",
    "        test_results.append(f\"❌ Checkpoint save/load failed: {e}\")\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nSMOKE TEST RESULTS:\")\n",
    "    for result in test_results:\n",
    "        print(f\"  {result}\")\n",
    "\n",
    "    # Overall status\n",
    "    passed_tests = sum(1 for r in test_results if r.startswith(\"✅\"))\n",
    "    total_tests = len(test_results)\n",
    "\n",
    "    print(f\"\\n📊 SUMMARY: {passed_tests}/{total_tests} tests passed\")\n",
    "\n",
    "    if passed_tests == total_tests:\n",
    "        print(\"🎉 ALL TESTS PASSED! Ready for production use.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"⚠️  Some tests failed. Please review the issues above.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run smoke tests\n",
    "smoke_test_success = run_smoke_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 10: Chapter Summary & Next Steps ===\n",
    "print(\"\\n\" + \"🎯 CHAPTER SUMMARY\" + \"\\n\" + \"=\" * 50)\n",
    "\n",
    "print(\"✅ COMPLETED ITEMS:\")\n",
    "print(\"  • 自訂 nn.Module 類別設計 (Custom nn.Module class design)\")\n",
    "print(\"  • 完整訓練迴圈實作 (Complete training loop implementation)\")\n",
    "print(\"  • 可重用 Trainer 類別 (Reusable Trainer class)\")\n",
    "print(\"  • Early stopping 與 learning rate scheduling\")\n",
    "print(\"  • 模型檢查點保存與載入 (Model checkpointing)\")\n",
    "print(\"  • 進階技巧：gradient accumulation & mixed precision\")\n",
    "print(\"  • 完整的驗收測試機制 (Comprehensive smoke testing)\")\n",
    "\n",
    "print(\"\\n🧠 CORE CONCEPTS:\")\n",
    "print(\"  • nn.Module 繼承與 forward() 方法實作\")\n",
    "print(\n",
    "    \"  • 訓練迴圈核心組件：loss.backward() → optimizer.step() → optimizer.zero_grad()\"\n",
    ")\n",
    "print(\"  • train()/eval() 模式切換的重要性\")\n",
    "print(\"  • 梯度累積用於模擬大 batch size\")\n",
    "print(\"  • Mixed precision 提升訓練效率\")\n",
    "print(\"  • 檢查點機制確保訓練可恢復性\")\n",
    "\n",
    "print(\"\\n⚠️  COMMON PITFALLS:\")\n",
    "print(\"  • 忘記 optimizer.zero_grad() 導致梯度累積\")\n",
    "print(\"  • train()/eval() 模式不正確影響 BatchNorm/Dropout\")\n",
    "print(\"  • 忽略 gradient clipping 導致梯度爆炸\")\n",
    "print(\"  • 未設置隨機種子導致結果不可重現\")\n",
    "print(\"  • GPU memory leak (忘記 detach() 或清理變數)\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"  • 進入 nb03: HF Datasets 前處理管線\")\n",
    "print(\"  • 學習處理大規模文本/圖像/語音資料\")\n",
    "print(\"  • 準備進入 Transformer 架構學習\")\n",
    "print(\"  • 建立可擴展的資料處理工作流程\")\n",
    "\n",
    "print(\"\\n💡 PRACTICAL RECOMMENDATIONS:\")\n",
    "print(\"  • 始終從小模型開始驗證，再擴展到大模型\")\n",
    "print(\"  • 使用 tensorboard 或 wandb 追蹤實驗（可選）\")\n",
    "print(\"  • 建立模型架構實驗的標準化模板\")\n",
    "print(\"  • 在移到 GPU 前先在 CPU 上驗證邏輯正確性\")\n",
    "print(\"  • 設置適當的 early stopping 避免過擬合\")\n",
    "\n",
    "if smoke_test_success:\n",
    "    print(\"\\n🎊 READY FOR NEXT CHAPTER!\")\n",
    "    print(\"    All components verified and working correctly.\")\n",
    "else:\n",
    "    print(\"\\n⚠️  PLEASE REVIEW FAILED TESTS BEFORE PROCEEDING\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation that everything works\n",
    "print(\"🔍 FINAL VALIDATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test model creation\n",
    "quick_model = SimpleMLP(input_dim=5, hidden_dims=[16, 8], output_dim=1)\n",
    "test_data = torch.randn(4, 5)\n",
    "output = quick_model(test_data)\n",
    "\n",
    "print(f\"✅ Model output shape: {output.shape}\")\n",
    "print(f\"✅ Model parameters: {quick_model.get_num_parameters():,}\")\n",
    "print(f\"✅ Forward pass successful\")\n",
    "\n",
    "# Test training components\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(quick_model.parameters(), lr=0.01)\n",
    "\n",
    "print(f\"✅ Loss function: {criterion.__class__.__name__}\")\n",
    "print(f\"✅ Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(\"🎉 All core components ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1091d",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 本章小結\n",
    "\n",
    "### ✅ 完成項目\n",
    "- **自訂 nn.Module 實作**：完整的 SimpleMLP 類別，支援動態層數配置\n",
    "- **標準訓練迴圈**：手動實作與 Trainer 類別封裝兩種方法\n",
    "- **進階訓練功能**：early stopping、learning rate scheduling、gradient clipping\n",
    "- **檢查點機制**：完整的模型狀態保存與恢復功能\n",
    "- **效能優化**：gradient accumulation 與 mixed precision 支援\n",
    "- **品質保證**：comprehensive smoke tests 確保代碼可靠性\n",
    "\n",
    "### 🧠 核心原理要點\n",
    "- **nn.Module 設計模式**：透過 `__init__` 定義層結構，`forward` 實作前向傳播\n",
    "- **訓練迴圈本質**：`loss.backward()` → `optimizer.step()` → `optimizer.zero_grad()` 循環\n",
    "- **模式切換重要性**：`train()/eval()` 影響 BatchNorm、Dropout 行為\n",
    "- **記憶體管理**：適當使用 `torch.no_grad()` 與變數清理避免記憶體洩漏\n",
    "- **可重現性**：設定隨機種子確保實驗結果一致\n",
    "\n",
    "### 🚀 下一步建議\n",
    "1. **立即進行**：開始 `nb03_data_preprocessing.ipynb`，學習 HF Datasets 處理大規模資料\n",
    "2. **中期目標**：完成 Part A 基礎部分，為 Transformer 學習做準備\n",
    "3. **長期規劃**：將 Trainer 類別擴展支援分散式訓練與更多優化器\n",
    "\n",
    "**準備就緒進入下一章！** 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
