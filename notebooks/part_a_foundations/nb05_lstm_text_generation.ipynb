{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3cb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb05_lstm_text_generation.ipynb\n",
    "# LSTM/GRU Text Generation - Character and Word Level\n",
    "\n",
    "# %% Cell 1: Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\n",
    "        f\"[GPU] {torch.cuda.get_device_name(0)} | VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Dependencies and Data Preparation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "\n",
    "# Download Shakespeare text for character-level generation\n",
    "def download_shakespeare():\n",
    "    \"\"\"Download and prepare Shakespeare text data\"\"\"\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        text = response.text\n",
    "        print(f\"[Data] Downloaded Shakespeare text: {len(text)} characters\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Download failed: {e}\")\n",
    "        # Fallback sample text (mix of English + Chinese)\n",
    "        return \"\"\"To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles.\n",
    "人工智能的發展正在改變世界。\n",
    "機器學習讓電腦能夠從數據中學習。\n",
    "深度學習是機器學習的一個分支。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Prepare sample text data\n",
    "text_data = download_shakespeare()\n",
    "print(f\"[Data] Text length: {len(text_data)} characters\")\n",
    "print(\"[Sample]\", repr(text_data[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Character-Level Data Processor\n",
    "class CharTokenizer:\n",
    "    \"\"\"Character-level tokenizer for text generation\"\"\"\n",
    "\n",
    "    def __init__(self, text, min_freq=1):\n",
    "        # Count character frequencies\n",
    "        char_counts = Counter(text)\n",
    "\n",
    "        # Filter by minimum frequency\n",
    "        self.chars = sorted(\n",
    "            [char for char, count in char_counts.items() if count >= min_freq]\n",
    "        )\n",
    "\n",
    "        # Create mappings\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(self.chars)}\n",
    "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        print(f\"[Tokenizer] Vocab size: {self.vocab_size}\")\n",
    "        print(f\"[Chars] {repr(''.join(self.chars[:20]))}...\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to token indices\"\"\"\n",
    "        return [self.char_to_idx.get(char, 0) for char in text]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert token indices back to text\"\"\"\n",
    "        return \"\".join([self.idx_to_char.get(idx, \"<UNK>\") for idx in indices])\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for sequence-to-sequence text modeling\"\"\"\n",
    "\n",
    "    def __init__(self, text, tokenizer, seq_length=50):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Encode entire text\n",
    "        self.tokens = tokenizer.encode(text)\n",
    "        print(f\"[Dataset] Total tokens: {len(self.tokens)}\")\n",
    "\n",
    "        # Create sequences\n",
    "        self.sequences = []\n",
    "        for i in range(len(self.tokens) - seq_length):\n",
    "            input_seq = self.tokens[i : i + seq_length]\n",
    "            target_seq = self.tokens[i + 1 : i + seq_length + 1]\n",
    "            self.sequences.append((input_seq, target_seq))\n",
    "\n",
    "        print(f\"[Dataset] Created {len(self.sequences)} sequences\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_seq = self.sequences[idx]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(\n",
    "            target_seq, dtype=torch.long\n",
    "        )\n",
    "\n",
    "\n",
    "# Create tokenizer and dataset\n",
    "tokenizer = CharTokenizer(text_data, min_freq=2)\n",
    "dataset = TextDataset(text_data, tokenizer, seq_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9cd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: LSTM Text Generation Model\n",
    "class LSTMTextGenerator(nn.Module):\n",
    "    \"\"\"LSTM-based text generation model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_proj = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.uniform_(param, -0.1, 0.1)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch, seq, embed)\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)  # (batch, seq, hidden)\n",
    "\n",
    "        # Dropout and projection\n",
    "        output = self.dropout(lstm_out)\n",
    "        logits = self.output_proj(output)  # (batch, seq, vocab)\n",
    "\n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device=\"cpu\"):\n",
    "        \"\"\"Initialize hidden state\"\"\"\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return (h_0, c_0)\n",
    "\n",
    "\n",
    "# Alternative GRU model for comparison\n",
    "class GRUTextGenerator(nn.Module):\n",
    "    \"\"\"GRU-based text generation model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(\n",
    "            embed_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_proj = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.uniform_(param, -0.1, 0.1)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, hidden = self.gru(embedded, hidden)\n",
    "        output = self.dropout(gru_out)\n",
    "        logits = self.output_proj(output)\n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device=\"cpu\"):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMTextGenerator(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=64,  # Smaller for low VRAM\n",
    "    hidden_dim=128,  # Smaller for low VRAM\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    ").to(device)\n",
    "\n",
    "print(\n",
    "    f\"[Model] LSTM Generator - {sum(p.numel() for p in model.parameters())} parameters\"\n",
    ")\n",
    "print(f\"[Device] {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: Training Loop with Gradient Accumulation\n",
    "def train_text_generator(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    gradient_accumulation_steps=1,\n",
    "):\n",
    "    \"\"\"Train text generation model with gradient accumulation for low VRAM\"\"\"\n",
    "\n",
    "    # Adjust batch size based on available memory\n",
    "    if torch.cuda.is_available():\n",
    "        available_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        if available_memory < 6:\n",
    "            batch_size = min(batch_size, 16)\n",
    "            gradient_accumulation_steps = max(gradient_accumulation_steps, 2)\n",
    "            print(\n",
    "                f\"[LowVRAM] Adjusted batch_size={batch_size}, grad_accum={gradient_accumulation_steps}\"\n",
    "            )\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, _ = model(input_seq)\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            logits_flat = logits.view(-1, model.vocab_size)\n",
    "            target_flat = target_seq.view(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits_flat, target_flat)\n",
    "\n",
    "            # Scale loss for gradient accumulation\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights every gradient_accumulation_steps\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += loss.item() * gradient_accumulation_steps\n",
    "            num_batches += 1\n",
    "\n",
    "            # Memory cleanup\n",
    "            if batch_idx % 50 == 0:\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Generate sample text every few epochs\n",
    "        if (epoch + 1) % 3 == 0:\n",
    "            sample = generate_text(\n",
    "                model, tokenizer, prompt=\"To be\", max_length=50, temperature=0.8\n",
    "            )\n",
    "            print(f\"[Sample] {repr(sample)}\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Temperature Sampling Generation Function\n",
    "def generate_text(\n",
    "    model, tokenizer, prompt=\"\", max_length=100, temperature=1.0, device=None\n",
    "):\n",
    "    \"\"\"Generate text using temperature sampling\"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Handle empty prompt\n",
    "    if not prompt:\n",
    "        prompt = random.choice(tokenizer.chars[:10])\n",
    "\n",
    "    # Encode prompt\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    input_seq = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    if hasattr(model, \"init_hidden\"):\n",
    "        hidden = model.init_hidden(1, device)\n",
    "    else:\n",
    "        hidden = None\n",
    "\n",
    "    generated_tokens = tokens.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass\n",
    "            logits, hidden = model(input_seq, hidden)\n",
    "\n",
    "            # Get last timestep logits\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "\n",
    "            # Apply softmax and sample\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            generated_tokens.append(next_token)\n",
    "\n",
    "            # Update input for next iteration\n",
    "            input_seq = torch.tensor([[next_token]], dtype=torch.long).to(device)\n",
    "\n",
    "    return tokenizer.decode(generated_tokens)\n",
    "\n",
    "\n",
    "def compare_temperatures(\n",
    "    model, tokenizer, prompt=\"The\", temperatures=[0.5, 0.8, 1.0, 1.2]\n",
    "):\n",
    "    \"\"\"Compare generation quality at different temperatures\"\"\"\n",
    "    print(f\"\\n[Temperature Comparison] Prompt: '{prompt}'\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for temp in temperatures:\n",
    "        generated = generate_text(\n",
    "            model, tokenizer, prompt, max_length=80, temperature=temp\n",
    "        )\n",
    "        print(f\"Temp {temp:3.1f}: {repr(generated[:100])}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa9884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Model Training and Checkpoints\n",
    "print(\"[Training] Starting LSTM text generator training...\")\n",
    "\n",
    "# Create smaller dataset for quick training if low VRAM\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory < 6e9:\n",
    "    # Use subset of data for low VRAM\n",
    "    subset_size = min(len(dataset), 1000)\n",
    "    dataset = torch.utils.data.Subset(dataset, range(subset_size))\n",
    "    print(f\"[LowVRAM] Using subset of {subset_size} samples\")\n",
    "\n",
    "# Train model\n",
    "train_losses = train_text_generator(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    epochs=8,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.002,\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"LSTM Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save model checkpoint\n",
    "checkpoint_path = f\"{AI_CACHE_ROOT}/models/lstm_text_gen_checkpoint.pt\"\n",
    "pathlib.Path(checkpoint_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"tokenizer_chars\": tokenizer.chars,\n",
    "        \"model_config\": {\n",
    "            \"vocab_size\": tokenizer.vocab_size,\n",
    "            \"embed_dim\": 64,\n",
    "            \"hidden_dim\": 128,\n",
    "            \"num_layers\": 2,\n",
    "        },\n",
    "    },\n",
    "    checkpoint_path,\n",
    ")\n",
    "print(f\"[Checkpoint] Saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4407105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 8: Text Generation Testing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEXT GENERATION TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test different temperature settings\n",
    "compare_temperatures(\n",
    "    model, tokenizer, prompt=\"To be\", temperatures=[0.3, 0.7, 1.0, 1.5]\n",
    ")\n",
    "\n",
    "# Test with Chinese characters (if available in vocab)\n",
    "chinese_chars = [char for char in tokenizer.chars if \"\\u4e00\" <= char <= \"\\u9fff\"]\n",
    "if chinese_chars:\n",
    "    print(f\"[Chinese] Found {len(chinese_chars)} Chinese characters in vocab\")\n",
    "    chinese_prompt = chinese_chars[0] if chinese_chars else \"人\"\n",
    "    print(f\"\\n[Chinese Generation] Prompt: '{chinese_prompt}'\")\n",
    "    chinese_text = generate_text(\n",
    "        model, tokenizer, prompt=chinese_prompt, max_length=50, temperature=0.8\n",
    "    )\n",
    "    print(f\"Generated: {repr(chinese_text)}\")\n",
    "\n",
    "# Creative prompts\n",
    "creative_prompts = [\"The future\", \"Once upon\", \"In the beginning\", \"人工智能\"]\n",
    "print(f\"\\n[Creative Generation]\")\n",
    "for prompt in creative_prompts:\n",
    "    # Check if prompt characters are in vocab\n",
    "    if all(char in tokenizer.char_to_idx for char in prompt):\n",
    "        generated = generate_text(\n",
    "            model, tokenizer, prompt, max_length=60, temperature=0.9\n",
    "        )\n",
    "        print(f\"'{prompt}' → {repr(generated[:80])}...\")\n",
    "    else:\n",
    "        print(f\"'{prompt}' → [Skipped - characters not in vocab]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab26e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 9: GRU vs LSTM Comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GRU vs LSTM COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create GRU model for comparison\n",
    "gru_model = GRUTextGenerator(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    ").to(device)\n",
    "\n",
    "print(f\"[GRU Model] {sum(p.numel() for p in gru_model.parameters())} parameters\")\n",
    "\n",
    "# Quick training for GRU (fewer epochs)\n",
    "print(\"[Training] GRU model (quick training)...\")\n",
    "gru_losses = train_text_generator(\n",
    "    model=gru_model,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    epochs=4,  # Fewer epochs for comparison\n",
    "    batch_size=16,\n",
    "    learning_rate=0.002,\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "\n",
    "# Compare generation quality\n",
    "test_prompt = \"To be\"\n",
    "print(f\"\\n[Model Comparison] Prompt: '{test_prompt}'\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "lstm_output = generate_text(\n",
    "    model, tokenizer, test_prompt, max_length=80, temperature=0.8\n",
    ")\n",
    "gru_output = generate_text(\n",
    "    gru_model, tokenizer, test_prompt, max_length=80, temperature=0.8\n",
    ")\n",
    "\n",
    "print(f\"LSTM: {repr(lstm_output[:70])}...\")\n",
    "print(f\"GRU:  {repr(gru_output[:70])}...\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_losses, label=\"LSTM\", marker=\"o\")\n",
    "plt.plot(gru_losses, label=\"GRU\", marker=\"s\")\n",
    "plt.title(\"LSTM vs GRU Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74419001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 10: Smoke Test - Verification\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SMOKE TEST - VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def smoke_test():\n",
    "    \"\"\"Quick verification that everything works\"\"\"\n",
    "    try:\n",
    "        # Test 1: Model can generate text\n",
    "        sample = generate_text(model, tokenizer, \"Test\", max_length=20, temperature=1.0)\n",
    "        assert len(sample) > 4, \"Generated text too short\"\n",
    "        print(\"✓ Text generation works\")\n",
    "\n",
    "        # Test 2: Different temperatures produce different outputs\n",
    "        out1 = generate_text(model, tokenizer, \"Hello\", max_length=30, temperature=0.1)\n",
    "        out2 = generate_text(model, tokenizer, \"Hello\", max_length=30, temperature=1.5)\n",
    "        print(\"✓ Temperature sampling works\")\n",
    "\n",
    "        # Test 3: Model can handle different prompt lengths\n",
    "        short_gen = generate_text(model, tokenizer, \"A\", max_length=10)\n",
    "        long_gen = generate_text(model, tokenizer, \"The quick brown\", max_length=10)\n",
    "        print(\"✓ Variable prompt lengths work\")\n",
    "\n",
    "        # Test 4: Tokenizer encode/decode consistency\n",
    "        test_text = \"Hello world!\"\n",
    "        if all(char in tokenizer.char_to_idx for char in test_text):\n",
    "            encoded = tokenizer.encode(test_text)\n",
    "            decoded = tokenizer.decode(encoded)\n",
    "            assert decoded == test_text, \"Encode/decode mismatch\"\n",
    "            print(\"✓ Tokenizer consistency verified\")\n",
    "\n",
    "        # Test 5: Model checkpoint saving worked\n",
    "        assert pathlib.Path(\n",
    "            f\"{AI_CACHE_ROOT}/models/lstm_text_gen_checkpoint.pt\"\n",
    "        ).exists()\n",
    "        print(\"✓ Model checkpoint saved successfully\")\n",
    "\n",
    "        print(\"\\n🎉 All smoke tests passed!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Smoke test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test()\n",
    "\n",
    "print(\n",
    "    f\"\\n[Memory] Peak VRAM used: {torch.cuda.max_memory_allocated()/1e6:.1f}MB\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"[Memory] CPU only\"\n",
    ")\n",
    "\n",
    "# %% Summary Cell\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTEBOOK SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "✅ 完成項目 (Completed Items):\n",
    "   • 實作字符級 LSTM/GRU 文字生成模型\n",
    "   • 支援溫度採樣與不同生成策略\n",
    "   • 低 VRAM 友善訓練（梯度累積、小 batch）\n",
    "   • LSTM vs GRU 架構比較\n",
    "   • 模型檢查點保存與載入\n",
    "\n",
    "🔑 核心概念 (Core Concepts):\n",
    "   • RNN 序列建模：LSTM 解決梯度消失問題\n",
    "   • 字符級 vs 詞彙級生成的權衡\n",
    "   • 溫度採樣控制生成多樣性\n",
    "   • 梯度累積減少記憶體需求\n",
    "\n",
    "⚠️  常見坑 (Common Pitfalls):\n",
    "   • 序列長度過長導致 VRAM 不足\n",
    "   • 溫度設置不當影響生成品質\n",
    "   • 詞彙表過大影響訓練效率\n",
    "   • 沒有梯度裁剪導致訓練不穩定\n",
    "\n",
    "🚀 下一步建議 (Next Steps):\n",
    "   • 進入 Part B: Transformer 架構學習\n",
    "   • 對比 RNN vs Transformer 的生成品質\n",
    "   • 學習 attention 機制改善長序列建模\n",
    "   • 探索更大規模的預訓練模型\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30582d9a",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 本章小結\n",
    "\n",
    "### ✅ 完成項目\n",
    "• **LSTM/GRU 文字生成**：實作完整的字符級序列模型\n",
    "• **低 VRAM 優化**：梯度累積、動態 batch size、記憶體清理\n",
    "• **溫度採樣**：多種生成策略比較\n",
    "• **模型比較**：LSTM vs GRU 架構效能對比\n",
    "• **中英文支援**：混合語言文本生成能力\n",
    "\n",
    "### 🔑 核心原理要點\n",
    "• **序列建模基礎**：理解 RNN 家族處理序列資料的方式\n",
    "• **LSTM 閘控機制**：遺忘閘、輸入閘、輸出閘控制資訊流\n",
    "• **文字生成策略**：貪婪解碼 vs 隨機採樣 vs 溫度控制\n",
    "• **訓練技巧**：梯度裁剪、學習率調度、檢查點保存\n",
    "\n",
    "### 🚀 下一步建議\n",
    "**立即行動**：進入 **nb06_attention_transformer.ipynb**，學習 Transformer 架構\n",
    "**技能建構**：對比 RNN 與 Transformer 在序列建模上的差異\n",
    "**實用擴展**：嘗試詞彙級生成、更大語料庫、多語言支援\n",
    "**效能優化**：探索模型蒸餾、量化等進一步的 VRAM 節省技巧\n",
    "\n",
    "---\n",
    "\n",
    "**何時使用這套技術**：\n",
    "- 需要理解 RNN 序列建模基礎時\n",
    "- 資源受限但需要文字生成功能時  \n",
    "- 作為 Transformer 學習前的打底準備\n",
    "- 進行序列模型架構比較研究時"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
