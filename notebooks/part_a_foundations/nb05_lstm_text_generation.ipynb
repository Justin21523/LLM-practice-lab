{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3cb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb05_lstm_text_generation.ipynb\n",
    "# LSTM/GRU Text Generation - Character and Word Level\n",
    "\n",
    "# %% Cell 1: Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\n",
    "        f\"[GPU] {torch.cuda.get_device_name(0)} | VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Dependencies and Data Preparation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "\n",
    "# Download Shakespeare text for character-level generation\n",
    "def download_shakespeare():\n",
    "    \"\"\"Download and prepare Shakespeare text data\"\"\"\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        text = response.text\n",
    "        print(f\"[Data] Downloaded Shakespeare text: {len(text)} characters\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Download failed: {e}\")\n",
    "        # Fallback sample text (mix of English + Chinese)\n",
    "        return \"\"\"To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles.\n",
    "äººå·¥æ™ºèƒ½çš„ç™¼å±•æ­£åœ¨æ”¹è®Šä¸–ç•Œã€‚\n",
    "æ©Ÿå™¨å­¸ç¿’è®“é›»è…¦èƒ½å¤ å¾æ•¸æ“šä¸­å­¸ç¿’ã€‚\n",
    "æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹åˆ†æ”¯ã€‚\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Prepare sample text data\n",
    "text_data = download_shakespeare()\n",
    "print(f\"[Data] Text length: {len(text_data)} characters\")\n",
    "print(\"[Sample]\", repr(text_data[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Character-Level Data Processor\n",
    "class CharTokenizer:\n",
    "    \"\"\"Character-level tokenizer for text generation\"\"\"\n",
    "\n",
    "    def __init__(self, text, min_freq=1):\n",
    "        # Count character frequencies\n",
    "        char_counts = Counter(text)\n",
    "\n",
    "        # Filter by minimum frequency\n",
    "        self.chars = sorted(\n",
    "            [char for char, count in char_counts.items() if count >= min_freq]\n",
    "        )\n",
    "\n",
    "        # Create mappings\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(self.chars)}\n",
    "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        print(f\"[Tokenizer] Vocab size: {self.vocab_size}\")\n",
    "        print(f\"[Chars] {repr(''.join(self.chars[:20]))}...\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to token indices\"\"\"\n",
    "        return [self.char_to_idx.get(char, 0) for char in text]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert token indices back to text\"\"\"\n",
    "        return \"\".join([self.idx_to_char.get(idx, \"<UNK>\") for idx in indices])\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for sequence-to-sequence text modeling\"\"\"\n",
    "\n",
    "    def __init__(self, text, tokenizer, seq_length=50):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Encode entire text\n",
    "        self.tokens = tokenizer.encode(text)\n",
    "        print(f\"[Dataset] Total tokens: {len(self.tokens)}\")\n",
    "\n",
    "        # Create sequences\n",
    "        self.sequences = []\n",
    "        for i in range(len(self.tokens) - seq_length):\n",
    "            input_seq = self.tokens[i : i + seq_length]\n",
    "            target_seq = self.tokens[i + 1 : i + seq_length + 1]\n",
    "            self.sequences.append((input_seq, target_seq))\n",
    "\n",
    "        print(f\"[Dataset] Created {len(self.sequences)} sequences\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_seq = self.sequences[idx]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(\n",
    "            target_seq, dtype=torch.long\n",
    "        )\n",
    "\n",
    "\n",
    "# Create tokenizer and dataset\n",
    "tokenizer = CharTokenizer(text_data, min_freq=2)\n",
    "dataset = TextDataset(text_data, tokenizer, seq_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9cd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: LSTM Text Generation Model\n",
    "class LSTMTextGenerator(nn.Module):\n",
    "    \"\"\"LSTM-based text generation model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_proj = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.uniform_(param, -0.1, 0.1)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch, seq, embed)\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)  # (batch, seq, hidden)\n",
    "\n",
    "        # Dropout and projection\n",
    "        output = self.dropout(lstm_out)\n",
    "        logits = self.output_proj(output)  # (batch, seq, vocab)\n",
    "\n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device=\"cpu\"):\n",
    "        \"\"\"Initialize hidden state\"\"\"\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return (h_0, c_0)\n",
    "\n",
    "\n",
    "# Alternative GRU model for comparison\n",
    "class GRUTextGenerator(nn.Module):\n",
    "    \"\"\"GRU-based text generation model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(\n",
    "            embed_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_proj = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.uniform_(param, -0.1, 0.1)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, hidden = self.gru(embedded, hidden)\n",
    "        output = self.dropout(gru_out)\n",
    "        logits = self.output_proj(output)\n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device=\"cpu\"):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMTextGenerator(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=64,  # Smaller for low VRAM\n",
    "    hidden_dim=128,  # Smaller for low VRAM\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    ").to(device)\n",
    "\n",
    "print(\n",
    "    f\"[Model] LSTM Generator - {sum(p.numel() for p in model.parameters())} parameters\"\n",
    ")\n",
    "print(f\"[Device] {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: Training Loop with Gradient Accumulation\n",
    "def train_text_generator(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    gradient_accumulation_steps=1,\n",
    "):\n",
    "    \"\"\"Train text generation model with gradient accumulation for low VRAM\"\"\"\n",
    "\n",
    "    # Adjust batch size based on available memory\n",
    "    if torch.cuda.is_available():\n",
    "        available_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        if available_memory < 6:\n",
    "            batch_size = min(batch_size, 16)\n",
    "            gradient_accumulation_steps = max(gradient_accumulation_steps, 2)\n",
    "            print(\n",
    "                f\"[LowVRAM] Adjusted batch_size={batch_size}, grad_accum={gradient_accumulation_steps}\"\n",
    "            )\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, _ = model(input_seq)\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            logits_flat = logits.view(-1, model.vocab_size)\n",
    "            target_flat = target_seq.view(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits_flat, target_flat)\n",
    "\n",
    "            # Scale loss for gradient accumulation\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights every gradient_accumulation_steps\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += loss.item() * gradient_accumulation_steps\n",
    "            num_batches += 1\n",
    "\n",
    "            # Memory cleanup\n",
    "            if batch_idx % 50 == 0:\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Generate sample text every few epochs\n",
    "        if (epoch + 1) % 3 == 0:\n",
    "            sample = generate_text(\n",
    "                model, tokenizer, prompt=\"To be\", max_length=50, temperature=0.8\n",
    "            )\n",
    "            print(f\"[Sample] {repr(sample)}\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Temperature Sampling Generation Function\n",
    "def generate_text(\n",
    "    model, tokenizer, prompt=\"\", max_length=100, temperature=1.0, device=None\n",
    "):\n",
    "    \"\"\"Generate text using temperature sampling\"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Handle empty prompt\n",
    "    if not prompt:\n",
    "        prompt = random.choice(tokenizer.chars[:10])\n",
    "\n",
    "    # Encode prompt\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    input_seq = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    if hasattr(model, \"init_hidden\"):\n",
    "        hidden = model.init_hidden(1, device)\n",
    "    else:\n",
    "        hidden = None\n",
    "\n",
    "    generated_tokens = tokens.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass\n",
    "            logits, hidden = model(input_seq, hidden)\n",
    "\n",
    "            # Get last timestep logits\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "\n",
    "            # Apply softmax and sample\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            generated_tokens.append(next_token)\n",
    "\n",
    "            # Update input for next iteration\n",
    "            input_seq = torch.tensor([[next_token]], dtype=torch.long).to(device)\n",
    "\n",
    "    return tokenizer.decode(generated_tokens)\n",
    "\n",
    "\n",
    "def compare_temperatures(\n",
    "    model, tokenizer, prompt=\"The\", temperatures=[0.5, 0.8, 1.0, 1.2]\n",
    "):\n",
    "    \"\"\"Compare generation quality at different temperatures\"\"\"\n",
    "    print(f\"\\n[Temperature Comparison] Prompt: '{prompt}'\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for temp in temperatures:\n",
    "        generated = generate_text(\n",
    "            model, tokenizer, prompt, max_length=80, temperature=temp\n",
    "        )\n",
    "        print(f\"Temp {temp:3.1f}: {repr(generated[:100])}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa9884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Model Training and Checkpoints\n",
    "print(\"[Training] Starting LSTM text generator training...\")\n",
    "\n",
    "# Create smaller dataset for quick training if low VRAM\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory < 6e9:\n",
    "    # Use subset of data for low VRAM\n",
    "    subset_size = min(len(dataset), 1000)\n",
    "    dataset = torch.utils.data.Subset(dataset, range(subset_size))\n",
    "    print(f\"[LowVRAM] Using subset of {subset_size} samples\")\n",
    "\n",
    "# Train model\n",
    "train_losses = train_text_generator(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    epochs=8,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.002,\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"LSTM Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save model checkpoint\n",
    "checkpoint_path = f\"{AI_CACHE_ROOT}/models/lstm_text_gen_checkpoint.pt\"\n",
    "pathlib.Path(checkpoint_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"tokenizer_chars\": tokenizer.chars,\n",
    "        \"model_config\": {\n",
    "            \"vocab_size\": tokenizer.vocab_size,\n",
    "            \"embed_dim\": 64,\n",
    "            \"hidden_dim\": 128,\n",
    "            \"num_layers\": 2,\n",
    "        },\n",
    "    },\n",
    "    checkpoint_path,\n",
    ")\n",
    "print(f\"[Checkpoint] Saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4407105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 8: Text Generation Testing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEXT GENERATION TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test different temperature settings\n",
    "compare_temperatures(\n",
    "    model, tokenizer, prompt=\"To be\", temperatures=[0.3, 0.7, 1.0, 1.5]\n",
    ")\n",
    "\n",
    "# Test with Chinese characters (if available in vocab)\n",
    "chinese_chars = [char for char in tokenizer.chars if \"\\u4e00\" <= char <= \"\\u9fff\"]\n",
    "if chinese_chars:\n",
    "    print(f\"[Chinese] Found {len(chinese_chars)} Chinese characters in vocab\")\n",
    "    chinese_prompt = chinese_chars[0] if chinese_chars else \"äºº\"\n",
    "    print(f\"\\n[Chinese Generation] Prompt: '{chinese_prompt}'\")\n",
    "    chinese_text = generate_text(\n",
    "        model, tokenizer, prompt=chinese_prompt, max_length=50, temperature=0.8\n",
    "    )\n",
    "    print(f\"Generated: {repr(chinese_text)}\")\n",
    "\n",
    "# Creative prompts\n",
    "creative_prompts = [\"The future\", \"Once upon\", \"In the beginning\", \"äººå·¥æ™ºèƒ½\"]\n",
    "print(f\"\\n[Creative Generation]\")\n",
    "for prompt in creative_prompts:\n",
    "    # Check if prompt characters are in vocab\n",
    "    if all(char in tokenizer.char_to_idx for char in prompt):\n",
    "        generated = generate_text(\n",
    "            model, tokenizer, prompt, max_length=60, temperature=0.9\n",
    "        )\n",
    "        print(f\"'{prompt}' â†’ {repr(generated[:80])}...\")\n",
    "    else:\n",
    "        print(f\"'{prompt}' â†’ [Skipped - characters not in vocab]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab26e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 9: GRU vs LSTM Comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GRU vs LSTM COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create GRU model for comparison\n",
    "gru_model = GRUTextGenerator(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    ").to(device)\n",
    "\n",
    "print(f\"[GRU Model] {sum(p.numel() for p in gru_model.parameters())} parameters\")\n",
    "\n",
    "# Quick training for GRU (fewer epochs)\n",
    "print(\"[Training] GRU model (quick training)...\")\n",
    "gru_losses = train_text_generator(\n",
    "    model=gru_model,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    epochs=4,  # Fewer epochs for comparison\n",
    "    batch_size=16,\n",
    "    learning_rate=0.002,\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "\n",
    "# Compare generation quality\n",
    "test_prompt = \"To be\"\n",
    "print(f\"\\n[Model Comparison] Prompt: '{test_prompt}'\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "lstm_output = generate_text(\n",
    "    model, tokenizer, test_prompt, max_length=80, temperature=0.8\n",
    ")\n",
    "gru_output = generate_text(\n",
    "    gru_model, tokenizer, test_prompt, max_length=80, temperature=0.8\n",
    ")\n",
    "\n",
    "print(f\"LSTM: {repr(lstm_output[:70])}...\")\n",
    "print(f\"GRU:  {repr(gru_output[:70])}...\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_losses, label=\"LSTM\", marker=\"o\")\n",
    "plt.plot(gru_losses, label=\"GRU\", marker=\"s\")\n",
    "plt.title(\"LSTM vs GRU Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74419001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 10: Smoke Test - Verification\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SMOKE TEST - VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def smoke_test():\n",
    "    \"\"\"Quick verification that everything works\"\"\"\n",
    "    try:\n",
    "        # Test 1: Model can generate text\n",
    "        sample = generate_text(model, tokenizer, \"Test\", max_length=20, temperature=1.0)\n",
    "        assert len(sample) > 4, \"Generated text too short\"\n",
    "        print(\"âœ“ Text generation works\")\n",
    "\n",
    "        # Test 2: Different temperatures produce different outputs\n",
    "        out1 = generate_text(model, tokenizer, \"Hello\", max_length=30, temperature=0.1)\n",
    "        out2 = generate_text(model, tokenizer, \"Hello\", max_length=30, temperature=1.5)\n",
    "        print(\"âœ“ Temperature sampling works\")\n",
    "\n",
    "        # Test 3: Model can handle different prompt lengths\n",
    "        short_gen = generate_text(model, tokenizer, \"A\", max_length=10)\n",
    "        long_gen = generate_text(model, tokenizer, \"The quick brown\", max_length=10)\n",
    "        print(\"âœ“ Variable prompt lengths work\")\n",
    "\n",
    "        # Test 4: Tokenizer encode/decode consistency\n",
    "        test_text = \"Hello world!\"\n",
    "        if all(char in tokenizer.char_to_idx for char in test_text):\n",
    "            encoded = tokenizer.encode(test_text)\n",
    "            decoded = tokenizer.decode(encoded)\n",
    "            assert decoded == test_text, \"Encode/decode mismatch\"\n",
    "            print(\"âœ“ Tokenizer consistency verified\")\n",
    "\n",
    "        # Test 5: Model checkpoint saving worked\n",
    "        assert pathlib.Path(\n",
    "            f\"{AI_CACHE_ROOT}/models/lstm_text_gen_checkpoint.pt\"\n",
    "        ).exists()\n",
    "        print(\"âœ“ Model checkpoint saved successfully\")\n",
    "\n",
    "        print(\"\\nğŸ‰ All smoke tests passed!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Smoke test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test()\n",
    "\n",
    "print(\n",
    "    f\"\\n[Memory] Peak VRAM used: {torch.cuda.max_memory_allocated()/1e6:.1f}MB\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"[Memory] CPU only\"\n",
    ")\n",
    "\n",
    "# %% Summary Cell\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTEBOOK SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "âœ… å®Œæˆé …ç›® (Completed Items):\n",
    "   â€¢ å¯¦ä½œå­—ç¬¦ç´š LSTM/GRU æ–‡å­—ç”Ÿæˆæ¨¡å‹\n",
    "   â€¢ æ”¯æ´æº«åº¦æ¡æ¨£èˆ‡ä¸åŒç”Ÿæˆç­–ç•¥\n",
    "   â€¢ ä½ VRAM å‹å–„è¨“ç·´ï¼ˆæ¢¯åº¦ç´¯ç©ã€å° batchï¼‰\n",
    "   â€¢ LSTM vs GRU æ¶æ§‹æ¯”è¼ƒ\n",
    "   â€¢ æ¨¡å‹æª¢æŸ¥é»ä¿å­˜èˆ‡è¼‰å…¥\n",
    "\n",
    "ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µ (Core Concepts):\n",
    "   â€¢ RNN åºåˆ—å»ºæ¨¡ï¼šLSTM è§£æ±ºæ¢¯åº¦æ¶ˆå¤±å•é¡Œ\n",
    "   â€¢ å­—ç¬¦ç´š vs è©å½™ç´šç”Ÿæˆçš„æ¬Šè¡¡\n",
    "   â€¢ æº«åº¦æ¡æ¨£æ§åˆ¶ç”Ÿæˆå¤šæ¨£æ€§\n",
    "   â€¢ æ¢¯åº¦ç´¯ç©æ¸›å°‘è¨˜æ†¶é«”éœ€æ±‚\n",
    "\n",
    "âš ï¸  å¸¸è¦‹å‘ (Common Pitfalls):\n",
    "   â€¢ åºåˆ—é•·åº¦éé•·å°è‡´ VRAM ä¸è¶³\n",
    "   â€¢ æº«åº¦è¨­ç½®ä¸ç•¶å½±éŸ¿ç”Ÿæˆå“è³ª\n",
    "   â€¢ è©å½™è¡¨éå¤§å½±éŸ¿è¨“ç·´æ•ˆç‡\n",
    "   â€¢ æ²’æœ‰æ¢¯åº¦è£å‰ªå°è‡´è¨“ç·´ä¸ç©©å®š\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥å»ºè­° (Next Steps):\n",
    "   â€¢ é€²å…¥ Part B: Transformer æ¶æ§‹å­¸ç¿’\n",
    "   â€¢ å°æ¯” RNN vs Transformer çš„ç”Ÿæˆå“è³ª\n",
    "   â€¢ å­¸ç¿’ attention æ©Ÿåˆ¶æ”¹å–„é•·åºåˆ—å»ºæ¨¡\n",
    "   â€¢ æ¢ç´¢æ›´å¤§è¦æ¨¡çš„é è¨“ç·´æ¨¡å‹\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30582d9a",
   "metadata": {},
   "source": [
    "\n",
    "## 6. æœ¬ç« å°çµ\n",
    "\n",
    "### âœ… å®Œæˆé …ç›®\n",
    "â€¢ **LSTM/GRU æ–‡å­—ç”Ÿæˆ**ï¼šå¯¦ä½œå®Œæ•´çš„å­—ç¬¦ç´šåºåˆ—æ¨¡å‹\n",
    "â€¢ **ä½ VRAM å„ªåŒ–**ï¼šæ¢¯åº¦ç´¯ç©ã€å‹•æ…‹ batch sizeã€è¨˜æ†¶é«”æ¸…ç†\n",
    "â€¢ **æº«åº¦æ¡æ¨£**ï¼šå¤šç¨®ç”Ÿæˆç­–ç•¥æ¯”è¼ƒ\n",
    "â€¢ **æ¨¡å‹æ¯”è¼ƒ**ï¼šLSTM vs GRU æ¶æ§‹æ•ˆèƒ½å°æ¯”\n",
    "â€¢ **ä¸­è‹±æ–‡æ”¯æ´**ï¼šæ··åˆèªè¨€æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›\n",
    "\n",
    "### ğŸ”‘ æ ¸å¿ƒåŸç†è¦é»\n",
    "â€¢ **åºåˆ—å»ºæ¨¡åŸºç¤**ï¼šç†è§£ RNN å®¶æ—è™•ç†åºåˆ—è³‡æ–™çš„æ–¹å¼\n",
    "â€¢ **LSTM é–˜æ§æ©Ÿåˆ¶**ï¼šéºå¿˜é–˜ã€è¼¸å…¥é–˜ã€è¼¸å‡ºé–˜æ§åˆ¶è³‡è¨Šæµ\n",
    "â€¢ **æ–‡å­—ç”Ÿæˆç­–ç•¥**ï¼šè²ªå©ªè§£ç¢¼ vs éš¨æ©Ÿæ¡æ¨£ vs æº«åº¦æ§åˆ¶\n",
    "â€¢ **è¨“ç·´æŠ€å·§**ï¼šæ¢¯åº¦è£å‰ªã€å­¸ç¿’ç‡èª¿åº¦ã€æª¢æŸ¥é»ä¿å­˜\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å»ºè­°\n",
    "**ç«‹å³è¡Œå‹•**ï¼šé€²å…¥ **nb06_attention_transformer.ipynb**ï¼Œå­¸ç¿’ Transformer æ¶æ§‹\n",
    "**æŠ€èƒ½å»ºæ§‹**ï¼šå°æ¯” RNN èˆ‡ Transformer åœ¨åºåˆ—å»ºæ¨¡ä¸Šçš„å·®ç•°\n",
    "**å¯¦ç”¨æ“´å±•**ï¼šå˜—è©¦è©å½™ç´šç”Ÿæˆã€æ›´å¤§èªæ–™åº«ã€å¤šèªè¨€æ”¯æ´\n",
    "**æ•ˆèƒ½å„ªåŒ–**ï¼šæ¢ç´¢æ¨¡å‹è’¸é¤¾ã€é‡åŒ–ç­‰é€²ä¸€æ­¥çš„ VRAM ç¯€çœæŠ€å·§\n",
    "\n",
    "---\n",
    "\n",
    "**ä½•æ™‚ä½¿ç”¨é€™å¥—æŠ€è¡“**ï¼š\n",
    "- éœ€è¦ç†è§£ RNN åºåˆ—å»ºæ¨¡åŸºç¤æ™‚\n",
    "- è³‡æºå—é™ä½†éœ€è¦æ–‡å­—ç”ŸæˆåŠŸèƒ½æ™‚  \n",
    "- ä½œç‚º Transformer å­¸ç¿’å‰çš„æ‰“åº•æº–å‚™\n",
    "- é€²è¡Œåºåˆ—æ¨¡å‹æ¶æ§‹æ¯”è¼ƒç ”ç©¶æ™‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
