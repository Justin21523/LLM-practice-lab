{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: Shared Cache Bootstrap ===\n",
    "import os, pathlib, torch\n",
    "\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"/mnt/ai/cache\")\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Cache] Root: {AI_CACHE_ROOT}\")\n",
    "print(f\"[GPU] Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"[GPU] Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac69bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Import Dependencies ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set device with memory-efficient defaults\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af26b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: CIFAR-10 Dataset Loading & Preprocessing ===\n",
    "# CIFAR-10 classes\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")\n",
    "\n",
    "# Data transforms with augmentation for training\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Download and load datasets (cached in AI_CACHE_ROOT)\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=f\"{AI_CACHE_ROOT}/data\", train=True, download=True, transform=transform_train\n",
    ")\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=f\"{AI_CACHE_ROOT}/data\", train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "# Low-VRAM friendly batch sizes\n",
    "batch_size = 64 if torch.cuda.is_available() else 32\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training samples: {len(trainset)}\")\n",
    "print(f\"Test samples: {len(testset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "\n",
    "# Visualize sample images\n",
    "def show_sample_images(dataloader, num_images=8):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = next(dataiter)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    for i in range(num_images):\n",
    "        ax = axes[i // 4, i % 4]\n",
    "        # Denormalize for display\n",
    "        img = images[i] * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "        img += torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "\n",
    "        ax.imshow(img.permute(1, 2, 0))\n",
    "        ax.set_title(f\"{classes[labels[i]]}\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_sample_images(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c341891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: CNN Model Architecture ===\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN for CIFAR-10 classification\n",
    "    Features: Conv layers, BatchNorm, Dropout, Global Average Pooling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.3):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1: 32x32 -> 16x16\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            # Block 2: 16x16 -> 8x8\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            # Block 3: 8x8 -> 4x4\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "        )\n",
    "\n",
    "        # Classifier with Global Average Pooling\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "\n",
    "# Model summary\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "print(f\"Model size: {count_parameters(model) * 4 / 1e6:.2f} MB (fp32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05fb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Training Configuration ===\n",
    "# Training hyperparameters (low-VRAM friendly)\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training tracking\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"Evaluate model on given dataloader\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataloader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Training Loop ===\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Training loop with progress bar\n",
    "    pbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for i, (inputs, labels) in enumerate(pbar):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        if i % 100 == 99:\n",
    "            avg_loss = running_loss / 100\n",
    "            accuracy = 100 * correct / total\n",
    "            pbar.set_postfix({\"Loss\": f\"{avg_loss:.3f}\", \"Acc\": f\"{accuracy:.1f}%\"})\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Epoch statistics\n",
    "    train_acc = 100 * correct / total\n",
    "    val_acc, val_loss = evaluate_model(model, testloader, device)\n",
    "\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "        f\"Train Acc: {train_acc:.2f}% - \"\n",
    "        f\"Val Acc: {val_acc:.2f}% - \"\n",
    "        f\"Val Loss: {val_loss:.3f}\"\n",
    "    )\n",
    "\n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, \"b-\", label=\"Training\")\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, \"r-\", label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(\n",
    "    range(1, num_epochs + 1),\n",
    "    [scheduler.get_last_lr()[0] * (0.5 ** (i // 5)) for i in range(num_epochs)],\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Learning Rate Schedule\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6308ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Model Evaluation & Visualization ===\n",
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "all_outputs = []\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "with torch.no_grad():\n",
    "    for data, targets in tqdm(testloader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "        all_outputs.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(all_labels, all_predicted, target_names=classes))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predicted)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes\n",
    ")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "for i, acc in enumerate(class_accuracy):\n",
    "    print(f\"{classes[i]}: {acc:.3f}\")\n",
    "\n",
    "\n",
    "# Visualize predictions\n",
    "def show_predictions(model, testloader, device, num_images=8):\n",
    "    model.eval()\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = next(dataiter)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "    for i in range(num_images):\n",
    "        ax = axes[i // 4, i % 4]\n",
    "\n",
    "        # Denormalize image for display\n",
    "        img = images[i].cpu()\n",
    "        img = img * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "        img += torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "\n",
    "        ax.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "        true_label = classes[labels[i]]\n",
    "        pred_label = classes[predicted[i]]\n",
    "        confidence = probabilities[i][predicted[i]].item()\n",
    "\n",
    "        color = \"green\" if labels[i] == predicted[i] else \"red\"\n",
    "        ax.set_title(\n",
    "            f\"True: {true_label}\\nPred: {pred_label} ({confidence:.2f})\",\n",
    "            color=color,\n",
    "            fontsize=10,\n",
    "        )\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_predictions(model, testloader, device)\n",
    "\n",
    "# Save model checkpoint\n",
    "model_path = f\"{AI_CACHE_ROOT}/models\"\n",
    "pathlib.Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": num_epochs,\n",
    "        \"val_accuracy\": val_accuracies[-1],\n",
    "        \"model_config\": {\"num_classes\": 10, \"dropout_rate\": 0.3},\n",
    "    },\n",
    "    f\"{model_path}/cifar10_cnn_checkpoint.pth\",\n",
    ")\n",
    "\n",
    "print(f\"Model saved to {model_path}/cifar10_cnn_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6aa59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Smoke Test ===\n",
    "print(\"=== Smoke Test: Model Loading & Inference ===\")\n",
    "\n",
    "# Test model loading\n",
    "checkpoint = torch.load(f\"{model_path}/cifar10_cnn_checkpoint.pth\", map_location=device)\n",
    "test_model = SimpleCNN(**checkpoint[\"model_config\"]).to(device)\n",
    "test_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# Test inference on a single batch\n",
    "test_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "    test_output = test_model(test_input)\n",
    "    test_prediction = torch.argmax(test_output, dim=1)\n",
    "\n",
    "print(f\"âœ… Model loading: SUCCESS\")\n",
    "print(f\"âœ… Inference shape: {test_output.shape}\")\n",
    "print(f\"âœ… Prediction class: {classes[test_prediction.item()]}\")\n",
    "\n",
    "# Memory usage summary\n",
    "if torch.cuda.is_available():\n",
    "    memory_used = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"âœ… Peak GPU memory: {memory_used:.2f} GB\")\n",
    "\n",
    "print(f\"âœ… Final validation accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "print(\"âœ… All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2417071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9: Usage Notes & Extensions ===\n",
    "print(\n",
    "    \"\"\"\n",
    "=== ä½•æ™‚ä½¿ç”¨ CNN (When to use CNN) ===\n",
    "\n",
    "ğŸ¯ é©ç”¨å ´æ™¯ (Use Cases):\n",
    "â€¢ å½±åƒåˆ†é¡ã€ç‰©ä»¶åµæ¸¬ã€å½±åƒåˆ†å‰²\n",
    "â€¢ ä»»ä½•å…·æœ‰ç©ºé–“çµæ§‹çš„è³‡æ–™ (åœ–ç‰‡ã€é†«å­¸å½±åƒã€è¡›æ˜Ÿåœ–)\n",
    "â€¢ éœ€è¦å¹³ç§»ä¸è®Šæ€§ (translation invariance) çš„ä»»å‹™\n",
    "\n",
    "âš™ï¸ é—œéµåƒæ•¸èª¿æ•´ (Key Parameters):\n",
    "â€¢ batch_size: æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´ (4GB: 32, 8GB: 64, 16GB+: 128)\n",
    "â€¢ learning_rate: 0.001 (Adam) æˆ– 0.01 (SGD)\n",
    "â€¢ dropout_rate: 0.2-0.5 é˜²æ­¢éæ“¬åˆ\n",
    "â€¢ data_augmentation: å°è³‡æ–™é›†å¿…ç”¨\n",
    "\n",
    "ğŸš€ æ•ˆèƒ½å„ªåŒ– (Performance Tips):\n",
    "â€¢ ä½¿ç”¨ BatchNorm åŠ é€Ÿæ”¶æ–‚\n",
    "â€¢ Global Average Pooling æ¸›å°‘åƒæ•¸\n",
    "â€¢ Mixed precision training (torch.cuda.amp)\n",
    "â€¢ æ¢¯åº¦ç´¯ç©è™•ç†å¤§ batch size\n",
    "\n",
    "ğŸ”§ å¸¸è¦‹å•é¡Œ (Common Issues):\n",
    "â€¢ éæ“¬åˆ: å¢åŠ  Dropoutã€Data Augmentation\n",
    "â€¢ æ”¶æ–‚æ…¢: æª¢æŸ¥å­¸ç¿’ç‡ã€ä½¿ç”¨é è¨“ç·´æ¨¡å‹\n",
    "â€¢ è¨˜æ†¶é«”ä¸è¶³: æ¸›å°‘ batch_sizeã€ä½¿ç”¨ gradient checkpointing\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Smoke Test Cell (5 lines) ===\n",
    "assert torch.cuda.is_available() or True  # Works on CPU too\n",
    "assert model.training == False  # Model in eval mode\n",
    "assert val_accuracies[-1] > 50.0  # Reasonable accuracy threshold\n",
    "assert os.path.exists(f\"{AI_CACHE_ROOT}/models/cifar10_cnn_checkpoint.pth\")\n",
    "print(\"âœ… All smoke tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396ea47",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. æœ¬ç« å°çµ\n",
    "\n",
    "### âœ… å®Œæˆé …ç›® (Completed Items)\n",
    "- **CNN æ¶æ§‹å¯¦ä½œ**ï¼šåŒ…å«å·ç©ã€æ‰¹æ¬¡æ­£è¦åŒ–ã€æ± åŒ–ã€Dropout çš„å®Œæ•´æ¨¡å‹\n",
    "- **CIFAR-10 è¨“ç·´æµç¨‹**ï¼šè³‡æ–™è¼‰å…¥ã€å¢å¼·ã€è¨“ç·´ã€è©•ä¼°çš„ç«¯å°ç«¯æµç¨‹\n",
    "- **ä½ VRAM å„ªåŒ–**ï¼šæ”¯æ´ 4GB+ GPUï¼ŒåŒ…å« CPU å¾Œå‚™æ–¹æ¡ˆ\n",
    "- **æ¨¡å‹è©•ä¼°**ï¼šæº–ç¢ºç‡ã€æ··æ·†çŸ©é™£ã€åˆ†é¡å ±å‘Šã€é æ¸¬è¦–è¦ºåŒ–\n",
    "- **æ¨¡å‹æŒä¹…åŒ–**ï¼šæª¢æŸ¥é»å„²å­˜èˆ‡è¼‰å…¥æ©Ÿåˆ¶\n",
    "\n",
    "### ğŸ§  æ ¸å¿ƒåŸç† (Core Concepts)\n",
    "- **ç©ºé–“ä¸è®Šæ€§ (Spatial Invariance)**ï¼šCNN é€éæ¬Šé‡å…±äº«å­¸ç¿’å±€éƒ¨ç‰¹å¾µ\n",
    "- **éšå±¤å¼ç‰¹å¾µå­¸ç¿’**ï¼šæ·ºå±¤å­¸ç¿’é‚Šç·£ï¼Œæ·±å±¤å­¸ç¿’è¤‡é›œæ¨¡å¼\n",
    "- **æ­£è¦åŒ–æŠ€è¡“**ï¼šBatchNorm åŠ é€Ÿè¨“ç·´ï¼ŒDropout é˜²æ­¢éæ“¬åˆ\n",
    "- **è³‡æ–™å¢å¼·**ï¼šRandomFlipã€Rotation æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›\n",
    "\n",
    "### âš ï¸ å¸¸è¦‹é™·é˜± (Common Pitfalls)\n",
    "- **è¨˜æ†¶é«”çˆ†ç‚¸**ï¼šbatch_size éå¤§å°è‡´ CUDA OOM\n",
    "- **æ¢¯åº¦æ¶ˆå¤±**ï¼šç¶²è·¯éæ·±æ™‚ä½¿ç”¨ ResNet æˆ– BatchNorm\n",
    "- **éæ“¬åˆ**ï¼šå°è³‡æ–™é›†æ™‚å¿…é ˆä½¿ç”¨ Dropout å’Œ Data Augmentation\n",
    "- **å­¸ç¿’ç‡è¨­å®š**ï¼šéå¤§å°è‡´éœ‡ç›ªï¼Œéå°å°è‡´æ”¶æ–‚æ…¢\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å»ºè­° (Next Steps)\n",
    "1. **è½‰ç§»å­¸ç¿’**ï¼šä½¿ç”¨é è¨“ç·´ ResNet/EfficientNet æå‡æ•ˆæœ\n",
    "2. **é€²éšå¢å¼·**ï¼šCutMixã€MixUpã€AutoAugment æŠ€è¡“\n",
    "3. **æ¶æ§‹æœç´¢**ï¼šå˜—è©¦ MobileNetã€ShuffleNet ç­‰è¼•é‡åŒ–æ¨¡å‹\n",
    "4. **å¤š GPU è¨“ç·´**ï¼šDataParallel æˆ– DistributedDataParallel\n",
    "\n",
    "**æº–å‚™é€²å…¥ nb05_lstm_text_generation.ipynb (LSTM æ–‡å­—ç”Ÿæˆ)ï¼Œæˆ–æ‚¨æƒ³å„ªå…ˆå­¸ç¿’å…¶ä»–ä¸»é¡Œï¼Ÿ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
